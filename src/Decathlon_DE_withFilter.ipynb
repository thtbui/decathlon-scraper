{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "472e20ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up\n",
    "from bs4 import BeautifulSoup as bts\n",
    "import pandas as pd\n",
    "import requests\n",
    "import math #for rounding numbers\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa544997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# country input\n",
    "country = 'DE'\n",
    "base_index = 'https://www.decathlon.de'\n",
    "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
    "base_sport = 'Zhpsxkd'\n",
    "sold_by = 'Z1da2q0e'\n",
    "prod_list_src = 'd5pqmr'\n",
    "sticker_src = '15lojui'\n",
    "total_page_src = '1uqvrhu'\n",
    "per_page = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df0d8d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(filter_link)\n",
    "\n",
    "#accept cookie popup\n",
    "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
    "\n",
    "\n",
    "#get all buttons and click on them all (show more) by \n",
    "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
    "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
    "\n",
    "#click all buttons\n",
    "for button in buttons:\n",
    "    driver.execute_script(\"arguments[0].click();\", button)\n",
    "    #getting the soup\n",
    "    soup = bts(driver.page_source)\n",
    "    \n",
    "# access all filters:\n",
    "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
    "\n",
    "# access sport filter at the last position:\n",
    "sports = filter[len(filter)-1]\n",
    "\n",
    "sport_list = []\n",
    "sport_url = sports.find_all(\"a\")\n",
    "\n",
    "#first item\n",
    "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
    "sport_code = base_sport\n",
    "sport_list.append({'sport_type': sport_type,\n",
    "                   'sport_code': sport_code})\n",
    "\n",
    "for i in range(1, len(sport_url)):\n",
    "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
    "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
    "    sport_list.append({'sport_type': sport_type,\n",
    "                       'sport_code': sport_code})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e1ce51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get category links\n",
    "def getCatLink(base_index, base_cat, sold_by, cat_list):\n",
    "    url = f'{base_index}{base_cat}{sold_by}'\n",
    "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
    "    soup = bts(result.text, 'html.parser')\n",
    "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
    "    for cat in categories[0: len(categories)-1]:\n",
    "        level_1 = soup.find(\"h1\").text\n",
    "        level_2 = cat.attrs['data-help']\n",
    "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
    "        cat_list.append({'level_1': level_1,\n",
    "                         'level_2': level_2,\n",
    "                         'cat_link': cat_link})\n",
    "    return cat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "703df522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing category_url data\n",
    "cat_url = []  \n",
    "# opening the file using \"with\" statement\n",
    "with open('cat_url.csv', 'r') as cat_data:\n",
    "    for cat in csv.DictReader(cat_data):\n",
    "        cat_url.append(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fa05133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting category links for all genders:\n",
    "cat_list =[]\n",
    "for i in cat_url:\n",
    "    base_index = i[\"base_index\"]\n",
    "    base_cat = i[\"base_cat\"]\n",
    "    sold_by = i[\"sold_by\"]\n",
    "    cat_list = getCatLink(base_index, base_cat, sold_by, cat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3271c33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7 + 5 + 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "792582d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74aa3d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine link for category level 2 and filtered by sport\n",
    "\n",
    "cat_level3=[]\n",
    "\n",
    "for cat in cat_list:\n",
    "    for sport in sport_list:\n",
    "        country = country\n",
    "        level_1 = cat[\"level_1\"]\n",
    "        level_2 = cat[\"level_2\"]\n",
    "        sport_type = sport[\"sport_type\"]\n",
    "        url = f'{cat[\"cat_link\"]}{sport[\"sport_code\"]}'\n",
    "        cat_level3.append({'country': country,\n",
    "                           'cat1': level_1,\n",
    "                           'cat2': level_2,\n",
    "                           'cat3': sport_type,\n",
    "                           'cat_url': url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de4ef203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': 'DE',\n",
       " 'cat1': 'Damen',\n",
       " 'cat2': 'Sportbekleidung',\n",
       " 'cat3': ' Fu√üball   ',\n",
       " 'cat_url': 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung/_/N-1peigq4Z1da2q0eZexlast'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_level3[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12aa1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving cat_level3\n",
    "\n",
    "with open(f\"{country}_cat_level3.csv\", \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter = \",\")\n",
    "    writer.writerow([\"country\",\"cat1\", \"cat2\", \"cat3\", \"cat_url\"])\n",
    "\n",
    "    for item in cat_level3:\n",
    "        writer.writerow([item['country'], item['cat1'], item['cat2'], item['cat3'], item['cat_url']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9771e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for parsing the URLs\n",
    "def cookSoup(url): \n",
    "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
    "    soup = bts(result.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3336a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for pagination - creating a list of urls from a category\n",
    "def pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page):\n",
    "    url_list = [cat_url]\n",
    "    total_prod = soup.find(\"div\",{\"class\":f\"plp-bar-info svelte-{total_page_src}\"}).find(\"span\", {\"class\":f\"svelte-{total_page_src}\"}).text\n",
    "    #Create list of urls within the cat\n",
    "    total_page = math.ceil(int(total_prod)/per_page)\n",
    "    for i in range(1, total_page + 1):\n",
    "            page = f'{cat_url}?from={per_page * i}&size={per_page}'\n",
    "            url_list.append(page)\n",
    "    print (f'{country}_{cat1}_{cat2}_{cat3}: There are {total_prod} products ({total_page} pages)')\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "722e1a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get main data\n",
    "def getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src):\n",
    "    for url in url_list:\n",
    "        page_soup = cookSoup(url)\n",
    "        prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
    "        \n",
    "        for product in prod:\n",
    "            cat1 = cat1\n",
    "            cat2 = cat2\n",
    "            sport = cat3\n",
    "            link = product.find(\"a\").attrs[\"href\"]\n",
    "            prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
    "            brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
    "            prod_url = f'{base_index}{link}'\n",
    "\n",
    "            #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
    "            #taking sku's even in case of more than 7 character id's:\n",
    "\n",
    "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
    "                prod_id = link.partition(\"?mc=\")[2]\n",
    "            else:\n",
    "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
    "\n",
    "            #Prices:\n",
    "            #for product without discount\n",
    "            if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
    "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
    "                act_price = None\n",
    "\n",
    "            #for product with discount\n",
    "            else:\n",
    "                if product.find(\"span\", {\"class\":\"prc__previous\"}) == None:\n",
    "                    reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
    "                    act_price = None\n",
    "                else:\n",
    "                    reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
    "                    act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
    "\n",
    "            #label:\n",
    "            if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
    "                prod_sticker = None\n",
    "            else:\n",
    "                prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
    "\n",
    "            prod_list.append({'title': prod_title,\n",
    "                             'sku': prod_id,\n",
    "                             'regular price': reg_price,\n",
    "                             'actual price' : act_price,\n",
    "                             'brand': brand_name,\n",
    "                             'url' : prod_url,\n",
    "                             'sticker' : prod_sticker,\n",
    "                             'cat_1' : cat1,\n",
    "                             'cat_2' : cat2,\n",
    "                             'cat_3' : cat3})\n",
    "\n",
    "    print(f'{country}_{cat1}_{cat2}_{cat3}: {len(prod_list)} products have been scraped!')\n",
    "        \n",
    "    return prod_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81558764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Damen_Sportbekleidung_ Wandern   : There are 184 products (5 pages)\n",
      "DE_Damen_Sportbekleidung_ Wandern   : 184 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Fu√üball   : There are 109 products (3 pages)\n",
      "DE_Damen_Sportbekleidung_ Fu√üball   : 293 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Laufen   : There are 170 products (5 pages)\n",
      "DE_Damen_Sportbekleidung_ Laufen   : 463 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Surfen   : There are 23 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Surfen   : 486 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Skifahren   : There are 107 products (3 pages)\n",
      "DE_Damen_Sportbekleidung_ Skifahren   : 593 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Trekking   : There are 105 products (3 pages)\n",
      "DE_Damen_Sportbekleidung_ Trekking   : 698 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Pilates   : There are 149 products (4 pages)\n",
      "DE_Damen_Sportbekleidung_ Pilates   : 847 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Schwimmen   : There are 3 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Schwimmen   : 850 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Cardio-Training   : There are 100 products (3 pages)\n",
      "DE_Damen_Sportbekleidung_ Cardio-Training   : 950 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Tennis   : There are 58 products (2 pages)\n",
      "DE_Damen_Sportbekleidung_ Tennis   : 1008 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Bodybuilding   : There are 46 products (2 pages)\n",
      "DE_Damen_Sportbekleidung_ Bodybuilding   : 1054 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Mountainbiking   : There are 36 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Mountainbiking   : 1090 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Badminton   : There are 47 products (2 pages)\n",
      "DE_Damen_Sportbekleidung_ Badminton   : 1137 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Padel-Tennis   : There are 66 products (2 pages)\n",
      "DE_Damen_Sportbekleidung_ Padel-Tennis   : 1203 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Urban Cycling   : There are 35 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Urban Cycling   : 1238 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Rennradfahren   : There are 64 products (2 pages)\n",
      "DE_Damen_Sportbekleidung_ Rennradfahren   : 1302 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Snowboarden   : There are 49 products (2 pages)\n",
      "DE_Damen_Sportbekleidung_ Snowboarden   : 1351 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Jagd   : There are 55 products (2 pages)\n",
      "DE_Damen_Sportbekleidung_ Jagd   : 1406 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Walking   : There are 43 products (2 pages)\n",
      "DE_Damen_Sportbekleidung_ Walking   : 1449 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Basketball   : There are 42 products (2 pages)\n",
      "DE_Damen_Sportbekleidung_ Basketball   : 1491 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Tourenski   : There are 42 products (2 pages)\n",
      "DE_Damen_Sportbekleidung_ Tourenski   : 1533 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Squash   : There are 40 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Squash   : 1573 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Segeln   : There are 51 products (2 pages)\n",
      "DE_Damen_Sportbekleidung_ Segeln   : 1624 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Stand Up Paddling   : There are 7 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Stand Up Paddling   : 1631 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Tauchen   : There are 7 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Tauchen   : 1638 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Bergsteigen   : There are 40 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Bergsteigen   : 1678 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Yoga   : There are 48 products (2 pages)\n",
      "DE_Damen_Sportbekleidung_ Yoga   : 1727 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Volleyball   : There are 22 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Volleyball   : 1749 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Hybrid-Rad   : There are 15 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Hybrid-Rad   : 1764 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Rodeln   : There are 42 products (2 pages)\n",
      "DE_Damen_Sportbekleidung_ Rodeln   : 1806 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Handball   : There are 21 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Handball   : 1827 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Kanu/Kajaksport   : There are 9 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Kanu/Kajaksport   : 1836 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Reiten   : There are 44 products (2 pages)\n",
      "DE_Damen_Sportbekleidung_ Reiten   : 1880 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Leichtathletik   : There are 55 products (2 pages)\n",
      "DE_Damen_Sportbekleidung_ Leichtathletik   : 1935 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Aquafitness   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Aquafitness   : 1936 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Baseball   : There are 50 products (2 pages)\n",
      "DE_Damen_Sportbekleidung_ Baseball   : 1986 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ American Football   : There are 40 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ American Football   : 2026 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Feldhockey   : There are 15 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Feldhockey   : 2041 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Rugby   : There are 27 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Rugby   : 2068 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Orientierungslauf   : There are 32 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Orientierungslauf   : 2100 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Triathlon   : There are 18 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Triathlon   : 2118 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Klettern   : There are 22 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Klettern   : 2140 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Pelota   : There are 31 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Pelota   : 2171 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Gehen   : There are 36 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Gehen   : 2207 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Golf   : There are 31 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Golf   : 2238 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Ski Nordisch   : There are 19 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Ski Nordisch   : 2257 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Boxen   : There are 8 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Boxen   : 2265 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Softball   : There are 26 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Softball   : 2291 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Modern Dance   : There are 29 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Modern Dance   : 2320 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Pickleball   : There are 31 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Pickleball   : 2351 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Racquetball   : There are 26 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Racquetball   : 2377 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Speedball   : There are 27 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Speedball   : 2404 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Inlineskaten   : There are 5 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Inlineskaten   : 2409 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Frescoball   : There are 28 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Frescoball   : 2437 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Crosstraining   : There are 5 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Crosstraining   : 2442 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Tischtennis   : There are 19 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Tischtennis   : 2461 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Wasserball   : There are 0 products (0 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Damen_Sportbekleidung_ Wasserball   : 2461 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Beachtennis   : There are 18 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Beachtennis   : 2479 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Roller   : There are 8 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Roller   : 2487 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Einradfahren   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportbekleidung_ Einradfahren   : 2487 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Skateboarden   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Skateboarden   : 2488 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Rollschuhfahren   : There are 6 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Rollschuhfahren   : 2494 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Windsurfen   : There are 6 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Windsurfen   : 2500 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Ballett   : There are 11 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Ballett   : 2511 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Gymnastik   : There are 11 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Gymnastik   : 2522 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Raubfisch-Angeln   : There are 11 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Raubfisch-Angeln   : 2533 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Crossminton   : There are 11 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Crossminton   : 2544 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Salzwasserangeln   : There are 9 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Salzwasserangeln   : 2553 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Kitesurfen   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Kitesurfen   : 2555 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Tamburello   : There are 13 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Tamburello   : 2568 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Eislaufen   : There are 4 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Eislaufen   : 2572 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Kinderradsport   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportbekleidung_ Kinderradsport   : 2572 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Cricket   : There are 9 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Cricket   : 2581 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Floorball   : There are 9 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Floorball   : 2590 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Netball   : There are 8 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Netball   : 2598 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Inlinehockey   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Inlinehockey   : 2600 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Swimrun   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Swimrun   : 2601 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Eishockey   : There are 3 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Eishockey   : 2604 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Lacrosse   : There are 8 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Lacrosse   : 2612 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ H√∂hlenwandern   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportbekleidung_ H√∂hlenwandern   : 2612 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Urban Dance   : There are 9 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Urban Dance   : 2621 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Kite Landboarding   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Kite Landboarding   : 2623 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Karpfenangeln   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Karpfenangeln   : 2625 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Slacklining   : There are 10 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Slacklining   : 2635 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Power Kiting   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Power Kiting   : 2637 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Snowkiting   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Snowkiting   : 2639 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Westernreiten   : There are 6 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Westernreiten   : 2645 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Grappling   : There are 6 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Grappling   : 2651 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Karate   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Karate   : 2653 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Wrestling   : There are 5 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Wrestling   : 2658 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Drachenboot   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportbekleidung_ Drachenboot   : 2658 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Jiu Jitsu   : There are 5 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Jiu Jitsu   : 2663 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Sepak Takraw   : There are 5 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Sepak Takraw   : 2668 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Canyoning   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Canyoning   : 2669 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Sambo   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Sambo   : 2671 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Schie√üen   : There are 3 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Schie√üen   : 2674 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Rudern   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportbekleidung_ Rudern   : 2674 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Pole Fishing   : There are 4 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Pole Fishing   : 2678 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Selbstverteidigung   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Selbstverteidigung   : 2680 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Taekwondo   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Taekwondo   : 2681 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ BMX   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ BMX   : 2682 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Kung Fu   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Kung Fu   : 2683 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Aikido   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Aikido   : 2685 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Fechten   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Fechten   : 2687 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Judo   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Judo   : 2688 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Kendo   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Kendo   : 2689 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Eisfischen   : There are 3 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Eisfischen   : 2692 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Wasserski   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Wasserski   : 2693 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Wakeboarden   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Wakeboarden   : 2694 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Ball Hockey   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Ball Hockey   : 2695 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Taijiquan    : There are 1 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Taijiquan    : 2696 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Fitness Dance   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Fitness Dance   : 2697 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Poledance   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Poledance   : 2699 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Damen_Sportbekleidung_ Gezogene aufblasbare Schwimmk√∂rper   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Gezogene aufblasbare Schwimmk√∂rper   : 2700 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Capoeira   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Capoeira   : 2701 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Kempo   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Kempo   : 2702 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Peteca   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Peteca   : 2703 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Bike & Run   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Bike & Run   : 2704 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Kitewing   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportbekleidung_ Kitewing   : 2704 products have been scraped!\n",
      "DE_Damen_Sportbekleidung_ Arnis   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportbekleidung_ Arnis   : 2705 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Wandern   : There are 141 products (4 pages)\n",
      "DE_Damen_Sportschuhe_ Wandern   : 2846 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Fu√üball   : There are 41 products (2 pages)\n",
      "DE_Damen_Sportschuhe_ Fu√üball   : 2887 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Laufen   : There are 64 products (2 pages)\n",
      "DE_Damen_Sportschuhe_ Laufen   : 2951 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Surfen   : There are 28 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Surfen   : 2979 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Skifahren   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Skifahren   : 2979 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Trekking   : There are 14 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Trekking   : 2993 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Pilates   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Pilates   : 2994 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Schwimmen   : There are 20 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Schwimmen   : 3014 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Cardio-Training   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Cardio-Training   : 3014 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Tennis   : There are 21 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Tennis   : 3035 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Bodybuilding   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Bodybuilding   : 3035 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Mountainbiking   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Mountainbiking   : 3035 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Badminton   : There are 12 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Badminton   : 3047 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Padel-Tennis   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Padel-Tennis   : 3047 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Urban Cycling   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Urban Cycling   : 3047 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Rennradfahren   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Rennradfahren   : 3047 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Snowboarden   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Snowboarden   : 3047 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Jagd   : There are 3 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Jagd   : 3050 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Walking   : There are 48 products (2 pages)\n",
      "DE_Damen_Sportschuhe_ Walking   : 3098 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Basketball   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Basketball   : 3099 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Tourenski   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Tourenski   : 3099 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Squash   : There are 12 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Squash   : 3111 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Segeln   : There are 5 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Segeln   : 3116 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Stand Up Paddling   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Stand Up Paddling   : 3116 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Tauchen   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Tauchen   : 3116 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Bergsteigen   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Bergsteigen   : 3118 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Yoga   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Yoga   : 3118 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Volleyball   : There are 7 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Volleyball   : 3125 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Hybrid-Rad   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Hybrid-Rad   : 3125 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Rodeln   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Rodeln   : 3125 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Handball   : There are 8 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Handball   : 3133 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Kanu/Kajaksport   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Kanu/Kajaksport   : 3133 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Reiten   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Reiten   : 3133 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Leichtathletik   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Leichtathletik   : 3134 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Aquafitness   : There are 12 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Aquafitness   : 3146 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Baseball   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Baseball   : 3146 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ American Football   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ American Football   : 3146 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Feldhockey   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Feldhockey   : 3146 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Rugby   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Rugby   : 3146 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Orientierungslauf   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Orientierungslauf   : 3146 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Triathlon   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Triathlon   : 3146 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Klettern   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Klettern   : 3146 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Pelota   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Pelota   : 3147 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Gehen   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Gehen   : 3149 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Golf   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Golf   : 3149 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Ski Nordisch   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Ski Nordisch   : 3149 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Boxen   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Boxen   : 3149 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Softball   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Softball   : 3149 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Modern Dance   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Modern Dance   : 3149 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Pickleball   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Pickleball   : 3150 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Racquetball   : There are 3 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Racquetball   : 3153 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Speedball   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Speedball   : 3154 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Inlineskaten   : There are 0 products (0 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Damen_Sportschuhe_ Inlineskaten   : 3154 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Frescoball   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Frescoball   : 3154 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Crosstraining   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Crosstraining   : 3154 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Tischtennis   : There are 4 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Tischtennis   : 3158 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Wasserball   : There are 15 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Wasserball   : 3173 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Beachtennis   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Beachtennis   : 3173 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Roller   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Roller   : 3173 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Einradfahren   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Einradfahren   : 3173 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Skateboarden   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Skateboarden   : 3174 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Rollschuhfahren   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Rollschuhfahren   : 3174 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Windsurfen   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Windsurfen   : 3174 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Ballett   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Ballett   : 3174 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Gymnastik   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Gymnastik   : 3174 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Raubfisch-Angeln   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Raubfisch-Angeln   : 3174 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Crossminton   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Crossminton   : 3175 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Salzwasserangeln   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Salzwasserangeln   : 3175 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Kitesurfen   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Kitesurfen   : 3175 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Tamburello   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Tamburello   : 3175 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Eislaufen   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Eislaufen   : 3175 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Kinderradsport   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Kinderradsport   : 3175 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Cricket   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Cricket   : 3175 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Floorball   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Floorball   : 3175 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Netball   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Netball   : 3175 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Inlinehockey   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Inlinehockey   : 3175 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Swimrun   : There are 5 products (1 pages)\n",
      "DE_Damen_Sportschuhe_ Swimrun   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Eishockey   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Eishockey   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Lacrosse   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Lacrosse   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ H√∂hlenwandern   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ H√∂hlenwandern   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Urban Dance   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Urban Dance   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Kite Landboarding   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Kite Landboarding   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Karpfenangeln   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Karpfenangeln   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Slacklining   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Slacklining   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Power Kiting   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Power Kiting   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Snowkiting   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Snowkiting   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Westernreiten   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Westernreiten   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Grappling   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Grappling   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Karate   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Karate   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Wrestling   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Wrestling   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Drachenboot   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Drachenboot   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Jiu Jitsu   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Jiu Jitsu   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Sepak Takraw   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Sepak Takraw   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Canyoning   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Canyoning   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Sambo   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Sambo   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Schie√üen   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Schie√üen   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Rudern   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Rudern   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Pole Fishing   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Pole Fishing   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Selbstverteidigung   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Selbstverteidigung   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Taekwondo   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Taekwondo   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ BMX   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ BMX   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Kung Fu   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Kung Fu   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Aikido   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Aikido   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Fechten   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Fechten   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Judo   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Judo   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Kendo   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Kendo   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Eisfischen   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Eisfischen   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Wasserski   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Wasserski   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Wakeboarden   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Wakeboarden   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Ball Hockey   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Ball Hockey   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Taijiquan    : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Taijiquan    : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Fitness Dance   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Fitness Dance   : 3180 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Damen_Sportschuhe_ Poledance   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Poledance   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Gezogene aufblasbare Schwimmk√∂rper   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Gezogene aufblasbare Schwimmk√∂rper   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Capoeira   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Capoeira   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Kempo   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Kempo   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Peteca   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Peteca   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Bike & Run   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Bike & Run   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Kitewing   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Kitewing   : 3180 products have been scraped!\n",
      "DE_Damen_Sportschuhe_ Arnis   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportschuhe_ Arnis   : 3180 products have been scraped!\n",
      "DE_Damen_Bademode_ Wandern   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Wandern   : 3180 products have been scraped!\n",
      "DE_Damen_Bademode_ Fu√üball   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Fu√üball   : 3180 products have been scraped!\n",
      "DE_Damen_Bademode_ Laufen   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Laufen   : 3180 products have been scraped!\n",
      "DE_Damen_Bademode_ Surfen   : There are 178 products (5 pages)\n",
      "DE_Damen_Bademode_ Surfen   : 3358 products have been scraped!\n",
      "DE_Damen_Bademode_ Skifahren   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Skifahren   : 3358 products have been scraped!\n",
      "DE_Damen_Bademode_ Trekking   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Trekking   : 3358 products have been scraped!\n",
      "DE_Damen_Bademode_ Pilates   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Pilates   : 3358 products have been scraped!\n",
      "DE_Damen_Bademode_ Schwimmen   : There are 89 products (3 pages)\n",
      "DE_Damen_Bademode_ Schwimmen   : 3447 products have been scraped!\n",
      "DE_Damen_Bademode_ Cardio-Training   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Cardio-Training   : 3447 products have been scraped!\n",
      "DE_Damen_Bademode_ Tennis   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Tennis   : 3447 products have been scraped!\n",
      "DE_Damen_Bademode_ Bodybuilding   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Bodybuilding   : 3447 products have been scraped!\n",
      "DE_Damen_Bademode_ Mountainbiking   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Mountainbiking   : 3447 products have been scraped!\n",
      "DE_Damen_Bademode_ Badminton   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Badminton   : 3447 products have been scraped!\n",
      "DE_Damen_Bademode_ Padel-Tennis   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Padel-Tennis   : 3447 products have been scraped!\n",
      "DE_Damen_Bademode_ Urban Cycling   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Urban Cycling   : 3447 products have been scraped!\n",
      "DE_Damen_Bademode_ Rennradfahren   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Rennradfahren   : 3447 products have been scraped!\n",
      "DE_Damen_Bademode_ Snowboarden   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Snowboarden   : 3447 products have been scraped!\n",
      "DE_Damen_Bademode_ Jagd   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Jagd   : 3447 products have been scraped!\n",
      "DE_Damen_Bademode_ Walking   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Walking   : 3447 products have been scraped!\n",
      "DE_Damen_Bademode_ Basketball   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Basketball   : 3447 products have been scraped!\n",
      "DE_Damen_Bademode_ Tourenski   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Tourenski   : 3447 products have been scraped!\n",
      "DE_Damen_Bademode_ Squash   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Squash   : 3447 products have been scraped!\n",
      "DE_Damen_Bademode_ Segeln   : There are 4 products (1 pages)\n",
      "DE_Damen_Bademode_ Segeln   : 3451 products have been scraped!\n",
      "DE_Damen_Bademode_ Stand Up Paddling   : There are 17 products (1 pages)\n",
      "DE_Damen_Bademode_ Stand Up Paddling   : 3468 products have been scraped!\n",
      "DE_Damen_Bademode_ Tauchen   : There are 43 products (2 pages)\n",
      "DE_Damen_Bademode_ Tauchen   : 3511 products have been scraped!\n",
      "DE_Damen_Bademode_ Bergsteigen   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Bergsteigen   : 3511 products have been scraped!\n",
      "DE_Damen_Bademode_ Yoga   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Yoga   : 3511 products have been scraped!\n",
      "DE_Damen_Bademode_ Volleyball   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Volleyball   : 3511 products have been scraped!\n",
      "DE_Damen_Bademode_ Hybrid-Rad   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Hybrid-Rad   : 3511 products have been scraped!\n",
      "DE_Damen_Bademode_ Rodeln   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Rodeln   : 3511 products have been scraped!\n",
      "DE_Damen_Bademode_ Handball   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Handball   : 3511 products have been scraped!\n",
      "DE_Damen_Bademode_ Kanu/Kajaksport   : There are 6 products (1 pages)\n",
      "DE_Damen_Bademode_ Kanu/Kajaksport   : 3517 products have been scraped!\n",
      "DE_Damen_Bademode_ Reiten   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Reiten   : 3517 products have been scraped!\n",
      "DE_Damen_Bademode_ Leichtathletik   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Leichtathletik   : 3517 products have been scraped!\n",
      "DE_Damen_Bademode_ Aquafitness   : There are 33 products (1 pages)\n",
      "DE_Damen_Bademode_ Aquafitness   : 3550 products have been scraped!\n",
      "DE_Damen_Bademode_ Baseball   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Baseball   : 3550 products have been scraped!\n",
      "DE_Damen_Bademode_ American Football   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ American Football   : 3550 products have been scraped!\n",
      "DE_Damen_Bademode_ Feldhockey   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Feldhockey   : 3550 products have been scraped!\n",
      "DE_Damen_Bademode_ Rugby   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Rugby   : 3550 products have been scraped!\n",
      "DE_Damen_Bademode_ Orientierungslauf   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Orientierungslauf   : 3550 products have been scraped!\n",
      "DE_Damen_Bademode_ Triathlon   : There are 9 products (1 pages)\n",
      "DE_Damen_Bademode_ Triathlon   : 3559 products have been scraped!\n",
      "DE_Damen_Bademode_ Klettern   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Klettern   : 3559 products have been scraped!\n",
      "DE_Damen_Bademode_ Pelota   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Pelota   : 3559 products have been scraped!\n",
      "DE_Damen_Bademode_ Gehen   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Gehen   : 3559 products have been scraped!\n",
      "DE_Damen_Bademode_ Golf   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Golf   : 3559 products have been scraped!\n",
      "DE_Damen_Bademode_ Ski Nordisch   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Ski Nordisch   : 3559 products have been scraped!\n",
      "DE_Damen_Bademode_ Boxen   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Boxen   : 3559 products have been scraped!\n",
      "DE_Damen_Bademode_ Softball   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Softball   : 3559 products have been scraped!\n",
      "DE_Damen_Bademode_ Modern Dance   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Modern Dance   : 3559 products have been scraped!\n",
      "DE_Damen_Bademode_ Pickleball   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Pickleball   : 3559 products have been scraped!\n",
      "DE_Damen_Bademode_ Racquetball   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Racquetball   : 3559 products have been scraped!\n",
      "DE_Damen_Bademode_ Speedball   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Speedball   : 3559 products have been scraped!\n",
      "DE_Damen_Bademode_ Inlineskaten   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Inlineskaten   : 3559 products have been scraped!\n",
      "DE_Damen_Bademode_ Frescoball   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Frescoball   : 3559 products have been scraped!\n",
      "DE_Damen_Bademode_ Crosstraining   : There are 0 products (0 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Damen_Bademode_ Crosstraining   : 3559 products have been scraped!\n",
      "DE_Damen_Bademode_ Tischtennis   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Tischtennis   : 3559 products have been scraped!\n",
      "DE_Damen_Bademode_ Wasserball   : There are 8 products (1 pages)\n",
      "DE_Damen_Bademode_ Wasserball   : 3567 products have been scraped!\n",
      "DE_Damen_Bademode_ Beachtennis   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Beachtennis   : 3567 products have been scraped!\n",
      "DE_Damen_Bademode_ Roller   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Roller   : 3567 products have been scraped!\n",
      "DE_Damen_Bademode_ Einradfahren   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Einradfahren   : 3567 products have been scraped!\n",
      "DE_Damen_Bademode_ Skateboarden   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Skateboarden   : 3567 products have been scraped!\n",
      "DE_Damen_Bademode_ Rollschuhfahren   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Rollschuhfahren   : 3567 products have been scraped!\n",
      "DE_Damen_Bademode_ Windsurfen   : There are 6 products (1 pages)\n",
      "DE_Damen_Bademode_ Windsurfen   : 3573 products have been scraped!\n",
      "DE_Damen_Bademode_ Ballett   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Ballett   : 3573 products have been scraped!\n",
      "DE_Damen_Bademode_ Gymnastik   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Gymnastik   : 3573 products have been scraped!\n",
      "DE_Damen_Bademode_ Raubfisch-Angeln   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Raubfisch-Angeln   : 3573 products have been scraped!\n",
      "DE_Damen_Bademode_ Crossminton   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Crossminton   : 3573 products have been scraped!\n",
      "DE_Damen_Bademode_ Salzwasserangeln   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Salzwasserangeln   : 3573 products have been scraped!\n",
      "DE_Damen_Bademode_ Kitesurfen   : There are 6 products (1 pages)\n",
      "DE_Damen_Bademode_ Kitesurfen   : 3579 products have been scraped!\n",
      "DE_Damen_Bademode_ Tamburello   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Tamburello   : 3579 products have been scraped!\n",
      "DE_Damen_Bademode_ Eislaufen   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Eislaufen   : 3579 products have been scraped!\n",
      "DE_Damen_Bademode_ Kinderradsport   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Kinderradsport   : 3579 products have been scraped!\n",
      "DE_Damen_Bademode_ Cricket   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Cricket   : 3579 products have been scraped!\n",
      "DE_Damen_Bademode_ Floorball   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Floorball   : 3579 products have been scraped!\n",
      "DE_Damen_Bademode_ Netball   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Netball   : 3579 products have been scraped!\n",
      "DE_Damen_Bademode_ Inlinehockey   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Inlinehockey   : 3579 products have been scraped!\n",
      "DE_Damen_Bademode_ Swimrun   : There are 4 products (1 pages)\n",
      "DE_Damen_Bademode_ Swimrun   : 3583 products have been scraped!\n",
      "DE_Damen_Bademode_ Eishockey   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Eishockey   : 3583 products have been scraped!\n",
      "DE_Damen_Bademode_ Lacrosse   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Lacrosse   : 3583 products have been scraped!\n",
      "DE_Damen_Bademode_ H√∂hlenwandern   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ H√∂hlenwandern   : 3583 products have been scraped!\n",
      "DE_Damen_Bademode_ Urban Dance   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Urban Dance   : 3583 products have been scraped!\n",
      "DE_Damen_Bademode_ Kite Landboarding   : There are 3 products (1 pages)\n",
      "DE_Damen_Bademode_ Kite Landboarding   : 3586 products have been scraped!\n",
      "DE_Damen_Bademode_ Karpfenangeln   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Karpfenangeln   : 3586 products have been scraped!\n",
      "DE_Damen_Bademode_ Slacklining   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Slacklining   : 3586 products have been scraped!\n",
      "DE_Damen_Bademode_ Power Kiting   : There are 4 products (1 pages)\n",
      "DE_Damen_Bademode_ Power Kiting   : 3590 products have been scraped!\n",
      "DE_Damen_Bademode_ Snowkiting   : There are 2 products (1 pages)\n",
      "DE_Damen_Bademode_ Snowkiting   : 3592 products have been scraped!\n",
      "DE_Damen_Bademode_ Westernreiten   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Westernreiten   : 3592 products have been scraped!\n",
      "DE_Damen_Bademode_ Grappling   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Grappling   : 3592 products have been scraped!\n",
      "DE_Damen_Bademode_ Karate   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Karate   : 3592 products have been scraped!\n",
      "DE_Damen_Bademode_ Wrestling   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Wrestling   : 3592 products have been scraped!\n",
      "DE_Damen_Bademode_ Drachenboot   : There are 1 products (1 pages)\n",
      "DE_Damen_Bademode_ Drachenboot   : 3593 products have been scraped!\n",
      "DE_Damen_Bademode_ Jiu Jitsu   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Jiu Jitsu   : 3593 products have been scraped!\n",
      "DE_Damen_Bademode_ Sepak Takraw   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Sepak Takraw   : 3593 products have been scraped!\n",
      "DE_Damen_Bademode_ Canyoning   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Canyoning   : 3593 products have been scraped!\n",
      "DE_Damen_Bademode_ Sambo   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Sambo   : 3593 products have been scraped!\n",
      "DE_Damen_Bademode_ Schie√üen   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Schie√üen   : 3593 products have been scraped!\n",
      "DE_Damen_Bademode_ Rudern   : There are 1 products (1 pages)\n",
      "DE_Damen_Bademode_ Rudern   : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ Pole Fishing   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Pole Fishing   : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ Selbstverteidigung   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Selbstverteidigung   : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ Taekwondo   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Taekwondo   : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ BMX   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ BMX   : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ Kung Fu   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Kung Fu   : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ Aikido   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Aikido   : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ Fechten   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Fechten   : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ Judo   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Judo   : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ Kendo   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Kendo   : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ Eisfischen   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Eisfischen   : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ Wasserski   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Wasserski   : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ Wakeboarden   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Wakeboarden   : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ Ball Hockey   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Ball Hockey   : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ Taijiquan    : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Taijiquan    : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ Fitness Dance   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Fitness Dance   : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ Poledance   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Poledance   : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ Gezogene aufblasbare Schwimmk√∂rper   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Gezogene aufblasbare Schwimmk√∂rper   : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ Capoeira   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Capoeira   : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ Kempo   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Kempo   : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ Peteca   : There are 0 products (0 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Damen_Bademode_ Peteca   : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ Bike & Run   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Bike & Run   : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ Kitewing   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Kitewing   : 3594 products have been scraped!\n",
      "DE_Damen_Bademode_ Arnis   : There are 0 products (0 pages)\n",
      "DE_Damen_Bademode_ Arnis   : 3594 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Wandern   : There are 86 products (3 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Wandern   : 3680 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Fu√üball   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Fu√üball   : 3680 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Laufen   : There are 38 products (1 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Laufen   : 3718 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Surfen   : There are 39 products (1 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Surfen   : 3757 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Skifahren   : There are 12 products (1 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Skifahren   : 3769 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Trekking   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Trekking   : 3769 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Pilates   : There are 77 products (2 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Pilates   : 3846 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Schwimmen   : There are 19 products (1 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Schwimmen   : 3865 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Cardio-Training   : There are 17 products (1 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Cardio-Training   : 3882 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Tennis   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Tennis   : 3882 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Bodybuilding   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Bodybuilding   : 3882 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Mountainbiking   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Mountainbiking   : 3882 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Badminton   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Badminton   : 3882 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Padel-Tennis   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Padel-Tennis   : 3882 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Urban Cycling   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Urban Cycling   : 3882 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Rennradfahren   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Rennradfahren   : 3882 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Snowboarden   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Snowboarden   : 3882 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Jagd   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Jagd   : 3882 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Walking   : There are 12 products (1 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Walking   : 3894 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Basketball   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Basketball   : 3894 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Tourenski   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Tourenski   : 3894 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Squash   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Squash   : 3894 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Segeln   : There are 1 products (1 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Segeln   : 3895 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Stand Up Paddling   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Stand Up Paddling   : 3895 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Tauchen   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Tauchen   : 3895 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Bergsteigen   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Bergsteigen   : 3895 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Yoga   : There are 12 products (1 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Yoga   : 3907 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Volleyball   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Volleyball   : 3907 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Hybrid-Rad   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Hybrid-Rad   : 3907 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Rodeln   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Rodeln   : 3907 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Handball   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Handball   : 3907 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Kanu/Kajaksport   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Kanu/Kajaksport   : 3907 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Reiten   : There are 7 products (1 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Reiten   : 3914 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Leichtathletik   : There are 9 products (1 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Leichtathletik   : 3923 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Aquafitness   : There are 21 products (1 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Aquafitness   : 3944 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Baseball   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Baseball   : 3944 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ American Football   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ American Football   : 3944 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Feldhockey   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Feldhockey   : 3944 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Rugby   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Rugby   : 3944 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Orientierungslauf   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Orientierungslauf   : 3944 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Triathlon   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Triathlon   : 3944 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Klettern   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Klettern   : 3944 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Pelota   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Pelota   : 3944 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Gehen   : There are 11 products (1 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Gehen   : 3955 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Golf   : There are 13 products (1 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Golf   : 3968 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Ski Nordisch   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Ski Nordisch   : 3968 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Boxen   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Boxen   : 3968 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Softball   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Softball   : 3968 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Modern Dance   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Modern Dance   : 3968 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Pickleball   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Pickleball   : 3968 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Racquetball   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Racquetball   : 3968 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Speedball   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Speedball   : 3968 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Inlineskaten   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Inlineskaten   : 3968 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Frescoball   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Frescoball   : 3968 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Crosstraining   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Crosstraining   : 3968 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Tischtennis   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Tischtennis   : 3968 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Damen_Gro√üe Gr√∂√üen_ Wasserball   : There are 8 products (1 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Wasserball   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Beachtennis   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Beachtennis   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Roller   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Roller   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Einradfahren   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Einradfahren   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Skateboarden   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Skateboarden   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Rollschuhfahren   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Rollschuhfahren   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Windsurfen   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Windsurfen   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Ballett   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Ballett   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Gymnastik   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Gymnastik   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Raubfisch-Angeln   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Raubfisch-Angeln   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Crossminton   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Crossminton   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Salzwasserangeln   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Salzwasserangeln   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Kitesurfen   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Kitesurfen   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Tamburello   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Tamburello   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Eislaufen   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Eislaufen   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Kinderradsport   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Kinderradsport   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Cricket   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Cricket   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Floorball   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Floorball   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Netball   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Netball   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Inlinehockey   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Inlinehockey   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Swimrun   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Swimrun   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Eishockey   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Eishockey   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Lacrosse   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Lacrosse   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ H√∂hlenwandern   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ H√∂hlenwandern   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Urban Dance   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Urban Dance   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Kite Landboarding   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Kite Landboarding   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Karpfenangeln   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Karpfenangeln   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Slacklining   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Slacklining   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Power Kiting   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Power Kiting   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Snowkiting   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Snowkiting   : 3976 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Westernreiten   : There are 7 products (1 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Westernreiten   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Grappling   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Grappling   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Karate   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Karate   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Wrestling   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Wrestling   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Drachenboot   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Drachenboot   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Jiu Jitsu   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Jiu Jitsu   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Sepak Takraw   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Sepak Takraw   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Canyoning   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Canyoning   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Sambo   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Sambo   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Schie√üen   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Schie√üen   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Rudern   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Rudern   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Pole Fishing   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Pole Fishing   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Selbstverteidigung   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Selbstverteidigung   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Taekwondo   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Taekwondo   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ BMX   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ BMX   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Kung Fu   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Kung Fu   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Aikido   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Aikido   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Fechten   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Fechten   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Judo   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Judo   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Kendo   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Kendo   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Eisfischen   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Eisfischen   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Wasserski   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Wasserski   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Wakeboarden   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Wakeboarden   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Ball Hockey   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Ball Hockey   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Taijiquan    : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Taijiquan    : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Fitness Dance   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Fitness Dance   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Poledance   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Poledance   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Gezogene aufblasbare Schwimmk√∂rper   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Gezogene aufblasbare Schwimmk√∂rper   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Capoeira   : There are 0 products (0 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Damen_Gro√üe Gr√∂√üen_ Capoeira   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Kempo   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Kempo   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Peteca   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Peteca   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Bike & Run   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Bike & Run   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Kitewing   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Kitewing   : 3983 products have been scraped!\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Arnis   : There are 0 products (0 pages)\n",
      "DE_Damen_Gro√üe Gr√∂√üen_ Arnis   : 3983 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Wandern   : There are 2 products (1 pages)\n",
      "DE_Damen_Umstandsmode_ Wandern   : 3985 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Fu√üball   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Fu√üball   : 3985 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Laufen   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Laufen   : 3985 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Surfen   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Surfen   : 3985 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Skifahren   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Skifahren   : 3985 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Trekking   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Trekking   : 3985 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Pilates   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Pilates   : 3985 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Schwimmen   : There are 2 products (1 pages)\n",
      "DE_Damen_Umstandsmode_ Schwimmen   : 3987 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Cardio-Training   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Cardio-Training   : 3987 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Tennis   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Tennis   : 3987 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Bodybuilding   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Bodybuilding   : 3987 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Mountainbiking   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Mountainbiking   : 3987 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Badminton   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Badminton   : 3987 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Padel-Tennis   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Padel-Tennis   : 3987 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Urban Cycling   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Urban Cycling   : 3987 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Rennradfahren   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Rennradfahren   : 3987 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Snowboarden   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Snowboarden   : 3987 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Jagd   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Jagd   : 3987 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Walking   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Walking   : 3987 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Basketball   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Basketball   : 3987 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Tourenski   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Tourenski   : 3987 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Squash   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Squash   : 3987 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Segeln   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Segeln   : 3987 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Stand Up Paddling   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Stand Up Paddling   : 3987 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Tauchen   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Tauchen   : 3987 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Bergsteigen   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Bergsteigen   : 3987 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Yoga   : There are 22 products (1 pages)\n",
      "DE_Damen_Umstandsmode_ Yoga   : 4009 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Volleyball   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Volleyball   : 4009 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Hybrid-Rad   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Hybrid-Rad   : 4009 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Rodeln   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Rodeln   : 4009 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Handball   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Handball   : 4009 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Kanu/Kajaksport   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Kanu/Kajaksport   : 4009 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Reiten   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Reiten   : 4009 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Leichtathletik   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Leichtathletik   : 4009 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Aquafitness   : There are 2 products (1 pages)\n",
      "DE_Damen_Umstandsmode_ Aquafitness   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Baseball   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Baseball   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ American Football   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ American Football   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Feldhockey   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Feldhockey   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Rugby   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Rugby   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Orientierungslauf   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Orientierungslauf   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Triathlon   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Triathlon   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Klettern   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Klettern   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Pelota   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Pelota   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Gehen   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Gehen   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Golf   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Golf   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Ski Nordisch   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Ski Nordisch   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Boxen   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Boxen   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Softball   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Softball   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Modern Dance   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Modern Dance   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Pickleball   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Pickleball   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Racquetball   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Racquetball   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Speedball   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Speedball   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Inlineskaten   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Inlineskaten   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Frescoball   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Frescoball   : 4011 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Damen_Umstandsmode_ Crosstraining   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Crosstraining   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Tischtennis   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Tischtennis   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Wasserball   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Wasserball   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Beachtennis   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Beachtennis   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Roller   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Roller   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Einradfahren   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Einradfahren   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Skateboarden   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Skateboarden   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Rollschuhfahren   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Rollschuhfahren   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Windsurfen   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Windsurfen   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Ballett   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Ballett   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Gymnastik   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Gymnastik   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Raubfisch-Angeln   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Raubfisch-Angeln   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Crossminton   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Crossminton   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Salzwasserangeln   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Salzwasserangeln   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Kitesurfen   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Kitesurfen   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Tamburello   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Tamburello   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Eislaufen   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Eislaufen   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Kinderradsport   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Kinderradsport   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Cricket   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Cricket   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Floorball   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Floorball   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Netball   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Netball   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Inlinehockey   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Inlinehockey   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Swimrun   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Swimrun   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Eishockey   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Eishockey   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Lacrosse   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Lacrosse   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ H√∂hlenwandern   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ H√∂hlenwandern   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Urban Dance   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Urban Dance   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Kite Landboarding   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Kite Landboarding   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Karpfenangeln   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Karpfenangeln   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Slacklining   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Slacklining   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Power Kiting   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Power Kiting   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Snowkiting   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Snowkiting   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Westernreiten   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Westernreiten   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Grappling   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Grappling   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Karate   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Karate   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Wrestling   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Wrestling   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Drachenboot   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Drachenboot   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Jiu Jitsu   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Jiu Jitsu   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Sepak Takraw   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Sepak Takraw   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Canyoning   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Canyoning   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Sambo   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Sambo   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Schie√üen   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Schie√üen   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Rudern   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Rudern   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Pole Fishing   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Pole Fishing   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Selbstverteidigung   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Selbstverteidigung   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Taekwondo   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Taekwondo   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ BMX   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ BMX   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Kung Fu   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Kung Fu   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Aikido   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Aikido   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Fechten   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Fechten   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Judo   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Judo   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Kendo   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Kendo   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Eisfischen   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Eisfischen   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Wasserski   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Wasserski   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Wakeboarden   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Wakeboarden   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Ball Hockey   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Ball Hockey   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Taijiquan    : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Taijiquan    : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Fitness Dance   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Fitness Dance   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Poledance   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Poledance   : 4011 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Damen_Umstandsmode_ Gezogene aufblasbare Schwimmk√∂rper   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Gezogene aufblasbare Schwimmk√∂rper   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Capoeira   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Capoeira   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Kempo   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Kempo   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Peteca   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Peteca   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Bike & Run   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Bike & Run   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Kitewing   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Kitewing   : 4011 products have been scraped!\n",
      "DE_Damen_Umstandsmode_ Arnis   : There are 0 products (0 pages)\n",
      "DE_Damen_Umstandsmode_ Arnis   : 4011 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Wandern   : There are 63 products (2 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Wandern   : 4074 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Fu√üball   : There are 103 products (3 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Fu√üball   : 4177 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Laufen   : There are 16 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Laufen   : 4193 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Surfen   : There are 18 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Surfen   : 4211 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Skifahren   : There are 100 products (3 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Skifahren   : 4311 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Trekking   : There are 32 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Trekking   : 4343 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Pilates   : There are 21 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Pilates   : 4364 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Schwimmen   : There are 57 products (2 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Schwimmen   : 4421 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Cardio-Training   : There are 60 products (2 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Cardio-Training   : 4481 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Tennis   : There are 57 products (2 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Tennis   : 4538 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Bodybuilding   : There are 39 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Bodybuilding   : 4577 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Mountainbiking   : There are 82 products (3 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Mountainbiking   : 4659 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Badminton   : There are 54 products (2 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Badminton   : 4713 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Padel-Tennis   : There are 43 products (2 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Padel-Tennis   : 4756 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Urban Cycling   : There are 57 products (2 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Urban Cycling   : 4813 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Rennradfahren   : There are 29 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Rennradfahren   : 4842 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Snowboarden   : There are 56 products (2 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Snowboarden   : 4898 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Jagd   : There are 21 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Jagd   : 4919 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Walking   : There are 8 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Walking   : 4927 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Basketball   : There are 45 products (2 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Basketball   : 4972 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Tourenski   : There are 50 products (2 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Tourenski   : 5022 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Squash   : There are 33 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Squash   : 5055 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Segeln   : There are 29 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Segeln   : 5084 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Stand Up Paddling   : There are 28 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Stand Up Paddling   : 5112 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Tauchen   : There are 39 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Tauchen   : 5151 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Bergsteigen   : There are 14 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Bergsteigen   : 5165 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Yoga   : There are 25 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Yoga   : 5190 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Volleyball   : There are 45 products (2 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Volleyball   : 5235 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Hybrid-Rad   : There are 33 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Hybrid-Rad   : 5268 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Rodeln   : There are 32 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Rodeln   : 5300 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Handball   : There are 39 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Handball   : 5339 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Kanu/Kajaksport   : There are 24 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Kanu/Kajaksport   : 5363 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Reiten   : There are 29 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Reiten   : 5392 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Leichtathletik   : There are 9 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Leichtathletik   : 5401 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Aquafitness   : There are 11 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Aquafitness   : 5412 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Baseball   : There are 7 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Baseball   : 5419 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ American Football   : There are 11 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ American Football   : 5430 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Feldhockey   : There are 35 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Feldhockey   : 5465 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Rugby   : There are 22 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Rugby   : 5487 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Orientierungslauf   : There are 6 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Orientierungslauf   : 5493 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Triathlon   : There are 20 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Triathlon   : 5513 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Klettern   : There are 20 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Klettern   : 5533 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Pelota   : There are 9 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Pelota   : 5542 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Gehen   : There are 6 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Gehen   : 5548 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Golf   : There are 6 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Golf   : 5554 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Ski Nordisch   : There are 20 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Ski Nordisch   : 5574 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Boxen   : There are 29 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Boxen   : 5603 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Softball   : There are 9 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Softball   : 5612 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Modern Dance   : There are 4 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Modern Dance   : 5616 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Pickleball   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Pickleball   : 5618 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Damen_Sportausr√ºstung_ Racquetball   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Racquetball   : 5620 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Speedball   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Speedball   : 5622 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Inlineskaten   : There are 26 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Inlineskaten   : 5648 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Frescoball   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Frescoball   : 5649 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Crosstraining   : There are 23 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Crosstraining   : 5672 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Tischtennis   : There are 4 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Tischtennis   : 5676 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Wasserball   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Wasserball   : 5677 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Beachtennis   : There are 9 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Beachtennis   : 5686 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Roller   : There are 15 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Roller   : 5701 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Einradfahren   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Einradfahren   : 5702 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Skateboarden   : There are 20 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Skateboarden   : 5722 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Rollschuhfahren   : There are 14 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Rollschuhfahren   : 5736 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Windsurfen   : There are 14 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Windsurfen   : 5750 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Ballett   : There are 7 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Ballett   : 5757 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Gymnastik   : There are 6 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Gymnastik   : 5763 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Raubfisch-Angeln   : There are 8 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Raubfisch-Angeln   : 5771 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Crossminton   : There are 4 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Crossminton   : 5775 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Salzwasserangeln   : There are 6 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Salzwasserangeln   : 5781 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Kitesurfen   : There are 10 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Kitesurfen   : 5791 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Tamburello   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Tamburello   : 5792 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Eislaufen   : There are 8 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Eislaufen   : 5800 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Kinderradsport   : There are 13 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Kinderradsport   : 5813 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Cricket   : There are 4 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Cricket   : 5817 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Floorball   : There are 4 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Floorball   : 5821 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Netball   : There are 4 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Netball   : 5825 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Inlinehockey   : There are 10 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Inlinehockey   : 5835 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Swimrun   : There are 4 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Swimrun   : 5839 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Eishockey   : There are 9 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Eishockey   : 5848 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Lacrosse   : There are 3 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Lacrosse   : 5851 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ H√∂hlenwandern   : There are 7 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ H√∂hlenwandern   : 5858 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Urban Dance   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Urban Dance   : 5860 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Kite Landboarding   : There are 9 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Kite Landboarding   : 5869 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Karpfenangeln   : There are 4 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Karpfenangeln   : 5873 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Slacklining   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Slacklining   : 5873 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Power Kiting   : There are 8 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Power Kiting   : 5881 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Snowkiting   : There are 8 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Snowkiting   : 5889 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Westernreiten   : There are 3 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Westernreiten   : 5892 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Grappling   : There are 3 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Grappling   : 5895 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Karate   : There are 6 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Karate   : 5901 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Wrestling   : There are 3 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Wrestling   : 5904 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Drachenboot   : There are 6 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Drachenboot   : 5910 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Jiu Jitsu   : There are 3 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Jiu Jitsu   : 5913 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Sepak Takraw   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Sepak Takraw   : 5915 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Canyoning   : There are 7 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Canyoning   : 5922 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Sambo   : There are 4 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Sambo   : 5926 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Schie√üen   : There are 5 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Schie√üen   : 5931 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Rudern   : There are 5 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Rudern   : 5936 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Pole Fishing   : There are 5 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Pole Fishing   : 5941 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Selbstverteidigung   : There are 3 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Selbstverteidigung   : 5944 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Taekwondo   : There are 4 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Taekwondo   : 5948 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ BMX   : There are 4 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ BMX   : 5952 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Kung Fu   : There are 3 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Kung Fu   : 5955 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Aikido   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Aikido   : 5957 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Fechten   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Fechten   : 5959 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Judo   : There are 3 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Judo   : 5962 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Kendo   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Kendo   : 5964 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Eisfischen   : There are 3 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Damen_Sportausr√ºstung_ Eisfischen   : 5967 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Wasserski   : There are 3 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Wasserski   : 5970 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Wakeboarden   : There are 3 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Wakeboarden   : 5973 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Ball Hockey   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Ball Hockey   : 5975 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Taijiquan    : There are 1 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Taijiquan    : 5976 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Fitness Dance   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Fitness Dance   : 5977 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Poledance   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Poledance   : 5977 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Gezogene aufblasbare Schwimmk√∂rper   : There are 2 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Gezogene aufblasbare Schwimmk√∂rper   : 5979 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Capoeira   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Capoeira   : 5980 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Kempo   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Kempo   : 5980 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Peteca   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Peteca   : 5980 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Bike & Run   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Bike & Run   : 5981 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Kitewing   : There are 1 products (1 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Kitewing   : 5982 products have been scraped!\n",
      "DE_Damen_Sportausr√ºstung_ Arnis   : There are 0 products (0 pages)\n",
      "DE_Damen_Sportausr√ºstung_ Arnis   : 5982 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Wandern   : There are 279 products (7 pages)\n",
      "DE_Damen_Outdoorwelt_ Wandern   : 6261 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Fu√üball   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Fu√üball   : 6261 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Laufen   : There are 1 products (1 pages)\n",
      "DE_Damen_Outdoorwelt_ Laufen   : 6262 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Surfen   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Surfen   : 6262 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Skifahren   : There are 1 products (1 pages)\n",
      "DE_Damen_Outdoorwelt_ Skifahren   : 6263 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Trekking   : There are 119 products (3 pages)\n",
      "DE_Damen_Outdoorwelt_ Trekking   : 6382 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Pilates   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Pilates   : 6382 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Schwimmen   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Schwimmen   : 6382 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Cardio-Training   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Cardio-Training   : 6382 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Tennis   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Tennis   : 6382 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Bodybuilding   : There are 1 products (1 pages)\n",
      "DE_Damen_Outdoorwelt_ Bodybuilding   : 6383 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Mountainbiking   : There are 19 products (1 pages)\n",
      "DE_Damen_Outdoorwelt_ Mountainbiking   : 6402 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Badminton   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Badminton   : 6402 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Padel-Tennis   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Padel-Tennis   : 6402 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Urban Cycling   : There are 19 products (1 pages)\n",
      "DE_Damen_Outdoorwelt_ Urban Cycling   : 6421 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Rennradfahren   : There are 21 products (1 pages)\n",
      "DE_Damen_Outdoorwelt_ Rennradfahren   : 6442 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Snowboarden   : There are 1 products (1 pages)\n",
      "DE_Damen_Outdoorwelt_ Snowboarden   : 6443 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Jagd   : There are 40 products (1 pages)\n",
      "DE_Damen_Outdoorwelt_ Jagd   : 6483 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Walking   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Walking   : 6483 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Basketball   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Basketball   : 6483 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Tourenski   : There are 1 products (1 pages)\n",
      "DE_Damen_Outdoorwelt_ Tourenski   : 6484 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Squash   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Squash   : 6484 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Segeln   : There are 34 products (1 pages)\n",
      "DE_Damen_Outdoorwelt_ Segeln   : 6518 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Stand Up Paddling   : There are 36 products (1 pages)\n",
      "DE_Damen_Outdoorwelt_ Stand Up Paddling   : 6554 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Tauchen   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Tauchen   : 6554 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Bergsteigen   : There are 5 products (1 pages)\n",
      "DE_Damen_Outdoorwelt_ Bergsteigen   : 6559 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Yoga   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Yoga   : 6559 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Volleyball   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Volleyball   : 6559 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Hybrid-Rad   : There are 21 products (1 pages)\n",
      "DE_Damen_Outdoorwelt_ Hybrid-Rad   : 6580 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Rodeln   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Rodeln   : 6580 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Handball   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Handball   : 6580 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Kanu/Kajaksport   : There are 31 products (1 pages)\n",
      "DE_Damen_Outdoorwelt_ Kanu/Kajaksport   : 6611 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Reiten   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Reiten   : 6611 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Leichtathletik   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Leichtathletik   : 6611 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Aquafitness   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Aquafitness   : 6611 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Baseball   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Baseball   : 6611 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ American Football   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ American Football   : 6611 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Feldhockey   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Feldhockey   : 6611 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Rugby   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Rugby   : 6611 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Orientierungslauf   : There are 9 products (1 pages)\n",
      "DE_Damen_Outdoorwelt_ Orientierungslauf   : 6620 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Triathlon   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Triathlon   : 6620 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Klettern   : There are 3 products (1 pages)\n",
      "DE_Damen_Outdoorwelt_ Klettern   : 6623 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Pelota   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Pelota   : 6623 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Gehen   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Gehen   : 6623 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Golf   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Golf   : 6623 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Ski Nordisch   : There are 0 products (0 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Damen_Outdoorwelt_ Ski Nordisch   : 6623 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Boxen   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Boxen   : 6623 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Softball   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Softball   : 6623 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Modern Dance   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Modern Dance   : 6623 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Pickleball   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Pickleball   : 6623 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Racquetball   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Racquetball   : 6623 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Speedball   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Speedball   : 6623 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Inlineskaten   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Inlineskaten   : 6623 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Frescoball   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Frescoball   : 6623 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Crosstraining   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Crosstraining   : 6623 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Tischtennis   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Tischtennis   : 6623 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Wasserball   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Wasserball   : 6623 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Beachtennis   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Beachtennis   : 6623 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Roller   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Roller   : 6623 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Einradfahren   : There are 21 products (1 pages)\n",
      "DE_Damen_Outdoorwelt_ Einradfahren   : 6644 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Skateboarden   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Skateboarden   : 6644 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Rollschuhfahren   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Rollschuhfahren   : 6644 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Windsurfen   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Windsurfen   : 6644 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Ballett   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Ballett   : 6644 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Gymnastik   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Gymnastik   : 6644 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Raubfisch-Angeln   : There are 8 products (1 pages)\n",
      "DE_Damen_Outdoorwelt_ Raubfisch-Angeln   : 6652 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Crossminton   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Crossminton   : 6652 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Salzwasserangeln   : There are 9 products (1 pages)\n",
      "DE_Damen_Outdoorwelt_ Salzwasserangeln   : 6661 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Kitesurfen   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Kitesurfen   : 6661 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Tamburello   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Tamburello   : 6661 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Eislaufen   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Eislaufen   : 6661 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Kinderradsport   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Kinderradsport   : 6661 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Cricket   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Cricket   : 6661 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Floorball   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Floorball   : 6661 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Netball   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Netball   : 6661 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Inlinehockey   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Inlinehockey   : 6661 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Swimrun   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Swimrun   : 6661 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Eishockey   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Eishockey   : 6661 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Lacrosse   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Lacrosse   : 6661 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ H√∂hlenwandern   : There are 4 products (1 pages)\n",
      "DE_Damen_Outdoorwelt_ H√∂hlenwandern   : 6665 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Urban Dance   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Urban Dance   : 6665 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Kite Landboarding   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Kite Landboarding   : 6665 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Karpfenangeln   : There are 6 products (1 pages)\n",
      "DE_Damen_Outdoorwelt_ Karpfenangeln   : 6671 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Slacklining   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Slacklining   : 6671 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Power Kiting   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Power Kiting   : 6671 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Snowkiting   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Snowkiting   : 6671 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Westernreiten   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Westernreiten   : 6671 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Grappling   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Grappling   : 6671 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Karate   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Karate   : 6671 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Wrestling   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Wrestling   : 6671 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Drachenboot   : There are 1 products (1 pages)\n",
      "DE_Damen_Outdoorwelt_ Drachenboot   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Jiu Jitsu   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Jiu Jitsu   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Sepak Takraw   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Sepak Takraw   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Canyoning   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Canyoning   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Sambo   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Sambo   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Schie√üen   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Schie√üen   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Rudern   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Rudern   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Pole Fishing   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Pole Fishing   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Selbstverteidigung   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Selbstverteidigung   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Taekwondo   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Taekwondo   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ BMX   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ BMX   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Kung Fu   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Kung Fu   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Aikido   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Aikido   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Fechten   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Fechten   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Judo   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Judo   : 6672 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Damen_Outdoorwelt_ Kendo   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Kendo   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Eisfischen   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Eisfischen   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Wasserski   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Wasserski   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Wakeboarden   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Wakeboarden   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Ball Hockey   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Ball Hockey   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Taijiquan    : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Taijiquan    : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Fitness Dance   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Fitness Dance   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Poledance   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Poledance   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Gezogene aufblasbare Schwimmk√∂rper   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Gezogene aufblasbare Schwimmk√∂rper   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Capoeira   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Capoeira   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Kempo   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Kempo   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Peteca   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Peteca   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Bike & Run   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Bike & Run   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Kitewing   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Kitewing   : 6672 products have been scraped!\n",
      "DE_Damen_Outdoorwelt_ Arnis   : There are 0 products (0 pages)\n",
      "DE_Damen_Outdoorwelt_ Arnis   : 6672 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Wandern   : There are 177 products (5 pages)\n",
      "DE_Herren_Sportbekleidung_ Wandern   : 6849 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Fu√üball   : There are 177 products (5 pages)\n",
      "DE_Herren_Sportbekleidung_ Fu√üball   : 7026 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Laufen   : There are 149 products (4 pages)\n",
      "DE_Herren_Sportbekleidung_ Laufen   : 7175 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Surfen   : There are 28 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Surfen   : 7203 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Skifahren   : There are 102 products (3 pages)\n",
      "DE_Herren_Sportbekleidung_ Skifahren   : 7305 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Trekking   : There are 103 products (3 pages)\n",
      "DE_Herren_Sportbekleidung_ Trekking   : 7408 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Pilates   : There are 102 products (3 pages)\n",
      "DE_Herren_Sportbekleidung_ Pilates   : 7510 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Schwimmen   : There are 4 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Schwimmen   : 7514 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Cardio-Training   : There are 67 products (2 pages)\n",
      "DE_Herren_Sportbekleidung_ Cardio-Training   : 7581 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Tennis   : There are 50 products (2 pages)\n",
      "DE_Herren_Sportbekleidung_ Tennis   : 7631 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Bodybuilding   : There are 30 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Bodybuilding   : 7661 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Mountainbiking   : There are 92 products (3 pages)\n",
      "DE_Herren_Sportbekleidung_ Mountainbiking   : 7753 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Badminton   : There are 38 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Badminton   : 7791 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Padel-Tennis   : There are 54 products (2 pages)\n",
      "DE_Herren_Sportbekleidung_ Padel-Tennis   : 7845 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Urban Cycling   : There are 48 products (2 pages)\n",
      "DE_Herren_Sportbekleidung_ Urban Cycling   : 7893 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Rennradfahren   : There are 117 products (3 pages)\n",
      "DE_Herren_Sportbekleidung_ Rennradfahren   : 8010 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Snowboarden   : There are 51 products (2 pages)\n",
      "DE_Herren_Sportbekleidung_ Snowboarden   : 8061 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Jagd   : There are 174 products (5 pages)\n",
      "DE_Herren_Sportbekleidung_ Jagd   : 8235 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Walking   : There are 39 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Walking   : 8274 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Basketball   : There are 52 products (2 pages)\n",
      "DE_Herren_Sportbekleidung_ Basketball   : 8326 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Tourenski   : There are 39 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Tourenski   : 8365 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Squash   : There are 31 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Squash   : 8396 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Segeln   : There are 51 products (2 pages)\n",
      "DE_Herren_Sportbekleidung_ Segeln   : 8447 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Stand Up Paddling   : There are 7 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Stand Up Paddling   : 8454 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Tauchen   : There are 7 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Tauchen   : 8461 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Bergsteigen   : There are 30 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Bergsteigen   : 8491 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Yoga   : There are 16 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Yoga   : 8507 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Volleyball   : There are 24 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Volleyball   : 8531 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Hybrid-Rad   : There are 52 products (2 pages)\n",
      "DE_Herren_Sportbekleidung_ Hybrid-Rad   : 8583 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Rodeln   : There are 40 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Rodeln   : 8623 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Handball   : There are 23 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Handball   : 8646 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Kanu/Kajaksport   : There are 9 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Kanu/Kajaksport   : 8655 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Reiten   : There are 28 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Reiten   : 8683 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Leichtathletik   : There are 44 products (2 pages)\n",
      "DE_Herren_Sportbekleidung_ Leichtathletik   : 8727 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Aquafitness   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportbekleidung_ Aquafitness   : 8727 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Baseball   : There are 54 products (2 pages)\n",
      "DE_Herren_Sportbekleidung_ Baseball   : 8781 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ American Football   : There are 46 products (2 pages)\n",
      "DE_Herren_Sportbekleidung_ American Football   : 8827 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Feldhockey   : There are 23 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Feldhockey   : 8850 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Rugby   : There are 45 products (2 pages)\n",
      "DE_Herren_Sportbekleidung_ Rugby   : 8895 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Orientierungslauf   : There are 31 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Orientierungslauf   : 8926 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Triathlon   : There are 18 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Triathlon   : 8944 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Klettern   : There are 19 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Herren_Sportbekleidung_ Klettern   : 8963 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Pelota   : There are 37 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Pelota   : 9000 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Gehen   : There are 26 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Gehen   : 9026 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Golf   : There are 38 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Golf   : 9064 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Ski Nordisch   : There are 21 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Ski Nordisch   : 9085 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Boxen   : There are 9 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Boxen   : 9094 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Softball   : There are 30 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Softball   : 9124 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Modern Dance   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Modern Dance   : 9125 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Pickleball   : There are 37 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Pickleball   : 9162 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Racquetball   : There are 32 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Racquetball   : 9194 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Speedball   : There are 33 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Speedball   : 9227 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Inlineskaten   : There are 5 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Inlineskaten   : 9232 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Frescoball   : There are 32 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Frescoball   : 9264 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Crosstraining   : There are 6 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Crosstraining   : 9270 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Tischtennis   : There are 16 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Tischtennis   : 9286 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Wasserball   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportbekleidung_ Wasserball   : 9286 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Beachtennis   : There are 19 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Beachtennis   : 9305 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Roller   : There are 7 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Roller   : 9312 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Einradfahren   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportbekleidung_ Einradfahren   : 9312 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Skateboarden   : There are 2 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Skateboarden   : 9314 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Rollschuhfahren   : There are 2 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Rollschuhfahren   : 9316 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Windsurfen   : There are 6 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Windsurfen   : 9322 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Ballett   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Ballett   : 9323 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Gymnastik   : There are 7 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Gymnastik   : 9330 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Raubfisch-Angeln   : There are 11 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Raubfisch-Angeln   : 9341 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Crossminton   : There are 10 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Crossminton   : 9351 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Salzwasserangeln   : There are 9 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Salzwasserangeln   : 9360 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Kitesurfen   : There are 2 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Kitesurfen   : 9362 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Tamburello   : There are 12 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Tamburello   : 9374 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Eislaufen   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportbekleidung_ Eislaufen   : 9374 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Kinderradsport   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportbekleidung_ Kinderradsport   : 9374 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Cricket   : There are 15 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Cricket   : 9389 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Floorball   : There are 15 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Floorball   : 9404 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Netball   : There are 13 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Netball   : 9417 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Inlinehockey   : There are 3 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Inlinehockey   : 9420 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Swimrun   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Swimrun   : 9421 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Eishockey   : There are 4 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Eishockey   : 9425 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Lacrosse   : There are 14 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Lacrosse   : 9439 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ H√∂hlenwandern   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportbekleidung_ H√∂hlenwandern   : 9439 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Urban Dance   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportbekleidung_ Urban Dance   : 9439 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Kite Landboarding   : There are 2 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Kite Landboarding   : 9441 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Karpfenangeln   : There are 3 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Karpfenangeln   : 9444 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Slacklining   : There are 7 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Slacklining   : 9451 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Power Kiting   : There are 2 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Power Kiting   : 9453 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Snowkiting   : There are 2 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Snowkiting   : 9455 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Westernreiten   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Westernreiten   : 9456 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Grappling   : There are 4 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Grappling   : 9460 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Karate   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Karate   : 9461 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Wrestling   : There are 4 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Wrestling   : 9465 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Drachenboot   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportbekleidung_ Drachenboot   : 9465 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Jiu Jitsu   : There are 4 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Jiu Jitsu   : 9469 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Sepak Takraw   : There are 10 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Sepak Takraw   : 9479 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Canyoning   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Canyoning   : 9480 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Sambo   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Sambo   : 9481 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Schie√üen   : There are 3 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Schie√üen   : 9484 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Rudern   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportbekleidung_ Rudern   : 9484 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Herren_Sportbekleidung_ Pole Fishing   : There are 4 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Pole Fishing   : 9488 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Selbstverteidigung   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Selbstverteidigung   : 9489 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Taekwondo   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Taekwondo   : 9490 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ BMX   : There are 4 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ BMX   : 9494 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Kung Fu   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Kung Fu   : 9495 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Aikido   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Aikido   : 9496 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Fechten   : There are 2 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Fechten   : 9498 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Judo   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Judo   : 9499 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Kendo   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Kendo   : 9500 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Eisfischen   : There are 3 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Eisfischen   : 9503 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Wasserski   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Wasserski   : 9504 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Wakeboarden   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Wakeboarden   : 9505 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Ball Hockey   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Ball Hockey   : 9506 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Taijiquan    : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Taijiquan    : 9507 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Fitness Dance   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportbekleidung_ Fitness Dance   : 9507 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Poledance   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportbekleidung_ Poledance   : 9507 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Gezogene aufblasbare Schwimmk√∂rper   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Gezogene aufblasbare Schwimmk√∂rper   : 9508 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Capoeira   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Capoeira   : 9509 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Kempo   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Kempo   : 9510 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Peteca   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Peteca   : 9511 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Bike & Run   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Bike & Run   : 9512 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Kitewing   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportbekleidung_ Kitewing   : 9512 products have been scraped!\n",
      "DE_Herren_Sportbekleidung_ Arnis   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportbekleidung_ Arnis   : 9513 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Wandern   : There are 191 products (5 pages)\n",
      "DE_Herren_Sportschuhe_ Wandern   : 9704 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Fu√üball   : There are 58 products (2 pages)\n",
      "DE_Herren_Sportschuhe_ Fu√üball   : 9762 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Laufen   : There are 66 products (2 pages)\n",
      "DE_Herren_Sportschuhe_ Laufen   : 9828 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Surfen   : There are 32 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Surfen   : 9860 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Skifahren   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Skifahren   : 9860 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Trekking   : There are 20 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Trekking   : 9880 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Pilates   : There are 7 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Pilates   : 9887 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Schwimmen   : There are 24 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Schwimmen   : 9911 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Cardio-Training   : There are 7 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Cardio-Training   : 9918 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Tennis   : There are 16 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Tennis   : 9934 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Bodybuilding   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Bodybuilding   : 9934 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Mountainbiking   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Mountainbiking   : 9934 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Badminton   : There are 23 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Badminton   : 9957 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Padel-Tennis   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Padel-Tennis   : 9957 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Urban Cycling   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Urban Cycling   : 9957 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Rennradfahren   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Rennradfahren   : 9957 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Snowboarden   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Snowboarden   : 9957 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Jagd   : There are 30 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Jagd   : 9987 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Walking   : There are 55 products (2 pages)\n",
      "DE_Herren_Sportschuhe_ Walking   : 10042 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Basketball   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Basketball   : 10043 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Tourenski   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Tourenski   : 10043 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Squash   : There are 23 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Squash   : 10066 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Segeln   : There are 4 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Segeln   : 10070 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Stand Up Paddling   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Stand Up Paddling   : 10070 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Tauchen   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Tauchen   : 10070 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Bergsteigen   : There are 3 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Bergsteigen   : 10073 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Yoga   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Yoga   : 10073 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Volleyball   : There are 5 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Volleyball   : 10078 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Hybrid-Rad   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Hybrid-Rad   : 10078 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Rodeln   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Rodeln   : 10078 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Handball   : There are 8 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Handball   : 10086 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Kanu/Kajaksport   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Kanu/Kajaksport   : 10086 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Reiten   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Reiten   : 10086 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Leichtathletik   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Leichtathletik   : 10087 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Aquafitness   : There are 12 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Aquafitness   : 10099 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Herren_Sportschuhe_ Baseball   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Baseball   : 10099 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ American Football   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ American Football   : 10099 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Feldhockey   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Feldhockey   : 10099 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Rugby   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Rugby   : 10099 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Orientierungslauf   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Orientierungslauf   : 10099 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Triathlon   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Triathlon   : 10100 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Klettern   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Klettern   : 10100 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Pelota   : There are 2 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Pelota   : 10102 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Gehen   : There are 3 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Gehen   : 10105 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Golf   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Golf   : 10105 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Ski Nordisch   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Ski Nordisch   : 10105 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Boxen   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Boxen   : 10105 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Softball   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Softball   : 10105 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Modern Dance   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Modern Dance   : 10105 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Pickleball   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Pickleball   : 10105 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Racquetball   : There are 9 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Racquetball   : 10114 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Speedball   : There are 5 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Speedball   : 10119 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Inlineskaten   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Inlineskaten   : 10119 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Frescoball   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Frescoball   : 10119 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Crosstraining   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Crosstraining   : 10120 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Tischtennis   : There are 2 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Tischtennis   : 10122 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Wasserball   : There are 15 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Wasserball   : 10137 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Beachtennis   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Beachtennis   : 10137 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Roller   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Roller   : 10137 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Einradfahren   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Einradfahren   : 10137 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Skateboarden   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Skateboarden   : 10137 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Rollschuhfahren   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Rollschuhfahren   : 10137 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Windsurfen   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Windsurfen   : 10137 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Ballett   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Ballett   : 10137 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Gymnastik   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Gymnastik   : 10137 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Raubfisch-Angeln   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Raubfisch-Angeln   : 10137 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Crossminton   : There are 7 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Crossminton   : 10144 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Salzwasserangeln   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Salzwasserangeln   : 10144 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Kitesurfen   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Kitesurfen   : 10144 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Tamburello   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Tamburello   : 10144 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Eislaufen   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Eislaufen   : 10144 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Kinderradsport   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Kinderradsport   : 10144 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Cricket   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Cricket   : 10144 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Floorball   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Floorball   : 10144 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Netball   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Netball   : 10144 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Inlinehockey   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Inlinehockey   : 10144 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Swimrun   : There are 3 products (1 pages)\n",
      "DE_Herren_Sportschuhe_ Swimrun   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Eishockey   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Eishockey   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Lacrosse   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Lacrosse   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ H√∂hlenwandern   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ H√∂hlenwandern   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Urban Dance   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Urban Dance   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Kite Landboarding   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Kite Landboarding   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Karpfenangeln   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Karpfenangeln   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Slacklining   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Slacklining   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Power Kiting   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Power Kiting   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Snowkiting   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Snowkiting   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Westernreiten   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Westernreiten   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Grappling   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Grappling   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Karate   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Karate   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Wrestling   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Wrestling   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Drachenboot   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Drachenboot   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Jiu Jitsu   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Jiu Jitsu   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Sepak Takraw   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Sepak Takraw   : 10147 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Herren_Sportschuhe_ Canyoning   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Canyoning   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Sambo   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Sambo   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Schie√üen   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Schie√üen   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Rudern   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Rudern   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Pole Fishing   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Pole Fishing   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Selbstverteidigung   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Selbstverteidigung   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Taekwondo   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Taekwondo   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ BMX   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ BMX   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Kung Fu   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Kung Fu   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Aikido   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Aikido   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Fechten   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Fechten   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Judo   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Judo   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Kendo   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Kendo   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Eisfischen   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Eisfischen   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Wasserski   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Wasserski   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Wakeboarden   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Wakeboarden   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Ball Hockey   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Ball Hockey   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Taijiquan    : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Taijiquan    : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Fitness Dance   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Fitness Dance   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Poledance   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Poledance   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Gezogene aufblasbare Schwimmk√∂rper   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Gezogene aufblasbare Schwimmk√∂rper   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Capoeira   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Capoeira   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Kempo   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Kempo   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Peteca   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Peteca   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Bike & Run   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Bike & Run   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Kitewing   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Kitewing   : 10147 products have been scraped!\n",
      "DE_Herren_Sportschuhe_ Arnis   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportschuhe_ Arnis   : 10147 products have been scraped!\n",
      "DE_Herren_Bademode_ Wandern   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Wandern   : 10147 products have been scraped!\n",
      "DE_Herren_Bademode_ Fu√üball   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Fu√üball   : 10147 products have been scraped!\n",
      "DE_Herren_Bademode_ Laufen   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Laufen   : 10147 products have been scraped!\n",
      "DE_Herren_Bademode_ Surfen   : There are 74 products (2 pages)\n",
      "DE_Herren_Bademode_ Surfen   : 10221 products have been scraped!\n",
      "DE_Herren_Bademode_ Skifahren   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Skifahren   : 10221 products have been scraped!\n",
      "DE_Herren_Bademode_ Trekking   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Trekking   : 10221 products have been scraped!\n",
      "DE_Herren_Bademode_ Pilates   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Pilates   : 10221 products have been scraped!\n",
      "DE_Herren_Bademode_ Schwimmen   : There are 78 products (2 pages)\n",
      "DE_Herren_Bademode_ Schwimmen   : 10299 products have been scraped!\n",
      "DE_Herren_Bademode_ Cardio-Training   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Cardio-Training   : 10299 products have been scraped!\n",
      "DE_Herren_Bademode_ Tennis   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Tennis   : 10299 products have been scraped!\n",
      "DE_Herren_Bademode_ Bodybuilding   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Bodybuilding   : 10299 products have been scraped!\n",
      "DE_Herren_Bademode_ Mountainbiking   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Mountainbiking   : 10299 products have been scraped!\n",
      "DE_Herren_Bademode_ Badminton   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Badminton   : 10299 products have been scraped!\n",
      "DE_Herren_Bademode_ Padel-Tennis   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Padel-Tennis   : 10299 products have been scraped!\n",
      "DE_Herren_Bademode_ Urban Cycling   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Urban Cycling   : 10299 products have been scraped!\n",
      "DE_Herren_Bademode_ Rennradfahren   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Rennradfahren   : 10299 products have been scraped!\n",
      "DE_Herren_Bademode_ Snowboarden   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Snowboarden   : 10299 products have been scraped!\n",
      "DE_Herren_Bademode_ Jagd   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Jagd   : 10299 products have been scraped!\n",
      "DE_Herren_Bademode_ Walking   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Walking   : 10299 products have been scraped!\n",
      "DE_Herren_Bademode_ Basketball   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Basketball   : 10299 products have been scraped!\n",
      "DE_Herren_Bademode_ Tourenski   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Tourenski   : 10299 products have been scraped!\n",
      "DE_Herren_Bademode_ Squash   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Squash   : 10299 products have been scraped!\n",
      "DE_Herren_Bademode_ Segeln   : There are 4 products (1 pages)\n",
      "DE_Herren_Bademode_ Segeln   : 10303 products have been scraped!\n",
      "DE_Herren_Bademode_ Stand Up Paddling   : There are 8 products (1 pages)\n",
      "DE_Herren_Bademode_ Stand Up Paddling   : 10311 products have been scraped!\n",
      "DE_Herren_Bademode_ Tauchen   : There are 53 products (2 pages)\n",
      "DE_Herren_Bademode_ Tauchen   : 10364 products have been scraped!\n",
      "DE_Herren_Bademode_ Bergsteigen   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Bergsteigen   : 10364 products have been scraped!\n",
      "DE_Herren_Bademode_ Yoga   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Yoga   : 10364 products have been scraped!\n",
      "DE_Herren_Bademode_ Volleyball   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Volleyball   : 10364 products have been scraped!\n",
      "DE_Herren_Bademode_ Hybrid-Rad   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Hybrid-Rad   : 10364 products have been scraped!\n",
      "DE_Herren_Bademode_ Rodeln   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Rodeln   : 10364 products have been scraped!\n",
      "DE_Herren_Bademode_ Handball   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Handball   : 10364 products have been scraped!\n",
      "DE_Herren_Bademode_ Kanu/Kajaksport   : There are 6 products (1 pages)\n",
      "DE_Herren_Bademode_ Kanu/Kajaksport   : 10370 products have been scraped!\n",
      "DE_Herren_Bademode_ Reiten   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Reiten   : 10370 products have been scraped!\n",
      "DE_Herren_Bademode_ Leichtathletik   : There are 0 products (0 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Herren_Bademode_ Leichtathletik   : 10370 products have been scraped!\n",
      "DE_Herren_Bademode_ Aquafitness   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Aquafitness   : 10370 products have been scraped!\n",
      "DE_Herren_Bademode_ Baseball   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Baseball   : 10370 products have been scraped!\n",
      "DE_Herren_Bademode_ American Football   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ American Football   : 10370 products have been scraped!\n",
      "DE_Herren_Bademode_ Feldhockey   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Feldhockey   : 10370 products have been scraped!\n",
      "DE_Herren_Bademode_ Rugby   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Rugby   : 10370 products have been scraped!\n",
      "DE_Herren_Bademode_ Orientierungslauf   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Orientierungslauf   : 10370 products have been scraped!\n",
      "DE_Herren_Bademode_ Triathlon   : There are 9 products (1 pages)\n",
      "DE_Herren_Bademode_ Triathlon   : 10379 products have been scraped!\n",
      "DE_Herren_Bademode_ Klettern   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Klettern   : 10379 products have been scraped!\n",
      "DE_Herren_Bademode_ Pelota   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Pelota   : 10379 products have been scraped!\n",
      "DE_Herren_Bademode_ Gehen   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Gehen   : 10379 products have been scraped!\n",
      "DE_Herren_Bademode_ Golf   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Golf   : 10379 products have been scraped!\n",
      "DE_Herren_Bademode_ Ski Nordisch   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Ski Nordisch   : 10379 products have been scraped!\n",
      "DE_Herren_Bademode_ Boxen   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Boxen   : 10379 products have been scraped!\n",
      "DE_Herren_Bademode_ Softball   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Softball   : 10379 products have been scraped!\n",
      "DE_Herren_Bademode_ Modern Dance   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Modern Dance   : 10379 products have been scraped!\n",
      "DE_Herren_Bademode_ Pickleball   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Pickleball   : 10379 products have been scraped!\n",
      "DE_Herren_Bademode_ Racquetball   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Racquetball   : 10379 products have been scraped!\n",
      "DE_Herren_Bademode_ Speedball   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Speedball   : 10379 products have been scraped!\n",
      "DE_Herren_Bademode_ Inlineskaten   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Inlineskaten   : 10379 products have been scraped!\n",
      "DE_Herren_Bademode_ Frescoball   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Frescoball   : 10379 products have been scraped!\n",
      "DE_Herren_Bademode_ Crosstraining   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Crosstraining   : 10379 products have been scraped!\n",
      "DE_Herren_Bademode_ Tischtennis   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Tischtennis   : 10379 products have been scraped!\n",
      "DE_Herren_Bademode_ Wasserball   : There are 5 products (1 pages)\n",
      "DE_Herren_Bademode_ Wasserball   : 10384 products have been scraped!\n",
      "DE_Herren_Bademode_ Beachtennis   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Beachtennis   : 10384 products have been scraped!\n",
      "DE_Herren_Bademode_ Roller   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Roller   : 10384 products have been scraped!\n",
      "DE_Herren_Bademode_ Einradfahren   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Einradfahren   : 10384 products have been scraped!\n",
      "DE_Herren_Bademode_ Skateboarden   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Skateboarden   : 10384 products have been scraped!\n",
      "DE_Herren_Bademode_ Rollschuhfahren   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Rollschuhfahren   : 10384 products have been scraped!\n",
      "DE_Herren_Bademode_ Windsurfen   : There are 6 products (1 pages)\n",
      "DE_Herren_Bademode_ Windsurfen   : 10390 products have been scraped!\n",
      "DE_Herren_Bademode_ Ballett   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Ballett   : 10390 products have been scraped!\n",
      "DE_Herren_Bademode_ Gymnastik   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Gymnastik   : 10390 products have been scraped!\n",
      "DE_Herren_Bademode_ Raubfisch-Angeln   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Raubfisch-Angeln   : 10390 products have been scraped!\n",
      "DE_Herren_Bademode_ Crossminton   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Crossminton   : 10390 products have been scraped!\n",
      "DE_Herren_Bademode_ Salzwasserangeln   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Salzwasserangeln   : 10390 products have been scraped!\n",
      "DE_Herren_Bademode_ Kitesurfen   : There are 6 products (1 pages)\n",
      "DE_Herren_Bademode_ Kitesurfen   : 10396 products have been scraped!\n",
      "DE_Herren_Bademode_ Tamburello   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Tamburello   : 10396 products have been scraped!\n",
      "DE_Herren_Bademode_ Eislaufen   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Eislaufen   : 10396 products have been scraped!\n",
      "DE_Herren_Bademode_ Kinderradsport   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Kinderradsport   : 10396 products have been scraped!\n",
      "DE_Herren_Bademode_ Cricket   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Cricket   : 10396 products have been scraped!\n",
      "DE_Herren_Bademode_ Floorball   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Floorball   : 10396 products have been scraped!\n",
      "DE_Herren_Bademode_ Netball   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Netball   : 10396 products have been scraped!\n",
      "DE_Herren_Bademode_ Inlinehockey   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Inlinehockey   : 10396 products have been scraped!\n",
      "DE_Herren_Bademode_ Swimrun   : There are 1 products (1 pages)\n",
      "DE_Herren_Bademode_ Swimrun   : 10397 products have been scraped!\n",
      "DE_Herren_Bademode_ Eishockey   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Eishockey   : 10397 products have been scraped!\n",
      "DE_Herren_Bademode_ Lacrosse   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Lacrosse   : 10397 products have been scraped!\n",
      "DE_Herren_Bademode_ H√∂hlenwandern   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ H√∂hlenwandern   : 10397 products have been scraped!\n",
      "DE_Herren_Bademode_ Urban Dance   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Urban Dance   : 10397 products have been scraped!\n",
      "DE_Herren_Bademode_ Kite Landboarding   : There are 1 products (1 pages)\n",
      "DE_Herren_Bademode_ Kite Landboarding   : 10398 products have been scraped!\n",
      "DE_Herren_Bademode_ Karpfenangeln   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Karpfenangeln   : 10398 products have been scraped!\n",
      "DE_Herren_Bademode_ Slacklining   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Slacklining   : 10398 products have been scraped!\n",
      "DE_Herren_Bademode_ Power Kiting   : There are 2 products (1 pages)\n",
      "DE_Herren_Bademode_ Power Kiting   : 10400 products have been scraped!\n",
      "DE_Herren_Bademode_ Snowkiting   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Snowkiting   : 10400 products have been scraped!\n",
      "DE_Herren_Bademode_ Westernreiten   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Westernreiten   : 10400 products have been scraped!\n",
      "DE_Herren_Bademode_ Grappling   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Grappling   : 10400 products have been scraped!\n",
      "DE_Herren_Bademode_ Karate   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Karate   : 10400 products have been scraped!\n",
      "DE_Herren_Bademode_ Wrestling   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Wrestling   : 10400 products have been scraped!\n",
      "DE_Herren_Bademode_ Drachenboot   : There are 1 products (1 pages)\n",
      "DE_Herren_Bademode_ Drachenboot   : 10401 products have been scraped!\n",
      "DE_Herren_Bademode_ Jiu Jitsu   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Jiu Jitsu   : 10401 products have been scraped!\n",
      "DE_Herren_Bademode_ Sepak Takraw   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Sepak Takraw   : 10401 products have been scraped!\n",
      "DE_Herren_Bademode_ Canyoning   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Canyoning   : 10401 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Herren_Bademode_ Sambo   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Sambo   : 10401 products have been scraped!\n",
      "DE_Herren_Bademode_ Schie√üen   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Schie√üen   : 10401 products have been scraped!\n",
      "DE_Herren_Bademode_ Rudern   : There are 1 products (1 pages)\n",
      "DE_Herren_Bademode_ Rudern   : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ Pole Fishing   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Pole Fishing   : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ Selbstverteidigung   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Selbstverteidigung   : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ Taekwondo   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Taekwondo   : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ BMX   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ BMX   : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ Kung Fu   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Kung Fu   : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ Aikido   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Aikido   : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ Fechten   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Fechten   : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ Judo   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Judo   : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ Kendo   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Kendo   : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ Eisfischen   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Eisfischen   : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ Wasserski   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Wasserski   : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ Wakeboarden   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Wakeboarden   : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ Ball Hockey   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Ball Hockey   : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ Taijiquan    : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Taijiquan    : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ Fitness Dance   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Fitness Dance   : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ Poledance   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Poledance   : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ Gezogene aufblasbare Schwimmk√∂rper   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Gezogene aufblasbare Schwimmk√∂rper   : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ Capoeira   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Capoeira   : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ Kempo   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Kempo   : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ Peteca   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Peteca   : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ Bike & Run   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Bike & Run   : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ Kitewing   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Kitewing   : 10402 products have been scraped!\n",
      "DE_Herren_Bademode_ Arnis   : There are 0 products (0 pages)\n",
      "DE_Herren_Bademode_ Arnis   : 10402 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Wandern   : There are 63 products (2 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Wandern   : 10465 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Fu√üball   : There are 104 products (3 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Fu√üball   : 10569 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Laufen   : There are 15 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Laufen   : 10584 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Surfen   : There are 20 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Surfen   : 10604 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Skifahren   : There are 108 products (3 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Skifahren   : 10712 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Trekking   : There are 34 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Trekking   : 10746 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Pilates   : There are 20 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Pilates   : 10766 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Schwimmen   : There are 54 products (2 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Schwimmen   : 10820 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Cardio-Training   : There are 60 products (2 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Cardio-Training   : 10880 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Tennis   : There are 57 products (2 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Tennis   : 10937 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Bodybuilding   : There are 39 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Bodybuilding   : 10976 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Mountainbiking   : There are 98 products (3 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Mountainbiking   : 11074 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Badminton   : There are 54 products (2 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Badminton   : 11128 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Padel-Tennis   : There are 42 products (2 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Padel-Tennis   : 11170 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Urban Cycling   : There are 73 products (2 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Urban Cycling   : 11243 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Rennradfahren   : There are 48 products (2 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Rennradfahren   : 11291 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Snowboarden   : There are 59 products (2 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Snowboarden   : 11350 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Jagd   : There are 32 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Jagd   : 11382 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Walking   : There are 7 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Walking   : 11389 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Basketball   : There are 46 products (2 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Basketball   : 11435 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Tourenski   : There are 50 products (2 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Tourenski   : 11485 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Squash   : There are 33 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Squash   : 11518 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Segeln   : There are 29 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Segeln   : 11547 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Stand Up Paddling   : There are 28 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Stand Up Paddling   : 11575 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Tauchen   : There are 39 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Tauchen   : 11614 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Bergsteigen   : There are 14 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Bergsteigen   : 11628 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Yoga   : There are 24 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Yoga   : 11652 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Volleyball   : There are 45 products (2 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Volleyball   : 11697 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Hybrid-Rad   : There are 39 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Hybrid-Rad   : 11736 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Rodeln   : There are 33 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Rodeln   : 11769 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Handball   : There are 39 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Handball   : 11808 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Kanu/Kajaksport   : There are 24 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Kanu/Kajaksport   : 11832 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Herren_Sportausr√ºstung_ Reiten   : There are 24 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Reiten   : 11856 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Leichtathletik   : There are 8 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Leichtathletik   : 11864 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Aquafitness   : There are 9 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Aquafitness   : 11873 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Baseball   : There are 8 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Baseball   : 11881 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ American Football   : There are 11 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ American Football   : 11892 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Feldhockey   : There are 35 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Feldhockey   : 11927 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Rugby   : There are 22 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Rugby   : 11949 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Orientierungslauf   : There are 5 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Orientierungslauf   : 11954 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Triathlon   : There are 20 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Triathlon   : 11974 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Klettern   : There are 20 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Klettern   : 11994 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Pelota   : There are 9 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Pelota   : 12003 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Gehen   : There are 5 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Gehen   : 12008 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Golf   : There are 12 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Golf   : 12020 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Ski Nordisch   : There are 21 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Ski Nordisch   : 12041 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Boxen   : There are 28 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Boxen   : 12069 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Softball   : There are 9 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Softball   : 12078 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Modern Dance   : There are 4 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Modern Dance   : 12082 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Pickleball   : There are 2 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Pickleball   : 12084 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Racquetball   : There are 2 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Racquetball   : 12086 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Speedball   : There are 2 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Speedball   : 12088 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Inlineskaten   : There are 22 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Inlineskaten   : 12110 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Frescoball   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Frescoball   : 12111 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Crosstraining   : There are 24 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Crosstraining   : 12135 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Tischtennis   : There are 4 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Tischtennis   : 12139 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Wasserball   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Wasserball   : 12140 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Beachtennis   : There are 9 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Beachtennis   : 12149 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Roller   : There are 15 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Roller   : 12164 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Einradfahren   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Einradfahren   : 12165 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Skateboarden   : There are 20 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Skateboarden   : 12185 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Rollschuhfahren   : There are 11 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Rollschuhfahren   : 12196 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Windsurfen   : There are 13 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Windsurfen   : 12209 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Ballett   : There are 5 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Ballett   : 12214 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Gymnastik   : There are 5 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Gymnastik   : 12219 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Raubfisch-Angeln   : There are 8 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Raubfisch-Angeln   : 12227 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Crossminton   : There are 4 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Crossminton   : 12231 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Salzwasserangeln   : There are 6 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Salzwasserangeln   : 12237 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Kitesurfen   : There are 9 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Kitesurfen   : 12246 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Tamburello   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Tamburello   : 12247 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Eislaufen   : There are 7 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Eislaufen   : 12254 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Kinderradsport   : There are 13 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Kinderradsport   : 12267 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Cricket   : There are 4 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Cricket   : 12271 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Floorball   : There are 4 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Floorball   : 12275 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Netball   : There are 4 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Netball   : 12279 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Inlinehockey   : There are 10 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Inlinehockey   : 12289 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Swimrun   : There are 4 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Swimrun   : 12293 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Eishockey   : There are 9 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Eishockey   : 12302 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Lacrosse   : There are 3 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Lacrosse   : 12305 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ H√∂hlenwandern   : There are 7 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ H√∂hlenwandern   : 12312 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Urban Dance   : There are 2 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Urban Dance   : 12314 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Kite Landboarding   : There are 8 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Kite Landboarding   : 12322 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Karpfenangeln   : There are 4 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Karpfenangeln   : 12326 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Slacklining   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Slacklining   : 12326 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Power Kiting   : There are 7 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Power Kiting   : 12333 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Snowkiting   : There are 7 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Snowkiting   : 12340 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Westernreiten   : There are 3 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Westernreiten   : 12343 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Herren_Sportausr√ºstung_ Grappling   : There are 3 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Grappling   : 12346 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Karate   : There are 6 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Karate   : 12352 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Wrestling   : There are 3 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Wrestling   : 12355 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Drachenboot   : There are 6 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Drachenboot   : 12361 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Jiu Jitsu   : There are 3 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Jiu Jitsu   : 12364 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Sepak Takraw   : There are 2 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Sepak Takraw   : 12366 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Canyoning   : There are 7 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Canyoning   : 12373 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Sambo   : There are 4 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Sambo   : 12377 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Schie√üen   : There are 5 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Schie√üen   : 12382 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Rudern   : There are 5 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Rudern   : 12387 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Pole Fishing   : There are 5 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Pole Fishing   : 12392 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Selbstverteidigung   : There are 3 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Selbstverteidigung   : 12395 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Taekwondo   : There are 4 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Taekwondo   : 12399 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ BMX   : There are 4 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ BMX   : 12403 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Kung Fu   : There are 3 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Kung Fu   : 12406 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Aikido   : There are 2 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Aikido   : 12408 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Fechten   : There are 2 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Fechten   : 12410 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Judo   : There are 3 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Judo   : 12413 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Kendo   : There are 2 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Kendo   : 12415 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Eisfischen   : There are 3 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Eisfischen   : 12418 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Wasserski   : There are 3 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Wasserski   : 12421 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Wakeboarden   : There are 3 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Wakeboarden   : 12424 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Ball Hockey   : There are 2 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Ball Hockey   : 12426 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Taijiquan    : There are 1 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Taijiquan    : 12427 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Fitness Dance   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Fitness Dance   : 12428 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Poledance   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Poledance   : 12428 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Gezogene aufblasbare Schwimmk√∂rper   : There are 2 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Gezogene aufblasbare Schwimmk√∂rper   : 12430 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Capoeira   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Capoeira   : 12431 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Kempo   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Kempo   : 12431 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Peteca   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Peteca   : 12431 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Bike & Run   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Bike & Run   : 12432 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Kitewing   : There are 1 products (1 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Kitewing   : 12433 products have been scraped!\n",
      "DE_Herren_Sportausr√ºstung_ Arnis   : There are 0 products (0 pages)\n",
      "DE_Herren_Sportausr√ºstung_ Arnis   : 12433 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Wandern   : There are 373 products (10 pages)\n",
      "DE_Herren_Outdoorwelt_ Wandern   : 12806 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Fu√üball   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Fu√üball   : 12806 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Laufen   : There are 10 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Laufen   : 12816 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Surfen   : There are 4 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Surfen   : 12820 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Skifahren   : There are 2 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Skifahren   : 12822 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Trekking   : There are 130 products (4 pages)\n",
      "DE_Herren_Outdoorwelt_ Trekking   : 12952 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Pilates   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Pilates   : 12952 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Schwimmen   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Schwimmen   : 12952 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Cardio-Training   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Cardio-Training   : 12952 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Tennis   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Tennis   : 12952 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Bodybuilding   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Bodybuilding   : 12952 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Mountainbiking   : There are 11 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Mountainbiking   : 12963 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Badminton   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Badminton   : 12963 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Padel-Tennis   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Padel-Tennis   : 12963 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Urban Cycling   : There are 10 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Urban Cycling   : 12973 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Rennradfahren   : There are 11 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Rennradfahren   : 12984 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Snowboarden   : There are 2 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Snowboarden   : 12986 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Jagd   : There are 123 products (4 pages)\n",
      "DE_Herren_Outdoorwelt_ Jagd   : 13109 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Walking   : There are 1 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Walking   : 13110 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Basketball   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Basketball   : 13110 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Tourenski   : There are 2 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Tourenski   : 13112 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Squash   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Squash   : 13112 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Segeln   : There are 15 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Segeln   : 13127 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Stand Up Paddling   : There are 39 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Stand Up Paddling   : 13166 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Herren_Outdoorwelt_ Tauchen   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Tauchen   : 13166 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Bergsteigen   : There are 5 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Bergsteigen   : 13171 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Yoga   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Yoga   : 13171 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Volleyball   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Volleyball   : 13171 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Hybrid-Rad   : There are 11 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Hybrid-Rad   : 13182 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Rodeln   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Rodeln   : 13182 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Handball   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Handball   : 13182 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Kanu/Kajaksport   : There are 36 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Kanu/Kajaksport   : 13218 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Reiten   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Reiten   : 13218 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Leichtathletik   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Leichtathletik   : 13218 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Aquafitness   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Aquafitness   : 13218 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Baseball   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Baseball   : 13218 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ American Football   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ American Football   : 13218 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Feldhockey   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Feldhockey   : 13218 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Rugby   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Rugby   : 13218 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Orientierungslauf   : There are 16 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Orientierungslauf   : 13234 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Triathlon   : There are 2 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Triathlon   : 13236 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Klettern   : There are 2 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Klettern   : 13238 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Pelota   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Pelota   : 13238 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Gehen   : There are 1 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Gehen   : 13239 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Golf   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Golf   : 13239 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Ski Nordisch   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Ski Nordisch   : 13239 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Boxen   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Boxen   : 13239 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Softball   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Softball   : 13239 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Modern Dance   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Modern Dance   : 13239 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Pickleball   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Pickleball   : 13239 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Racquetball   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Racquetball   : 13239 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Speedball   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Speedball   : 13239 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Inlineskaten   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Inlineskaten   : 13239 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Frescoball   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Frescoball   : 13239 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Crosstraining   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Crosstraining   : 13239 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Tischtennis   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Tischtennis   : 13239 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Wasserball   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Wasserball   : 13239 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Beachtennis   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Beachtennis   : 13239 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Roller   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Roller   : 13239 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Einradfahren   : There are 11 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Einradfahren   : 13250 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Skateboarden   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Skateboarden   : 13250 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Rollschuhfahren   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Rollschuhfahren   : 13250 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Windsurfen   : There are 4 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Windsurfen   : 13254 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Ballett   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Ballett   : 13254 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Gymnastik   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Gymnastik   : 13254 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Raubfisch-Angeln   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Raubfisch-Angeln   : 13254 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Crossminton   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Crossminton   : 13254 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Salzwasserangeln   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Salzwasserangeln   : 13254 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Kitesurfen   : There are 4 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Kitesurfen   : 13258 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Tamburello   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Tamburello   : 13258 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Eislaufen   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Eislaufen   : 13258 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Kinderradsport   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Kinderradsport   : 13258 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Cricket   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Cricket   : 13258 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Floorball   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Floorball   : 13258 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Netball   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Netball   : 13258 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Inlinehockey   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Inlinehockey   : 13258 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Swimrun   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Swimrun   : 13258 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Eishockey   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Eishockey   : 13258 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Lacrosse   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Lacrosse   : 13258 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ H√∂hlenwandern   : There are 4 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ H√∂hlenwandern   : 13262 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Urban Dance   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Urban Dance   : 13262 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Kite Landboarding   : There are 4 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Kite Landboarding   : 13266 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Karpfenangeln   : There are 0 products (0 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Herren_Outdoorwelt_ Karpfenangeln   : 13266 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Slacklining   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Slacklining   : 13266 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Power Kiting   : There are 4 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Power Kiting   : 13270 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Snowkiting   : There are 4 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Snowkiting   : 13274 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Westernreiten   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Westernreiten   : 13274 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Grappling   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Grappling   : 13274 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Karate   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Karate   : 13274 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Wrestling   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Wrestling   : 13274 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Drachenboot   : There are 6 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Drachenboot   : 13280 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Jiu Jitsu   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Jiu Jitsu   : 13280 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Sepak Takraw   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Sepak Takraw   : 13280 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Canyoning   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Canyoning   : 13280 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Sambo   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Sambo   : 13280 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Schie√üen   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Schie√üen   : 13280 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Rudern   : There are 5 products (1 pages)\n",
      "DE_Herren_Outdoorwelt_ Rudern   : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Pole Fishing   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Pole Fishing   : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Selbstverteidigung   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Selbstverteidigung   : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Taekwondo   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Taekwondo   : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ BMX   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ BMX   : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Kung Fu   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Kung Fu   : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Aikido   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Aikido   : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Fechten   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Fechten   : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Judo   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Judo   : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Kendo   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Kendo   : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Eisfischen   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Eisfischen   : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Wasserski   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Wasserski   : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Wakeboarden   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Wakeboarden   : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Ball Hockey   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Ball Hockey   : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Taijiquan    : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Taijiquan    : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Fitness Dance   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Fitness Dance   : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Poledance   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Poledance   : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Gezogene aufblasbare Schwimmk√∂rper   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Gezogene aufblasbare Schwimmk√∂rper   : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Capoeira   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Capoeira   : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Kempo   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Kempo   : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Peteca   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Peteca   : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Bike & Run   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Bike & Run   : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Kitewing   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Kitewing   : 13285 products have been scraped!\n",
      "DE_Herren_Outdoorwelt_ Arnis   : There are 0 products (0 pages)\n",
      "DE_Herren_Outdoorwelt_ Arnis   : 13285 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Wandern   : There are 166 products (5 pages)\n",
      "DE_Kinder_Sportbekleidung_ Wandern   : 13451 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Fu√üball   : There are 89 products (3 pages)\n",
      "DE_Kinder_Sportbekleidung_ Fu√üball   : 13540 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Laufen   : There are 32 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Laufen   : 13572 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Surfen   : There are 26 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Surfen   : 13598 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Skifahren   : There are 102 products (3 pages)\n",
      "DE_Kinder_Sportbekleidung_ Skifahren   : 13700 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Trekking   : There are 12 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Trekking   : 13712 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Pilates   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Pilates   : 13712 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Schwimmen   : There are 9 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Schwimmen   : 13721 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Cardio-Training   : There are 4 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Cardio-Training   : 13725 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Tennis   : There are 43 products (2 pages)\n",
      "DE_Kinder_Sportbekleidung_ Tennis   : 13768 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Bodybuilding   : There are 9 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Bodybuilding   : 13777 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Mountainbiking   : There are 21 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Mountainbiking   : 13798 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Badminton   : There are 20 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Badminton   : 13818 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Padel-Tennis   : There are 34 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Padel-Tennis   : 13852 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Urban Cycling   : There are 13 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Urban Cycling   : 13865 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Rennradfahren   : There are 20 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Rennradfahren   : 13885 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Snowboarden   : There are 37 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Snowboarden   : 13922 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Jagd   : There are 16 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Jagd   : 13938 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Walking   : There are 31 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Walking   : 13969 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Basketball   : There are 50 products (2 pages)\n",
      "DE_Kinder_Sportbekleidung_ Basketball   : 14019 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Kinder_Sportbekleidung_ Tourenski   : There are 16 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Tourenski   : 14035 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Squash   : There are 26 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Squash   : 14061 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Segeln   : There are 19 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Segeln   : 14080 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Stand Up Paddling   : There are 5 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Stand Up Paddling   : 14085 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Tauchen   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Tauchen   : 14087 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Bergsteigen   : There are 3 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Bergsteigen   : 14090 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Yoga   : There are 4 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Yoga   : 14094 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Volleyball   : There are 14 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Volleyball   : 14108 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Hybrid-Rad   : There are 11 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Hybrid-Rad   : 14119 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Rodeln   : There are 48 products (2 pages)\n",
      "DE_Kinder_Sportbekleidung_ Rodeln   : 14167 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Handball   : There are 11 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Handball   : 14178 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Kanu/Kajaksport   : There are 7 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Kanu/Kajaksport   : 14185 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Reiten   : There are 41 products (2 pages)\n",
      "DE_Kinder_Sportbekleidung_ Reiten   : 14226 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Leichtathletik   : There are 34 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Leichtathletik   : 14260 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Aquafitness   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Aquafitness   : 14260 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Baseball   : There are 33 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Baseball   : 14293 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ American Football   : There are 35 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ American Football   : 14328 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Feldhockey   : There are 17 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Feldhockey   : 14345 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Rugby   : There are 28 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Rugby   : 14373 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Orientierungslauf   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Orientierungslauf   : 14375 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Triathlon   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Triathlon   : 14375 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Klettern   : There are 4 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Klettern   : 14379 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Pelota   : There are 18 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Pelota   : 14397 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Gehen   : There are 23 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Gehen   : 14420 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Golf   : There are 13 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Golf   : 14433 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Ski Nordisch   : There are 16 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Ski Nordisch   : 14449 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Boxen   : There are 4 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Boxen   : 14453 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Softball   : There are 13 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Softball   : 14466 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Modern Dance   : There are 14 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Modern Dance   : 14480 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Pickleball   : There are 15 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Pickleball   : 14495 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Racquetball   : There are 13 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Racquetball   : 14508 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Speedball   : There are 15 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Speedball   : 14523 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Inlineskaten   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Inlineskaten   : 14524 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Frescoball   : There are 11 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Frescoball   : 14535 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Crosstraining   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Crosstraining   : 14535 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Tischtennis   : There are 16 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Tischtennis   : 14551 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Wasserball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Wasserball   : 14551 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Beachtennis   : There are 8 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Beachtennis   : 14559 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Roller   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Roller   : 14560 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Einradfahren   : There are 8 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Einradfahren   : 14568 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Skateboarden   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Skateboarden   : 14568 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Rollschuhfahren   : There are 7 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Rollschuhfahren   : 14575 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Windsurfen   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Windsurfen   : 14577 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Ballett   : There are 13 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Ballett   : 14590 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Gymnastik   : There are 61 products (2 pages)\n",
      "DE_Kinder_Sportbekleidung_ Gymnastik   : 14651 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Raubfisch-Angeln   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Raubfisch-Angeln   : 14651 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Crossminton   : There are 4 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Crossminton   : 14655 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Salzwasserangeln   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Salzwasserangeln   : 14655 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Kitesurfen   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Kitesurfen   : 14656 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Tamburello   : There are 7 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Tamburello   : 14663 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Eislaufen   : There are 8 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Eislaufen   : 14671 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Kinderradsport   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Kinderradsport   : 14671 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Cricket   : There are 8 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Cricket   : 14679 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Floorball   : There are 9 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Floorball   : 14688 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Netball   : There are 8 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Netball   : 14696 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Kinder_Sportbekleidung_ Inlinehockey   : There are 3 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Inlinehockey   : 14699 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Swimrun   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Swimrun   : 14699 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Eishockey   : There are 4 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Eishockey   : 14703 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Lacrosse   : There are 8 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Lacrosse   : 14711 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ H√∂hlenwandern   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ H√∂hlenwandern   : 14711 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Urban Dance   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Urban Dance   : 14711 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Kite Landboarding   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Kite Landboarding   : 14711 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Karpfenangeln   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Karpfenangeln   : 14711 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Slacklining   : There are 4 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Slacklining   : 14715 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Power Kiting   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Power Kiting   : 14715 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Snowkiting   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Snowkiting   : 14715 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Westernreiten   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Westernreiten   : 14715 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Grappling   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Grappling   : 14716 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Karate   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Karate   : 14717 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Wrestling   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Wrestling   : 14718 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Drachenboot   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Drachenboot   : 14718 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Jiu Jitsu   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Jiu Jitsu   : 14719 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Sepak Takraw   : There are 7 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Sepak Takraw   : 14726 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Canyoning   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Canyoning   : 14726 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Sambo   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Sambo   : 14727 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Schie√üen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Schie√üen   : 14727 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Rudern   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Rudern   : 14727 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Pole Fishing   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Pole Fishing   : 14727 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Selbstverteidigung   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Selbstverteidigung   : 14728 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Taekwondo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Taekwondo   : 14728 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ BMX   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ BMX   : 14730 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Kung Fu   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Kung Fu   : 14730 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Aikido   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Aikido   : 14731 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Fechten   : There are 5 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Fechten   : 14736 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Judo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Judo   : 14736 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Kendo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Kendo   : 14736 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Eisfischen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Eisfischen   : 14736 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Wasserski   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Wasserski   : 14736 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Wakeboarden   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Wakeboarden   : 14736 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Ball Hockey   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportbekleidung_ Ball Hockey   : 14737 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Taijiquan    : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Taijiquan    : 14737 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Fitness Dance   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Fitness Dance   : 14737 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Poledance   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Poledance   : 14737 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Gezogene aufblasbare Schwimmk√∂rper   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Gezogene aufblasbare Schwimmk√∂rper   : 14737 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Capoeira   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Capoeira   : 14737 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Kempo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Kempo   : 14737 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Peteca   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Peteca   : 14737 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Bike & Run   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Bike & Run   : 14737 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Kitewing   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Kitewing   : 14737 products have been scraped!\n",
      "DE_Kinder_Sportbekleidung_ Arnis   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportbekleidung_ Arnis   : 14737 products have been scraped!\n",
      "DE_Kinder_Bademode_ Wandern   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Wandern   : 14737 products have been scraped!\n",
      "DE_Kinder_Bademode_ Fu√üball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Fu√üball   : 14737 products have been scraped!\n",
      "DE_Kinder_Bademode_ Laufen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Laufen   : 14737 products have been scraped!\n",
      "DE_Kinder_Bademode_ Surfen   : There are 83 products (3 pages)\n",
      "DE_Kinder_Bademode_ Surfen   : 14820 products have been scraped!\n",
      "DE_Kinder_Bademode_ Skifahren   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Skifahren   : 14820 products have been scraped!\n",
      "DE_Kinder_Bademode_ Trekking   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Trekking   : 14820 products have been scraped!\n",
      "DE_Kinder_Bademode_ Pilates   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Pilates   : 14820 products have been scraped!\n",
      "DE_Kinder_Bademode_ Schwimmen   : There are 97 products (3 pages)\n",
      "DE_Kinder_Bademode_ Schwimmen   : 14917 products have been scraped!\n",
      "DE_Kinder_Bademode_ Cardio-Training   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Cardio-Training   : 14917 products have been scraped!\n",
      "DE_Kinder_Bademode_ Tennis   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Tennis   : 14917 products have been scraped!\n",
      "DE_Kinder_Bademode_ Bodybuilding   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Bodybuilding   : 14917 products have been scraped!\n",
      "DE_Kinder_Bademode_ Mountainbiking   : There are 0 products (0 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Kinder_Bademode_ Mountainbiking   : 14917 products have been scraped!\n",
      "DE_Kinder_Bademode_ Badminton   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Badminton   : 14917 products have been scraped!\n",
      "DE_Kinder_Bademode_ Padel-Tennis   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Padel-Tennis   : 14917 products have been scraped!\n",
      "DE_Kinder_Bademode_ Urban Cycling   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Urban Cycling   : 14917 products have been scraped!\n",
      "DE_Kinder_Bademode_ Rennradfahren   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Rennradfahren   : 14917 products have been scraped!\n",
      "DE_Kinder_Bademode_ Snowboarden   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Snowboarden   : 14917 products have been scraped!\n",
      "DE_Kinder_Bademode_ Jagd   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Jagd   : 14917 products have been scraped!\n",
      "DE_Kinder_Bademode_ Walking   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Walking   : 14917 products have been scraped!\n",
      "DE_Kinder_Bademode_ Basketball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Basketball   : 14917 products have been scraped!\n",
      "DE_Kinder_Bademode_ Tourenski   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Tourenski   : 14917 products have been scraped!\n",
      "DE_Kinder_Bademode_ Squash   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Squash   : 14917 products have been scraped!\n",
      "DE_Kinder_Bademode_ Segeln   : There are 2 products (1 pages)\n",
      "DE_Kinder_Bademode_ Segeln   : 14919 products have been scraped!\n",
      "DE_Kinder_Bademode_ Stand Up Paddling   : There are 4 products (1 pages)\n",
      "DE_Kinder_Bademode_ Stand Up Paddling   : 14923 products have been scraped!\n",
      "DE_Kinder_Bademode_ Tauchen   : There are 13 products (1 pages)\n",
      "DE_Kinder_Bademode_ Tauchen   : 14936 products have been scraped!\n",
      "DE_Kinder_Bademode_ Bergsteigen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Bergsteigen   : 14936 products have been scraped!\n",
      "DE_Kinder_Bademode_ Yoga   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Yoga   : 14936 products have been scraped!\n",
      "DE_Kinder_Bademode_ Volleyball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Volleyball   : 14936 products have been scraped!\n",
      "DE_Kinder_Bademode_ Hybrid-Rad   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Hybrid-Rad   : 14936 products have been scraped!\n",
      "DE_Kinder_Bademode_ Rodeln   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Rodeln   : 14936 products have been scraped!\n",
      "DE_Kinder_Bademode_ Handball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Handball   : 14936 products have been scraped!\n",
      "DE_Kinder_Bademode_ Kanu/Kajaksport   : There are 2 products (1 pages)\n",
      "DE_Kinder_Bademode_ Kanu/Kajaksport   : 14938 products have been scraped!\n",
      "DE_Kinder_Bademode_ Reiten   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Reiten   : 14938 products have been scraped!\n",
      "DE_Kinder_Bademode_ Leichtathletik   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Leichtathletik   : 14938 products have been scraped!\n",
      "DE_Kinder_Bademode_ Aquafitness   : There are 1 products (1 pages)\n",
      "DE_Kinder_Bademode_ Aquafitness   : 14939 products have been scraped!\n",
      "DE_Kinder_Bademode_ Baseball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Baseball   : 14939 products have been scraped!\n",
      "DE_Kinder_Bademode_ American Football   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ American Football   : 14939 products have been scraped!\n",
      "DE_Kinder_Bademode_ Feldhockey   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Feldhockey   : 14939 products have been scraped!\n",
      "DE_Kinder_Bademode_ Rugby   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Rugby   : 14939 products have been scraped!\n",
      "DE_Kinder_Bademode_ Orientierungslauf   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Orientierungslauf   : 14939 products have been scraped!\n",
      "DE_Kinder_Bademode_ Triathlon   : There are 2 products (1 pages)\n",
      "DE_Kinder_Bademode_ Triathlon   : 14941 products have been scraped!\n",
      "DE_Kinder_Bademode_ Klettern   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Klettern   : 14941 products have been scraped!\n",
      "DE_Kinder_Bademode_ Pelota   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Pelota   : 14941 products have been scraped!\n",
      "DE_Kinder_Bademode_ Gehen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Gehen   : 14941 products have been scraped!\n",
      "DE_Kinder_Bademode_ Golf   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Golf   : 14941 products have been scraped!\n",
      "DE_Kinder_Bademode_ Ski Nordisch   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Ski Nordisch   : 14941 products have been scraped!\n",
      "DE_Kinder_Bademode_ Boxen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Boxen   : 14941 products have been scraped!\n",
      "DE_Kinder_Bademode_ Softball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Softball   : 14941 products have been scraped!\n",
      "DE_Kinder_Bademode_ Modern Dance   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Modern Dance   : 14941 products have been scraped!\n",
      "DE_Kinder_Bademode_ Pickleball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Pickleball   : 14941 products have been scraped!\n",
      "DE_Kinder_Bademode_ Racquetball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Racquetball   : 14941 products have been scraped!\n",
      "DE_Kinder_Bademode_ Speedball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Speedball   : 14941 products have been scraped!\n",
      "DE_Kinder_Bademode_ Inlineskaten   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Inlineskaten   : 14941 products have been scraped!\n",
      "DE_Kinder_Bademode_ Frescoball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Frescoball   : 14941 products have been scraped!\n",
      "DE_Kinder_Bademode_ Crosstraining   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Crosstraining   : 14941 products have been scraped!\n",
      "DE_Kinder_Bademode_ Tischtennis   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Tischtennis   : 14941 products have been scraped!\n",
      "DE_Kinder_Bademode_ Wasserball   : There are 5 products (1 pages)\n",
      "DE_Kinder_Bademode_ Wasserball   : 14946 products have been scraped!\n",
      "DE_Kinder_Bademode_ Beachtennis   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Beachtennis   : 14946 products have been scraped!\n",
      "DE_Kinder_Bademode_ Roller   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Roller   : 14946 products have been scraped!\n",
      "DE_Kinder_Bademode_ Einradfahren   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Einradfahren   : 14946 products have been scraped!\n",
      "DE_Kinder_Bademode_ Skateboarden   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Skateboarden   : 14946 products have been scraped!\n",
      "DE_Kinder_Bademode_ Rollschuhfahren   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Rollschuhfahren   : 14946 products have been scraped!\n",
      "DE_Kinder_Bademode_ Windsurfen   : There are 5 products (1 pages)\n",
      "DE_Kinder_Bademode_ Windsurfen   : 14951 products have been scraped!\n",
      "DE_Kinder_Bademode_ Ballett   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Ballett   : 14951 products have been scraped!\n",
      "DE_Kinder_Bademode_ Gymnastik   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Gymnastik   : 14951 products have been scraped!\n",
      "DE_Kinder_Bademode_ Raubfisch-Angeln   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Raubfisch-Angeln   : 14951 products have been scraped!\n",
      "DE_Kinder_Bademode_ Crossminton   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Crossminton   : 14951 products have been scraped!\n",
      "DE_Kinder_Bademode_ Salzwasserangeln   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Salzwasserangeln   : 14951 products have been scraped!\n",
      "DE_Kinder_Bademode_ Kitesurfen   : There are 6 products (1 pages)\n",
      "DE_Kinder_Bademode_ Kitesurfen   : 14957 products have been scraped!\n",
      "DE_Kinder_Bademode_ Tamburello   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Tamburello   : 14957 products have been scraped!\n",
      "DE_Kinder_Bademode_ Eislaufen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Eislaufen   : 14957 products have been scraped!\n",
      "DE_Kinder_Bademode_ Kinderradsport   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Kinderradsport   : 14957 products have been scraped!\n",
      "DE_Kinder_Bademode_ Cricket   : There are 0 products (0 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Kinder_Bademode_ Cricket   : 14957 products have been scraped!\n",
      "DE_Kinder_Bademode_ Floorball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Floorball   : 14957 products have been scraped!\n",
      "DE_Kinder_Bademode_ Netball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Netball   : 14957 products have been scraped!\n",
      "DE_Kinder_Bademode_ Inlinehockey   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Inlinehockey   : 14957 products have been scraped!\n",
      "DE_Kinder_Bademode_ Swimrun   : There are 2 products (1 pages)\n",
      "DE_Kinder_Bademode_ Swimrun   : 14959 products have been scraped!\n",
      "DE_Kinder_Bademode_ Eishockey   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Eishockey   : 14959 products have been scraped!\n",
      "DE_Kinder_Bademode_ Lacrosse   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Lacrosse   : 14959 products have been scraped!\n",
      "DE_Kinder_Bademode_ H√∂hlenwandern   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ H√∂hlenwandern   : 14959 products have been scraped!\n",
      "DE_Kinder_Bademode_ Urban Dance   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Urban Dance   : 14959 products have been scraped!\n",
      "DE_Kinder_Bademode_ Kite Landboarding   : There are 1 products (1 pages)\n",
      "DE_Kinder_Bademode_ Kite Landboarding   : 14960 products have been scraped!\n",
      "DE_Kinder_Bademode_ Karpfenangeln   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Karpfenangeln   : 14960 products have been scraped!\n",
      "DE_Kinder_Bademode_ Slacklining   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Slacklining   : 14960 products have been scraped!\n",
      "DE_Kinder_Bademode_ Power Kiting   : There are 1 products (1 pages)\n",
      "DE_Kinder_Bademode_ Power Kiting   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Snowkiting   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Snowkiting   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Westernreiten   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Westernreiten   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Grappling   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Grappling   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Karate   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Karate   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Wrestling   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Wrestling   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Drachenboot   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Drachenboot   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Jiu Jitsu   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Jiu Jitsu   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Sepak Takraw   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Sepak Takraw   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Canyoning   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Canyoning   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Sambo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Sambo   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Schie√üen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Schie√üen   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Rudern   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Rudern   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Pole Fishing   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Pole Fishing   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Selbstverteidigung   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Selbstverteidigung   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Taekwondo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Taekwondo   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ BMX   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ BMX   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Kung Fu   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Kung Fu   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Aikido   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Aikido   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Fechten   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Fechten   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Judo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Judo   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Kendo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Kendo   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Eisfischen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Eisfischen   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Wasserski   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Wasserski   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Wakeboarden   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Wakeboarden   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Ball Hockey   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Ball Hockey   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Taijiquan    : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Taijiquan    : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Fitness Dance   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Fitness Dance   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Poledance   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Poledance   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Gezogene aufblasbare Schwimmk√∂rper   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Gezogene aufblasbare Schwimmk√∂rper   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Capoeira   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Capoeira   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Kempo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Kempo   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Peteca   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Peteca   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Bike & Run   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Bike & Run   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Kitewing   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Kitewing   : 14961 products have been scraped!\n",
      "DE_Kinder_Bademode_ Arnis   : There are 0 products (0 pages)\n",
      "DE_Kinder_Bademode_ Arnis   : 14961 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Wandern   : There are 98 products (3 pages)\n",
      "DE_Kinder_Sportschuhe_ Wandern   : 15059 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Fu√üball   : There are 38 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Fu√üball   : 15097 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Laufen   : There are 23 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Laufen   : 15120 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Surfen   : There are 20 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Surfen   : 15140 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Skifahren   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Skifahren   : 15142 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Trekking   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Trekking   : 15144 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Pilates   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Pilates   : 15145 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Schwimmen   : There are 16 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Schwimmen   : 15161 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Cardio-Training   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Cardio-Training   : 15161 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Tennis   : There are 17 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Tennis   : 15178 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Bodybuilding   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Bodybuilding   : 15178 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Mountainbiking   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Mountainbiking   : 15178 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Badminton   : There are 8 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Badminton   : 15186 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Padel-Tennis   : There are 0 products (0 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Kinder_Sportschuhe_ Padel-Tennis   : 15186 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Urban Cycling   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Urban Cycling   : 15186 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Rennradfahren   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Rennradfahren   : 15186 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Snowboarden   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Snowboarden   : 15186 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Jagd   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Jagd   : 15188 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Walking   : There are 18 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Walking   : 15206 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Basketball   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Basketball   : 15207 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Tourenski   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Tourenski   : 15207 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Squash   : There are 8 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Squash   : 15215 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Segeln   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Segeln   : 15216 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Stand Up Paddling   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Stand Up Paddling   : 15216 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Tauchen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Tauchen   : 15216 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Bergsteigen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Bergsteigen   : 15216 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Yoga   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Yoga   : 15216 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Volleyball   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Volleyball   : 15218 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Hybrid-Rad   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Hybrid-Rad   : 15218 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Rodeln   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Rodeln   : 15220 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Handball   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Handball   : 15222 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Kanu/Kajaksport   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Kanu/Kajaksport   : 15222 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Reiten   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Reiten   : 15223 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Leichtathletik   : There are 9 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Leichtathletik   : 15232 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Aquafitness   : There are 4 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Aquafitness   : 15236 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Baseball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Baseball   : 15236 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ American Football   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ American Football   : 15236 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Feldhockey   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Feldhockey   : 15236 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Rugby   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Rugby   : 15236 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Orientierungslauf   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Orientierungslauf   : 15236 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Triathlon   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Triathlon   : 15237 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Klettern   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Klettern   : 15237 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Pelota   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Pelota   : 15238 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Gehen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Gehen   : 15238 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Golf   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Golf   : 15238 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Ski Nordisch   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Ski Nordisch   : 15238 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Boxen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Boxen   : 15238 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Softball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Softball   : 15238 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Modern Dance   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Modern Dance   : 15240 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Pickleball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Pickleball   : 15240 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Racquetball   : There are 3 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Racquetball   : 15243 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Speedball   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Speedball   : 15244 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Inlineskaten   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Inlineskaten   : 15244 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Frescoball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Frescoball   : 15244 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Crosstraining   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Crosstraining   : 15244 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Tischtennis   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Tischtennis   : 15245 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Wasserball   : There are 4 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Wasserball   : 15249 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Beachtennis   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Beachtennis   : 15249 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Roller   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Roller   : 15249 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Einradfahren   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Einradfahren   : 15249 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Skateboarden   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Skateboarden   : 15249 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Rollschuhfahren   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Rollschuhfahren   : 15249 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Windsurfen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Windsurfen   : 15249 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Ballett   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Ballett   : 15249 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Gymnastik   : There are 7 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Gymnastik   : 15256 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Raubfisch-Angeln   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Raubfisch-Angeln   : 15256 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Crossminton   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Crossminton   : 15256 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Salzwasserangeln   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Salzwasserangeln   : 15256 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Kitesurfen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Kitesurfen   : 15256 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Tamburello   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Tamburello   : 15256 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Eislaufen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Eislaufen   : 15256 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Kinderradsport   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Kinderradsport   : 15256 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Kinder_Sportschuhe_ Cricket   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Cricket   : 15256 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Floorball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Floorball   : 15256 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Netball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Netball   : 15256 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Inlinehockey   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Inlinehockey   : 15256 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Swimrun   : There are 3 products (1 pages)\n",
      "DE_Kinder_Sportschuhe_ Swimrun   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Eishockey   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Eishockey   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Lacrosse   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Lacrosse   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ H√∂hlenwandern   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ H√∂hlenwandern   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Urban Dance   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Urban Dance   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Kite Landboarding   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Kite Landboarding   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Karpfenangeln   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Karpfenangeln   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Slacklining   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Slacklining   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Power Kiting   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Power Kiting   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Snowkiting   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Snowkiting   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Westernreiten   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Westernreiten   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Grappling   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Grappling   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Karate   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Karate   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Wrestling   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Wrestling   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Drachenboot   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Drachenboot   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Jiu Jitsu   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Jiu Jitsu   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Sepak Takraw   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Sepak Takraw   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Canyoning   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Canyoning   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Sambo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Sambo   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Schie√üen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Schie√üen   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Rudern   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Rudern   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Pole Fishing   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Pole Fishing   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Selbstverteidigung   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Selbstverteidigung   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Taekwondo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Taekwondo   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ BMX   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ BMX   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Kung Fu   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Kung Fu   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Aikido   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Aikido   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Fechten   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Fechten   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Judo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Judo   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Kendo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Kendo   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Eisfischen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Eisfischen   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Wasserski   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Wasserski   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Wakeboarden   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Wakeboarden   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Ball Hockey   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Ball Hockey   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Taijiquan    : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Taijiquan    : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Fitness Dance   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Fitness Dance   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Poledance   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Poledance   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Gezogene aufblasbare Schwimmk√∂rper   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Gezogene aufblasbare Schwimmk√∂rper   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Capoeira   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Capoeira   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Kempo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Kempo   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Peteca   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Peteca   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Bike & Run   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Bike & Run   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Kitewing   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Kitewing   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportschuhe_ Arnis   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportschuhe_ Arnis   : 15259 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Wandern   : There are 24 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Wandern   : 15283 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Fu√üball   : There are 85 products (3 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Fu√üball   : 15368 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Laufen   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Laufen   : 15370 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Surfen   : There are 16 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Surfen   : 15386 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Skifahren   : There are 56 products (2 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Skifahren   : 15442 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Trekking   : There are 6 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Trekking   : 15448 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Pilates   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Pilates   : 15448 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Schwimmen   : There are 74 products (2 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Schwimmen   : 15522 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Cardio-Training   : There are 4 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Cardio-Training   : 15526 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Tennis   : There are 40 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Tennis   : 15566 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Bodybuilding   : There are 3 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Kinder_Sportausr√ºstung_ Bodybuilding   : 15569 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Mountainbiking   : There are 26 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Mountainbiking   : 15595 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Badminton   : There are 18 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Badminton   : 15613 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Padel-Tennis   : There are 9 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Padel-Tennis   : 15622 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Urban Cycling   : There are 22 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Urban Cycling   : 15644 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Rennradfahren   : There are 8 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Rennradfahren   : 15652 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Snowboarden   : There are 26 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Snowboarden   : 15678 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Jagd   : There are 4 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Jagd   : 15682 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Walking   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Walking   : 15683 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Basketball   : There are 39 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Basketball   : 15722 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Tourenski   : There are 22 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Tourenski   : 15744 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Squash   : There are 34 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Squash   : 15778 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Segeln   : There are 22 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Segeln   : 15800 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Stand Up Paddling   : There are 24 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Stand Up Paddling   : 15824 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Tauchen   : There are 31 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Tauchen   : 15855 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Bergsteigen   : There are 9 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Bergsteigen   : 15864 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Yoga   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Yoga   : 15865 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Volleyball   : There are 30 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Volleyball   : 15895 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Hybrid-Rad   : There are 33 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Hybrid-Rad   : 15928 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Rodeln   : There are 41 products (2 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Rodeln   : 15969 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Handball   : There are 30 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Handball   : 15999 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Kanu/Kajaksport   : There are 19 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Kanu/Kajaksport   : 16018 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Reiten   : There are 24 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Reiten   : 16042 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Leichtathletik   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Leichtathletik   : 16042 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Aquafitness   : There are 7 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Aquafitness   : 16049 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Baseball   : There are 6 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Baseball   : 16055 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ American Football   : There are 9 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ American Football   : 16064 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Feldhockey   : There are 25 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Feldhockey   : 16089 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Rugby   : There are 9 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Rugby   : 16098 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Orientierungslauf   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Orientierungslauf   : 16098 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Triathlon   : There are 14 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Triathlon   : 16112 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Klettern   : There are 15 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Klettern   : 16127 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Pelota   : There are 12 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Pelota   : 16139 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Gehen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Gehen   : 16139 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Golf   : There are 8 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Golf   : 16147 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Ski Nordisch   : There are 5 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Ski Nordisch   : 16152 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Boxen   : There are 10 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Boxen   : 16162 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Softball   : There are 7 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Softball   : 16169 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Modern Dance   : There are 5 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Modern Dance   : 16174 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Pickleball   : There are 4 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Pickleball   : 16178 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Racquetball   : There are 4 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Racquetball   : 16182 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Speedball   : There are 3 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Speedball   : 16185 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Inlineskaten   : There are 20 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Inlineskaten   : 16205 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Frescoball   : There are 3 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Frescoball   : 16208 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Crosstraining   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Crosstraining   : 16209 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Tischtennis   : There are 4 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Tischtennis   : 16213 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Wasserball   : There are 4 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Wasserball   : 16217 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Beachtennis   : There are 7 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Beachtennis   : 16224 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Roller   : There are 24 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Roller   : 16248 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Einradfahren   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Einradfahren   : 16248 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Skateboarden   : There are 31 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Skateboarden   : 16279 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Rollschuhfahren   : There are 15 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Rollschuhfahren   : 16294 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Windsurfen   : There are 8 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Windsurfen   : 16302 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Ballett   : There are 6 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Ballett   : 16308 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Gymnastik   : There are 18 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Gymnastik   : 16326 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Raubfisch-Angeln   : There are 1 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Kinder_Sportausr√ºstung_ Raubfisch-Angeln   : 16327 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Crossminton   : There are 4 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Crossminton   : 16331 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Salzwasserangeln   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Salzwasserangeln   : 16333 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Kitesurfen   : There are 7 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Kitesurfen   : 16340 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Tamburello   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Tamburello   : 16341 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Eislaufen   : There are 9 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Eislaufen   : 16350 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Kinderradsport   : There are 13 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Kinderradsport   : 16363 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Cricket   : There are 3 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Cricket   : 16366 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Floorball   : There are 5 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Floorball   : 16371 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Netball   : There are 3 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Netball   : 16374 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Inlinehockey   : There are 15 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Inlinehockey   : 16389 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Swimrun   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Swimrun   : 16391 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Eishockey   : There are 12 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Eishockey   : 16403 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Lacrosse   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Lacrosse   : 16405 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ H√∂hlenwandern   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ H√∂hlenwandern   : 16407 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Urban Dance   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Urban Dance   : 16409 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Kite Landboarding   : There are 6 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Kite Landboarding   : 16415 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Karpfenangeln   : There are 4 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Karpfenangeln   : 16419 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Slacklining   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Slacklining   : 16419 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Power Kiting   : There are 5 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Power Kiting   : 16424 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Snowkiting   : There are 5 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Snowkiting   : 16429 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Westernreiten   : There are 3 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Westernreiten   : 16432 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Grappling   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Grappling   : 16434 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Karate   : There are 3 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Karate   : 16437 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Wrestling   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Wrestling   : 16438 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Drachenboot   : There are 6 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Drachenboot   : 16444 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Jiu Jitsu   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Jiu Jitsu   : 16445 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Sepak Takraw   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Sepak Takraw   : 16447 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Canyoning   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Canyoning   : 16447 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Sambo   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Sambo   : 16449 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Schie√üen   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Schie√üen   : 16450 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Rudern   : There are 5 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Rudern   : 16455 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Pole Fishing   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Pole Fishing   : 16456 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Selbstverteidigung   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Selbstverteidigung   : 16458 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Taekwondo   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Taekwondo   : 16460 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ BMX   : There are 12 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ BMX   : 16472 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Kung Fu   : There are 2 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Kung Fu   : 16474 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Aikido   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Aikido   : 16475 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Fechten   : There are 4 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Fechten   : 16479 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Judo   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Judo   : 16480 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Kendo   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Kendo   : 16481 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Eisfischen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Eisfischen   : 16481 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Wasserski   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Wasserski   : 16482 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Wakeboarden   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Wakeboarden   : 16483 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Ball Hockey   : There are 4 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Ball Hockey   : 16487 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Taijiquan    : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Taijiquan    : 16488 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Fitness Dance   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Fitness Dance   : 16489 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Poledance   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Poledance   : 16489 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Gezogene aufblasbare Schwimmk√∂rper   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Gezogene aufblasbare Schwimmk√∂rper   : 16489 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Capoeira   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Capoeira   : 16490 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Kempo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Kempo   : 16490 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Peteca   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Peteca   : 16490 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Bike & Run   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Bike & Run   : 16490 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Kitewing   : There are 1 products (1 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Kitewing   : 16491 products have been scraped!\n",
      "DE_Kinder_Sportausr√ºstung_ Arnis   : There are 0 products (0 pages)\n",
      "DE_Kinder_Sportausr√ºstung_ Arnis   : 16491 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Wandern   : There are 39 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Kinder_Babys & Kleinkinder_ Wandern   : 16530 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Fu√üball   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Fu√üball   : 16532 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Laufen   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Laufen   : 16534 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Surfen   : There are 7 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Surfen   : 16541 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Skifahren   : There are 20 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Skifahren   : 16561 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Trekking   : There are 5 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Trekking   : 16566 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Pilates   : There are 3 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Pilates   : 16569 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Schwimmen   : There are 66 products (2 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Schwimmen   : 16635 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Cardio-Training   : There are 4 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Cardio-Training   : 16639 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Tennis   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Tennis   : 16641 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Bodybuilding   : There are 4 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Bodybuilding   : 16645 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Mountainbiking   : There are 10 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Mountainbiking   : 16655 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Badminton   : There are 4 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Badminton   : 16659 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Padel-Tennis   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Padel-Tennis   : 16661 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Urban Cycling   : There are 10 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Urban Cycling   : 16671 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Rennradfahren   : There are 10 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Rennradfahren   : 16681 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Snowboarden   : There are 5 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Snowboarden   : 16686 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Jagd   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Jagd   : 16688 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Walking   : There are 7 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Walking   : 16695 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Basketball   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Basketball   : 16697 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Tourenski   : There are 4 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Tourenski   : 16701 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Squash   : There are 8 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Squash   : 16709 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Segeln   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Segeln   : 16711 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Stand Up Paddling   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Stand Up Paddling   : 16713 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Tauchen   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Tauchen   : 16715 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Bergsteigen   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Bergsteigen   : 16717 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Yoga   : There are 3 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Yoga   : 16720 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Volleyball   : There are 4 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Volleyball   : 16724 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Hybrid-Rad   : There are 10 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Hybrid-Rad   : 16734 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Rodeln   : There are 16 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Rodeln   : 16750 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Handball   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Handball   : 16752 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Kanu/Kajaksport   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Kanu/Kajaksport   : 16754 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Reiten   : There are 4 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Reiten   : 16758 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Leichtathletik   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Leichtathletik   : 16760 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Aquafitness   : There are 3 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Aquafitness   : 16763 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Baseball   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Baseball   : 16765 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ American Football   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ American Football   : 16767 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Feldhockey   : There are 5 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Feldhockey   : 16772 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Rugby   : There are 3 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Rugby   : 16775 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Orientierungslauf   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Orientierungslauf   : 16777 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Triathlon   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Triathlon   : 16779 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Klettern   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Klettern   : 16781 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Pelota   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Pelota   : 16783 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Gehen   : There are 3 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Gehen   : 16786 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Golf   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Golf   : 16788 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Ski Nordisch   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Ski Nordisch   : 16790 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Boxen   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Boxen   : 16792 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Softball   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Softball   : 16794 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Modern Dance   : There are 3 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Modern Dance   : 16797 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Pickleball   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Pickleball   : 16799 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Racquetball   : There are 4 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Racquetball   : 16803 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Speedball   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Speedball   : 16805 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Inlineskaten   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Inlineskaten   : 16807 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Frescoball   : There are 2 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Kinder_Babys & Kleinkinder_ Frescoball   : 16809 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Crosstraining   : There are 3 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Crosstraining   : 16812 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Tischtennis   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Tischtennis   : 16814 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Wasserball   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Wasserball   : 16816 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Beachtennis   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Beachtennis   : 16818 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Roller   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Roller   : 16820 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Einradfahren   : There are 8 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Einradfahren   : 16828 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Skateboarden   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Skateboarden   : 16830 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Rollschuhfahren   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Rollschuhfahren   : 16832 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Windsurfen   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Windsurfen   : 16834 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Ballett   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Ballett   : 16836 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Gymnastik   : There are 7 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Gymnastik   : 16843 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Raubfisch-Angeln   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Raubfisch-Angeln   : 16845 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Crossminton   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Crossminton   : 16847 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Salzwasserangeln   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Salzwasserangeln   : 16849 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Kitesurfen   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Kitesurfen   : 16851 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Tamburello   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Tamburello   : 16853 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Eislaufen   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Eislaufen   : 16855 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Kinderradsport   : There are 0 products (0 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Kinderradsport   : 16855 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Cricket   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Cricket   : 16857 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Floorball   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Floorball   : 16859 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Netball   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Netball   : 16861 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Inlinehockey   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Inlinehockey   : 16863 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Swimrun   : There are 0 products (0 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Swimrun   : 16863 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Eishockey   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Eishockey   : 16865 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Lacrosse   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Lacrosse   : 16867 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ H√∂hlenwandern   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ H√∂hlenwandern   : 16869 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Urban Dance   : There are 3 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Urban Dance   : 16872 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Kite Landboarding   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Kite Landboarding   : 16874 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Karpfenangeln   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Karpfenangeln   : 16876 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Slacklining   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Slacklining   : 16878 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Power Kiting   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Power Kiting   : 16880 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Snowkiting   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Snowkiting   : 16882 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Westernreiten   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Westernreiten   : 16884 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Grappling   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Grappling   : 16886 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Karate   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Karate   : 16888 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Wrestling   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Wrestling   : 16890 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Drachenboot   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Drachenboot   : 16892 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Jiu Jitsu   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Jiu Jitsu   : 16894 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Sepak Takraw   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Sepak Takraw   : 16896 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Canyoning   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Canyoning   : 16898 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Sambo   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Sambo   : 16900 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Schie√üen   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Schie√üen   : 16902 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Rudern   : There are 0 products (0 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Rudern   : 16902 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Pole Fishing   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Pole Fishing   : 16904 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Selbstverteidigung   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Selbstverteidigung   : 16906 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Taekwondo   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Taekwondo   : 16908 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ BMX   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ BMX   : 16910 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Kung Fu   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Kung Fu   : 16912 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Aikido   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Aikido   : 16914 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Fechten   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Fechten   : 16916 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Judo   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Judo   : 16918 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Kendo   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Kendo   : 16920 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Kinder_Babys & Kleinkinder_ Eisfischen   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Eisfischen   : 16922 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Wasserski   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Wasserski   : 16924 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Wakeboarden   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Wakeboarden   : 16926 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Ball Hockey   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Ball Hockey   : 16928 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Taijiquan    : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Taijiquan    : 16930 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Fitness Dance   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Fitness Dance   : 16932 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Poledance   : There are 0 products (0 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Poledance   : 16932 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Gezogene aufblasbare Schwimmk√∂rper   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Gezogene aufblasbare Schwimmk√∂rper   : 16934 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Capoeira   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Capoeira   : 16936 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Kempo   : There are 2 products (1 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Kempo   : 16938 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Peteca   : There are 0 products (0 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Peteca   : 16938 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Bike & Run   : There are 0 products (0 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Bike & Run   : 16938 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Kitewing   : There are 0 products (0 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Kitewing   : 16938 products have been scraped!\n",
      "DE_Kinder_Babys & Kleinkinder_ Arnis   : There are 0 products (0 pages)\n",
      "DE_Kinder_Babys & Kleinkinder_ Arnis   : 16938 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Wandern   : There are 192 products (5 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Wandern   : 17130 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Fu√üball   : There are 13 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Fu√üball   : 17143 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Laufen   : There are 22 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Laufen   : 17165 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Surfen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Surfen   : 17165 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Skifahren   : There are 17 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Skifahren   : 17182 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Trekking   : There are 2 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Trekking   : 17184 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Pilates   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Pilates   : 17184 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Schwimmen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Schwimmen   : 17184 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Cardio-Training   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Cardio-Training   : 17184 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Tennis   : There are 1 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Tennis   : 17185 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Bodybuilding   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Bodybuilding   : 17185 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Mountainbiking   : There are 8 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Mountainbiking   : 17193 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Badminton   : There are 25 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Badminton   : 17218 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Padel-Tennis   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Padel-Tennis   : 17218 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Urban Cycling   : There are 8 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Urban Cycling   : 17226 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Rennradfahren   : There are 8 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Rennradfahren   : 17234 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Snowboarden   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Snowboarden   : 17234 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Jagd   : There are 13 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Jagd   : 17247 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Walking   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Walking   : 17247 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Basketball   : There are 20 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Basketball   : 17267 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Tourenski   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Tourenski   : 17267 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Squash   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Squash   : 17267 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Segeln   : There are 9 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Segeln   : 17276 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Stand Up Paddling   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Stand Up Paddling   : 17276 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Tauchen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Tauchen   : 17276 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Bergsteigen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Bergsteigen   : 17276 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Yoga   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Yoga   : 17276 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Volleyball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Volleyball   : 17276 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Hybrid-Rad   : There are 8 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Hybrid-Rad   : 17284 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Rodeln   : There are 6 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Rodeln   : 17290 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Handball   : There are 2 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Handball   : 17292 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Kanu/Kajaksport   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Kanu/Kajaksport   : 17292 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Reiten   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Reiten   : 17292 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Leichtathletik   : There are 9 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Leichtathletik   : 17301 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Aquafitness   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Aquafitness   : 17301 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Baseball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Baseball   : 17301 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ American Football   : There are 1 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ American Football   : 17302 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Feldhockey   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Feldhockey   : 17302 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Rugby   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Rugby   : 17302 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Orientierungslauf   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Orientierungslauf   : 17302 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Kinder_Outdoorwelt Kids_ Triathlon   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Triathlon   : 17302 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Klettern   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Klettern   : 17302 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Pelota   : There are 1 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Pelota   : 17303 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Gehen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Gehen   : 17303 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Golf   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Golf   : 17303 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Ski Nordisch   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Ski Nordisch   : 17303 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Boxen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Boxen   : 17303 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Softball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Softball   : 17303 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Modern Dance   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Modern Dance   : 17303 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Pickleball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Pickleball   : 17303 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Racquetball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Racquetball   : 17303 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Speedball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Speedball   : 17303 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Inlineskaten   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Inlineskaten   : 17303 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Frescoball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Frescoball   : 17303 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Crosstraining   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Crosstraining   : 17303 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Tischtennis   : There are 32 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Tischtennis   : 17335 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Wasserball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Wasserball   : 17335 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Beachtennis   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Beachtennis   : 17335 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Roller   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Roller   : 17335 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Einradfahren   : There are 8 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Einradfahren   : 17343 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Skateboarden   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Skateboarden   : 17343 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Rollschuhfahren   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Rollschuhfahren   : 17343 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Windsurfen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Windsurfen   : 17343 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Ballett   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Ballett   : 17343 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Gymnastik   : There are 5 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Gymnastik   : 17348 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Raubfisch-Angeln   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Raubfisch-Angeln   : 17348 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Crossminton   : There are 4 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Crossminton   : 17352 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Salzwasserangeln   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Salzwasserangeln   : 17352 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Kitesurfen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Kitesurfen   : 17352 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Tamburello   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Tamburello   : 17352 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Eislaufen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Eislaufen   : 17352 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Kinderradsport   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Kinderradsport   : 17352 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Cricket   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Cricket   : 17352 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Floorball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Floorball   : 17352 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Netball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Netball   : 17352 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Inlinehockey   : There are 1 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Inlinehockey   : 17353 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Swimrun   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Swimrun   : 17353 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Eishockey   : There are 1 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Eishockey   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Lacrosse   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Lacrosse   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ H√∂hlenwandern   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ H√∂hlenwandern   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Urban Dance   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Urban Dance   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Kite Landboarding   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Kite Landboarding   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Karpfenangeln   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Karpfenangeln   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Slacklining   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Slacklining   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Power Kiting   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Power Kiting   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Snowkiting   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Snowkiting   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Westernreiten   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Westernreiten   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Grappling   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Grappling   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Karate   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Karate   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Wrestling   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Wrestling   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Drachenboot   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Drachenboot   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Jiu Jitsu   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Jiu Jitsu   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Sepak Takraw   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Sepak Takraw   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Canyoning   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Canyoning   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Sambo   : There are 0 products (0 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Kinder_Outdoorwelt Kids_ Sambo   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Schie√üen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Schie√üen   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Rudern   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Rudern   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Pole Fishing   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Pole Fishing   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Selbstverteidigung   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Selbstverteidigung   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Taekwondo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Taekwondo   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ BMX   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ BMX   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Kung Fu   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Kung Fu   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Aikido   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Aikido   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Fechten   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Fechten   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Judo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Judo   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Kendo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Kendo   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Eisfischen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Eisfischen   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Wasserski   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Wasserski   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Wakeboarden   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Wakeboarden   : 17354 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Ball Hockey   : There are 1 products (1 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Ball Hockey   : 17355 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Taijiquan    : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Taijiquan    : 17355 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Fitness Dance   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Fitness Dance   : 17355 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Poledance   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Poledance   : 17355 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Gezogene aufblasbare Schwimmk√∂rper   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Gezogene aufblasbare Schwimmk√∂rper   : 17355 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Capoeira   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Capoeira   : 17355 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Kempo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Kempo   : 17355 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Peteca   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Peteca   : 17355 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Bike & Run   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Bike & Run   : 17355 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Kitewing   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Kitewing   : 17355 products have been scraped!\n",
      "DE_Kinder_Outdoorwelt Kids_ Arnis   : There are 0 products (0 pages)\n",
      "DE_Kinder_Outdoorwelt Kids_ Arnis   : 17355 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Wandern   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Wandern   : 17355 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Fu√üball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Fu√üball   : 17355 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Laufen   : There are 6 products (1 pages)\n",
      "DE_Kinder_Schulsport_ Laufen   : 17361 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Surfen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Surfen   : 17361 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Skifahren   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Skifahren   : 17361 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Trekking   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Trekking   : 17361 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Pilates   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Pilates   : 17361 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Schwimmen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Schwimmen   : 17361 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Cardio-Training   : There are 4 products (1 pages)\n",
      "DE_Kinder_Schulsport_ Cardio-Training   : 17365 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Tennis   : There are 14 products (1 pages)\n",
      "DE_Kinder_Schulsport_ Tennis   : 17379 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Bodybuilding   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Bodybuilding   : 17379 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Mountainbiking   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Mountainbiking   : 17379 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Badminton   : There are 5 products (1 pages)\n",
      "DE_Kinder_Schulsport_ Badminton   : 17384 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Padel-Tennis   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Padel-Tennis   : 17384 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Urban Cycling   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Urban Cycling   : 17384 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Rennradfahren   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Rennradfahren   : 17384 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Snowboarden   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Snowboarden   : 17384 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Jagd   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Jagd   : 17384 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Walking   : There are 9 products (1 pages)\n",
      "DE_Kinder_Schulsport_ Walking   : 17393 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Basketball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Basketball   : 17393 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Tourenski   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Tourenski   : 17393 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Squash   : There are 8 products (1 pages)\n",
      "DE_Kinder_Schulsport_ Squash   : 17401 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Segeln   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Segeln   : 17401 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Stand Up Paddling   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Stand Up Paddling   : 17401 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Tauchen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Tauchen   : 17401 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Bergsteigen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Bergsteigen   : 17401 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Yoga   : There are 1 products (1 pages)\n",
      "DE_Kinder_Schulsport_ Yoga   : 17402 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Volleyball   : There are 2 products (1 pages)\n",
      "DE_Kinder_Schulsport_ Volleyball   : 17404 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Hybrid-Rad   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Hybrid-Rad   : 17404 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Rodeln   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Rodeln   : 17404 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Handball   : There are 2 products (1 pages)\n",
      "DE_Kinder_Schulsport_ Handball   : 17406 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Kanu/Kajaksport   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Kanu/Kajaksport   : 17406 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Kinder_Schulsport_ Reiten   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Reiten   : 17406 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Leichtathletik   : There are 7 products (1 pages)\n",
      "DE_Kinder_Schulsport_ Leichtathletik   : 17413 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Aquafitness   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Aquafitness   : 17413 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Baseball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Baseball   : 17413 products have been scraped!\n",
      "DE_Kinder_Schulsport_ American Football   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ American Football   : 17413 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Feldhockey   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Feldhockey   : 17413 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Rugby   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Rugby   : 17413 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Orientierungslauf   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Orientierungslauf   : 17413 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Triathlon   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Triathlon   : 17413 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Klettern   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Klettern   : 17413 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Pelota   : There are 1 products (1 pages)\n",
      "DE_Kinder_Schulsport_ Pelota   : 17414 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Gehen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Gehen   : 17414 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Golf   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Golf   : 17414 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Ski Nordisch   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Ski Nordisch   : 17414 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Boxen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Boxen   : 17414 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Softball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Softball   : 17414 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Modern Dance   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Modern Dance   : 17414 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Pickleball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Pickleball   : 17414 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Racquetball   : There are 3 products (1 pages)\n",
      "DE_Kinder_Schulsport_ Racquetball   : 17417 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Speedball   : There are 1 products (1 pages)\n",
      "DE_Kinder_Schulsport_ Speedball   : 17418 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Inlineskaten   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Inlineskaten   : 17418 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Frescoball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Frescoball   : 17418 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Crosstraining   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Crosstraining   : 17418 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Tischtennis   : There are 1 products (1 pages)\n",
      "DE_Kinder_Schulsport_ Tischtennis   : 17419 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Wasserball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Wasserball   : 17419 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Beachtennis   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Beachtennis   : 17419 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Roller   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Roller   : 17419 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Einradfahren   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Einradfahren   : 17419 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Skateboarden   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Skateboarden   : 17419 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Rollschuhfahren   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Rollschuhfahren   : 17419 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Windsurfen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Windsurfen   : 17419 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Ballett   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Ballett   : 17419 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Gymnastik   : There are 20 products (1 pages)\n",
      "DE_Kinder_Schulsport_ Gymnastik   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Raubfisch-Angeln   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Raubfisch-Angeln   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Crossminton   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Crossminton   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Salzwasserangeln   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Salzwasserangeln   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Kitesurfen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Kitesurfen   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Tamburello   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Tamburello   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Eislaufen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Eislaufen   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Kinderradsport   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Kinderradsport   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Cricket   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Cricket   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Floorball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Floorball   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Netball   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Netball   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Inlinehockey   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Inlinehockey   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Swimrun   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Swimrun   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Eishockey   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Eishockey   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Lacrosse   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Lacrosse   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ H√∂hlenwandern   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ H√∂hlenwandern   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Urban Dance   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Urban Dance   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Kite Landboarding   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Kite Landboarding   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Karpfenangeln   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Karpfenangeln   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Slacklining   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Slacklining   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Power Kiting   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Power Kiting   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Snowkiting   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Snowkiting   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Westernreiten   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Westernreiten   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Grappling   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Grappling   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Karate   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Karate   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Wrestling   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Wrestling   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Drachenboot   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Drachenboot   : 17439 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE_Kinder_Schulsport_ Jiu Jitsu   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Jiu Jitsu   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Sepak Takraw   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Sepak Takraw   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Canyoning   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Canyoning   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Sambo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Sambo   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Schie√üen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Schie√üen   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Rudern   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Rudern   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Pole Fishing   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Pole Fishing   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Selbstverteidigung   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Selbstverteidigung   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Taekwondo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Taekwondo   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ BMX   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ BMX   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Kung Fu   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Kung Fu   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Aikido   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Aikido   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Fechten   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Fechten   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Judo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Judo   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Kendo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Kendo   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Eisfischen   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Eisfischen   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Wasserski   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Wasserski   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Wakeboarden   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Wakeboarden   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Ball Hockey   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Ball Hockey   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Taijiquan    : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Taijiquan    : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Fitness Dance   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Fitness Dance   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Poledance   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Poledance   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Gezogene aufblasbare Schwimmk√∂rper   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Gezogene aufblasbare Schwimmk√∂rper   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Capoeira   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Capoeira   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Kempo   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Kempo   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Peteca   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Peteca   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Bike & Run   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Bike & Run   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Kitewing   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Kitewing   : 17439 products have been scraped!\n",
      "DE_Kinder_Schulsport_ Arnis   : There are 0 products (0 pages)\n",
      "DE_Kinder_Schulsport_ Arnis   : 17439 products have been scraped!\n"
     ]
    }
   ],
   "source": [
    "prod_list = []\n",
    "\n",
    "for item in cat_level3:\n",
    "    country = item[\"country\"]\n",
    "    cat_url = item[\"cat_url\"]\n",
    "    cat1 = item[\"cat1\"]\n",
    "    cat2 = item[\"cat2\"]\n",
    "    cat3 = item[\"cat3\"]\n",
    "    soup = cookSoup(cat_url)\n",
    "    url_list = pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page)\n",
    "    prod_list = getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "121e9207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving cat_level3\n",
    "with open(f\"{country}_decathlon.csv\", \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter = \",\")\n",
    "    writer.writerow([\"country\",\"title\", \"sku\", \"reg_pr\", \"act_pr\", \"brand\", \"sticker\", \"cat_1\", \"cat_2\", \"cat_3\", \"url\"])\n",
    "\n",
    "    for item in prod_list:\n",
    "        writer.writerow([country, item['title'], item['sku'], item['regular price'], \n",
    "                         item['actual price'], item['brand'], item['sticker'], \n",
    "                         item['cat_1'], item['cat_2'], item['cat_3'], item['url']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5dc2e78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/1:\n",
      "a = 1 \n",
      "b = 1\n",
      "print(a > b)\n",
      "print(a < b)\n",
      "print(a >= b)\n",
      "print(a <= b)\n",
      " 2/2: a>b\n",
      " 2/3:\n",
      "a > b\n",
      "a < b\n",
      "a >= b\n",
      "a <=b\n",
      " 2/4:\n",
      "a > b\n",
      "a < b\n",
      "a >= b\n",
      "a <=b\n",
      " 2/5:\n",
      "a > b\n",
      "a < b\n",
      "a >= b\n",
      "a <=b\n",
      " 2/6: url = \"https://www.airbnb.com/s/Amsterdam/homes?place_id=ChIJVXealLU_xkcRja_At0z9AGY&refinement_paths%5B%5D=%2Fhomes&search_type=section_navigation\"\n",
      " 2/7:\n",
      "import requests\n",
      "content = requests.get(url)\n",
      " 2/8:\n",
      "import requests\n",
      "content = requests.get(url)\n",
      " 2/9: content.text\n",
      "2/10: ! ahaha\n",
      "2/11: captured_content = \"Page 1\"\n",
      "2/12: captured_content = \"Page 1\"\n",
      "2/13: numeric_content = int(captured_content.replace( 'Page ', ''))\n",
      "2/14: urls = ['https://www.airbnb.com/rooms/30042149?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba', 'https://www.airbnb.com/rooms/14173621?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba', 'https://www.airbnb.com/rooms/13040996?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba']\n",
      "2/15: urls = ['https://www.airbnb.com/rooms/30042149?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba', 'https://www.airbnb.com/rooms/14173621?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba', 'https://www.airbnb.com/rooms/13040996?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba']\n",
      "2/16:\n",
      "#length of URLs\n",
      "len(urls)\n",
      "2/17: import pandas\n",
      "2/18: import this\n",
      "2/19:\n",
      "def scrape(url):\n",
      "    print('Im scraping' + url )\n",
      "2/20:\n",
      "def scrape(url):\n",
      "    print('Im scraping' + url )\n",
      "2/21: scrape ('airbnb.com')\n",
      " 3/1:\n",
      "# your code goes here!\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "    elif study == \"Research Master\":\n",
      "        print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "        else:\n",
      "            print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      " 3/2:\n",
      "# solution\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course.\")\n",
      " 3/3:\n",
      "# your code goes here!\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "    elif study == \"Research Master\":\n",
      "        print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "        else:\n",
      "            print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      " 3/4:\n",
      "# your code goes here!\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      " 3/5:\n",
      "# your code goes here!\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "    elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      "2/22: urls\n",
      "2/23:\n",
      "club1 = {'url':'https://www.airbnb.com/rooms/30042149?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba', \n",
      "             'country': 'england', \n",
      "             'champions_league': True}\n",
      "club2 = {'url':'https://www.airbnb.com/rooms/30042149?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba', \n",
      "             'country': 'france', \n",
      "             'champions_league': True}\n",
      "club3 = {'url':'https://www.airbnb.com/rooms/30042149?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba', \n",
      "             'country': 'england', \n",
      "             'champions_league': True}\n",
      "2/24: list_of_dics = [club1, club2, club3]\n",
      "2/25: list_of_dics\n",
      "2/26:\n",
      "# Functions can or cannot have Arguemnts\n",
      "\n",
      "def scrape3(url):\n",
      "    print(url)\n",
      "    return(True)\n",
      "2/27: scrape3(urls[0])\n",
      "2/28: urls\n",
      "2/29: print(urls)\n",
      " 5/1:\n",
      "import os\n",
      "os.getcwd()\n",
      " 5/2:\n",
      "import os\n",
      "os.getcwd()\n",
      " 5/3: os.chdir('D:\\OneDrive\\00_Tilburg\\00_RM2021\\Unit 1\\oDCM\\W1\\python-bootcamp')\n",
      " 5/4: os.chdir('D:\\\\OneDrive\\\\00_Tilburg\\\\00_RM2021\\\\Unit 1\\\\oDCM\\\\W1\\\\python-bootcamp')\n",
      " 5/5: os.getcwd()\n",
      " 6/1:\n",
      "a = 1 \n",
      "b = 1\n",
      "print(a > b)\n",
      "print(a < b)\n",
      "print(a >= b)\n",
      "print(a <= b)\n",
      " 6/2: a > b\n",
      " 6/3:\n",
      "balance = 100\n",
      "if balance < 0: \n",
      "    print(\"You should top up your checking account to avoid paying interest\")\n",
      "elif balance == 0: \n",
      "    print(\"Your checking account balance is exactly ‚Ç¨0.00, be careful when making new payments!\")\n",
      "else: \n",
      "    print(\"You have a positive balance\")\n",
      " 6/4:\n",
      "balance = 100\n",
      "if balance < 0: \n",
      "    \"You should top up your checking account to avoid paying interest\"\n",
      "elif balance == 0: \n",
      "    \"Your checking account balance is exactly ‚Ç¨0.00, be careful when making new payments!\"\n",
      "else: \n",
      "    \"You have a positive balance\"\n",
      " 6/5:\n",
      "balance = 100\n",
      "if balance < 0: \n",
      "    \"You should top up your checking account to avoid paying interest\"\n",
      "elif balance == 0: \n",
      "    \"Your checking account balance is exactly ‚Ç¨0.00, be careful when making new payments!\"\n",
      "else: \n",
      "    \"You have a positive balance\"\n",
      " 6/6:\n",
      "balance = 100\n",
      "if balance < 0: \n",
      "    print(\"You should top up your checking account to avoid paying interest\")\n",
      "elif balance == 0: \n",
      "    print(\"Your checking account balance is exactly ‚Ç¨0.00, be careful when making new payments!\")\n",
      "else: \n",
      "    print(\"You have a positive balance\")\n",
      " 6/7:\n",
      "balance = -10\n",
      "if balance < 0: \n",
      "    print(\"You should top up your checking account to avoid paying interest\")\n",
      "elif balance == 0: \n",
      "    print(\"Your checking account balance is exactly ‚Ç¨0.00, be careful when making new payments!\")\n",
      "else: \n",
      "    print(\"You have a positive balance\")\n",
      " 6/8:\n",
      "# your code goes here!\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      " 6/9:\n",
      "workday = False\n",
      "no_holiday = True\n",
      "\n",
      "if workday and no_holiday:\n",
      "    print(\"Go to work!\")\n",
      "6/10:\n",
      "workday = True\n",
      "no_holiday = True\n",
      "\n",
      "if workday and no_holiday:\n",
      "    print(\"Go to work!\")\n",
      "6/11:\n",
      "workday = False\n",
      "no_holiday = True\n",
      "\n",
      "if workday and no_holiday:\n",
      "    print(\"Go to work!\")\n",
      "else:\n",
      "    print(\"Back to sleep!\")\n",
      "6/12:\n",
      "workday = False\n",
      "no_holiday = False\n",
      "\n",
      "if workday and no_holiday:\n",
      "    print(\"Go to work!\")\n",
      "else:\n",
      "    print(\"Back to sleep!\")\n",
      "6/13:\n",
      "workday = True\n",
      "no_holiday = False\n",
      "\n",
      "if workday and no_holiday:\n",
      "    print(\"Go to work!\")\n",
      "else:\n",
      "    print(\"Back to sleep!\")\n",
      "6/14:\n",
      "# your code goes here!\n",
      "prior_knowledge == True\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\" and prior_knowledge:\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\" and prior_knowledge:\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      "6/15:\n",
      "# your code goes here!\n",
      "prior_knowledge = True\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\" and prior_knowledge:\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\" and prior_knowledge:\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      "6/16:\n",
      "student_dict = student_dict + {\"age\": 23}\n",
      "print(strudent_dict)\n",
      "6/17:\n",
      "student_dict = {\"name\": \"Lotte\", \n",
      "                \"study\": \"Marketing Analytics\", \n",
      "                \"prior_knowledge\": True}\n",
      "\n",
      "print(student_dict)\n",
      "6/18:\n",
      "student_dict = student_dict + {\"age\": 23}\n",
      "print(strudent_dict)\n",
      "6/19:\n",
      "student_dict = {student_dict, \"age\": 23}\n",
      "print(strudent_dict)\n",
      "6/20:\n",
      "student_dict[2]\n",
      "print(strudent_dict)\n",
      "6/21: student_dict[2]\n",
      "6/22: student_dict[\"name\"]\n",
      "6/23: student_dict[\"age\"] = 23\n",
      "6/24:\n",
      "student_dict[\"age\"] = 23\n",
      "print(student_dict)\n",
      "6/25:\n",
      "print(student_dict(\"email\"))\n",
      "print(student_dict.get(\"name\"))\n",
      "print(student_dict.get(\"email\"))\n",
      "6/26:\n",
      "print(student_dict.get(\"email\"))\n",
      "print(student_dict.get(\"name\"))\n",
      "print(student_dict.get(\"email\"))\n",
      "6/27:\n",
      "# your code goes here!\n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "6/28:\n",
      "enrollments = [\n",
      "    {\n",
      "        \"course\": \"Online Data Collection & Management\",\n",
      "        \"instructor\": \"Hannes Datta\",\n",
      "        \"students\": [\n",
      "            {\n",
      "                \"name\": \"Lotte\",\n",
      "                \"study\": \"Marketing Analytics\",\n",
      "                \"prior_knowledge\": True\n",
      "                \n",
      "            }, \n",
      "            {\n",
      "                \"name\": \"Joep\",\n",
      "                \"study\": \"Research Master\",\n",
      "                \"prior_knowledge\": True,\n",
      "                \"honors\": \"The Societal Challenge of Migration\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "6/29:\n",
      "# your code goes here!\n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "6/30:\n",
      "# your code goes here!\n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "6/31: print(enrollments[0][\"students\"][,].get[\"honors\"])\n",
      "6/32: print(enrollments[0][\"students\"][].get[\"honors\"])\n",
      "6/33: print(enrollments[0][\"students\"][0:1].get[\"honors\"])\n",
      "6/34: print(enrollments[0][\"students\"][0].get[\"honors\"])\n",
      "6/35: print(enrollments[0][\"students\"][].get(\"honors\"))\n",
      "6/36: print(enrollments[0][\"students\"][0:1].get(\"honors\"))\n",
      "6/37: print(enrollments[0][\"students\"][0].get(\"honors\"))\n",
      "6/38:\n",
      "# Question 2\n",
      "print(enrollments[0][\"students\"][0].get(\"honors\"))\n",
      "6/39: enrollments\n",
      "6/40: enrollments\n",
      "6/41:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "print(enrollments)\n",
      "6/42:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "enrollments\n",
      "6/43:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@uvt.edu\"\n",
      "enrollments\n",
      "6/44:\n",
      "enrollments = enrollments + [\n",
      "    {\"course\": \"Data Preparation and Workflow Management\",\n",
      "     \"instructor\": \"Hannes Datta\",\n",
      "     \"students\":[{\"name\": \"Sanne\",\n",
      "                  \"study\": \"Marketing Analytics\",\n",
      "                  \"prior_knowledge\": True\n",
      "         \n",
      "     }]\n",
      "        \n",
      "    }\n",
      "]\n",
      "print(enrollments)\n",
      "6/45:\n",
      "enrollments = enrollments + [\n",
      "    {\"course\": \"Data Preparation and Workflow Management\",\n",
      "     \"instructor\": \"Hannes Datta\",\n",
      "     \"students\":[{\"name\": \"Sanne\",\n",
      "                  \"study\": \"Marketing Analytics\",\n",
      "                  \"prior_knowledge\": True\n",
      "         \n",
      "     }]\n",
      "        \n",
      "    }\n",
      "]\n",
      "enrollments\n",
      "6/46:\n",
      "enrollments = enrollments + [,\n",
      "    {\"course\": \"Data Preparation and Workflow Management\",\n",
      "     \"instructor\": \"Hannes Datta\",\n",
      "     \"students\":[{\"name\": \"Sanne\",\n",
      "                  \"study\": \"Marketing Analytics\",\n",
      "                  \"prior_knowledge\": True\n",
      "         \n",
      "     }]\n",
      "        \n",
      "    }\n",
      "]\n",
      "enrollments\n",
      "6/47:\n",
      "enrollments.append([\n",
      "    {\"course\": \"Data Preparation and Workflow Management\",\n",
      "     \"instructor\": \"Hannes Datta\",\n",
      "     \"students\":[{\"name\": \"Sanne\",\n",
      "                  \"study\": \"Marketing Analytics\",\n",
      "                  \"prior_knowledge\": True\n",
      "         \n",
      "     }]\n",
      "        \n",
      "    }\n",
      "])\n",
      "enrollments\n",
      "6/48:\n",
      "enrollments.append({\"course\": \"Data Preparation and Workflow Management\",\n",
      "     \"instructor\": \"Hannes Datta\",\n",
      "     \"students\":[{\"name\": \"Sanne\",\n",
      "                  \"study\": \"Marketing Analytics\",\n",
      "                  \"prior_knowledge\": True\n",
      "         \n",
      "     })\n",
      "enrollments\n",
      "6/49:\n",
      "enrollments.append({\"course\": \"Data Preparation and Workflow Management\",\n",
      "     \"instructor\": \"Hannes Datta\",\n",
      "     \"students\":[{\"name\": \"Sanne\",\n",
      "                  \"study\": \"Marketing Analytics\",\n",
      "                  \"prior_knowledge\": True \n",
      "     })\n",
      "enrollments\n",
      "6/50:\n",
      "enrollments.append({\"course\": \"Data Preparation and Workflow Management\",\n",
      "     \"instructor\": \"Hannes Datta\",\n",
      "     \"students\":[{\"name\": \"Sanne\",\n",
      "                  \"study\": \"Marketing Analytics\",\n",
      "                  \"prior_knowledge\": True \n",
      "    }\n",
      "    }\n",
      "    )\n",
      "enrollments\n",
      "6/51:\n",
      "enrollments.append({\n",
      "    \"course\": \"Data Preparation and Workflow Management\",\n",
      "    \"instructor\": \"Hannes Datta\",\n",
      "    \"students\":[{\n",
      "        \"name\": \"Sanne\",\n",
      "        \"study\": \"Marketing Analytics\",\n",
      "        \"prior_knowledge\": True \n",
      "    }]\n",
      "    })\n",
      "enrollments\n",
      "6/52:\n",
      "enrollments = [\n",
      "    {\n",
      "        \"course\": \"Online Data Collection & Management\",\n",
      "        \"instructor\": \"Hannes Datta\",\n",
      "        \"students\": [\n",
      "            {\n",
      "                \"name\": \"Lotte\",\n",
      "                \"study\": \"Marketing Analytics\",\n",
      "                \"prior_knowledge\": True\n",
      "                \n",
      "            }, \n",
      "            {\n",
      "                \"name\": \"Joep\",\n",
      "                \"study\": \"Research Master\",\n",
      "                \"prior_knowledge\": True,\n",
      "                \"honors\": \"The Societal Challenge of Migration\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "6/53:\n",
      "# your code goes here!\n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "6/54: print(enrollments[0][\"students\"][0].get(\"honors\"))\n",
      "6/55: student_dict[\"email\"] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "6/56:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "enrollments\n",
      "6/57:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@uvt.edu\"\n",
      "enrollments\n",
      "6/58:\n",
      "enrollments.append({\n",
      "    \"course\": \"Data Preparation and Workflow Management\",\n",
      "    \"instructor\": \"Hannes Datta\",\n",
      "    \"students\":[{\n",
      "        \"name\": \"Sanne\",\n",
      "        \"study\": \"Marketing Analytics\",\n",
      "        \"prior_knowledge\": True \n",
      "    }]\n",
      "    })\n",
      "enrollments\n",
      "6/59:\n",
      "prices = [9.99, 3.95, 24.95]\n",
      "\n",
      "for price in prices:\n",
      "    print(price)\n",
      "6/60:\n",
      "prices = [9.99, 3.95, 24.95]\n",
      "\n",
      "for i in prices:\n",
      "    print(i)\n",
      "6/61:\n",
      "prices = [9.99, 3.95, 24.95, 8.95]\n",
      "\n",
      "for price in prices:\n",
      "    print(price)\n",
      "6/62:\n",
      "price_discount = []\n",
      "\n",
      "for price in prices:\n",
      "    price_dis = price * 0.85\n",
      "    price_discount.append(price_dis)\n",
      "    \n",
      "print(price_discount)\n",
      "6/63:\n",
      "price_discount = []\n",
      "\n",
      "for price in prices:\n",
      "    price_dis = price * 0.85\n",
      "    price_discount.append(price_dis)\n",
      "    \n",
      "print(round(price_discount))\n",
      "6/64:\n",
      "price_discount = []\n",
      "\n",
      "for price in prices:\n",
      "    price_dis = round(price * 0.85)\n",
      "    price_discount.append(price_dis)\n",
      "    \n",
      "print(round(price_discount))\n",
      "6/65:\n",
      "price_discount = []\n",
      "\n",
      "for price in prices:\n",
      "    price_dis = round(price * 0.85)\n",
      "    price_discount.append(price_dis)\n",
      "    \n",
      "print(price_discount)\n",
      "6/66:\n",
      "price_discount = []\n",
      "\n",
      "for price in prices:\n",
      "    price_dis = round((price * 0.85),2)\n",
      "    price_discount.append(price_dis)\n",
      "    \n",
      "print(price_discount)\n",
      "6/67: len(prices)\n",
      "6/68:\n",
      "# your code goes here!\n",
      "prices = [9.99, 3.95, 24.95, 8.95]\n",
      "sorted(prices, reverse=True)\n",
      "6/69:\n",
      "# your code goes here!\n",
      "prices = [9.99, 3.95, 24.95, 8.95]\n",
      "sorted(prices, reverse=True) \n",
      "\n",
      "total_prices = 3.95 * 60% + 8.95 * 70% + 9.99*80% + 24.95 *90%\n",
      "6/70:\n",
      "# your code goes here!\n",
      "\n",
      "total_prices = 3.95 * 60% + 8.95 * 70% + 9.99*80% + 24.95 *90%\n",
      "print(total_prices)\n",
      "6/71:\n",
      "# your code goes here!\n",
      "total_prices = 3.95 * 60% + 8.95 * 70% + 9.99 * 80% + 24.95 * 90%\n",
      "print(total_prices)\n",
      "6/72:\n",
      "# your code goes here!\n",
      "total_prices = 3.95 * 0.60 + 8.95 * 0.70 + 9.99 * 0.80 + 24.95 * 0.90\n",
      "print(total_prices)\n",
      "6/73:\n",
      "# solution (it's a bit more difficult, so no worries if you don't get it right away!)\n",
      "total_price = 0\n",
      "discount_rate = 0.10\n",
      "\n",
      "prices = sorted(prices, reverse=True)\n",
      "\n",
      "for price in prices: \n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += 0.10\n",
      "\n",
      "print(total_price)\n",
      " 7/1: prices\n",
      " 7/2:\n",
      "a = 1 \n",
      "b = 1\n",
      "print(a > b)\n",
      "print(a < b)\n",
      "print(a >= b)\n",
      "print(a <= b)\n",
      " 7/3: a > b\n",
      " 7/4:\n",
      "savings = 100 # variable assignment\n",
      "print(savings == 100) # comparison \n",
      "print(savings != 100) # comparison (False because it is 100!)\n",
      " 7/5:\n",
      "balance = -10\n",
      "if balance < 0: \n",
      "    print(\"You should top up your checking account to avoid paying interest\")\n",
      "elif balance == 0: \n",
      "    print(\"Your checking account balance is exactly ‚Ç¨0.00, be careful when making new payments!\")\n",
      "else: \n",
      "    print(\"You have a positive balance\")\n",
      " 7/6:\n",
      "# your code goes here!\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      " 7/7:\n",
      "# solution\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course.\")\n",
      " 7/8:\n",
      "workday = True\n",
      "no_holiday = False\n",
      "\n",
      "if workday and no_holiday:\n",
      "    print(\"Go to work!\")\n",
      "else:\n",
      "    print(\"Back to sleep!\")\n",
      " 7/9:\n",
      "tired = True\n",
      "bed_time = False\n",
      "\n",
      "if tired or bed_time:\n",
      "    print(\"Go to sleep!\")\n",
      "7/10:\n",
      "weekend = False\n",
      "\n",
      "if not weekend:\n",
      "    print(\"Go to work!\")\n",
      "7/11:\n",
      "# your code goes here!\n",
      "prior_knowledge = True\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\" and prior_knowledge:\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\" and prior_knowledge:\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      "7/12:\n",
      "# solution (it's not necessary to check whether prior_knowledge == True)\n",
      "if study == \"Marketing Analytics\" and prior_knowledge:\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\" and prior_knowledge:\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course.\")\n",
      "7/13:\n",
      "final_grade = 5.4\n",
      "exam_grade = 4.3\n",
      "resit_grade = 5.6\n",
      "\n",
      "final_grade > 5.5 and exam_grade > 5.5 or resit_grade > 5.5\n",
      "7/14: final_grade > 5.5 and (exam_grade > 5.5 or resit_grade > 5.5)\n",
      "7/15:\n",
      "driver_license = False\n",
      "age = 17\n",
      "coach = True\n",
      "\n",
      "if driver_license and age >= 18 or age == 17 and coach: \n",
      "    print(\"You're allowed to drive!\")    \n",
      "else: \n",
      "    print(\"You're not allowed to drive!\")\n",
      "7/16: # your code goes here!\n",
      "7/17:\n",
      "# solution\n",
      "if driver_license and (age >= 18 or age == 17 and coach): \n",
      "    print(\"You're allowed to drive!\")    \n",
      "else: \n",
      "    print(\"You're not allowed to drive!\")\n",
      "7/18:\n",
      "students_list = [[\"Lotte\", \"Marketing Analytics\", True], \n",
      "            [\"Joep\", \"Research Master\", True]]\n",
      "7/19:\n",
      "student_dict = {\"name\": \"Lotte\", \n",
      "                \"study\": \"Marketing Analytics\", \n",
      "                \"prior_knowledge\": True}\n",
      "\n",
      "print(student_dict)\n",
      "7/20:\n",
      "student_dict[\"age\"] = 23\n",
      "print(student_dict)\n",
      "7/21: print(student_dict[\"name\"])\n",
      "7/22:\n",
      "print(student_dict.get(\"name\"))\n",
      "print(student_dict.get(\"email\"))\n",
      "7/23:\n",
      "enrollments = [\n",
      "    {\n",
      "        \"course\": \"Online Data Collection & Management\",\n",
      "        \"instructor\": \"Hannes Datta\",\n",
      "        \"students\": [\n",
      "            {\n",
      "                \"name\": \"Lotte\",\n",
      "                \"study\": \"Marketing Analytics\",\n",
      "                \"prior_knowledge\": True\n",
      "                \n",
      "            }, \n",
      "            {\n",
      "                \"name\": \"Joep\",\n",
      "                \"study\": \"Research Master\",\n",
      "                \"prior_knowledge\": True,\n",
      "                \"honors\": \"The Societal Challenge of Migration\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "7/24:\n",
      "# your code goes here!\n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "7/25: print(enrollments[0][\"students\"][0].get(\"honors\"))\n",
      "7/26:\n",
      "# Question 1 \n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "7/27:\n",
      "# Question 2\n",
      "print(enrollments[0][\"students\"][0].get(\"honors\"))\n",
      "7/28: student_dict[\"email\"] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "7/29:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "enrollments\n",
      "7/30:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@uvt.edu\"\n",
      "enrollments\n",
      "7/31:\n",
      "enrollments.append({\n",
      "    \"course\": \"Data Preparation and Workflow Management\",\n",
      "    \"instructor\": \"Hannes Datta\",\n",
      "    \"students\":[{\n",
      "        \"name\": \"Sanne\",\n",
      "        \"study\": \"Marketing Analytics\",\n",
      "        \"prior_knowledge\": True \n",
      "    }]\n",
      "    })\n",
      "enrollments\n",
      "7/32: # your code goes here!\n",
      "7/33:\n",
      "# solution\n",
      "enrollments.append({\n",
      "    \"course\": \"Data Preparation and Workflow Management\",\n",
      "    \"instructor\": \"Hannes Datta\",\n",
      "    \"students\": [{\n",
      "        \"name\": \"Sanne\",\n",
      "        \"study\": \"Marketing Analytics\", \n",
      "        \"prior_knowledge\": True\n",
      "    }]\n",
      "})\n",
      "7/34:\n",
      "prices = [9.99, 3.95, 24.95, 8.95]\n",
      "\n",
      "for price in prices:\n",
      "    print(price)\n",
      "7/35:\n",
      "price_discount = []\n",
      "\n",
      "for price in prices:\n",
      "    price_dis = round((price * 0.85),2)\n",
      "    price_discount.append(price_dis)\n",
      "    \n",
      "print(price_discount)\n",
      "7/36:\n",
      "prices_discount = []\n",
      "\n",
      "for price in prices: \n",
      "    price_discount = price * 0.85\n",
      "    prices_discount.append(price_discount)\n",
      "\n",
      "print(prices_discount) # in a future lessson we learn you how to round figures!\n",
      "7/37: prices\n",
      "7/38:\n",
      "# your code goes here!\n",
      "sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "7/39:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "7/40:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "7/41:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "\n",
      "\n",
      "a = 24.95 * 0.9 + 9.99 * 0.8 + 8.95 * 0.7 + 3.95 * 0.6\n",
      "7/42:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "\n",
      "\n",
      "a = 24.95 * 0.9 + 9.99 * 0.8 + 8.95 * 0.7 + 3.95 * 0.6\n",
      "print(a)\n",
      "7/43:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "7/44:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "7/45:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "7/46:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "7/47:\n",
      "# first name, study, prior knowledge\n",
      "students = [[\"Lotte\", \"Marketing Analytics\", True], \n",
      "            [\"Joep\", \"Research Master\", True], \n",
      "            [\"Mirte\", \"Marketing Analytics\", False], \n",
      "            [\"Dirk\", \"Economics\", True], \n",
      "            [\"Sanne\", \"Marketing Analytics\", True], \n",
      "            [\"Roy\", \"Research Master\", False]]\n",
      "\n",
      "# your code goes here!\n",
      "names =[]\n",
      "\n",
      "for student in students:\n",
      "    if student[1] == \"Marketing Analytics\" or student[1] ==\"Research Master\" and student[2] == True:\n",
      "        names.append(student[0])\n",
      "\n",
      "print(names)\n",
      "7/48:\n",
      "# first name, study, prior knowledge\n",
      "students = [[\"Lotte\", \"Marketing Analytics\", True], \n",
      "            [\"Joep\", \"Research Master\", True], \n",
      "            [\"Mirte\", \"Marketing Analytics\", False], \n",
      "            [\"Dirk\", \"Economics\", True], \n",
      "            [\"Sanne\", \"Marketing Analytics\", True], \n",
      "            [\"Roy\", \"Research Master\", False]]\n",
      "\n",
      "# your code goes here!\n",
      "names =[]\n",
      "\n",
      "for student in students:\n",
      "    if (student[1] == \"Marketing Analytics\" or student[1] ==\"Research Master\") and student[2] == True:\n",
      "        names.append(student[0])\n",
      "\n",
      "print(names)\n",
      "7/49:\n",
      "# solution (alternative 2 - counter)\n",
      "for counter in range(len(students)): \n",
      "    if (students[counter][1] == \"Marketing Analytics\" or students[counter][1] == \"Research Master\") and students[counter][2]:\n",
      "        print(students[counter][0])\n",
      "7/50: ?range\n",
      "7/51: range(len(students))\n",
      "7/52:\n",
      "print(list(range(-10,10))) # from 1 to 9\n",
      "print(list(range(20,10))) # from 0 to 9\n",
      "7/53:\n",
      "print(list(range(1,10))) # from 1 to 9\n",
      "print(list(range(10))) # from 0 to 9\n",
      "7/54:\n",
      "print(list(range(3)))\n",
      "print(list(range(len(prices))))\n",
      "7/55:\n",
      "prices.append(10)\n",
      "print(prices)\n",
      "7/56:\n",
      "prices.append(10)\n",
      "print(list(range(len(prices))))\n",
      "7/57:\n",
      "prices.append(10)\n",
      "print(price)\n",
      "print(list(range(len(prices))))\n",
      "7/58:\n",
      "prices.append(10)\n",
      "print(prices)\n",
      "print(list(range(len(prices))))\n",
      "7/59:\n",
      "prices = [9.99, 3.95, 24.95]\n",
      "print(list(range(3)))\n",
      "print(list(range(len(prices))))\n",
      "7/60:\n",
      "prices.append(10)\n",
      "print(prices)\n",
      "print(list(range(len(prices))))\n",
      "7/61:\n",
      "for counter in range(len(prices)):\n",
      "    print(prices[counter]*.21)\n",
      "7/62:\n",
      "for counter in range(len(prices)):\n",
      "    print(prices[counter])\n",
      "7/63:\n",
      "prices = [9.99, 3.95, 24.95]\n",
      "print(list(range(3)))\n",
      "print(list(range(len(prices))))\n",
      "7/64:\n",
      "for counter in range(len(prices)):\n",
      "    print(prices[counter])\n",
      "7/65:\n",
      "for counter in range(len(prices)):\n",
      "    print(prices[counter]*.21)\n",
      "7/66:\n",
      "counter = 0\n",
      "while counter < len(prices):\n",
      "    print(prices[counter])\n",
      " 8/1:\n",
      "a = 1 \n",
      "b = 1\n",
      "print(a > b)\n",
      "print(a < b)\n",
      "print(a >= b)\n",
      "print(a <= b)\n",
      " 8/2: a > b\n",
      " 8/3:\n",
      "savings = 100 # variable assignment\n",
      "print(savings == 100) # comparison \n",
      "print(savings != 100) # comparison (False because it is 100!)\n",
      " 8/4:\n",
      "balance = -10\n",
      "if balance < 0: \n",
      "    print(\"You should top up your checking account to avoid paying interest\")\n",
      "elif balance == 0: \n",
      "    print(\"Your checking account balance is exactly ‚Ç¨0.00, be careful when making new payments!\")\n",
      "else: \n",
      "    print(\"You have a positive balance\")\n",
      " 8/5:\n",
      "# your code goes here!\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      " 8/6:\n",
      "# solution\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course.\")\n",
      " 8/7:\n",
      "workday = True\n",
      "no_holiday = False\n",
      "\n",
      "if workday and no_holiday:\n",
      "    print(\"Go to work!\")\n",
      "else:\n",
      "    print(\"Back to sleep!\")\n",
      " 8/8:\n",
      "tired = True\n",
      "bed_time = False\n",
      "\n",
      "if tired or bed_time:\n",
      "    print(\"Go to sleep!\")\n",
      " 8/9:\n",
      "weekend = False\n",
      "\n",
      "if not weekend:\n",
      "    print(\"Go to work!\")\n",
      "8/10:\n",
      "# your code goes here!\n",
      "prior_knowledge = True\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\" and prior_knowledge:\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\" and prior_knowledge:\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      "8/11:\n",
      "# solution (it's not necessary to check whether prior_knowledge == True)\n",
      "if study == \"Marketing Analytics\" and prior_knowledge:\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\" and prior_knowledge:\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course.\")\n",
      "8/12:\n",
      "final_grade = 5.4\n",
      "exam_grade = 4.3\n",
      "resit_grade = 5.6\n",
      "\n",
      "final_grade > 5.5 and exam_grade > 5.5 or resit_grade > 5.5\n",
      "8/13: final_grade > 5.5 and (exam_grade > 5.5 or resit_grade > 5.5)\n",
      "8/14:\n",
      "driver_license = False\n",
      "age = 17\n",
      "coach = True\n",
      "\n",
      "if driver_license and age >= 18 or age == 17 and coach: \n",
      "    print(\"You're allowed to drive!\")    \n",
      "else: \n",
      "    print(\"You're not allowed to drive!\")\n",
      "8/15: # your code goes here!\n",
      "8/16:\n",
      "# solution\n",
      "if driver_license and (age >= 18 or age == 17 and coach): \n",
      "    print(\"You're allowed to drive!\")    \n",
      "else: \n",
      "    print(\"You're not allowed to drive!\")\n",
      "8/17:\n",
      "students_list = [[\"Lotte\", \"Marketing Analytics\", True], \n",
      "            [\"Joep\", \"Research Master\", True]]\n",
      "8/18:\n",
      "student_dict = {\"name\": \"Lotte\", \n",
      "                \"study\": \"Marketing Analytics\", \n",
      "                \"prior_knowledge\": True}\n",
      "\n",
      "print(student_dict)\n",
      "8/19:\n",
      "student_dict[\"age\"] = 23\n",
      "print(student_dict)\n",
      "8/20: print(student_dict[\"name\"])\n",
      "8/21:\n",
      "print(student_dict.get(\"name\"))\n",
      "print(student_dict.get(\"email\"))\n",
      "8/22:\n",
      "enrollments = [\n",
      "    {\n",
      "        \"course\": \"Online Data Collection & Management\",\n",
      "        \"instructor\": \"Hannes Datta\",\n",
      "        \"students\": [\n",
      "            {\n",
      "                \"name\": \"Lotte\",\n",
      "                \"study\": \"Marketing Analytics\",\n",
      "                \"prior_knowledge\": True\n",
      "                \n",
      "            }, \n",
      "            {\n",
      "                \"name\": \"Joep\",\n",
      "                \"study\": \"Research Master\",\n",
      "                \"prior_knowledge\": True,\n",
      "                \"honors\": \"The Societal Challenge of Migration\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "8/23:\n",
      "# your code goes here!\n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "8/24: print(enrollments[0][\"students\"][0].get(\"honors\"))\n",
      "8/25:\n",
      "# Question 1 \n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "8/26:\n",
      "# Question 2\n",
      "print(enrollments[0][\"students\"][0].get(\"honors\"))\n",
      "8/27: student_dict[\"email\"] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "8/28:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "enrollments\n",
      "8/29:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@uvt.edu\"\n",
      "enrollments\n",
      "8/30:\n",
      "enrollments.append({\n",
      "    \"course\": \"Data Preparation and Workflow Management\",\n",
      "    \"instructor\": \"Hannes Datta\",\n",
      "    \"students\":[{\n",
      "        \"name\": \"Sanne\",\n",
      "        \"study\": \"Marketing Analytics\",\n",
      "        \"prior_knowledge\": True \n",
      "    }]\n",
      "    })\n",
      "enrollments\n",
      "8/31: # your code goes here!\n",
      "8/32:\n",
      "# solution\n",
      "enrollments.append({\n",
      "    \"course\": \"Data Preparation and Workflow Management\",\n",
      "    \"instructor\": \"Hannes Datta\",\n",
      "    \"students\": [{\n",
      "        \"name\": \"Sanne\",\n",
      "        \"study\": \"Marketing Analytics\", \n",
      "        \"prior_knowledge\": True\n",
      "    }]\n",
      "})\n",
      "8/33:\n",
      "prices = [9.99, 3.95, 24.95, 8.95]\n",
      "\n",
      "for price in prices:\n",
      "    print(price)\n",
      "8/34:\n",
      "price_discount = []\n",
      "\n",
      "for price in prices:\n",
      "    price_dis = round((price * 0.85),2)\n",
      "    price_discount.append(price_dis)\n",
      "    \n",
      "print(price_discount)\n",
      "8/35:\n",
      "prices_discount = []\n",
      "\n",
      "for price in prices: \n",
      "    price_discount = price * 0.85\n",
      "    prices_discount.append(price_discount)\n",
      "\n",
      "print(prices_discount) # in a future lessson we learn you how to round figures!\n",
      "8/36: prices\n",
      "8/37:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "8/38:\n",
      "# solution (it's a bit more difficult, so no worries if you don't get it right away!)\n",
      "total_price = 0\n",
      "discount_rate = 0.10\n",
      "\n",
      "prices = sorted(prices, reverse=True)\n",
      "\n",
      "for price in prices: \n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += 0.10\n",
      "\n",
      "print(total_price)\n",
      "8/39:\n",
      "# first name, study, prior knowledge\n",
      "students = [[\"Lotte\", \"Marketing Analytics\", True], \n",
      "            [\"Joep\", \"Research Master\", True], \n",
      "            [\"Mirte\", \"Marketing Analytics\", False], \n",
      "            [\"Dirk\", \"Economics\", True], \n",
      "            [\"Sanne\", \"Marketing Analytics\", True], \n",
      "            [\"Roy\", \"Research Master\", False]]\n",
      "\n",
      "# your code goes here!\n",
      "names =[]\n",
      "\n",
      "for student in students:\n",
      "    if (student[1] == \"Marketing Analytics\" or student[1] ==\"Research Master\") and student[2]:\n",
      "        names.append(student[0])\n",
      "\n",
      "print(names)\n",
      "8/40:\n",
      "# solution (most elegant)\n",
      "for student in students: \n",
      "    if student[1] in [\"Marketing Analytics\", \"Research Master\"] and student[2]:\n",
      "        print(student[0])\n",
      "8/41:\n",
      "# solution (alternative 1 - iterator)\n",
      "for student in students: \n",
      "    if (student[1] == \"Marketing Analytics\" or student[1] == \"Research Master\") and student[2]: # mind the brackets!\n",
      "        print(student[0])\n",
      "8/42:\n",
      "# solution (alternative 2 - counter)\n",
      "for counter in range(len(students)): \n",
      "    if (students[counter][1] == \"Marketing Analytics\" or students[counter][1] == \"Research Master\") and students[counter][2]:\n",
      "        print(students[counter][0])\n",
      "8/43:\n",
      "print(list(range(1,10))) # from 1 to 9\n",
      "print(list(range(10))) # from 0 to 9\n",
      "8/44:\n",
      "prices = [9.99, 3.95, 24.95]\n",
      "print(list(range(3)))\n",
      "print(list(range(len(prices))))\n",
      "8/45:\n",
      "prices.append(10)\n",
      "print(prices)\n",
      "print(list(range(len(prices))))\n",
      "8/46:\n",
      "for counter in range(len(prices)):\n",
      "    print(prices[counter])\n",
      "8/47:\n",
      "for counter in range(len(prices)):\n",
      "    print(prices[counter])\n",
      "8/48: print(prices)\n",
      "8/49:\n",
      "prices = prices[0:2]\n",
      "prices\n",
      "8/50:\n",
      "prices = prices[0:3]\n",
      "prices\n",
      "8/51:\n",
      "prices = prices[0:3]\n",
      "prices\n",
      "8/52:\n",
      "prices = [9.99, 3.95, 24.95]\n",
      "print(list(range(3)))\n",
      "print(list(range(len(prices))))\n",
      "8/53: prices\n",
      "8/54:\n",
      "prices.append(10)\n",
      "for counter in range(len(prices)):\n",
      "    print(prices[counter]*.21)\n",
      "8/55:\n",
      "while counter < 100:\n",
      "    print(\\U0001f600)\n",
      "8/56:\n",
      "while counter < 100:\n",
      "    print(\"\\U0001f600\")\n",
      "8/57:\n",
      "while counter in range(10):\n",
      "    print(\"\\U0001f600\" * counter)\n",
      "8/58:\n",
      "for counter in range(10):\n",
      "    print(\"\\U0001f600\" * counter)\n",
      "8/59:\n",
      "# solution\n",
      "for counter in range(10):\n",
      "    print(\"\\U0001f600\" * 10)\n",
      "8/60:\n",
      "for counter in range(10):\n",
      "    print(\"\\U0001f600\" * 10)\n",
      "8/61: student_dict\n",
      "8/62:\n",
      "for key in student_dict.keys(): \n",
      "    print(key)\n",
      "8/63: student_dict\n",
      "8/64: student_dict.keys()\n",
      "8/65:\n",
      "student_dict.keys()\n",
      "student_dict.value()\n",
      "8/66:\n",
      "student_dict.keys()\n",
      "student_dict.values()\n",
      "8/67:\n",
      "student_dict.keys()\n",
      "print(student_dict.values())\n",
      "8/68:\n",
      "student_dict.keys()\n",
      "\n",
      "for value in student_dict.values():\n",
      "    print(student_dict.values())\n",
      "8/69:\n",
      "student_dict.keys()\n",
      "\n",
      "for value in student_dict.values():\n",
      "    print(value)\n",
      "8/70:\n",
      "student_dict.keys()\n",
      "student_dict.values()\n",
      "8/71:\n",
      "student_dict.keys()\n",
      "student_dict.values()\n",
      "student_dict.items()\n",
      "8/72:\n",
      "print(student_dict.keys())\n",
      "print(student_dict.values())\n",
      "print(student_dict.items())\n",
      "8/73:\n",
      "print(student_dict.keys())\n",
      "print(student_dict.values())\n",
      "print(student_dict.items())\n",
      "\n",
      "for item in student_dict.items():\n",
      "    print (item)\n",
      "8/74:\n",
      "print(student_dict.keys())\n",
      "print(student_dict.values())\n",
      "8/75:\n",
      "print(student_dict.items())\n",
      "\n",
      "for item in student_dict.items():\n",
      "    print (item)\n",
      "    \n",
      "for key, value in student_dict.items():\n",
      "    print (key, value)\n",
      "8/76:\n",
      "print(student_dict.items())\n",
      "\n",
      "for item in student_dict.items():\n",
      "    print (item)\n",
      "8/77:\n",
      "for key, value in student_dict.items():\n",
      "    print (key, value)\n",
      "8/78:\n",
      "students_grades = {\"Lotte\": 6.7, \"Joep\": 7.2, \"Mirte\": 9.3, \"Dirk\": 5.2, \"Sanne\": 7.5, \"Roy\": 6.9}\n",
      "# your code goes here\n",
      "8/79:\n",
      "avg = 0\n",
      "\n",
      "for counter in range(len(students_grades))\n",
      "    total += students_grades.values(counter)\n",
      "    avg = total/(counter+1)\n",
      "    counter += 1\n",
      "    print(avg)\n",
      "8/80:\n",
      "students_grades = {\"Lotte\": 6.7, \"Joep\": 7.2, \"Mirte\": 9.3, \"Dirk\": 5.2, \"Sanne\": 7.5, \"Roy\": 6.9}\n",
      "# your code goes here\n",
      "8/81:\n",
      "students_grades = {\"Lotte\": 6.7, \"Joep\": 7.2, \"Mirte\": 9.3, \"Dirk\": 5.2, \"Sanne\": 7.5, \"Roy\": 6.9}\n",
      "# your code goes here\n",
      "8/82: range(len(students_grades))\n",
      "8/83: print(range(len(students_grades)))\n",
      "8/84:\n",
      "avg = 0\n",
      "for counter in range(len(students_grades))\n",
      "    total += students_grades.values(counter)\n",
      "    avg = total/(counter+1)\n",
      "    counter += 1\n",
      "    print(avg)\n",
      "8/85:\n",
      "avg = 0\n",
      "for counter in range(len(students_grades)):\n",
      "    total += students_grades.values(counter)\n",
      "    avg = total/(counter+1)\n",
      "    counter += 1\n",
      "    print(avg)\n",
      "8/86:\n",
      "avg = 0\n",
      "total = 0\n",
      "for counter in range(len(students_grades)):\n",
      "    total += students_grades.values(counter)\n",
      "    avg = total/(counter+1)\n",
      "    counter += 1\n",
      "    print(avg)\n",
      "8/87:\n",
      "avg = 0\n",
      "total = 0\n",
      "for counter in range(len(students_grades)):\n",
      "    total += students_grades.values(counter)\n",
      "    avg = total/(counter+1)\n",
      "    counter += 1\n",
      "    print(avg)\n",
      "8/88:\n",
      "avg = 0\n",
      "total = 0\n",
      "students_grades.values(1)\n",
      "for counter in range(len(students_grades)):\n",
      "    total += students_grades.values(counter)\n",
      "    avg = total/(counter+1)\n",
      "    counter += 1\n",
      "    print(avg)\n",
      "8/89:\n",
      "avg = 0\n",
      "total = 0\n",
      "students_grades.values()\n",
      "for counter in range(len(students_grades)):\n",
      "    total += students_grades.values(counter)\n",
      "    avg = total/(counter+1)\n",
      "    counter += 1\n",
      "    print(avg)\n",
      "8/90: range(len(students_grades)))\n",
      "8/91:\n",
      "students_grades = {\"Lotte\": 6.7, \"Joep\": 7.2, \"Mirte\": 9.3, \"Dirk\": 5.2, \"Sanne\": 7.5, \"Roy\": 6.9}\n",
      "# your code goes here\n",
      "8/92: range(len(students_grades))\n",
      "8/93: students_grades.values[0]\n",
      "8/94: students_grades.values()\n",
      "8/95: students_grades.values()[0]\n",
      "8/96:\n",
      "students_grades.values()\n",
      "dict_values[0]\n",
      "8/97: students_grades.values()\n",
      "8/98: type(students_grades.values())\n",
      "8/99:\n",
      "total = 0\n",
      "\n",
      "for value in students_grades.values():\n",
      "    print(value)\n",
      "8/100:\n",
      "total = 0\n",
      "\n",
      "for value in students_grades.values():\n",
      "    total += value\n",
      "    \n",
      "avg = total/len(students_grades)\n",
      "8/101:\n",
      "total = 0\n",
      "\n",
      "for value in students_grades.values():\n",
      "    total += value\n",
      "    \n",
      "avg = total/len(students_grades)\n",
      "\n",
      "print(avg)\n",
      "8/102:\n",
      "total = 0\n",
      "\n",
      "for value in students_grades.values():\n",
      "    total += value\n",
      "    \n",
      "total/len(students_grades)\n",
      "8/103: print(enrollments)\n",
      "8/104: enrollments\n",
      "8/105: enrollments <- enrollments[0:3]\n",
      "8/106: enrollments <- enrollments[0,3]\n",
      "8/107: enrollments <- enrollments[0]\n",
      "8/108: enrollments[0]\n",
      "8/109: enrollments[1]\n",
      "8/110: enrollments[0:1]\n",
      "8/111: enrollments[0:3]\n",
      "8/112: enrollments[0:2]\n",
      "8/113: enrollments = enrollments[0:2]\n",
      "8/114:\n",
      "enrollments = enrollments[0:2]\n",
      "print(enrollments)\n",
      "8/115:\n",
      "enrollments = enrollments[0:2]\n",
      "enrollments\n",
      "8/116:\n",
      "name = []\n",
      "\n",
      "for course in enrollments:\n",
      "    for student in enrollments[course][\"students\"]:\n",
      "        name.append(enrollments[course][\"students\"][\"name\"])\n",
      "8/117:\n",
      "name = []\n",
      "\n",
      "for course in enrollments:\n",
      "    for student in course[\"students\"]:\n",
      "        name.append(student[\"name\"])\n",
      "8/118:\n",
      "name = []\n",
      "for course in enrollments:\n",
      "    for student in course[\"students\"]:\n",
      "        name.append(student[\"name\"])\n",
      "8/119:\n",
      "name = []\n",
      "for course in enrollments:\n",
      "    for student in course[\"students\"]:\n",
      "        name.append(student[\"name\"])\n",
      "8/120:\n",
      "name = []\n",
      "for course in enrollments:\n",
      "    for student in course[\"students\"]:\n",
      "        name.append(student[\"name\"])\n",
      "print(name)\n",
      "8/121:\n",
      "prices_dict = {\"beer\": 9.99, \"meat\": 3.95, \"toaster\": 24.95}\n",
      "prices_dict_discount = {}\n",
      "\n",
      "for product, price in prices_dict.items():\n",
      "    price_discount = price * 0.85\n",
      "    prices_dict_discount[product] = price_discount\n",
      "    \n",
      "prices_dict_discount\n",
      "8/122:\n",
      "prices_dict = {\"beer\": 9.99, \"meat\": 3.95, \"toaster\": 24.95}\n",
      "\n",
      "price_list = {}\n",
      "for product, price in prices_dict:\n",
      "    price_dis = price * 0.85\n",
      "    price_list[product] = price_dis\n",
      "\n",
      "price_list\n",
      "8/123:\n",
      "prices_dict = {\"beer\": 9.99, \"meat\": 3.95, \"toaster\": 24.95}\n",
      "price_list = {}\n",
      "for product, price in prices_dict.items():\n",
      "    price_dis = price * 0.85\n",
      "    price_list[product] = price_dis\n",
      "\n",
      "price_list\n",
      "8/124:\n",
      "# solution\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "    for student in course[\"students\"]:\n",
      "        students_enrolled.append(student[\"name\"])\n",
      "    \n",
      "print(students_enrolled)\n",
      "8/125: [students_enrolled.append(student[\"name\"]) for course in enrollments, student in course[\"students\"]]\n",
      "8/126: [students_enrolled.append(student[\"name\"]) for course in enrollments: for student in course[\"students\"]]\n",
      "8/127:\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "    [students_enrolled.append(student[\"name\"]) for student in course[\"students\"]]\n",
      "8/128:\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "    print[students_enrolled.append(student[\"name\"]) for student in course[\"students\"]]\n",
      "8/129:\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "    print(students_enrolled.append(student[\"name\"]) for student in course[\"students\"]])\n",
      "8/130:\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "    print(students_enrolled.append(student[\"name\"]) for student in course[\"students\"])\n",
      "8/131:\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "   students_enrolled.append(student[\"name\"]) for student in course[\"students\"]\n",
      "8/132:\n",
      "students_enrolled = []\n",
      "for course in enrollments: \n",
      "   [students_enrolled.append(student[\"name\"])for student in course[\"students\"]]\n",
      "8/133:\n",
      "students_enrolled = []\n",
      "for course in enrollments: \n",
      "   print[students_enrolled.append(student[\"name\"])for student in course[\"students\"]]\n",
      "8/134:\n",
      "students_enrolled = []\n",
      "for course in enrollments: \n",
      "   [students_enrolled.append(student[\"name\"])for student in course[\"students\"]]\n",
      "print(students_enrolled)\n",
      "8/135:\n",
      "students_enrolled = []\n",
      "[students_enrolled.append(student[\"name\"])for student in course[\"students\"] for course in enrollments]\n",
      "print(students_enrolled)\n",
      "8/136:\n",
      "enrollments = enrollments[0:2]\n",
      "enrollments\n",
      "8/137:\n",
      "students_enrolled = []\n",
      "[students_enrolled.append(student[\"name\"])for student in course[\"students\"] for course in enrollments]\n",
      "print(students_enrolled)\n",
      "8/138:\n",
      "# solution\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "    for student in course[\"students\"]:\n",
      "        students_enrolled.append(student[\"name\"])\n",
      "    \n",
      "print(students_enrolled)\n",
      "8/139: enrollments\n",
      "8/140:\n",
      "students_enrolled = []\n",
      "[students_enrolled.append(student[\"name\"])for student in course[\"students\"] for course in enrollments]\n",
      "print(students_enrolled)\n",
      "8/141:\n",
      "students_enrolled = []\n",
      "[students_enrolled.append(student[\"name\"])for student in course[\"students\"] for course in enrollments]\n",
      "8/142: enrollments\n",
      "8/143:\n",
      "students_enrolled = []\n",
      "[students_enrolled.append(student[\"name\"])for student in course[\"students\"] for course in enrollments]\n",
      "8/144:\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "   [student[\"name\"]for student in course[\"students\"]]\n",
      "8/145:\n",
      "for course in enrollments: \n",
      "   print([student[\"name\"]for student in course[\"students\"]])\n",
      "8/146: print([student[\"name\"]for student in course[\"students\"] for course in enrollments]\n",
      "8/147: [student[\"name\"]for student in course[\"students\"] for course in enrollments]\n",
      "8/148:\n",
      "for course in enrollments:\n",
      "    [student[\"name\"]for student in course[\"students\"]]\n",
      "8/149:\n",
      "for course in enrollments:\n",
      "    print([student[\"name\"]for student in course[\"students\"]])\n",
      "8/150:\n",
      "for course in enrollments:\n",
      "    name =[student[\"name\"]for student in course[\"students\"]]\n",
      "8/151:\n",
      "for course in enrollments:\n",
      "    name =[student[\"name\"]for student in course[\"students\"]]\n",
      "    \n",
      "name\n",
      "8/152:\n",
      "for course in enrollments:\n",
      "    name +=[student[\"name\"]for student in course[\"students\"]]\n",
      "    \n",
      "name\n",
      "8/153:\n",
      "name = []\n",
      "for course in enrollments:\n",
      "    name +=[student[\"name\"]for student in course[\"students\"]]\n",
      "    \n",
      "name\n",
      "8/154:\n",
      "# solution\n",
      "total = 0\n",
      "for value in students_grades.values(): \n",
      "    total += value\n",
      "total/len(students_grades)\n",
      "8/155:\n",
      "total += value for value in students_grades.values()\n",
      "avg = total/len(students_grades)\n",
      "8/156:\n",
      "total = 0\n",
      "total+= value for value in students_grades.values()\n",
      "avg = total/len(students_grades)\n",
      "8/157:\n",
      "total = 0\n",
      "[total+= value for value in students_grades.values()]\n",
      "avg = total/len(students_grades)\n",
      "8/158:\n",
      "total = 0\n",
      "(total+= value for value in students_grades.values())\n",
      "avg = total/len(students_grades)\n",
      "8/159:\n",
      "total = 0\n",
      "total+= value for value in students_grades.values()\n",
      "avg = total/len(students_grades)\n",
      "8/160:\n",
      "total = 0\n",
      "total += [value for value in students_grades.values()]\n",
      "avg = total/len(students_grades)\n",
      "8/161:\n",
      "total = 0\n",
      "total += (value for value in students_grades.values())\n",
      "avg = total/len(students_grades)\n",
      "8/162:\n",
      "total = 0\n",
      "total += value for value in students_grades.values()\n",
      "avg = total/len(students_grades)\n",
      "8/163:\n",
      "total = 0\n",
      "[value for value in students_grades.values()]\n",
      "8/164:\n",
      "total = 0\n",
      "[value for value in students_grades.values()][0]\n",
      "8/165: total += [value for value in students_grades.values()]\n",
      "8/166: total += value for value in students_grades.values()\n",
      "8/167: value for value in students_grades.values()\n",
      "8/168: [value for value in students_grades.values()]\n",
      "8/169: [students_grades.values()]\n",
      "8/170: [value for value in students_grades.values()]\n",
      "8/171: total += [value for value in students_grades.values()]\n",
      "8/172: total += [value for value in students_grades.values()][value]\n",
      "8/173:\n",
      "name = []\n",
      "name +=[student[\"name\"]for student in course[\"students\"]] for course in enrollments\n",
      "    \n",
      "name\n",
      "8/174:\n",
      "name = []\n",
      "name +=[student[\"name\"] for course in enrollments for student in course[\"students\"]] \n",
      "    \n",
      "name\n",
      "8/175:\n",
      "name =[student[\"name\"] for course in enrollments for student in course[\"students\"]] \n",
      "    \n",
      "name\n",
      "8/176: name =[student[\"name\"] for course in enrollments for student in course[\"students\"]]\n",
      "8/177:\n",
      "name =[student[\"name\"] for course in enrollments for student in course[\"students\"]] \n",
      "name\n",
      "8/178:\n",
      "book_price_usd = 19\n",
      "book_price_eur = book_price_usd * .82 \n",
      "print(f\"The price in euros is {book_price_eur}\")\n",
      "\n",
      "tablet_price_usd = 349\n",
      "tablet_price_eur = tablet_price_usd * .82 \n",
      "print(f\"The price in euros is {tablet_price_eur}\")\n",
      "\n",
      "laptop_price_usd = 699\n",
      "laptop_price_eur = laptop_price_usd * .82 \n",
      "print(f\"The price in euros is {laptop_price_eur}\")\n",
      "8/179:\n",
      "book_price_usd = 19\n",
      "book_price_eur = book_price_usd * .82 \n",
      "print(\"The price in euros is {book_price_eur}\")\n",
      "\n",
      "tablet_price_usd = 349\n",
      "tablet_price_eur = tablet_price_usd * .82 \n",
      "print(f\"The price in euros is {tablet_price_eur}\")\n",
      "\n",
      "laptop_price_usd = 699\n",
      "laptop_price_eur = laptop_price_usd * .82 \n",
      "print(f\"The price in euros is {laptop_price_eur}\")\n",
      "8/180:\n",
      "book_price_usd = 19\n",
      "book_price_eur = book_price_usd * .82 \n",
      "print(f\"The price in euros is {book_price_eur}\")\n",
      "\n",
      "tablet_price_usd = 349\n",
      "tablet_price_eur = tablet_price_usd * .82 \n",
      "print(f\"The price in euros is {tablet_price_eur}\")\n",
      "\n",
      "laptop_price_usd = 699\n",
      "laptop_price_eur = laptop_price_usd * .82 \n",
      "print(f\"The price in euros is {laptop_price_eur}\")\n",
      "8/181:\n",
      "def sing_happy_birthday(): \n",
      "    print(\"Happy Birthday To You\")\n",
      "    print(\"Happy Birthday To You\")\n",
      "    print(\"Happy Birthday Dear You\")\n",
      "    print(\"Happy Birthday To You\")\n",
      "\n",
      "sing_happy_birthday()\n",
      "8/182:\n",
      "sing_happy_birthday()\n",
      "sing_happy_birthday()\n",
      "sing_happy_birthday()\n",
      "8/183:\n",
      "instructor = \"Hannes\"\n",
      "\n",
      "def say_hello():\n",
      "    print(\"Hello\" + instructor)\n",
      "    \n",
      "print(instructor)\n",
      "8/184:\n",
      "instructor = \"Hannes\"\n",
      "\n",
      "def say_hello():\n",
      "    print(\"Hello\" + instructor)\n",
      "8/185:\n",
      "instructor = \"Hannes\"\n",
      "\n",
      "def say_hello():\n",
      "    print(\"Hello\" + instructor)\n",
      "    \n",
      "say_hello()\n",
      "print(instructor)\n",
      "8/186:\n",
      "instructor = \"Hannes\"\n",
      "\n",
      "def say_hello():\n",
      "    print(\"Hello \" + instructor)\n",
      "    \n",
      "say_hello()\n",
      "print(instructor)\n",
      "8/187:\n",
      "def say_hello(first_name):\n",
      "    print(\"Hello \" + first_name)\n",
      "    \n",
      "say_hello(\"Hannes\")\n",
      "8/188:\n",
      "def say_hello(first_name, last_name):\n",
      "    print(\"Hello \" + first_name + last_name)\n",
      "\n",
      "say_hello(Trang, Bui)\n",
      "8/189:\n",
      "def say_hello(first_name, last_name):\n",
      "    print(\"Hello \" + first_name + last_name)\n",
      "\n",
      "say_hello(\"Trang\", \"Bui\")\n",
      "8/190:\n",
      "def say_hello(first_name, last_name):\n",
      "    print(\"Hello \" + first_name +\" \"+ last_name)\n",
      "\n",
      "say_hello(\"Trang\", \"Bui\")\n",
      "8/191:\n",
      "def add(a,b):\n",
      "    print(result = a + b)\n",
      "\n",
      "add(3,4)\n",
      "8/192:\n",
      "def add(a,b):\n",
      "    print(a + b)\n",
      "\n",
      "add(3,4)\n",
      "8/193:\n",
      "def add(a,b):\n",
      "    print(a + b)\n",
      "\n",
      "add(5,-2)\n",
      "8/194:\n",
      "def add(a,b):\n",
      "    return a + b\n",
      "\n",
      "add(5,-2)\n",
      "8/195:\n",
      "def add(a,b):\n",
      "    return a + b\n",
      "\n",
      "add(add(5,-2),4)\n",
      "8/196:\n",
      "def add(a,b):\n",
      "    return a + b\n",
      "\n",
      "add(add(add(5,-2),4),3)\n",
      "8/197:\n",
      "# your code goes here!\n",
      "day_of_the_week = [Mon, Tue, Wed, Thu, Fri, Sat, Sun]\n",
      "def return_day(x):\n",
      "    if x > 0 and x < 8:\n",
      "        return day_of_the_week[x-1]\n",
      "    else:\n",
      "        return print(\"None\")\n",
      "\n",
      "return_day(1)\n",
      "8/198:\n",
      "# your code goes here!\n",
      "day_of_the_week = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
      "def return_day(x):\n",
      "    if x > 0 and x < 8:\n",
      "        return day_of_the_week[x-1]\n",
      "    else:\n",
      "        return print(\"None\")\n",
      "\n",
      "return_day(1)\n",
      "8/199:\n",
      "# your code goes here!\n",
      "day_of_the_week = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
      "def return_day(x):\n",
      "    if x > 0 and x < 8:\n",
      "        return day_of_the_week[x-1]\n",
      "    else:\n",
      "        return print(\"None\")\n",
      "\n",
      "return_day(8)\n",
      "8/200:\n",
      "# your code goes here!\n",
      "day_of_the_week = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
      "def return_day(x):\n",
      "    if x > 0 and x < 8:\n",
      "        return day_of_the_week[x-1]\n",
      "    else:\n",
      "        return print(\"None\")\n",
      "\n",
      "return_day(7)\n",
      "8/201:\n",
      "# your code goes here!\n",
      "day_of_the_week = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
      "def return_day(x):\n",
      "    if x > 0 and x < 8:\n",
      "        return day_of_the_week[x-1]\n",
      "    else:\n",
      "        return print(\"None\")\n",
      "\n",
      "return_day(1)\n",
      "8/202:\n",
      "# your code goes here!\n",
      "day_of_the_week = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
      "def return_day(x):\n",
      "    if x > 0 and x < 8:\n",
      "        return day_of_the_week[x-1]\n",
      "    else:\n",
      "        return print(\"None\")\n",
      "\n",
      "return_day(8)\n",
      "8/203:\n",
      "# your code goes here!\n",
      "day_of_the_week = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
      "def return_day(x):\n",
      "    if x > 0 and x < 8:\n",
      "        return day_of_the_week[x-1]\n",
      "    else:\n",
      "        return print(\"None\")\n",
      "\n",
      "return_day(8)\n",
      "8/204:\n",
      "def return_day(x):\n",
      "    days={1 : \"Mon\", 2: \"Tue\", 3: \"Wed\", 4 : \"Thu\", 5 : \"Fri\", 6: \"Sat\", 7: \"Sun\"}\n",
      "    return days.get(num)\n",
      "return_day(1)\n",
      "8/205:\n",
      "def return_day(x):\n",
      "    days={1 : \"Mon\", 2: \"Tue\", 3: \"Wed\", 4 : \"Thu\", 5 : \"Fri\", 6: \"Sat\", 7: \"Sun\"}\n",
      "    return days.get(x)\n",
      "return_day(1)\n",
      "8/206:\n",
      "def return_day(x):\n",
      "    days={1 : \"Mon\", 2: \"Tue\", 3: \"Wed\", 4 : \"Thu\", 5 : \"Fri\", 6: \"Sat\", 7: \"Sun\"}\n",
      "    return days.get(x)\n",
      "return_day(8)\n",
      "8/207:\n",
      "def return_day(x):\n",
      "    days={1 : \"Mon\", 2: \"Tue\", 3: \"Wed\", 4 : \"Thu\", 5 : \"Fri\", 6: \"Sat\", 7: \"Sun\"}\n",
      "    return days.get(x)\n",
      "print(return_day(8))\n",
      "8/208:\n",
      "def convert_usd_eur(usd_amounts, currency_rate):\n",
      "    eur_amounts = []\n",
      "    for usd_amount in usd_amounts: \n",
      "        eur_amounts.append(usd_amount * currency_rate)\n",
      "        return eur_amounts\n",
      "        \n",
      "convert_usd_eur([10, 100, 1000], .82)\n",
      "8/209:\n",
      "def convert2(usd, rate):\n",
      "    eur = []\n",
      "    [eur = usd * rate for usd in usd]\n",
      "    return eur\n",
      "\n",
      "covert2([10, 100, 1000], .82)\n",
      "8/210:\n",
      "def convert2(usd, rate):\n",
      "    eur = [usd * rate for usd in usd]\n",
      "    return eur\n",
      "\n",
      "covert2([10, 100, 1000], .82)\n",
      "8/211:\n",
      "def convert2(usd, rate):\n",
      "    return [usd * rate for usd in usd]\n",
      "    \n",
      "covert2([10, 100, 1000], .82)\n",
      "8/212:\n",
      "def convert2(usd, rate):\n",
      "    return [usd * rate for usd in usd]\n",
      "    \n",
      "convert2([10, 100, 1000], .82)\n",
      "8/213:\n",
      "def convert_usd_eur(usd_amounts, currency_rate):\n",
      "    eur_amounts = []\n",
      "    for usd_amount in usd_amounts: \n",
      "        eur_amounts.append(usd_amount * currency_rate)\n",
      "    return eur_amounts\n",
      "        \n",
      "convert_usd_eur([10, 100, 1000], .82)\n",
      "8/214:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        return total += num_list[num] \n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/215:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        return total = total + num_list[num]\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/216:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        return total = total + num_list[num]\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/217:\n",
      "num_list = [3, -4, 1, 2, 0]\n",
      "num_list[0]\n",
      "8/218:\n",
      "num_list = [3, -4, 1, 2, 0]\n",
      "num_list[0] + num_list[1]\n",
      "8/219:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        return total + num_list[num]\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/220:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        total += num_list[num]\n",
      "    return total\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/221:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        total += num_list[num]\n",
      "        return total\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/222:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        return total + num\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/223:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        total += num\n",
      "        return total\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/224:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        total += num\n",
      "    return total\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/225:\n",
      "import os\n",
      "os.getcwd()\n",
      "8/226:\n",
      "# solution\n",
      "def add_list(num_list):\n",
      "    \"\"\"adding up all the numbers in the list\"\"\"\n",
      "    total = 0\n",
      "    for num in num_list: \n",
      "        total += num\n",
      "    return total\n",
      "\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "10/1: import requests\n",
      "10/2: import requests\n",
      "10/3: url = \"https://www.twitch.tv/directory/game/The%20Sims%204\"\n",
      "10/4: url = \"https://www.twitch.tv/directory/game/The%20Sims%204\"\n",
      "10/5:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "10/6:\n",
      "url = \"https://www.twitch.tv/directory/game/The%20Sims%204\"\n",
      "request = request.get(url)\n",
      "10/7:\n",
      "url = \"https://www.twitch.tv/directory/game/The%20Sims%204\"\n",
      "request = requests.get(url)\n",
      "10/8: source.code = request.test\n",
      "10/9: source.code = request.test\n",
      "10/10: source.code = request.text\n",
      "10/11:\n",
      "url = \"https://www.twitch.tv/directory/game/The%20Sims%204\"\n",
      "request = requests.get(url)\n",
      "10/12: source.code = request.text\n",
      "10/13: src = request.text\n",
      "10/14:\n",
      "src = request.text\n",
      "\n",
      "soup = BeautifulSoup(src)\n",
      "10/15:\n",
      "src = request.text\n",
      "\n",
      "soup = BeautifulSoup(src)\n",
      "print(soup.find('h1'))\n",
      "10/16:\n",
      "src = request.text\n",
      "\n",
      "soup = BeautifulSoup(src)\n",
      "print(soup.find('h3'))\n",
      "10/17:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "10/18:\n",
      "url = \"https://www.twitch.tv/directory/game/The%20Sims%204\"\n",
      "request = requests.get(url)\n",
      "10/19:\n",
      "src = request.text\n",
      "\n",
      "soup = BeautifulSoup(src)\n",
      "print(soup.find('h3'))\n",
      "10/20:\n",
      "src = request.text\n",
      "\n",
      "soup = BeautifulSoup(src)\n",
      "print(soup.find('h1 class=\"CoreText-sc-cpl358-0 ScTitleText-sc-1gsen4-0 gAYdrP tw-title'))\n",
      "10/21:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "10/22:\n",
      "url = \"https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=sustainability&oq=\"\n",
      "request = requests.get(url)\n",
      "10/23:\n",
      "src = request.text\n",
      "\n",
      "soup = BeautifulSoup(src)\n",
      "print(soup.find('h3'))\n",
      "10/24:\n",
      "src = request.text\n",
      "\n",
      "soup = BeautifulSoup(src)\n",
      "print(soup.find_all('h3'))\n",
      "12/1: import requests\n",
      "12/2: import requests\n",
      "12/3: import requests\n",
      "12/4:\n",
      "url = \"https://www.hornbach.nl/shop/Planten/Tuinplanten/Vaste-planten/S5120/artikeloverzicht.html\"\n",
      "plant_request = request.get(url)\n",
      "12/5:\n",
      "url = \"https://www.hornbach.nl/shop/Planten/Tuinplanten/Vaste-planten/S5120/artikeloverzicht.html\"\n",
      "plant_request = requests.get(url)\n",
      "12/6: plant_src = plant_request.text\n",
      "12/7:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src)\n",
      "12/8:\n",
      "plant_src = plant_request.text\n",
      "type(plant_request)\n",
      "12/9: ?type\n",
      "12/10: print(plant_src[1:10])\n",
      "12/11:\n",
      "plant_src = plant_request.text\n",
      "print(plant_request)\n",
      "12/12:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src)\n",
      "12/13: print(plant_src[5000:10000])\n",
      "12/14:\n",
      "a = \"<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n",
      "<!DOCTYPE html>\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"nl\" id=\"ng-app\">\n",
      "\n",
      "\n",
      "<head>\n",
      "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=EDGE\" />\n",
      "    <meta name=\"csrf-token\" content=\"V4W7-KLVS-P9PL-WFST-LKYY-WGO6-K83E-WUQC\" />\n",
      "    <title>\"\n",
      "len(a)\n",
      "12/15:\n",
      "a = '<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n",
      "<!DOCTYPE html>\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"nl\" id=\"ng-app\">\n",
      "\n",
      "\n",
      "<head>\n",
      "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=EDGE\" />\n",
      "    <meta name=\"csrf-token\" content=\"V4W7-KLVS-P9PL-WFST-LKYY-WGO6-K83E-WUQC\" />\n",
      "    <title>'\n",
      "len(a)\n",
      "12/16:\n",
      "a = \"<?xml version=\"1.0\" encoding=\"utf-8\" ?><!DOCTYPE html><html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"nl\" id=\"ng-app\">\n",
      "<head>  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=EDGE\" />\n",
      "    <meta name=\"csrf-token\" content=\"V4W7-KLVS-P9PL-WFST-LKYY-WGO6-K83E-WUQC\" />\n",
      "    <title>\"\n",
      "len(a)\n",
      "12/17:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [2000:2500])\n",
      "12/18:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [1000:1100])\n",
      "12/19:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [500:1100])\n",
      "12/20:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:500])\n",
      "12/21:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:300])\n",
      "12/22:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:350])\n",
      "12/23:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:340])\n",
      "12/24:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:330])\n",
      "12/25:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:335])\n",
      "12/26:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:332])\n",
      "12/27:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [200:332])\n",
      "12/28:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [280:332])\n",
      "12/29:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [270:332])\n",
      "12/30:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [278:332])\n",
      "12/31:\n",
      "#html src from the retrieved webpage\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:332])\n",
      "12/32:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(soup.find('title'))\n",
      "12/33:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(soup.find('title')[0:60])\n",
      "12/34:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print((soup.find('title'))[0:60]))\n",
      "12/35:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print((soup.find('title'))[0:60])\n",
      "12/36:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "type(soup.find('title')\n",
      "12/37:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "type(soup.find('title'))\n",
      "12/38:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "soup.find('title')\n",
      "12/39:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "soup.find('title')[1]\n",
      "12/40:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "string(soup.find('title'))\n",
      "12/41:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "str(soup.find('title'))\n",
      "12/42:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "str(soup.find('title'))[0:60]\n",
      "12/43:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(str(soup.find('title'))[0:60])\n",
      "12/44:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(str(soup.find('title'))[8:60])\n",
      "12/45:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(str(soup.find('title'))[7:60])\n",
      "12/46:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(str(soup.find('title'))[7:61])\n",
      "12/47:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(str(soup.find('title'))[7:62])\n",
      "12/48:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(str(soup.find('title'))[7:61])\n",
      "12/49:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(str(soup.find('title'))[7:61])\n",
      "12/50: soup.find_all('h1')\n",
      "12/51: soup.find_all('h2')\n",
      "12/52: soup.find_all('meta')\n",
      "12/53: soup.find_all('price')\n",
      "12/54: soup.find_all('prijs')\n",
      "12/55: soup.find_all('rating')\n",
      "12/56: soup.find_all('vaste')\n",
      "12/57: soup.find_all('meta')\n",
      "12/58: soup.find_all('meta')[1]\n",
      "12/59: soup.find_all('title')[1]\n",
      "12/60: soup.find_all('title')\n",
      "12/61: soup.find_all('title').get_text()\n",
      "12/62: soup.find_all('title').[1]get_text()\n",
      "12/63: soup.find_all('title')[1].get_text()\n",
      "12/64: soup.find_all('title')[0].get_text()\n",
      "12/65: type(soup.find_all('title')[0].get_text())\n",
      "12/66: print(soup.find_all('title')[0].get_text())\n",
      "12/67:\n",
      "url2 = \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n",
      "book = requests.get(url)\n",
      "12/68:\n",
      "url2 = \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n",
      "book = requests.get(url)\n",
      "12/69:\n",
      "url2 = \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n",
      "book = requests.get(url)\n",
      "book_src = book.text\n",
      "soup2 = BeautifulSoup(book_src)\n",
      "12/70:\n",
      "url2 = \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n",
      "book = requests.get(url)\n",
      "book_src = book.text\n",
      "soup2 = BeautifulSoup(book_src)\n",
      "book\n",
      "12/71: soup2.find(table)\n",
      "12/72: soup2.find(\"table\")\n",
      "12/73: print(soup2.find(\"table\"))\n",
      "12/74: print(soup2.find('table'))\n",
      "12/75: print(soup2.find('h1'))\n",
      "12/76:\n",
      "url2 = \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n",
      "book = requests.get(url)\n",
      "book_src = book.text\n",
      "soup2 = BeautifulSoup(book_src)\n",
      "12/77: print(soup2.find('h1'))\n",
      "12/78:\n",
      "url2 = \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n",
      "book = requests.get(url2)\n",
      "book_src = book.text\n",
      "soup2 = BeautifulSoup(book_src)\n",
      "12/79: print(soup2.find('table'))\n",
      "12/80: print(soup2.find.all('table'))\n",
      "12/81: print(soup2.find_all('table'))\n",
      "12/82: print(soup2.find('table'))\n",
      "12/83: print(soup2.find('table').get_text())\n",
      "12/84: soup2.find('table').get_text()\n",
      "12/85: print(soup2.find('table'))\n",
      "12/86: print(soup2.find('table'))\n",
      "12/87: print(soup2.find('table').find_all('td'))\n",
      "12/88: print(soup2.find('table').find_all('td')[4])\n",
      "12/89: print(soup2.find('table').find_all('td')[4].get_text())\n",
      "12/90: print(soup2.find('table').find_all('td')[4].get_text())\n",
      "12/91:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find_all(\"h2\").get_text())\n",
      "12/92:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find_all(\"h2\").get_text())\n",
      "coursera\n",
      "12/93:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "coursera\n",
      "12/94:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find_all('h2').get_text())\n",
      "12/95:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find_all('h2'))\n",
      "12/96:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find_all('h2 class=\"color-primary-text card-title headline-1-text\"'))\n",
      "12/97:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find_all('color-primary-text card-title headline-1-text'))\n",
      "12/98:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find_all('color-primary-text'))\n",
      "12/99:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find('color-primary-text'))\n",
      "12/100:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find('color-primary-text'))\n",
      "12/101:\n",
      "url4 = \"https://icanhazdadjoke.com/\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "joke = response.json()\n",
      "print(joke)\n",
      "11/1:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "joke_request = response.json() \n",
      "print(joke_request)\n",
      "11/2:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "joke_request = response.json() \n",
      "print(joke_request)\n",
      "12/102:\n",
      "url4 = \"https://icanhazdadjoke.com/\"\n",
      "response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "joke = response.json()\n",
      "print(joke)\n",
      "12/103:\n",
      "url4 = \"https://icanhazdadjoke.com/\"\n",
      "response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "joke = response.json()\n",
      "print(joke)\n",
      "12/104:\n",
      "url4 = \"https://icanhazdadjoke.com/\"\n",
      "response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "joke = response.json()\n",
      "print(joke)\n",
      "12/105:\n",
      "url4 = \"https://icanhazdadjoke.com/\"\n",
      "response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "joke = response.json()\n",
      "print(joke)\n",
      "12/106:\n",
      "url4 = \"https://icanhazdadjoke.com/\"\n",
      "response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "joke = response.json()\n",
      "print(joke['joke'])\n",
      "12/107:\n",
      "\n",
      "for i in range(9):\n",
      "    response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "    joke = response.json()\n",
      "    print(joke['joke'])\n",
      "12/108:\n",
      "\n",
      "for i in range(10):\n",
      "    response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "    joke = response.json()\n",
      "    print(joke['joke'])\n",
      "11/3:\n",
      "# Question 3\n",
      "jokes = [] \n",
      "\n",
      "for counter in range(10):\n",
      "    url = \"https://icanhazdadjoke.com\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "    joke_request = response.json() \n",
      "    jokes.append(joke_request)\n",
      "11/4:\n",
      "# Question 3\n",
      "jokes = [] \n",
      "\n",
      "for counter in range(10):\n",
      "    url = \"https://icanhazdadjoke.com\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "    joke_request = response.json() \n",
      "    jokes.append(joke_request)\n",
      "11/5:\n",
      "# Question 3\n",
      "jokes = [] \n",
      "\n",
      "for counter in range(10):\n",
      "    url = \"https://icanhazdadjoke.com\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "    joke_request = response.json() \n",
      "    jokes.append(joke_request)\n",
      "    \n",
      "print(jokes)\n",
      "11/6:\n",
      "# Question 3\n",
      "jokes = [] \n",
      "\n",
      "for counter in range(10):\n",
      "    url = \"https://icanhazdadjoke.com\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "    joke_request = response.json() \n",
      "    jokes.append(joke_request)\n",
      "    \n",
      "print(jokes['joke'])\n",
      "11/7:\n",
      "# Question 3\n",
      "jokes = [] \n",
      "\n",
      "for counter in range(10):\n",
      "    url = \"https://icanhazdadjoke.com\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "    joke_request = response.json() \n",
      "    jokes.append(joke_request)\n",
      "    \n",
      "print(jokes_request)\n",
      "11/8:\n",
      "# Question 3\n",
      "jokes = [] \n",
      "\n",
      "for counter in range(10):\n",
      "    url = \"https://icanhazdadjoke.com\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "    joke_request = response.json() \n",
      "    jokes.append(joke_request)\n",
      "    \n",
      "print(jokes)\n",
      "12/109:\n",
      "jokes = []\n",
      "for i in range(10):\n",
      "    response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "    joke = response.json()\n",
      "    jokes.append = joke\n",
      "12/110:\n",
      "jokes = []\n",
      "for i in range(10):\n",
      "    response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "    joke = response.json()\n",
      "    jokes.append (joke)\n",
      "12/111:\n",
      "jokes = []\n",
      "for i in range(10):\n",
      "    response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "    joke = response.json()\n",
      "    jokes.append(joke)\n",
      "    print(jokes)\n",
      "12/112:\n",
      "jokes = []\n",
      "for i in range(10):\n",
      "    response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "    joke = response.json()\n",
      "    jokes.append(joke['joke'])\n",
      "    print(jokes)\n",
      "12/113:\n",
      "jokes = []\n",
      "for i in range(10):\n",
      "    response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "    joke = response.json()\n",
      "    jokes.append(joke['joke'])\n",
      "print(jokes)\n",
      "17/1:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text)\n",
      "17/2: ?BeautifulSoup\n",
      "17/3:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text)\n",
      "print(soup)\n",
      "17/4:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "print(soup)\n",
      "17/5:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text)\n",
      "print(soup)\n",
      "17/6:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "print(soup)\n",
      "17/7:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "print(soup)\n",
      "17/8:\n",
      "soup2 = BeautifulSoup(res.text)\n",
      "print(soup2)\n",
      "17/9:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "print(soup)\n",
      "17/10:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "type(soup)\n",
      "17/11: soup.find_all(\"a\")\n",
      "15/1:\n",
      "# Run this code now\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find_all(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "\n",
      "soup.find(\"a\", href=True)[\"href\"]\n",
      "15/2:\n",
      "# Run this code now\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find_all(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "17/12:\n",
      "for link in soup.find_all(\"a\"):\n",
      "    print(link.attrs[\"href\"])\n",
      "17/13:\n",
      "for link in soup.find_all(\"a\"):\n",
      "    print(link.attrs[\"href\"])\n",
      "17/14:\n",
      "for link in soup.find_all(\"a\"):\n",
      "    print(link.attrs[\"href\"])\n",
      "17/15: soup.find(\"a\", href=True)[\"href\"]\n",
      "17/16: soup.find_all(\"a\", href=True)[\"href\"]\n",
      "17/17: soup.find(\"a\", href=True)\n",
      "17/18: soup.find(\"a\", href=True)[\"href\"]\n",
      "17/19: type(soup.find(\"a\", href=True))\n",
      "17/20: ?.find_all\n",
      "17/21: ?BeautifulSoup.find_all\n",
      "17/22: doc(BeautifulSoup.find_all)\n",
      "17/23: doc(BeautifulSoup)\n",
      "17/24: help(BeautifulSoup)\n",
      "17/25: help(BeautifulSoup.find_all)\n",
      "17/26:\n",
      "for link in soup.find_all(\"a\"):\n",
      "    print(link.attrs[\"href\"])\n",
      "17/27:\n",
      "for link in soup.find_all('a'):\n",
      "    print(link.attrs)\n",
      "17/28:\n",
      "for link in soup.find_all('a'):\n",
      "    print(link.attrs)\n",
      "17/29: soup.find_all('a')\n",
      "17/30: soup.find_all('a')\n",
      "17/31:\n",
      "for link in soup.find_all('a'):\n",
      "    print(link.attrs)\n",
      "17/32: soup.find_all(\"a\", class_=\"product_pod\")\n",
      "17/33: soup.find_all(class_= 'product_pod')\n",
      "17/34: soup.find_all(class_= 'product_pod')[0]\n",
      "17/35: soup.find_all(class_= 'product_pod')[0].find('a').attrs['href']\n",
      "17/36: soup.find_all(class_= 'product_pod')[0].find('a').attrs['href']\n",
      "17/37: soup.find_all(class_='product_pod')[1].find('a').attrs['href']\n",
      "17/38: soup.find_all(class_='product_pod')[1].find('a').attrs['href']\n",
      "17/39: ?slicing\n",
      "17/40: help(slicing)\n",
      "17/41: link = soup.find_all(class_='product_pod')[1].find('a').attrs['href']\n",
      "17/42: type(link)\n",
      "17/43:\n",
      "link = soup.find_all(class_='product_pod')[1].find('a').attrs['href']\n",
      "print(link)\n",
      "17/44: link[5:]\n",
      "17/45: book_url = 'https://books.toscrape.com/catalogue/' + link[5:]\n",
      "17/46:\n",
      "book_url = 'https://books.toscrape.com/catalogue/' + link[5:]\n",
      "\n",
      "print(book_url)\n",
      "17/47:\n",
      "book_url = 'https://books.toscrape.com/catalogue/' + link[6:]\n",
      "\n",
      "print(book_url)\n",
      "17/48:\n",
      "book_url = 'https://books.toscrape.com/catalogue/' + link[6:]\n",
      "\n",
      "print(book_url)\n",
      "17/49:\n",
      "book_url2 = link.replace('../../', 'https://books.toscrape.com/catalogue/')\n",
      "print(book_url2)\n",
      "17/50:\n",
      "book_url2 = link.replace('../../', 'https://books.toscrape.com/catalogue/')\n",
      "print(book_url2)\n",
      "17/51: soup.find_all(class_='product_pod')[0:5]\n",
      "17/52:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    for i in book:\n",
      "        link = book[i].find('a').attrs['href']\n",
      "        book_links.append(link)\n",
      "        print(book_links)\n",
      "17/53:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    for i in book:\n",
      "        link = book[i].find(\"a\").attrs[\"href\"]\n",
      "        book_links.append(link)\n",
      "        print(book_links)\n",
      "17/54:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    book_links.append(link)\n",
      "print(book_links)\n",
      "17/55: soup.find_all(class_='product_pod')[0]\n",
      "17/56:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    book_links.append(link)\n",
      "print(book_links\n",
      "17/57:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    book_links.append(link)\n",
      "print(book_links)\n",
      "17/58:\n",
      "book_links ={}\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links = {title : link}\n",
      "print(book_links)\n",
      "17/59:\n",
      "book_links ={}\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links += {title : link}\n",
      "print(book_links)\n",
      "17/60:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "print(book_links)\n",
      "17/61:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "print(book_links)[0]\n",
      "17/62:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "print(book_links)[0]\n",
      "17/63:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "print(book_links)\n",
      "17/64:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    book_links.append(link)\n",
      "print(book_links)[1]\n",
      "17/65:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    book_links.append(link)\n",
      "book_links[1]\n",
      "17/66:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "book_links[1]\n",
      "17/67:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "book_links[0]\n",
      "17/68:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "print(book_links[0])\n",
      "17/69:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "\n",
      "book_links.keys\n",
      "17/70:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "\n",
      "book_links[0:5]\n",
      "17/71:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "\n",
      "book_links[0:5]\n",
      "17/72:\n",
      "for book in book_links:\n",
      "    print(book.keys)\n",
      "17/73:\n",
      "for book in book_links:\n",
      "    if book.keys(\"title\")=\"A Light in the Attic\":\n",
      "        print book.values(\"link\")\n",
      "17/74:\n",
      "for book in book_links:\n",
      "    if book[\"title\"]=\"A Light in the Attic\":\n",
      "        print book(\"link\")\n",
      "17/75:\n",
      "for book in book_links:\n",
      "    if book[\"title\"]=\"A Light in the Attic\":\n",
      "        print book[\"link\"]\n",
      "17/76:\n",
      "for book in book_links:\n",
      "    if book.get[\"title\"]=\"A Light in the Attic\":\n",
      "        print book.get[\"link\"]\n",
      "17/77:\n",
      "for book in book_links:\n",
      "    if book.get[\"title\"]==\"A Light in the Attic\":\n",
      "        print book.get[\"link\"]\n",
      "17/78:\n",
      "for book in book_links:\n",
      "    if book[\"title\"] == \"A Light in the Attic\":\n",
      "        print book\n",
      "17/79:\n",
      "for book in book_links:\n",
      "    if book[\"title\"] == \"A Light in the Attic\":\n",
      "        print (book)\n",
      "17/80:\n",
      "for book in book_links:\n",
      "    if book[\"title\"] == \"A Light in the Attic\":\n",
      "        print (book)\n",
      "17/81: ?next\n",
      "15/3: next((book for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "17/82: next(book for book in book_links if book[\"title\"] == \"A Light in the Attic\")\n",
      "17/83: next(book for book in book_links if book[\"title\"] == \"A Light in the Attic\", None)\n",
      "17/84: next((book for book in book_links if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "17/85: next((book for book in book_links if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "17/86:\n",
      "full_url = []\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"].replace('../../','https://books.toscrape.com/catalogue/')\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "17/87:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "\n",
      "book_links[0:5]\n",
      "17/88:\n",
      "full_url = []\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"].replace('../../','https://books.toscrape.com/catalogue/')\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    full_url.append({'title': title, 'link': link})\n",
      "prink()\n",
      "17/89:\n",
      "full_url = []\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"].replace('../../','https://books.toscrape.com/catalogue/')\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    full_url.append({'title': title, 'link': link})\n",
      "prink(full_url)\n",
      "17/90:\n",
      "full_url = []\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"].replace('../../','https://books.toscrape.com/catalogue/')\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    full_url.append({'title': title, 'link': link})\n",
      "print(full_url)\n",
      "17/91:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_url = []\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"].replace('../../','https://books.toscrape.com/catalogue/')\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    full_url.append({'title': title, 'link': link})\n",
      "print(full_url)\n",
      "17/92: book_links[0:2]\n",
      "17/93: book_links[0:2]\n",
      "17/94: ?enumerate\n",
      "17/95:\n",
      "counter = 1\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/page-\"\n",
      "full_url = base_url + str(counter) + \".html\"\n",
      "print(full_url)\n",
      "17/96:\n",
      "counter = 1\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/page-\"\n",
      "full_url = base_url + str(counter) + \".html\"\n",
      "print(full_url)\n",
      "17/97: counter = range[50]\n",
      "17/98: counter = range(50)\n",
      "17/99: print(range(50))\n",
      "17/100: print(range(1,50))\n",
      "17/101:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/page-\"\n",
      "full_urls = []\n",
      "for counter in range(1,50):\n",
      "    full_urls.append(base_url + str(counter) + \".html\")\n",
      "\n",
      "print(full_urls)\n",
      "17/102:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/page-\"\n",
      "full_urls = []\n",
      "for counter in range(1,51):\n",
      "    full_urls.append(base_url + str(counter) + \".html\")\n",
      "\n",
      "print(full_urls)\n",
      "17/103:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/page-\"\n",
      "page = []\n",
      "for counter in range(1,51):\n",
      "    page.append(base_url + str(counter) + \".html\")\n",
      "\n",
      "print(pages)\n",
      "17/104:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/page-\"\n",
      "pages = []\n",
      "for counter in range(1,51):\n",
      "    pages.append(base_url + str(counter) + \".html\")\n",
      "\n",
      "print(pages)\n",
      "17/105: print(range(1, 50))\n",
      "17/106:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in rang(1, 101):\n",
      "    quote_page_urls.append (base + str(counter) + \"/\")\n",
      "print(quote_page_urls)(0:2)\n",
      "17/107:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in rang(1, 101):\n",
      "    quote_page_urls.append (base + str(counter) + \"/\")\n",
      "print(quote_page_urls)[0:2]\n",
      "17/108:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append (base + str(counter) + \"/\")\n",
      "print(quote_page_urls)[0:2]\n",
      "17/109:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "print(quote_page_urls)(0,2)\n",
      "17/110:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "print(quote_page_urls)(0:2)\n",
      "17/111:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "type(quote_page_urls)\n",
      "17/112:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "print(quote_page_urls [0,2])\n",
      "17/113:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "print(quote_page_urls [0:2])\n",
      "17/114:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "print(quote_page_urls [0:3])\n",
      "17/115:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "print(quote_page_urls [0:3])\n",
      "17/116:\n",
      "from time import sleep\n",
      "sleep(3)\n",
      "print(\"Done\")\n",
      "17/117:\n",
      "from time import sleep\n",
      "sleep(3)\n",
      "print(\"Done\")\n",
      "17/118:\n",
      "def gen_page_urls(base, page_num):\n",
      "    page_urls =[]\n",
      "    \n",
      "    for counter in range(1, page_num + 1):\n",
      "        full_url = base + \"page-\" + str(counter) + \".html\"\n",
      "        page_urls.append(full_url)\n",
      "    \n",
      "    return page_urls\n",
      "17/119:\n",
      "def gen_page_urls(base, page_num):\n",
      "    page_urls =[]\n",
      "    \n",
      "    for counter in range(1, page_num + 1):\n",
      "        full_url = base + \"page-\" + str(counter) + \".html\"\n",
      "        page_urls.append(full_url)\n",
      "    \n",
      "    return page_urls\n",
      "\n",
      "gen_page_urls(\"https://books.toscrape.com/catalogue/category/books_1/\", 3)\n",
      "17/120:\n",
      "def gen_page_urls(base, page_num):\n",
      "    page_urls =[]\n",
      "    \n",
      "    for counter in range(1, page_num + 1):\n",
      "        full_url = base + \"page-\" + str(counter) + \".html\"\n",
      "        page_urls.append(full_url)\n",
      "    \n",
      "    return page_urls\n",
      "\n",
      "gen_page_urls(\"https://books.toscrape.com/catalogue/category/books_1/\", 3)\n",
      "17/121:\n",
      "def extr_book_urls(page_urls):\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = Beautiful.Soup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class =\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "17/122:\n",
      "def extr_book_urls(page_urls):\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = Beautiful.Soup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "17/123:\n",
      "def extr_book_urls(page_urls):\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = Beautiful.Soup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "17/124:\n",
      "def extr_book_urls(page_urls):\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = Beautiful.Soup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "17/125:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = generate_page_urls(base_url, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extract_book_urls(page_urls)\n",
      "17/126:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extr_book_urls(page_urls)\n",
      "17/127:\n",
      "def extr_book_urls(page_urls):\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "17/128:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extr_book_urls(page_urls)\n",
      "17/129:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extr_book_urls(page_urls)\n",
      "book_list[0:2]\n",
      "20/1:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extr_book_urls(page_urls)\n",
      "book_list[0:2]\n",
      "20/2:\n",
      "def gen_page_urls(base, page_num):\n",
      "    page_urls =[]\n",
      "    \n",
      "    for counter in range(1, page_num + 1):\n",
      "        full_url = base + \"page-\" + str(counter) + \".html\"\n",
      "        page_urls.append(full_url)\n",
      "    \n",
      "    return page_urls\n",
      "\n",
      "gen_page_urls(\"https://books.toscrape.com/catalogue/category/books_1/\", 3)\n",
      "20/3:\n",
      "def extr_book_urls(page_urls):\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "20/4:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extr_book_urls(page_urls)\n",
      "book_list[0:2]\n",
      "20/5:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "20/6:\n",
      "for link in soup.find_all(\"a\"):\n",
      "    print(link.attrs[\"href\"])\n",
      "20/7:\n",
      "for link in soup.find_all('a'):\n",
      "    print(link.attrs)\n",
      "20/8: soup.find_all('a')\n",
      "20/9: soup.find_all(class_= 'product_pod')[0].find('a').attrs['href']\n",
      "20/10:\n",
      "link = soup.find_all(class_='product_pod')[1].find('a').attrs['href']\n",
      "print(link)\n",
      "20/11:\n",
      "book_url = 'https://books.toscrape.com/catalogue/' + link[6:]\n",
      "\n",
      "print(book_url)\n",
      "20/12:\n",
      "book_url2 = link.replace('../../', 'https://books.toscrape.com/catalogue/')\n",
      "print(book_url2)\n",
      "20/13: soup.find_all(class_='product_pod')[0]\n",
      "20/14:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    book_links.append(link)\n",
      "book_links[1]\n",
      "20/15:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "\n",
      "book_links[0:5]\n",
      "20/16:\n",
      "for book in book_links:\n",
      "    if book[\"title\"] == \"A Light in the Attic\":\n",
      "        print (book)\n",
      "20/17: next((book for book in book_links if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "20/18:\n",
      "full_url = []\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"].replace('../../','https://books.toscrape.com/catalogue/')\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    full_url.append({'title': title, 'link': link})\n",
      "print(full_url)\n",
      "20/19: book_links[0:2]\n",
      "20/20:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/page-\"\n",
      "pages = []\n",
      "for counter in range(1,51):\n",
      "    pages.append(base_url + str(counter) + \".html\")\n",
      "\n",
      "print(pages)\n",
      "20/21: print(range(1, 50))\n",
      "20/22:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "print(quote_page_urls [0:3])\n",
      "20/23:\n",
      "from time import sleep\n",
      "sleep(3)\n",
      "print(\"Done\")\n",
      "20/24:\n",
      "def gen_page_urls(base, page_num):\n",
      "    page_urls =[]\n",
      "    \n",
      "    for counter in range(1, page_num + 1):\n",
      "        full_url = base + \"page-\" + str(counter) + \".html\"\n",
      "        page_urls.append(full_url)\n",
      "    \n",
      "    return page_urls\n",
      "\n",
      "gen_page_urls(\"https://books.toscrape.com/catalogue/category/books_1/\", 3)\n",
      "20/25:\n",
      "def extr_book_urls(page_urls):\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "20/26:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extr_book_urls(page_urls)\n",
      "book_list[0:2]\n",
      "20/27:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book_urls =[]\n",
      "20/28:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book_urls =[]\n",
      "\n",
      "books[0]\n",
      "20/29:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base,5)\n",
      "20/30:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base,5)\n",
      "page_urls\n",
      "20/31:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base,5)\n",
      "book_list = extr_book_urls(page_urls)\n",
      "20/32: book_list[0]\n",
      "20/33: books = soup.find_all(class_=\"product_pod\")\n",
      "20/34: books = soup.find_all(class_=\"product_pod\")\n",
      "20/35:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "books[0]\n",
      "20/36:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book.find(\"p\", class_=\"instock availability\")\n",
      "20/37:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book.find(\"a\")\n",
      "20/38:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book.find(\"p\", class_=\"instock availability\")\n",
      "20/39:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book.find(\"p\", class_=\"instock availability\")==\"In stock\"\n",
      "20/40:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book.find(\"p\", class_=\"instock availability\")== \"In stock\"\n",
      "20/41:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book.find(\"p\", class_=\"instock availability\")\n",
      "20/42:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book.find(\"p\", class_=\"instock availability\").text\n",
      "20/43:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url})\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "        sleep(1)\n",
      "    return book_list\n",
      "20/44:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url})\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "        sleep(1)\n",
      "    return book_list\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list = extr_book_urls2(page_urls)\n",
      "book_list[0:2]\n",
      "20/45:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list = extr_book_urls2(page_urls)\n",
      "book_list[0:2]\n",
      "20/46:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url, \"avail\": book_instock})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list = extr_book_urls2(page_urls)\n",
      "book_list[0:2]\n",
      "20/47:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "            book_list2.append({\"title\": book_title,\n",
      "                             \"url\": book_url, \n",
      "                              \"avail\": book_instock})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list = extr_book_urls2(page_urls)\n",
      "book_list[0:2]\n",
      "20/48:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "            book_list2.append({\"title\": book_title,\n",
      "                             \"url\": book_url, \n",
      "                              \"avail\": book_instock})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list2 = extr_book_urls2(page_urls)\n",
      "book_list2[0:2]\n",
      "20/49:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "            book_list2.append({\"title\": book_title,\n",
      "                             \"url\": book_url, \n",
      "                              \"avail\": book_instock})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "extr_book_urls2(page_urls)\n",
      "book_list2[0:2]\n",
      "20/50:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "            book_list2.append({\"title\": book_title,\n",
      "                             \"url\": book_url, \n",
      "                              \"avail\": book_instock})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list2\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list2 = extr_book_urls2(page_urls)\n",
      "book_list2[0:2]\n",
      "20/51:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text.replace(\"\\n\",\"\").replace(\"  \",\"\")\n",
      "            book_list2.append({\"title\": book_title,\n",
      "                             \"url\": book_url, \n",
      "                              \"avail\": book_instock})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list2\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list2 = extr_book_urls2(page_urls)\n",
      "book_list2[0:2]\n",
      "20/52:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text.replace(\"\\n\",\"\").replace(\"  \",\"\")\n",
      "            book_list2.append({\"title\": book_title,\n",
      "                             \"url\": book_url, \n",
      "                              \"avail\": book_instock})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list2\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list2 = extr_book_urls2(page_urls)\n",
      "book_list2[0:2]\n",
      "20/53:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, 'html.parser')\n",
      "    next_btn = soup.find(class_=\"next\")\n",
      "    return next_btn.find('a').attrs['href'] if next_btn else None\n",
      "\n",
      "page_1 = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "print(\"The next page is: \" + str(check_next_page(page_1)))\n",
      "20/54:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, 'html.parser')\n",
      "    next_btn = soup.find(class_=\"next\")\n",
      "    return next_btn.find('a').attrs['href'] if next_btn else None\n",
      "\n",
      "page_1 = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "print(\"The next page is: \" + str(check_next_page(page_1)))\n",
      "\n",
      "type(check_next_page(page_1))\n",
      "20/55:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, 'html.parser')\n",
      "    next_btn = soup.find(class_=\"next\")\n",
      "    return next_btn.find('a').attrs['href'] if next_btn else None\n",
      "\n",
      "page_1 = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "print(\"The next page is: \" + check_next_page(page_1))\n",
      "\n",
      "type(check_next_page(page_1))\n",
      "20/56: check_next_page(page_50)\n",
      "20/57: check_next_page(page_50)\n",
      "20/58:\n",
      "check_next_page(page_50)\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "def next_page_url(next_btn):\n",
      "    if next_btn != None:\n",
      "        page_url = base + next_btn\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "20/59:\n",
      "check_next_page(https://books.toscrape.com/catalogue/page-1.html)\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "def next_page_url(next_btn):\n",
      "    if next_btn != None:\n",
      "        page_url = base + next_btn\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "  \n",
      "next_page_url\n",
      "20/60:\n",
      "check_next_page(\"https://books.toscrape.com/catalogue/page-1.html\")\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "def next_page_url(next_btn):\n",
      "    if next_btn != None:\n",
      "        page_url = base + next_btn\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "  \n",
      "next_page_url()\n",
      "20/61:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "def next_page_url(check_next_page(url)):\n",
      "    if next_btn != None:\n",
      "        page_url = base + next_btn\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "  \n",
      "next_page_url()\n",
      "20/62:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "check_next_page(url)\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "def next_page_url(check_next_page(url)):\n",
      "    if next_btn != None:\n",
      "        page_url = base + next_btn\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "20/63:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "check_next_page(url)\n",
      "20/64:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "link = check_next_page(url)\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "\n",
      "def next_page_url(link):\n",
      "    if link != None:\n",
      "        page_url = base + link\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "        \n",
      "next_page_url(link)\n",
      "20/65:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "link = check_next_page(url)\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "\n",
      "def next_page_url(link):\n",
      "    if link != None:\n",
      "        return page_url = base + link\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "        \n",
      "next_page_url(link)\n",
      "20/66:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "link = check_next_page(url)\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "\n",
      "def next_page_url(link):\n",
      "    if link != None:\n",
      "        page_url = base + link\n",
      "        print(page_url)\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "        \n",
      "next_page_url(link)\n",
      "20/67:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "link = check_next_page(url)\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "\n",
      "def next_page_url(link):\n",
      "    if link != None:\n",
      "        page_url = base + link\n",
      "        print(\"Next page link:\" + page_url)\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "        \n",
      "next_page_url(link)\n",
      "20/68:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "link = check_next_page(url)\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "\n",
      "def next_page_url(link):\n",
      "    if link != None:\n",
      "        page_url = base + link\n",
      "        print(\"Next page link: \" + page_url)\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "        \n",
      "next_page_url(link)\n",
      "23/1:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "joke_request = response.json() \n",
      "print(joke_request)\n",
      "23/2:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "#get the resquests using the url using the header\n",
      "joke_request = response.json() #convert data into json file\n",
      "print(joke_request)\n",
      "23/3:\n",
      "import requests\n",
      "import time\n",
      "\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "\n",
      "i = 0\n",
      "while i<10:\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "    #get the resquests using the url using the header\n",
      "    joke_request = response.json() #convert data into json file\n",
      "    print(i)\n",
      "    print(joke_request [\"id\"])\n",
      "    print(joke_request [\"joke\"])\n",
      "    i = i+1\n",
      "    time.sleep(2)\n",
      "23/4:\n",
      "# Question 2 \n",
      "import time\n",
      "\n",
      "i = 1\n",
      "while i <= 3:\n",
      "    url = 'https://www.reddit.com/r/marketing/about/.json'\n",
      "    headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "    response = requests.get(url, headers=headers)\n",
      "    json_response = response.json()\n",
      "    \n",
      "    print(json_response['data']['active_user_count'])\n",
      "    i += 1\n",
      "    time.sleep(5)\n",
      "24/1:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "#get the resquests using the url using the header\n",
      "joke_request = response.json() #convert data into json file\n",
      "print(joke_request)\n",
      "24/2:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "#get the resquests using the url using the header\n",
      "joke_request = response.json() #convert data into json file\n",
      "print(joke_request)\n",
      "24/3:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "#get the resquests using the url using the header\n",
      "joke_request = response.json() #convert data into json file\n",
      "print(joke_request)\n",
      "24/4:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "\n",
      "print(response)\n",
      "24/5:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "\n",
      "print(response.url)\n",
      "24/6:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "\n",
      "response\n",
      "24/7:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "\n",
      "type(response)\n",
      "24/8:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "\n",
      "view(response)\n",
      "24/9:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "\n",
      "dir(response)\n",
      "24/10:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "response.url\n",
      "24/11:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "response.text\n",
      "24/12:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "response.text\n",
      "24/13:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "BeautifulSoup(response.text)\n",
      "24/14:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "BeautifulSoup(response)\n",
      "24/15:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "dir(response)\n",
      "24/16:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "response.json\n",
      "24/17:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "dir(response.json)\n",
      "24/18:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "response.json()\n",
      "24/19:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "response.text()\n",
      "24/20:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "response.json()\n",
      "24/21:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "24/22: dir(response)\n",
      "24/23: response.links()\n",
      "24/24: response.links\n",
      "24/25: print(response.links)\n",
      "24/26: print(response.raw)\n",
      "24/27: dir(response.raw)\n",
      "24/28: dir(response.raw)\n",
      "24/29: a = ['head', 'tail']\n",
      "24/30:\n",
      "a = ['head', 'tail']\n",
      "a.head\n",
      "24/31: dir(response.__attrs__)\n",
      "24/32: type(response)\n",
      "24/33: ?requests.models.Response\n",
      "24/34: help(requests.models.Response)\n",
      "24/35: dir(response)\n",
      "24/36:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "response.url\n",
      "24/37:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "24/38:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request\n",
      "24/39:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "print(joke_request)\n",
      "24/40:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request.results\n",
      "24/41:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request[results]\n",
      "24/42:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "24/43:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "type(joke_request)\n",
      "24/44:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "dir(joke_request)\n",
      "24/45: ?dir\n",
      "24/46: help(dir)\n",
      "24/47:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "dir(joke_request)\n",
      "joke_request.class\n",
      "24/48:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "dir(joke_request)\n",
      "24/49:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "joke_request.get()\n",
      "24/50:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "joke_request.get\n",
      "24/51:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "joke_request.get('results')\n",
      "24/52:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in joke_request.get('results'):\n",
      "    print joke.get['joke']\n",
      "24/53:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in joke_request.get('results'):\n",
      "    print (joke.get['joke'])\n",
      "24/54:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in joke_request.get('results'):\n",
      "    print (joke.get('joke')\n",
      "24/55:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in joke_request.get('results'):\n",
      "    print (joke.get('joke'))\n",
      "24/56:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in joke_request.get('results'):\n",
      "    print (joke['joke'])\n",
      "24/57:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in joke_request['results']:\n",
      "    print (joke['joke'])\n",
      "24/58:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in range(5):\n",
      "    for joke in joke_request['results']:\n",
      "    print (joke['joke'])\n",
      "24/59:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in range(5):\n",
      "    for joke in joke_request['results']:\n",
      "    print (joke['joke'])\n",
      "24/60:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in range(5):\n",
      "    for joke in joke_request['results']:\n",
      "        print (joke['joke'])\n",
      "24/61:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in joke_request['results']:\n",
      "        if joke < 5:\n",
      "            print (joke['joke'])\n",
      "24/62:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "24/63:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results'](0:4)\n",
      "24/64:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results'][0:4]\n",
      "24/65:\n",
      "##### import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results'][0:5]\n",
      "24/66:\n",
      "##### import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results'][0:5]\n",
      "for joke in joke_request['results'] [0:5]:\n",
      "        print (joke['joke'])\n",
      "24/67:\n",
      "##### import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"dog\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results'][0:5]\n",
      "for joke in joke_request['results'] [0:5]:\n",
      "        print (joke['joke'])\n",
      "24/68:\n",
      "##### import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"dog\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results'][0:5]\n",
      "for joke in joke_request['results'] [0:5]:\n",
      "        print (joke['joke'])\n",
      "24/69:\n",
      "##### import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"dog\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "24/70:\n",
      "##### import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"dog\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request\n",
      "24/71:\n",
      "def find_joke(q):\n",
      "    response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": q})\n",
      "    joke_request = response.json()\n",
      "    print (\"There are\" + str(joke_request['total_jokes']) + \"jokes\")\n",
      "q = \"dog\"\n",
      "find_joke(q)\n",
      "24/72:\n",
      "def find_joke(q):\n",
      "    response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": q})\n",
      "    joke_request = response.json()\n",
      "    print (\"There are \" + str(joke_request['total_jokes']) + \" jokes\")\n",
      "q = \"dog\"\n",
      "find_joke(q)\n",
      "24/73:\n",
      "def find_joke(q):\n",
      "    response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": q})\n",
      "    joke_request = response.json()\n",
      "    print (\"There are \" + str(joke_request['total_jokes']) + \" jokes about\" + q)\n",
      "q = \"dog\"\n",
      "find_joke(q)\n",
      "24/74:\n",
      "def find_joke(q):\n",
      "    response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": q})\n",
      "    joke_request = response.json()\n",
      "    print (\"There are \" + str(joke_request['total_jokes']) + \" jokes about \" + q)\n",
      "q = \"dog\"\n",
      "find_joke(q)\n",
      "24/75:\n",
      "def find_joke(q):\n",
      "    response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": q})\n",
      "    joke_request = response.json()\n",
      "    print (\"There are \" + str(joke_request['total_jokes']) + \" jokes containing \" + q)\n",
      "q = \"dog\"\n",
      "find_joke(q)\n",
      "24/76:\n",
      "def find_joke(q):\n",
      "    response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": q})\n",
      "    joke_request = response.json()\n",
      "    print (f\"There are \" + str(joke_request['total_jokes']) + \" jokes containing \" + q)\n",
      "q = \"dog\"\n",
      "find_joke(q)\n",
      "24/77:\n",
      "def find_joke(q):\n",
      "    response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": q})\n",
      "    joke_request = response.json()\n",
      "    print (\"There are \" + str(joke_request['total_jokes']) + \" jokes containing \" + q)\n",
      "q = \"dog\"\n",
      "find_joke(q)\n",
      "24/78: help(requests.models.Response)\n",
      "24/79:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"})\n",
      "joke_request = response.json()\n",
      "24/80:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"})\n",
      "joke_request = response.json()\n",
      "joke_request\n",
      "24/81:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"})\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "24/82:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"})\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = ''\n",
      "24/83:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"})\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = ''\n",
      "joke_request\n",
      "24/84:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"}) #return all jokes\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = ''\n",
      "joke_request.url\n",
      "24/85:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"}) #return all jokes\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = ''\n",
      "response.url\n",
      "24/86:\n",
      "search_url = \"https://icanhazdadjoke.com\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}) #return all jokes\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = ''\n",
      "response.url\n",
      "24/87:\n",
      "search_url = \"https://icanhazdadjoke.com\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}) #return all jokes\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = ''\n",
      "joke_request\n",
      "24/88:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "for limit in range (10, 31, 10)\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": limit}) #return all jokes with limits = 10, 20 or 30\n",
      "joke_request = response.json()\n",
      "print(f\"Limit {limit} gives {joke_request['total_pages']} pages\")\n",
      "24/89:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "for limit in range (10, 31, 10):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": limit}) #return all jokes with limits = 10, 20 or 30\n",
      "joke_request = response.json()\n",
      "print(f\"Limit {limit} gives {joke_request['total_pages']} pages\")\n",
      "24/90:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "for limit in range (10, 31, 10):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": limit}) #return all jokes with limits = 10, 20 or 30\n",
      "    joke_request = response.json()\n",
      "    print(f\"Limit {limit} gives {joke_request['total_pages']} pages\")\n",
      "24/91:\n",
      "response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\"}) #return all jokes with limits = 10, 20 or 30\n",
      "joke_request = response.json()\n",
      "24/92:\n",
      "response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\"}) #return all jokes with limits = 10, 20 or 30\n",
      "joke_request = response.json()\n",
      "joke_request\n",
      "24/93:\n",
      "response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": 5,\n",
      "                                   \"page\": 2})\n",
      "response.json()\n",
      "24/94:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "        response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "        total_pages = response.json()[\"total_pages\"]\n",
      "        for page in range (1, total_pages + 1):\n",
      "                response = requests.get(search_url, \n",
      "                                        headers={\"Accept\": \"application/json\"}, \n",
      "                                        params={\"term\": term,\n",
      "                                               \"page\": page})\n",
      "                joke_request = response.json()\n",
      "                for joke in joke_request[\"results\"]:\n",
      "                    joke_list.append = joke\n",
      "                \n",
      "    return joke_list\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "24/96:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "        response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "        total_pages = response.json()[\"total_pages\"]\n",
      "        for page in range (1, total_pages + 1):\n",
      "                response = requests.get(search_url, \n",
      "                                        headers={\"Accept\": \"application/json\"}, \n",
      "                                        params={\"term\": term,\n",
      "                                               \"page\": page})\n",
      "                joke_request = response.json()\n",
      "                for joke in joke_request[\"results\"]:\n",
      "                    joke_list.append = joke\n",
      "                \n",
      "    print(joke_list)\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "24/98:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "        response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "        total_pages = response.json()[\"total_pages\"]\n",
      "        for page in range (1, total_pages + 1):\n",
      "                response = requests.get(search_url, \n",
      "                                        headers={\"Accept\": \"application/json\"}, \n",
      "                                        params={\"term\": term,\n",
      "                                               \"page\": page})\n",
      "                joke_request = response.json()\n",
      "                for joke in joke_request[\"results\"]:\n",
      "                    joke_list.append(joke)\n",
      "                \n",
      "    print(joke_list)\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "24/100:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "        response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "        total_pages = response.json()[\"total_pages\"]\n",
      "        for page in range (1, total_pages + 1):\n",
      "                response = requests.get(search_url, \n",
      "                                        headers={\"Accept\": \"application/json\"}, \n",
      "                                        params={\"term\": term,\n",
      "                                               \"page\": page})\n",
      "                joke_request = response.json()\n",
      "                for joke in joke_request[\"results\"]:\n",
      "                    joke_list.append(joke)\n",
      "                \n",
      "        print(joke_list)\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "24/101:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "        response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "        total_pages = response.json()[\"total_pages\"]\n",
      "        for page in range (1, total_pages + 1):\n",
      "                response = requests.get(search_url, \n",
      "                                        headers={\"Accept\": \"application/json\"}, \n",
      "                                        params={\"term\": term,\n",
      "                                               \"page\": page})\n",
      "                joke_request = response.json()\n",
      "                for joke in joke_request[\"results\"]:\n",
      "                    joke_list.append(joke)\n",
      "                \n",
      "                print(joke_list)\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "24/102:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "        response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "        total_pages = response.json()[\"total_pages\"]\n",
      "        for page in range (1, total_pages + 1):\n",
      "                response = requests.get(search_url, \n",
      "                                        headers={\"Accept\": \"application/json\"}, \n",
      "                                        params={\"term\": term,\n",
      "                                               \"page\": page})\n",
      "                joke_request = response.json()\n",
      "                for joke in joke_request[\"results\"]:\n",
      "                    joke_list.append(joke)\n",
      "                print(joke_list)\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "24/103:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "        response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "        total_pages = response.json()[\"total_pages\"]\n",
      "        for page in range (1, total_pages + 1):\n",
      "                response = requests.get(search_url, \n",
      "                                        headers={\"Accept\": \"application/json\"}, \n",
      "                                        params={\"term\": term,\n",
      "                                               \"page\": page})\n",
      "                joke_request = response.json()\n",
      "                for joke in joke_request[\"results\"]:\n",
      "                    joke_list.append(joke)\n",
      "        print(joke_list)\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "24/104:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    total_pages = response.json()[\"total_pages\"]\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        for joke in joke_request[\"results\"]:\n",
      "            joke_list.append(joke)\n",
      "    print(joke_list)\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "26/1:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "joke_request = response.json() \n",
      "print(joke_request)\n",
      "26/2:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "joke_request = response.json()\n",
      "print(joke_request)\n",
      "26/3: # your answer goes here!\n",
      "26/4:\n",
      "# Question 1 \n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"dog\"})\n",
      "joke_request = response.json()\n",
      "print(f\"The number of dog jokes is: {joke_request['total_jokes']}\")\n",
      "26/5:\n",
      "# Question 2\n",
      "def find_jokes(term):\n",
      "    search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    joke_request = response.json()\n",
      "    num_results = joke_request['total_jokes']\n",
      "    return num_results\n",
      "\n",
      "find_jokes(\"some-searchterm-you-would-like-to-try-out\")\n",
      "26/6:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"})\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = '' # let's remove all jokes, and only look at the other attributes in the JSON response\n",
      "joke_request\n",
      "26/7:\n",
      "for limit in range(10, 31, 10):  # note that range(a, b) runs from a to b-1; so the last value is exclusive (so from 10 to 30 with steps of 10)\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": limit})\n",
      "    joke_request = response.json()\n",
      "    print(f\"Limit {limit} gives {joke_request['total_pages']} pages\")\n",
      "26/8:\n",
      "response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": 5,\n",
      "                                   \"page\": 2})\n",
      "response.json()\n",
      "26/9: # your answer goes here!\n",
      "26/10:\n",
      "def find_jokes(term):\n",
      "    search_url = \"https://icanhazdadjoke.com/search\"\n",
      "    page = 1\n",
      "    jokes = []\n",
      "\n",
      "    while True:  # alternatively you can also use a for-loop that goes from page 1 to total_results / 20 (rounded up)\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,  # optionally you can add \"limit\": 20 but that's already the default so it doesn't change anything\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        jokes.extend(joke_request['results'])\n",
      "        if joke_request['current_page'] <= joke_request['total_pages']:\n",
      "            page += 1\n",
      "        else: \n",
      "            return jokes\n",
      "\n",
      "output = find_jokes(\"cat\") # try running it with \"\", too!\n",
      "26/11: print(f\"You've collected {len(output)} jokes\")\n",
      "24/105:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    total_pages = response.json()[\"total_pages\"]\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        for joke in joke_request[\"results\"]:\n",
      "            joke_list.append(joke)\n",
      "    print(joke_list)\n",
      "term = \"cat\"\n",
      "find_joke(term)\n",
      "24/106:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    total_pages = response.json()[\"total_pages\"]\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        for joke in joke_request[\"results\"]:\n",
      "            joke_list.append(joke)\n",
      "    print(joke_list)\n",
      "term = \"cat\"\n",
      "len(find_joke(term))\n",
      "24/107:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    total_pages = response.json()[\"total_pages\"]\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        for joke in joke_request[\"results\"]:\n",
      "            joke_list.append(joke)\n",
      "    return joke_list\n",
      "term = \"cat\"\n",
      "len(find_joke(term))\n",
      "24/108:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    total_pages = response.json()[\"total_pages\"]\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        for joke in joke_request[\"results\"]:\n",
      "            joke_list.append(joke)\n",
      "    return joke_list\n",
      "term = \"dog\"\n",
      "len(find_joke(term))\n",
      "24/109:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    total_pages = response.json()[\"total_pages\"]\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        for joke in joke_request[\"results\"]:\n",
      "            joke_list.append(joke)\n",
      "    return joke_list\n",
      "term = \"rain\"\n",
      "len(find_joke(term))\n",
      "26/12:\n",
      "def find_jokes(term):\n",
      "    search_url = \"https://icanhazdadjoke.com/search\"\n",
      "    page = 1\n",
      "    jokes = []\n",
      "\n",
      "    while True:  # alternatively you can also use a for-loop that goes from page 1 to total_results / 20 (rounded up)\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,  # optionally you can add \"limit\": 20 but that's already the default so it doesn't change anything\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        jokes.extend(joke_request['results'])\n",
      "        if joke_request['current_page'] <= joke_request['total_pages']:\n",
      "            page += 1\n",
      "        else: \n",
      "            return jokes\n",
      "\n",
      "output = find_jokes(\"rain\") # try running it with \"\", too!\n",
      "26/13: print(f\"You've collected {len(output)} jokes\")\n",
      "28/1:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://github.com/search?q=open+education\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/2:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories?q=open+education\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "27/1:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "joke_request = response.json() \n",
      "print(joke_request)\n",
      "28/3:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"term\": \"open education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/4:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": \"open education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/5:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"term\": \"open education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/6:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": \"open education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/7:\n",
      "# request JSON output from github search API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": \"open education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/8:\n",
      "# request JSON output from github search API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": \"open+education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/9:\n",
      "# request JSON output from github search API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": \"open education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/10: dir(git_request)\n",
      "28/11: git_request['total_count']\n",
      "28/12: git_request['total_count']\n",
      "28/13: git_request['total_count']\n",
      "28/14:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {response_request['total_git']} respositories that contains the term {term})\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/15:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_git']} respositories that contains the term {term})\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/16:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_git']} respositories that contains the term {term}\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/17:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contains the term {term}\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/18:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term {term}\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/19:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/20:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term \"{term}\"\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/21:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*)\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/22:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\"\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/23:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*)\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/24:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/25:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/26: git_request\n",
      "28/27: git_request['items'] = ''\n",
      "28/28: git_request['items'] = ''\n",
      "28/29:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "28/30: git_request['items']\n",
      "28/31: git_request\n",
      "28/32:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "28/33: git_request\n",
      "28/34:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "28/35:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "print (f\"There are {git_request['total_count']} respositories\")\n",
      "28/36: git_request\n",
      "28/37: len(git_request['items'])\n",
      "28/38: git_request['items'] = \"\"\n",
      "28/39:\n",
      "git_request['items'] = \"\"\n",
      "git_request\n",
      "28/40:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "print (f\"There are {git_request['total_count']} respositories\")\n",
      "28/41:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"limit\": 50})\n",
      "git_request = response.json()\n",
      "print (f\"There are {git_request['total_count']} respositories\")\n",
      "28/42: len(git_request['items'])\n",
      "28/43:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"limit\": 10,\n",
      "                                 \"page\": 2})\n",
      "git_request = response.json()\n",
      "print (f\"There are {git_request['total_count']} respositories\")\n",
      "28/44: len(git_request['items'])\n",
      "28/45:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"page\": 2})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "28/46:\n",
      "# url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "28/47:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"python\")\n",
      "28/48:\n",
      "# url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "28/49: len(git_request['items'])\n",
      "28/50:\n",
      "total_page = git_request['total_count']/len(git_request['items']\n",
      "total_page\n",
      "28/51: total_page = git_request['total_count']/len(git_request['items']\n",
      "28/52: git_request['total_count']\n",
      "28/53: git_request['total_count']/(len(git_request['items']))\n",
      "28/54: total_page = git_request['total_count']/(len(git_request['items']))\n",
      "28/55:\n",
      "total_page = git_request['total_count']/(len(git_request['items']))\n",
      "total_page\n",
      "28/56:\n",
      "total_page = git_request['total_count']/(len(git_request['items']))\n",
      "math.ceil(total_page)\n",
      "28/57:\n",
      "import math\n",
      "total_page = git_request['total_count']/(len(git_request['items']))\n",
      "math.ceil(total_page)\n",
      "28/58:\n",
      "import math\n",
      "total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "28/59:\n",
      "import math\n",
      "total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "total_page\n",
      "28/60:\n",
      "import math\n",
      "\n",
      "for page in range(1, total_page + 1):\n",
      "\n",
      "repo_list =[]\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 100})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "28/61:\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 100})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "28/62:\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 100})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "28/63:\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "28/64:\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/1:\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/2:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/3:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    total_page = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/4:\n",
      "#using parameter per_page to set the number of results for each request\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"page\": 2})\n",
      "git_request = response.json()\n",
      "git_request['items']\n",
      "30/5:\n",
      "#using parameter per_page to set the number of results for each request\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"page\": 2})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "30/6:\n",
      "#using parameter per_page to set the number of results for each request\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"page\": 2})\n",
      "git_request = response.json()\n",
      "git_request['items']\n",
      "30/7:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    total_page = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/8:\n",
      "#using parameter per_page to set the number of results for each request\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"page\": 2})\n",
      "git_request = response.json()\n",
      "git_request['items']\n",
      "30/9:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"python\")\n",
      "30/10:\n",
      "#using parameter per_page to set the number of results for each request\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"page\": 1})\n",
      "git_request = response.json()\n",
      "git_request['items']\n",
      "30/11:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    total_page = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/12:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    search_url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "\n",
      "    while True:\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 50,\n",
      "                                        \"page\": page})\n",
      "        git_request = response.json()\n",
      "        total_page = math.ceil(git_request['total_count']/50)\n",
      "        repo_list.extend(git_request['items'])\n",
      "        if page <= total_page:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "30/13:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    search_url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "\n",
      "    while True:\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 50,\n",
      "                                        \"page\": page})\n",
      "        repo_request = response.json()\n",
      "        total_page = math.ceil(repo_request['total_count']/50)\n",
      "        repo_list.extend(repo_request['items'])\n",
      "        if page <= total_page:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "30/14:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "\n",
      "    while True:\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 50,\n",
      "                                        \"page\": page})\n",
      "        repo_request = response.json()\n",
      "        total_page = math.ceil(repo_request['total_count']/50)\n",
      "        repo_list.extend(repo_request['items'])\n",
      "        if page <= total_page:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "30/15:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    total_page = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    for page in range (1, 50):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/16:\n",
      "#using parameter per_page to set the number of results for each request\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"page\": 1})\n",
      "git_request = response.json()\n",
      "git_request['items']\n",
      "30/17:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    total_page = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "\n",
      "def find_repo(term):\n",
      "    total_page(term)\n",
      "    for page in range (1, total_page+1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/18:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "\n",
      "def find_repo(term):\n",
      "    total_page(term)\n",
      "    for page in range (1, pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/19:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "def find_repo(term):\n",
      "    total_page(term)\n",
      "    for page in range (1, pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/20:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "def find_repo(term):\n",
      "    total_page(term)\n",
      "    for page in range (1, pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/21:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "term = \"open education\"\n",
      "total_page(term)\n",
      "30/22:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "term = \"open education\"\n",
      "total_page(term)\n",
      "30/23:\n",
      "repo_list =[]\n",
      "def find_repo(term):\n",
      "    total_page(term)\n",
      "    for page in range (1, pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "30/24:\n",
      "repo_list =[]\n",
      "def find_repo(term):\n",
      "    total_page(term)\n",
      "    for page in range (1, pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "30/25:\n",
      "repo_list =[]\n",
      "def find_repo(term):\n",
      "    pages = total_page(term)\n",
      "    for page in range (1, pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "30/26:\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    for page in range (1, pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "pages = total_page(term)\n",
      "find_repo(term)\n",
      "30/27:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"open education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "git_request = response.json() \n",
      "while 'next' in response.links.keys():\n",
      "  response = requests.get(response.links['next']['url'],headers={\"Authorization\": git_token})\n",
      "  git_request.extend(response.json())\n",
      "30/28:\n",
      "git_token = \"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\"\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "res=requests.get(url,\n",
      "                 headers={\"Authorization\": git_token},\n",
      "                 params = {\"q\": \"open education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "repos=res.json()\n",
      "while 'next' in res.links.keys():\n",
      "  res=requests.get(res.links['next']['url'],headers={\"Authorization\": git_token})\n",
      "  repos.extend(res.json())\n",
      "30/29:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"open education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "git_request = response.json() \n",
      "git_request\n",
      "30/30:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"open education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "git_request = response.json() \n",
      "response.headers\n",
      "git_request\n",
      "30/31:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"open education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "git_request = response.json() \n",
      "response.headers\n",
      "30/32:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"open education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "git_request = response.json() \n",
      "response.headers['Link']\n",
      "30/33: type(response.headers['Link'])\n",
      "30/34:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"python\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "git_request = response.json() \n",
      "response.headers['Link']\n",
      "30/35:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"python\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":15})\n",
      "git_request = response.json() \n",
      "response.headers['Link']\n",
      "30/36:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"python\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":15})\n",
      "git_request = response.json() \n",
      "git_request\n",
      "33/1: runfile('D:/OneDrive/00_Tilburg/00_RM2021/Unit 1/oDCM/github-api/scraping/scap.py', wdir='D:/OneDrive/00_Tilburg/00_RM2021/Unit 1/oDCM/github-api/scraping')\n",
      "33/2: runfile('D:/OneDrive/00_Tilburg/00_RM2021/Unit 1/oDCM/github-api/scraping/scap.py', wdir='D:/OneDrive/00_Tilburg/00_RM2021/Unit 1/oDCM/github-api/scraping')\n",
      "35/1:\n",
      "# Run this code now\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find_all(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "35/2:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "soup.find_all(class_=\"product_pod\")[0].find(\"a\").attrs[\"href\"]\n",
      "35/3: # your answer goes here!\n",
      "35/4:\n",
      "# Question 1\n",
      "url_book = soup.find_all(class_=\"product_pod\")[1].find(\"a\").attrs[\"href\"]\n",
      "print(url_book)\n",
      "35/5:\n",
      "# Question 2 \n",
      "base_url = \"https://books.toscrape.com/catalogue/\" # gives a 403 error if you run the URL separately but works as expected once combined with the book url\n",
      "book_url = base_url + url_book[6:] # so we skip characters with index 0, 1, 2, 3, 4, 5: \"../../\"\n",
      "print(book_url)\n",
      "35/6:\n",
      "# Question 3\n",
      "base_url = \"https://books.toscrape.com/catalogue/\"\n",
      "book_url = base_url + url_book\n",
      "book_url = book_url.replace('../', '')\n",
      "print(book_url)\n",
      "35/7:\n",
      "# list of all books on the overview page\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book_urls = []\n",
      "\n",
      "for book in books: \n",
      "    book_url = book.find(\"a\").attrs[\"href\"]\n",
      "    book_urls.append(book_url)\n",
      "    \n",
      "# print the first five urls\n",
      "print(book_urls[0:5])\n",
      "35/8:\n",
      "book_list = []\n",
      "\n",
      "for book in books: \n",
      "    book_title = book.find(\"img\").attrs[\"alt\"] \n",
      "    book_url = book.find(\"a\").attrs[\"href\"]\n",
      "    book_list.append({'title': book_title,\n",
      "                      'url': book_url})\n",
      "35/9: next((book for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "35/10: # your answer goes here!\n",
      "35/11:\n",
      "# Question 1\n",
      "for id, book in enumerate(book_list):\n",
      "    book[\"full_url\"] = (base_url + book[\"url\"]).replace('../','')\n",
      "\n",
      "# show the first five elements\n",
      "book_list[0:5]\n",
      "35/12:\n",
      "# Question 2 \n",
      "next((book for book in book_list if book[\"title\"] == \"Black Dust\"), None)\n",
      "\n",
      "# it does not return any result because the book does not exist (this book is on shown on the 2nd page and we only scraped the first one!)\n",
      "35/13:\n",
      "counter = 1\n",
      "full_url = base_url + \"page-\" + str(counter) + \".html\" \n",
      "print(full_url)\n",
      "35/14:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = []\n",
      "\n",
      "for counter in range(1, 51):\n",
      "    full_url = base_url + \"page-\" + str(counter) + \".html\" \n",
      "    page_urls.append(full_url)\n",
      "35/15:\n",
      "# print the last five page urls (btw, run print(page_urls) for yourself to see all page URLs!)\n",
      "print(\"The number of page urls in the list is: \" + str(len(page_urls)))\n",
      "35/16: # your answer goes here!\n",
      "35/17:\n",
      "# Question 2\n",
      "base_url = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "\n",
      "for counter in range(1, 11):\n",
      "    full_url = base_url + str(counter)\n",
      "    quote_page_urls.append(full_url)\n",
      "\n",
      "print(quote_page_urls)\n",
      "35/18:\n",
      "# run this cell again to see the timer in action yourself!\n",
      "from time import sleep\n",
      "sleep(5)\n",
      "print(\"I'll be printed to the console after 5 seconds!\")\n",
      "35/19: # your answer goes here!\n",
      "35/20:\n",
      "sleep(2*60)\n",
      "print(\"Done!\")\n",
      "35/21:\n",
      "def generate_page_urls(base_url, num_pages):\n",
      "    '''generate a list of full page urls from a base url and counter that has takes on the values between 1 and num_pages'''\n",
      "    page_urls = []\n",
      "    \n",
      "    for counter in range(1, num_pages + 1):\n",
      "        full_url = base_url + \"page-\" + str(counter) + \".html\"\n",
      "        page_urls.append(full_url)\n",
      "        \n",
      "    return page_urls\n",
      "35/22: generate_page_urls(\"https://books.toscrape.com/catalogue/category/books_1/\", 5)\n",
      "35/23:\n",
      "def extract_book_urls(page_urls):\n",
      "    '''collect the book title and url for every book on all page urls'''\n",
      "    book_list = []\n",
      "    \n",
      "    # collect all books on page_url\n",
      "    for page_url in page_urls: \n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "        \n",
      "        # for each book on that page look up the title and url and store it in a list\n",
      "        for book in books: \n",
      "            book_title = book.find(\"img\").attrs[\"alt\"] \n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "            \n",
      "        sleep(1)  # pause 1 second after each request\n",
      "            \n",
      "    return book_list\n",
      "35/24:\n",
      "# this cell references functions in other cells, therefore make sure you have loaded all cells above first! (Cell > Run All Above)\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = generate_page_urls(base_url, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extract_book_urls(page_urls)\n",
      "35/25:\n",
      "# Preview the results\n",
      "book_list[0:5]\n",
      "35/26: # Your answer goes here\n",
      "35/27:\n",
      "# Question 1\n",
      "\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = generate_page_urls(base_url, 5) \n",
      "book_list = extract_book_urls(page_urls)\n",
      "book_list\n",
      "35/28:\n",
      "# Question 2\n",
      "def extract_book_urls(page_urls):\n",
      "    '''collect the book title and url for every book on all page urls'''\n",
      "    book_list = []\n",
      "    \n",
      "    # this part is the same as above\n",
      "    for page_url in page_urls: \n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "\n",
      "        for book in books: \n",
      "            book_title = book.find(\"img\").attrs[\"alt\"] \n",
      "            book_url = (\"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"]).replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text # only this changed!\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url,\n",
      "                             \"instock\": book_instock}) # and this line!\n",
      "            \n",
      "        sleep(1)  \n",
      "            \n",
      "    return book_list\n",
      "35/29:\n",
      "# Question 3\n",
      "def extract_book_urls(page_urls):\n",
      "    '''collect the book title and url for every book on all page urls'''\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls: \n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "\n",
      "        for book in books: \n",
      "            book_title = book.find(\"img\").attrs[\"alt\"] \n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "            \n",
      "            # addition to clean up the text (the rest remains the same!)\n",
      "            book_instock = book_instock.replace('\\n','').replace(' ','') # first replace a line-break (`\\n`) by an empty space, then replace a space (' ') by an empty space\n",
      "            \n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url,\n",
      "                             \"instock\": book_instock})\n",
      "            \n",
      "        sleep(1) \n",
      "            \n",
      "    return book_list\n",
      "\n",
      "# test function!\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = generate_page_urls(base_url, 2) \n",
      "book_list = extract_book_urls(page_urls)\n",
      "book_list\n",
      "35/30:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_btn = soup.find(class_= \"next\") # observe the similarity with the code snippet used above\n",
      "    return next_btn.find(\"a\").attrs[\"href\"] if next_btn else None\n",
      "\n",
      "page_1 = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "print(\"The next page is: \" + check_next_page(page_1))\n",
      "35/31: # your answer goes here!\n",
      "35/32:\n",
      "# Question 1 \n",
      "output = check_next_page(\"https://books.toscrape.com/catalogue/page-50.html\")\n",
      "print(output) # the output is None because page 50 is the last one\n",
      "35/33:\n",
      "# Question 2 \n",
      "def next_page_url(url):\n",
      "    base_url = \"https://books.toscrape.com/catalogue/\"\n",
      "    if url != None: \n",
      "        page_url = base_url + url \n",
      "        return page_url \n",
      "    else: \n",
      "        print(\"This is already the last page!\")\n",
      "        \n",
      "next_page_url(check_next_page(\"https://books.toscrape.com/catalogue/page-50.html\"))\n",
      "35/34:\n",
      "def extract_all_books(page_url):\n",
      "    books = []\n",
      "    while page_url:\n",
      "        print(page_url)\n",
      "        for book in extract_book_urls([page_url]):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(page_url) != None: \n",
      "            page_url = \"https://books.toscrape.com/catalogue/category/books_1/\" + check_next_page(page_url)\n",
      "        else: \n",
      "            break\n",
      "        \n",
      "        # if \"page-4\" in page_url: break # (activate this if you don't want to run the entire loop)\n",
      "    return books\n",
      "35/35: book_list = extract_all_books(\"https://books.toscrape.com/catalogue/page-1.html\")\n",
      "35/36: book_list\n",
      "35/37: # your answer goes here!\n",
      "35/38:\n",
      "# Question 1\n",
      "# There are 1000 books \n",
      "\n",
      "len(books)\n",
      "\n",
      "#That's 50 pages into 20 products, which matches our expectations.\n",
      "35/39: books[0]\n",
      "35/40:\n",
      "# Question 2\n",
      "# we use one of the code snippets from above to search for the title\n",
      "\n",
      "next((book for book in book_list if book[\"title\"] == 'The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics'), None)\n",
      "\n",
      "# we can view the URL and open it in the browser.\n",
      "35/41:\n",
      "# Question 3\n",
      "books_instock = [book for book in book_list if book[\"instock\"] == \"Instock\"]\n",
      "len(books_instock)\n",
      "\n",
      "# All books are in stock!\n",
      "35/42:\n",
      "# Question 4\n",
      "len([book for book in book_list if \"boat\" in book[\"title\"].lower()])\n",
      "\n",
      "# here, we're checking for the appearence of the word \"boat\" in the title.\n",
      "35/43: [book for book in book_list if book[\"title\"] == \"Black Dust\"]\n",
      "35/44:\n",
      "res = requests.get('https://books.toscrape.com/catalogue/black-dust_976/index.html')\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "len(soup.find(id=\"content_inner\").find(\"p\", class_ = \"star-rating\").find_all(class_ = \"icon-star\"))\n",
      "35/45: soup\n",
      "35/46: soup.find(id=\"content_inner\")\n",
      "35/47: soup.find(id=\"content_inner\").find_all(\"p\")\n",
      "35/48: soup.find(id=\"content_inner\").find_all(\"p\", class_ = \"star-rating\")\n",
      "35/49: soup.find(id=\"content_inner\").find(\"p\", class_ = \"star-rating\")\n",
      "35/50: soup.find(id=\"content_inner\").find(\"p\", class_ = \"star-rating\").find_all(class_ = \"icon-star\")\n",
      "35/51: len(soup.find(id=\"content_inner\").find(\"p\", class_ = \"star-rating\").find_all(class_ = \"icon-star\"))\n",
      "35/52: # your answer goes here!\n",
      "35/53:\n",
      "# Question 1\n",
      "def get_book_description(books):\n",
      "    book_descriptions = []\n",
      "    \n",
      "    for book in books: \n",
      "        page_url = book[\"url\"]\n",
      "\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        # tip: look at the Google Inspector screenshot below \n",
      "        description = soup.find(id=\"content_inner\").find_all(\"p\")[3].get_text()\n",
      "        title = soup.find(id=\"content_inner\").find('img')['alt']\n",
      "        book_descriptions.append({'url': page_url,\n",
      "                                  'title': title,\n",
      "                                  'description': description})\n",
      "    return book_descriptions\n",
      "\n",
      "book_descriptions = get_book_description(book_list[0:5])\n",
      "book_descriptions\n",
      "\n",
      "# Question 2\n",
      "# t√É¬©g√É¬© (or similarly encoded strings) are characters from languages other than English, which use an extended character space.\n",
      "35/54:\n",
      "from datetime import datetime\n",
      "\n",
      "now = datetime.now()\n",
      "print(now)\n",
      "35/55:\n",
      "import csv \n",
      "\n",
      "with open(\"book_descriptions.csv\", \"w\") as csv_file: # <<- this is the line with the \"flag\"l see exercises below\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"description\", \"date_time\"])\n",
      "    now = datetime.now()\n",
      "    for book in book_descriptions: # here we reference the book_descriptions list - make sure it's loaded otherwise you get an error! (Cell > Run All Above)\n",
      "        writer.writerow([book['title'], book['description'], now])\n",
      "print('done!')\n",
      "36/1:\n",
      "# request JSON output from github search API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": \"open education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "36/2:\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\",                                     },\n",
      "                       params = {\"q\": \"python\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":15})\n",
      "git_request = response.json() \n",
      "git_request\n",
      "36/3:\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\",                                     },\n",
      "                       params = {\"q\": \"python\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "git_request = response.json() \n",
      "git_request\n",
      "36/4:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"python\")\n",
      "36/5:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "36/6:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} r\"espositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "36/7:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "36/8:\n",
      "# Function to calculate the total pages\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "term = \"open education\"\n",
      "total_page(term)\n",
      "36/9:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "36/10:\n",
      "# Function to calculate the total pages\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "term = \"open education\"\n",
      "total_page(term)\n",
      "36/11:\n",
      "# Building a function to request more pages but didn't succeed yet\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "\n",
      "    while True: #while page <= total page, keep requesting and adding repository data to repo_list\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 50,\n",
      "                                        \"page\": page}) #setting 50 items per page\n",
      "        repo_request = response.json()\n",
      "        total_page = math.ceil(repo_request['total_count']/50) #total pages = total items/items per page\n",
      "        repo_list.extend(repo_request['items'])\n",
      "        if page <= total_page:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "36/12:\n",
      "# Tried with personal authentication token but it didn't work (limited results to only 1000 first items)\n",
      "git_token = \"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\" \n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "res=requests.get(url,\n",
      "                 headers={\"Authorization\": git_token},\n",
      "                 params = {\"q\": \"open education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "repos=res.json()\n",
      "while 'next' in res.links.keys():\n",
      "  res=requests.get(res.links['next']['url'],headers={\"Authorization\": git_token})\n",
      "  repos.extend(res.json())\n",
      "36/13:\n",
      "# Tried with personal authentication token but it didn't work (limited results to only 1000 first items)\n",
      "git_token = \"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\" \n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "res=requests.get(url,\n",
      "                 headers={\"Authorization\": git_token},\n",
      "                 params = {\"q\": \"python\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":15})\n",
      "repos=res.json()\n",
      "36/14:\n",
      "# Tried with personal authentication token but it didn't work (limited results to only 1000 first items)\n",
      "git_token = \"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\" \n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "res=requests.get(url,\n",
      "                 headers={\"Authorization\": git_token},\n",
      "                 params = {\"q\": \"python\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":15})\n",
      "repos=res.json()\n",
      "38/1:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "38/2:\n",
      "# Function to calculate the total pages\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "term = \"open education\"\n",
      "total_page(term)\n",
      "38/3:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "\n",
      "git_request.links\n",
      "38/4: git_request['headers']\n",
      "38/5:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "38/6: git_request\n",
      "38/7:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    returns (git_request)\n",
      "   \n",
      "find_repo(\"open education\")\n",
      "38/8:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    return git_request\n",
      "   \n",
      "find_repo(\"open education\")\n",
      "38/9:\n",
      "# Building a function to request more pages but didn't succeed yet\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "\n",
      "    while True: #while page <= total page, keep requesting and adding repository data to repo_list\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 50,\n",
      "                                        \"page\": page}) #setting 50 items per page\n",
      "        repo_request = response.json()\n",
      "        total_page = math.ceil(repo_request['total_count']/50) #total pages = total items/items per page\n",
      "        repo_list.extend(repo_request['items'])\n",
      "        if page <= total_page:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "38/10: git_request['headers']\n",
      "38/11: git_request\n",
      "38/12:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    return git_request\n",
      "   \n",
      "find_repo(\"open education\")\n",
      "38/13: git_request[\"Headers\"]\n",
      "38/14: git_request\n",
      "38/15:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    return git_request\n",
      "   \n",
      "git_request = find_repo(\"open education\")\n",
      "38/16:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    return git_request\n",
      "   \n",
      "git_request = find_repo(\"open education\")\n",
      "38/17: git_request\n",
      "38/18:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    git_request = find_repo(\"open education\")\n",
      "   \n",
      "git_request\n",
      "38/19:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "       \n",
      "git_request\n",
      "38/20:\n",
      "# Building a function to request more pages but didn't succeed yet\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "\n",
      "    while True: #while page <= total page, keep requesting and adding repository data to repo_list\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 50,\n",
      "                                        \"page\": page}) #setting 50 items per page\n",
      "        repo_request = response.json()\n",
      "        total_page = math.ceil(repo_request['total_count']/50) #total pages = total items/items per page\n",
      "        repo_list.extend(repo_request['items'])\n",
      "        if page <= total_page:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "41/1:\n",
      "# Make selenium and chromedriver work for github.com\n",
      "# install also - pip install selenium , pip install webdriver_manager\n",
      "\n",
      "import selenium.webdriver\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.chrome.options import Options\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "#driver = webdriver.Chrome()\n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "base_url = \"https://github.com/search?\"\n",
      "driver.get(base_url)\n",
      "43/1: ?HTMLParser\n",
      "43/2: ??HTMLParser\n",
      "43/3:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, params = {\"q\": term, \"per_page\":100, \"page\": 1})\n",
      "soup = BeautifulSoup(response.text, \"html.parser\")\n",
      "response=soup.find_all(class_=\"repo-list-item hx_hit-repo d-flex flex-justify-start py-4 public source\")\n",
      "43/4:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, params = {\"q\": \"education\", \"per_page\":100, \"page\": 1})\n",
      "soup = BeautifulSoup(response.text, \"html.parser\")\n",
      "response=soup.find_all(class_=\"repo-list-item hx_hit-repo d-flex flex-justify-start py-4 public source\")\n",
      "43/5: response\n",
      "43/6: soup\n",
      "43/7:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://github.com/search/repositories\"\n",
      "response = requests.get(url, params = {\"q\": \"education\", \"per_page\":100, \"page\": 1})\n",
      "soup = BeautifulSoup(response.text, \"html.parser\")\n",
      "response=soup.find_all(class_=\"repo-list-item hx_hit-repo d-flex flex-justify-start py-4 public source\")\n",
      "43/8: soup\n",
      "43/9: response\n",
      "43/10: soup\n",
      "43/11:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://github.com/search\"\n",
      "response = requests.get(url, params = {\"q\": \"education\", \"per_page\":100, \"page\": 1, \"type\": Repositories})\n",
      "soup = BeautifulSoup(response.text, \"html.parser\")\n",
      "response=soup.find_all(class_=\"repo-list-item hx_hit-repo d-flex flex-justify-start py-4 public source\")\n",
      "43/12:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://github.com/search\"\n",
      "response = requests.get(url, params = {\"q\": \"education\", \n",
      "                                       \"per_page\":100, \n",
      "                                       \"page\": 1, \n",
      "                                       \"type\": \"Repositories\"})\n",
      "soup = BeautifulSoup(response.text, \"html.parser\")\n",
      "response=soup.find_all(class_=\"repo-list-item hx_hit-repo d-flex flex-justify-start py-4 public source\")\n",
      "43/13: response\n",
      "43/14: soup\n",
      "45/1:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    total_items = git_request['total_count']\n",
      "    return total_items\n",
      "\n",
      "find_repo(\"open education\")\n",
      "45/2:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    total_pages = math.ceil(git_request['total_count'])/len(git_request['items'])\n",
      "    return total_pages\n",
      "\n",
      "find_repo(\"open education\")\n",
      "45/3:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    total_pages = math.ceil((git_request['total_count'])/len(git_request['items']))\n",
      "    return total_pages\n",
      "\n",
      "find_repo(\"open education\")\n",
      "45/4:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    total_pages = math.ceil((git_request['total_count'])/len(git_request['items']))\n",
      "    return total_pages\n",
      "\n",
      "find_repo(\"python\")\n",
      "45/5: find_repo(\"python\")\n",
      "45/6:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def total_pages(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    total_pages = math.ceil((git_request['total_count'])/len(git_request['items']))\n",
      "    return total_pages\n",
      "45/7: total_pages(\"python\")\n",
      "45/8:\n",
      "def find_repo(term):\n",
      "    no_of_pages = total_pages(term)\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "    while True:\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 100,\n",
      "                                        \"page\": page})\n",
      "        repo_request = response.json()\n",
      "        repo_list.extend(repo_request['items'])\n",
      "        if page < no_of_pages:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "45/9:\n",
      "def find_repo(term):\n",
      "    no_of_pages = total_pages(term)\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "    while True:\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 100,\n",
      "                                        \"page\": page})\n",
      "        repo_request = response.json()\n",
      "        repo_list.extend(repo_request['items'])\n",
      "        if page < no_of_pages:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "\n",
      "find_repo(\"education\")\n",
      "45/10:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "page = 1\n",
      "repo_list =[]\n",
      "while True:\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": term,\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": page})\n",
      "    repo_request = response.json()\n",
      "    repo_list.extend(repo_request['items'])\n",
      "    if page < 50:\n",
      "        page += 1\n",
      "    else: \n",
      "        return repo_list\n",
      "45/11:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "page = 1\n",
      "repo_list =[]\n",
      "while True:\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": term,\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": page})\n",
      "    repo_request = response.json()\n",
      "    repo_list.extend(repo_request['items'])\n",
      "    if page < 50:\n",
      "        page += 1\n",
      "    else: \n",
      "        repo_list\n",
      "45/12:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "page = 1\n",
      "repo_list =[]\n",
      "while True:\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": education,\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": page})\n",
      "    repo_request = response.json()\n",
      "    repo_list.extend(repo_request['items'])\n",
      "    if page < 50:\n",
      "        page += 1\n",
      "    else: \n",
      "        repo_list\n",
      "45/13:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "page = 1\n",
      "repo_list =[]\n",
      "while True:\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": \"education\",\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": page})\n",
      "    repo_request = response.json()\n",
      "    repo_list.extend(repo_request['items'])\n",
      "    if page < 50:\n",
      "        page += 1\n",
      "    else: \n",
      "        repo_list\n",
      "45/14:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url,\n",
      "                        headers={\"Accept\": \"application/json\"},\n",
      "                        params={\"q\": \"education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\": 1})\n",
      "repo_request = response.json()\n",
      "repo_request['items']\n",
      "45/15:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "repo_list[]\n",
      "i = 1\n",
      "for i in range (1: (no_of_pages + 1)):\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": \"education\",\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": i})\n",
      "    repo_request = response.json()\n",
      "    repo_list.append(repo_request['items'])\n",
      "45/16:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "repo_list = []\n",
      "i = 1\n",
      "for i in range (1: (no_of_pages + 1)):\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": \"education\",\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": i})\n",
      "    repo_request = response.json()\n",
      "    repo_list.append(repo_request['items'])\n",
      "45/17:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "repo_list = []\n",
      "i = 1\n",
      "for i in range (1, (no_of_pages + 1)):\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": \"education\",\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": i})\n",
      "    repo_request = response.json()\n",
      "    repo_list.append(repo_request['items'])\n",
      "45/18:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "repo_list = []\n",
      "i = 1\n",
      "for i in range (1, 10):\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": \"education\",\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": i})\n",
      "    repo_request = response.json()\n",
      "    repo_list.append(repo_request['items'])\n",
      "45/19:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "repo_list = []\n",
      "i = 1\n",
      "for i in range (1, 10):\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": \"education\",\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": i})\n",
      "    repo_request = response.json()\n",
      "    repo_list.append(repo_request['items'])\n",
      "45/20:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "repo_list = []\n",
      "i = 1\n",
      "for i in range (1, 10):\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": \"education\",\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": i})\n",
      "    repo_request = response.json()\n",
      "    repo_list.append(repo_request['items'])\n",
      "    i+=1\n",
      "repo_list\n",
      "45/21:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url,\n",
      "                        headers={\"Accept\": \"application/json\"},\n",
      "                        params={\"q\": \"education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\": i})\n",
      "repo_request = response.json()\n",
      "repo_request['items']\n",
      "45/22:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url,\n",
      "                        headers={\"Accept\": \"application/json\"},\n",
      "                        params={\"q\": \"education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\": 1})\n",
      "repo_request = response.json()\n",
      "repo_request['items']\n",
      "45/23:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url,\n",
      "                        headers={\"Accept\": \"application/json\"},\n",
      "                        params={\"q\": \"education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\": 15})\n",
      "repo_request = response.json()\n",
      "repo_request['items']\n",
      "45/24:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url,\n",
      "                        headers={\"Accept\": \"application/json\"},\n",
      "                        params={\"q\": \"education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\": 10})\n",
      "repo_request = response.json()\n",
      "repo_request['items']\n",
      "45/25:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url,\n",
      "                        headers={\"Accept\": \"application/json\"},\n",
      "                        params={\"q\": \"education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\": 11})\n",
      "repo_request = response.json()\n",
      "repo_request['items']\n",
      "45/26:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url,\n",
      "                        headers={\"Accept\": \"application/json\"},\n",
      "                        params={\"q\": \"education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\": 11})\n",
      "repo_request = response.json()\n",
      "repo_request['items']\n",
      "\n",
      "total_pages(\"education\")\n",
      "45/27: total_pages(\"education\")\n",
      "45/28: total_pages(\"open education\")\n",
      "45/29: total_pages(\"python\")\n",
      "45/30:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": term})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/31:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/32:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "response.headers\n",
      "45/33:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(response.headers, \"html.parser\")\n",
      "45/34:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "\n",
      "response.headers\n",
      "45/35:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\";\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "\n",
      "response.headers\n",
      "45/36:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "\n",
      "response.headers\n",
      "45/37:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "\n",
      "response.headers\n",
      "45/38:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python\" + \"created: 2020-01-01T00:00:00..2021-01-01T00:00:00\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "45/39:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python\" + \"created: 2020-01-01T00:00:00..2021-01-01T00:00:00\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/40:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python\" + \"created: 2020-01-01..2021-01-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/41:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"cats created:2016-01-01T00:00:00Z..2017-01-01T00:00:00Z\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/42:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2016-01-01T00:00:00Z..2017-01-01T00:00:00Z\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/43:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2016-01-01T00:00:00Z..2016-01-08T00:00:00Z\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/44:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2016-01-01T00:00:00..2016-01-07T00:00:00\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/45:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2016-01-01T00:00:00..2016-01-05T00:00:00\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/46:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2020-01-01T00:00:00..2020-01-05T00:00:00\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/47:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2020-01-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/48:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2020-10-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/49:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2020-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/50:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2016-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/51:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2021-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/52:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2015-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/53:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2021-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/54:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2021-01-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/55:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education created:2021-01-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/56:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education created:2021-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/57:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education pushed:>2021-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/58:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python pushed:>2021-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/59:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"customer behavior pushed:>2021-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/60:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning created:>2021-01-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/61:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/62:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2021-01-01T00:00:00Z..2021-01-02T00:00:00Z\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/63:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2021-05-01T00:00:00Z..2021-05-02T00:00:00Z\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/64:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2021-05-01T00:00:00Z..2021-05-01T00:12:00Z\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/65:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2021-05-01T00:00:00Z..2021-05-01T12:00:00Z\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/66:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today()\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/67:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today()\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/68:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today()\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/69:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today()\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/70: ?datetime.today\n",
      "45/71: ?datetime\n",
      "45/72:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "45/73:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today()\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/74:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today()\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/75:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today()\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/76: datetime.today()\n",
      "45/77:\n",
      "datetime.today()\n",
      "until\n",
      "45/78:\n",
      "datetime.today()\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)\n",
      "45/79: since\n",
      "45/80: until\n",
      "45/81: until < datetime.today()\n",
      "45/82:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/83:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {git_request[\"total_count\"]}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/84:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {git_request(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/85:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {git_request['total_count']}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/86:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/87:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request['total_count']\n",
      "45/88:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "print(f'Repositories created: {git_request['total_count']}')\n",
      "45/89:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f\"Repositories created between {since} and {until}: {git_request['total_count']}\")\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/90:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    git_request['total_count']\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/91:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "count = ()\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    count.append(git_request['total_count']) \n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/92:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "count = []\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    count.append(git_request['total_count']) \n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/93:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "count = []\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    \n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/94: git_request\n",
      "45/95:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"tilburguniversity\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "count = []\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    \n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/96: git_request\n",
      "45/97:\n",
      "# Function to calculate the total pages\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "term = \"open education\"\n",
      "total_page(term)\n",
      "45/98:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"tilburguniversity\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "count = []\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    git_request['total_count']\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/99:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "URL = f'https://api.github.com/search/repositories?q=open education created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/100:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"Tilburg University\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=3)  # Since 3 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/101:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=3)  # Since 3 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/102:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=3)  # Since 3 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/103:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 3 days ago\n",
      "until = since + timedelta(hour=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/104:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 3 days ago\n",
      "until = since + timedelta(hours=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/105:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=2)  # Since 3 days ago\n",
      "until = since + timedelta(hours=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/106:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(hours=24)  # Since 3 days ago\n",
      "until = since + timedelta(hours=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/107:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=24)  # Since 3 days ago\n",
      "until = since + timedelta(hours=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/108:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 3 days ago\n",
      "until = since + timedelta(hours=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=1)\n",
      "45/109:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 3 days ago\n",
      "until = since + timedelta(hours=12)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/110:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "45/111:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/112:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request(\"items\")\n",
      "45/113:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request.get(\"items\")\n",
      "45/114:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    repo_list.append(repo_request.json().get(\"items\"))\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/115: len(repo_list)\n",
      "45/116: repo_list\n",
      "45/117:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "len(git_request.get(\"items\"))\n",
      "45/118: git_request.get(\"items\")\n",
      "45/119: git_request.get(\"items\")[1]\n",
      "45/120:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    repo_list.extend(repo_request.json().get(\"items\"))\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/121: repo_list\n",
      "45/122: len(repo_list)\n",
      "45/123:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    repo_list.extend(repo_request.json().get(\"items\"))\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/124: repo_request\n",
      "45/125: repo_request(\"items\")\n",
      "45/126: repo_request.json[\"items\"]\n",
      "45/127: repo_request.json().get(\"items\")\n",
      "45/128: len(repo_request.json().get(\"items\"))\n",
      "45/129:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    repo_list.extend(repo_request.json().get(\"items\"))\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/130:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    repo_list.extend(repo_request.json().get(\"items\"))\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/131:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    repo_list.extend(repo_request.json().get(\"items\"))\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/132:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'No. of pages between {since} and {until}: {repo_request.json().get(\"total_count\")/100}')\n",
      "    \n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/133:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'No. of pages between {since} and {until}: {math.ceil(repo_request.json().get(\"total_count\")/100)}')\n",
      "    \n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/134:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/135: len(repo_list)\n",
      "45/136: repo_list\n",
      "45/137: repo_list[1]\n",
      "45/138: class(repo_list[1])\n",
      "45/139: len(repo_list[1])\n",
      "45/140: len(repo_list[15])\n",
      "45/141: len(repo_list)\n",
      "45/142: len(repo_list[2])\n",
      "45/143: len(repo_list[10])\n",
      "45/144: len(repo_list[11])\n",
      "45/145: len(repo_list[14])\n",
      "45/146: len(repo_list[15])\n",
      "45/147: len(repo_list[0])\n",
      "45/148: len(repo_list[15])\n",
      "45/149: len(repo_list[14])\n",
      "45/150:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/151:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"consumer\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/152: len(repo_list[14])\n",
      "45/153: len(repo_list)\n",
      "45/154: len(repo_list(1))\n",
      "45/155: len(repo_list[1])\n",
      "45/156: len(repo_list[2])\n",
      "45/157: len(repo_list[0])\n",
      "45/158:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/159: len(repo_list)\n",
      "45/160: len(repo_list[0])\n",
      "45/161: len(repo_list[1])\n",
      "45/162: len(repo_list[2])\n",
      "45/163:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/164: len(repo_list[7])\n",
      "45/165: len(repo_list[6])\n",
      "45/166: len(repo_list[14])\n",
      "48/1:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(months=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/2:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(months=2)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/3:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=2)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/4:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=2)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/5: len(repo_list)\n",
      "48/6: repo_list\n",
      "48/7: repo_list[1]\n",
      "48/8:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=2)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/9: len(repo_list)\n",
      "48/10: len(repo_list[1])\n",
      "48/11: repo_list[1]\n",
      "48/12:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(years=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/13:\n",
      "datetime.today()\n",
      "since = datetime.today() - timedelta(years=1)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)\n",
      "48/14:\n",
      "datetime.today()\n",
      "since = datetime.today() - timedelta(month=1)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)\n",
      "48/15:\n",
      "from dateutil.relativedelta import relativedelta\n",
      "datetime.today()\n",
      "since = datetime.today() + relativedelta(months=-6)  # Since 6 months ago\n",
      "until = since + timedelta(days=1)\n",
      "48/16:\n",
      "from dateutil.relativedelta import relativedelta\n",
      "datetime.today()\n",
      "since = datetime.today() + relativedelta(months=-6)  # Since 6 months ago\n",
      "until = since + timedelta(days=1) \n",
      "since\n",
      "48/17:\n",
      "from dateutil.relativedelta import relativedelta\n",
      "datetime.today()\n",
      "since = datetime.today() + relativedelta(months=-6)  # Since 6 months ago\n",
      "until = since + timedelta(days=1) \n",
      "since\n",
      "until\n",
      "48/18:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(years=-1)  # Since 1 year ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/19:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Since 1 month ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/20:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Since 1 month ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/21:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Since 1 month ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/22:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Since 1 month ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/23:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Since 1 month ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/24: len(repo_list[])\n",
      "48/25: len(repo_list())\n",
      "48/26: len(repo_list)\n",
      "48/27: repo_list\n",
      "48/28: repo_list[101]\n",
      "48/29: len(repo_list)\n",
      "48/30: repo_list[100]\n",
      "48/31: len(repo_list[1])\n",
      "48/32: len(repo_list[0])\n",
      "48/33:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "48/34:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "48/35: len(repo_list)\n",
      "48/36:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "48/37:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "48/38:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/1:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/2:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/3:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/4:\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/5:\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/6:\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/7:\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/8:\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/9:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/10: math.ceil(122/100)\n",
      "50/11:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(days=-1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/12: len(repo_list)\n",
      "50/13:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(days=-30)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/14:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(days=-5)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/15: len(repo_list)\n",
      "50/16:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    return repo_list\n",
      "50/17: find_repo(\"machine learning\",2)\n",
      "50/18: len(find_repo(\"machine learning\",2))\n",
      "50/19:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=2)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/20: repo_list = find_repo(\"education\",1))\n",
      "50/21: repo_list = find_repo(\"python\",1))\n",
      "50/22: repo_list = find_repo(\"python\",1)\n",
      "50/23: len(repo_list)\n",
      "50/24: repo_list[1]\n",
      "50/25: repo_list[1].get(\"id\")\n",
      "50/26: repo_list[1].get(\"id\",\"name\")\n",
      "50/27: repo_list[1].get(\"id\",\"name\")\n",
      "50/28: repo_list[1].get(\"id\").(\"name\")\n",
      "50/29: repo_list[1].get(\"id\").get(\"name\")\n",
      "50/30: repo_list[1].get(\"name\")\n",
      "50/31: repo_list[1]\n",
      "50/32: repo_list[1].items['id']\n",
      "50/33: repo_list[1].items('id')\n",
      "50/34:\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/35:\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\", default=0)}')\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/36:\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\", 0)}')\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/37:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=30)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\",0)/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/38:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=30)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\",0)/100) #calculating the total No. of pages\n",
      "    while no_page > 0:\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\",0)) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/39:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=30)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\",0)/100) #calculating the total No. of pages\n",
      "    if no_page > 0:\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    else:\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/40:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=30)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\",0)/100) #calculating the total No. of pages\n",
      "    if no_page > 0:\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    else:\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/41:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=30)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\",0)/100) #calculating the total No. of pages\n",
      "    if no_page > 0:\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    else:\n",
      "    # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "50/42: repo_list[1](1)\n",
      "50/43: repo_list[1]\n",
      "50/44: repo_list[1][1]\n",
      "50/45: repo_list[1].items(\"id\")\n",
      "50/46: repo_list[1].items(1)\n",
      "50/47: repo_list[1].items\n",
      "50/48: repo_list[1].items()\n",
      "50/49:\n",
      "col = ('id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count')\n",
      "col(1)\n",
      "50/50:\n",
      "col = ('id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count')\n",
      "col[0]\n",
      "50/51:\n",
      "col = ('id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count')\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    for col in col:\n",
      "        col = item.get(col)\n",
      "        repo.append(col)\n",
      "    dt.append(repo)\n",
      "dt\n",
      "50/52: type(col)\n",
      "50/53:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    for col in col:\n",
      "        col = item.get(col)\n",
      "        repo.append(col)\n",
      "    dt.append(repo)\n",
      "dt\n",
      "50/54: type(col)\n",
      "50/55:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": fork})\n",
      "dt[1]\n",
      "50/56:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt[1]\n",
      "50/57:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt[2]\n",
      "50/58:\n",
      "repo_list = find_repo(\"python\",1)\n",
      "len(repo_list)\n",
      "50/59:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "len(dt)\n",
      "50/60:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt[14]\n",
      "50/61:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt[100]\n",
      "50/62:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt[1000]\n",
      "50/63:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt[50]\n",
      "50/64:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt\n",
      "50/65:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt[0]\n",
      "50/66:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "    return dt\n",
      "50/67: dt = find_repo(\"python\",1)\n",
      "50/68: len(dt)\n",
      "50/69:\n",
      "#function to search for a specific \"term\" and fletch all repositories created in the last {day} days\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    return repo_list\n",
      "50/70:\n",
      "repo_list = find_repo(\"python\",1)\n",
      "len(repo_list)\n",
      "50/71:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "    return dt\n",
      "50/72: dt = find_repo(\"python\",1)\n",
      "50/73: len(dt)\n",
      "50/74: dt[1]\n",
      "50/75: datetime.today()\n",
      "50/76: datetime.date()\n",
      "50/77: datetime.datetime.now()\n",
      "50/78: datetime.now()\n",
      "50/79: datetime.now().date\n",
      "50/80: datetime.now().date()\n",
      "50/81: print(f\"today's date is: {datetime.now().date()}\")\n",
      "50/82:\n",
      "import csv\n",
      "\n",
      "with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "    for repo in dt:\n",
      "        writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "print('done!')\n",
      "50/83: pwd()\n",
      "50/84:\n",
      "import csv\n",
      "import pandas as pd   \n",
      "\n",
      "with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "    for repo in dt:\n",
      "        writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\")\n",
      "\n",
      "rep\n",
      "50/85:\n",
      "import csv\n",
      "import pandas as pd   \n",
      "\n",
      "with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "    for repo in dt:\n",
      "        writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= ‚Äô;‚Äô)\n",
      "\n",
      "rep\n",
      "50/86:\n",
      "import csv\n",
      "import pandas as pd   \n",
      "\n",
      "with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "    for repo in dt:\n",
      "        writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "\n",
      "rep\n",
      "50/87:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "50/88: find_repo(\"python\",1)\n",
      "55/1:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today:\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables into a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "55/2: find_repo(\"python\",1)\n",
      "58/1:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables into a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "58/2:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables into a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "58/3:\n",
      "#Give it a try:\n",
      "find_repo(\"education\",30)\n",
      "58/4:\n",
      "#Give it a try:\n",
      "find_repo(\"education\",1)\n",
      "58/5:\n",
      "#Give it a try:\n",
      "find_repo(\"education\",10)\n",
      "58/6:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables into a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "58/7:\n",
      "#Give it a try:\n",
      "find_repo(\"education\",5)\n",
      "58/8:\n",
      "#Give it a try:\n",
      "find_repo(\"machine learning\",5)\n",
      "58/9:\n",
      "#Give it a try:\n",
      "find_repo(\"python\",1)\n",
      "58/10:\n",
      "#Give it a try:\n",
      "find_repo(\"education\",10)\n",
      "58/11:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables into a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "58/12:\n",
      "#Give it a try:\n",
      "find_repo(\"machine learning\",5)\n",
      "58/13:\n",
      "#Give it a try:\n",
      "find_repo(\"education\",30)\n",
      "58/14:\n",
      "def print_total_repo(term, day):\n",
      "    URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days=1)  # Start fetching repo created 1 month ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\", 0)}')\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "58/15: print_total_repo(\"education\",30)\n",
      "58/16:\n",
      "def print_total_repo(term, day):\n",
      "    URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created 1 month ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\", 0)}')\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "58/17: print_total_repo(\"education\",30)\n",
      "58/18:\n",
      "def print_total_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created 1 month ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\", 0)}')\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "58/19: print_total_repo(\"education\",30)\n",
      "58/20: print_total_repo(\"education\",5)\n",
      "58/21:\n",
      "#Give it a try:\n",
      "find_repo(\"python\",1)\n",
      "60/1:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created N days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the retrieved repositories to a list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables to a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "60/2: find_repo(\"education\",5)\n",
      "60/3: find_repo(\"education\",30)\n",
      "60/4:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created N days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            print(i)\n",
      "            print(requests.status_code)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the retrieved repositories to a list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables to a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "60/5:\n",
      "#Give it a try:\n",
      "find_repo(\"python\",1)\n",
      "60/6:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created N days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            print(i)\n",
      "            print(page_request.status_code)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the retrieved repositories to a list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables to a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "60/7:\n",
      "#Give it a try:\n",
      "find_repo(\"python\",1)\n",
      "60/8:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created N days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            print(i)\n",
      "            print(page_request.status_code)\n",
      "            if (page_request.status_code!=200): print(page_request.text)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the retrieved repositories to a list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables to a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "60/9:\n",
      "#Give it a try:\n",
      "find_repo(\"python\",1)\n",
      "63/1:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GithubToken']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    #HEADERS = {'Authorization': 'token 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'}\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 12 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        a=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{a}.json','a',encoding='utf-8')\n",
      "        \n",
      "        file_list.append(f'{term}_{a}.json')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "    # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=4)\n",
      "    return file_list\n",
      "63/2: find_repo(\"education\",2)\n",
      "63/3: os.environ\n",
      "63/4:\n",
      "import os\n",
      "os.environ\n",
      "64/1:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GithubToken']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    #HEADERS = {'Authorization': 'token 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'}\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 12 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        a=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{a}.json','a',encoding='utf-8')\n",
      "        \n",
      "        file_list.append(f'{term}_{a}.json')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "    # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=4)\n",
      "    return file_list\n",
      "64/2:\n",
      "import os\n",
      "os.environ\n",
      "64/3:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    #HEADERS = {'Authorization': 'token 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'}\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 12 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        a=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{a}.json','a',encoding='utf-8')\n",
      "        \n",
      "        file_list.append(f'{term}_{a}.json')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "    # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=4)\n",
      "    return file_list\n",
      "64/4: find_repo(\"education\",2)\n",
      "64/5:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{fetch_time}.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        file_list.append(f'{term}_{fetch_time}.json')\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "    return file_list\n",
      "64/6:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{fetch_time}.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        file_list.append(f'{term}_{fetch_time}.json')\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "    return file_list\n",
      "64/7: file_list = find_repo(\"education\",2)\n",
      "64/8: file_list\n",
      "66/1:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "66/2:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "66/3:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content['items']\n",
      "66/4:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "66/5:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "len(content)\n",
      "66/6:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[]('items')\n",
      "66/7:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "66/8:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[1]\n",
      "66/9:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content(0)\n",
      "66/10:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[0]\n",
      "66/11:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[0]['items']\n",
      "66/12:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[0]('items')\n",
      "66/13:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[0]['items']\n",
      "66/14:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[0].get('items')\n",
      "66/15:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[0]\n",
      "66/16:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "json.loads(content)\n",
      "66/17:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "json.loads(content)\n",
      "66/18:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "66/19:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "json.loads(content[1])\n",
      "66/20:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "json.loads(content[0])\n",
      "66/21:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "json.loads(content[0])('items')\n",
      "66/22:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "json.loads(content[0])['items']\n",
      "66/23:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "json.loads(f.readlines()[0])['items']\n",
      "66/24:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo_list.append(json.loads(f.readlines()[0])['items'])\n",
      "repo_list\n",
      "66/25: file_list = find_repo(\"education\",2)\n",
      "66/26:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{fetch_time}.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        file_list.append(f'{term}_{fetch_time}.json')\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "    return file_list\n",
      "66/27: file_list = find_repo(\"education\",2)\n",
      "66/28:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo_list.append(json.loads(f.readlines()[0])['items'])\n",
      "repo_list\n",
      "66/29:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "\n",
      "json.loads(f.readlines()[0])['items']\n",
      "66/30:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.append(repo)\n",
      "repo_list\n",
      "66/31: file_list\n",
      "66/32:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "66/33:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/34: file_list = find_repo(\"education\",1)\n",
      "66/35:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.append(repo)\n",
      "repo_list\n",
      "66/36: file_list = find_repo(\"education\",2)\n",
      "66/37:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.append(repo)\n",
      "repo_list\n",
      "66/38:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/39:\n",
      "import json\n",
      "f=open('education_2021-10-05 12_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/40:\n",
      "import json\n",
      "f=open('education_2021-10-06 20_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/41:\n",
      "import json\n",
      "f=open('education_2021-10-06 16_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/42:\n",
      "import json\n",
      "f=open('education_2021-10-06 12_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/43:\n",
      "import json\n",
      "f=open('education_2021-10-06 08_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/44:\n",
      "import json\n",
      "f=open('education_2021-10-06 04_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/45:\n",
      "import json\n",
      "f=open('education_2021-10-06 00_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/46:\n",
      "import json\n",
      "f=open('education_2021-10-05 20_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/47:\n",
      "import json\n",
      "f=open('education_2021-10-05 16_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/48:\n",
      "import json\n",
      "f=open('education_2021-10-05 12_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/49:\n",
      "import json\n",
      "f=open('education_2021-10-07 00_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/50:\n",
      "import json\n",
      "f=open('education_2021-10-07 04_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/51:\n",
      "import json\n",
      "f=open('education_2021-10-07 04_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])\n",
      "repo\n",
      "66/52:\n",
      "import json\n",
      "f=open('education_2021-10-07 08_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])\n",
      "repo\n",
      "66/53: file_list = find_repo(\"education\",1)\n",
      "66/54:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.append(repo)\n",
      "repo_list\n",
      "66/55:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.extend(repo)\n",
      "repo_list\n",
      "66/56:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.extend(repo)\n",
      "len(repo_list)\n",
      "66/57:\n",
      "dt=[]\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name = repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name,\n",
      "               \"url\": url,\n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt\n",
      "66/58:\n",
      "#Sometimes this doesn't work because of rate-limit.\n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.extend(repo)\n",
      "repo_list\n",
      "66/59:\n",
      "#try the function\n",
      "file_list = find_repo(\"education\",3)\n",
      "66/60:\n",
      "#Sometimes this doesn't work because of rate-limit.\n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.extend(repo)\n",
      "repo_list\n",
      "66/61:\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{fetch_time}.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        file_list.append(f'{term}_{fetch_time}.json')\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        \n",
      "    return file_list #return the list of file names\n",
      "66/62:\n",
      "#try the function\n",
      "file_list = find_repo(\"education\",3)\n",
      "66/63:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{fetch_time}.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        file_list.append(f'{term}_{fetch_time}.json')\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "        \n",
      "    return file_list #return the list of file names\n",
      "66/64:\n",
      "#try the function\n",
      "file_list = find_repo(\"education\",3)\n",
      "66/65: file_list[11]\n",
      "66/66: file_list\n",
      "66/67:\n",
      "f=open('education_2021-10-07 09_07_08.537482.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "66/68:\n",
      "f=open('education_2021-10-07 09_07_08.537482.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "con[1]\n",
      "66/69:\n",
      "f=open('education_2021-10-07 09_07_08.537482.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "con\n",
      "66/70:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    # clean file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        file_list.append(filename)\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "        \n",
      "    return file_list #return the list of file names\n",
      "66/71:\n",
      "#try the function\n",
      "file_list = find_repo(\"education\",3)\n",
      "66/72: file_list\n",
      "66/73:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json, 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "con\n",
      "66/74:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "con\n",
      "66/75:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "for item in con:\n",
      "    jsonobj = json.loads(item)\n",
      "    print(jsonobj['total_count'])\n",
      "66/76: con[11]\n",
      "66/77: con[11]['total_count']\n",
      "66/78:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "for item in con:\n",
      "    jsonobj = json.loads(item)\n",
      "    try:\n",
      "        jsonobj['total_count']\n",
      "    except:\n",
      "        next\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    print(jsonobj['total_count'])\n",
      "66/79:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "for item in con:\n",
      "    jsonobj = json.loads(item)\n",
      "    try:\n",
      "        print(jsonobj['total_count'])\n",
      "    except:\n",
      "        next\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    print(jsonobj['total_count'])\n",
      "66/80:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "for item in con:\n",
      "    try:\n",
      "        jsonobj = json.loads(item)\n",
      "        print(jsonobj['total_count'])\n",
      "    except:\n",
      "        next\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    print(jsonobj['total_count'])\n",
      "66/81:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "for item in con:\n",
      "    try:\n",
      "        jsonobj = json.loads(item)\n",
      "        print(jsonobj['total_count'])\n",
      "    except:\n",
      "        next\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    print(jsonobj['total_count'])\n",
      "66/82:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "cntr=0\n",
      "for item in con:\n",
      "    print(cntr)\n",
      "    try:\n",
      "        jsonobj = json.loads(item)\n",
      "        print(jsonobj['total_count'])\n",
      "    except:\n",
      "        next\n",
      "    cnt+=1\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    print(jsonobj['total_count'])\n",
      "66/83:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "cntr=0\n",
      "for item in con:\n",
      "    print(cntr)\n",
      "    try:\n",
      "        jsonobj = json.loads(item)\n",
      "        #print(jsonobj['total_count'])\n",
      "    except:\n",
      "        next\n",
      "    cnt+=1\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    #print(jsonobj['total_count'])\n",
      "66/84:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "cntr=0\n",
      "for item in con:\n",
      "    print(cntr)\n",
      "    try:\n",
      "        jsonobj = json.loads(item)\n",
      "        #print(jsonobj['total_count'])\n",
      "    except:\n",
      "        next\n",
      "    cntr+=1\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    #print(jsonobj['total_count'])\n",
      "66/85:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "cntr=0\n",
      "for item in con:\n",
      "    print(cntr)\n",
      "    try:\n",
      "        jsonobj = json.loads(item)\n",
      "        #print(jsonobj['total_count'])\n",
      "    except:\n",
      "        next\n",
      "    cntr+=1\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    print(jsonobj['total_count'])\n",
      "69/1:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    # clean file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return file_list #return the list of file names\n",
      "69/2:\n",
      "#try the function\n",
      "file_list = find_repo(\"education\",3)\n",
      "69/3: file_list\n",
      "69/4:\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "69/5:\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "\n",
      "con\n",
      "69/6:\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "\n",
      "len(con)\n",
      "69/7:\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "\n",
      "con\n",
      "69/8:\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "\n",
      "con[0]\n",
      "69/9:\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "\n",
      "con[1]\n",
      "69/10:\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt=f.readlines()\n",
      "repo = json.loads(dt[0])\n",
      "69/11:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt=f.readlines()\n",
      "repo = json.loads(dt[0])\n",
      "69/12:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt=f.readlines()\n",
      "repo = json.loads(dt[0])\n",
      "repo\n",
      "69/13:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "69/14:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "69/15:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "len(repo)\n",
      "69/16:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = json.loads(f.readlines()[0])\n",
      "69/17:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = json.loads(f.readlines()[0])\n",
      "69/18:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = json.loads(f.readlines()[0])\n",
      "dt\n",
      "69/19:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = json.loads(f.readlines()[1])\n",
      "dt\n",
      "69/20:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = json.loads(f.readlines())\n",
      "\n",
      "dt\n",
      "69/21:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = json.loads(f.readlines()[0])\n",
      "len(repo)\n",
      "69/22:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[0])\n",
      "69/23:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[0])\n",
      "len(dt)\n",
      "69/24:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[0])\n",
      "dt\n",
      "69/25:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[0])\n",
      "dt[1]\n",
      "69/26:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[0])\n",
      "dt[2]\n",
      "69/27:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[0])\n",
      "len(dt)\n",
      "69/28:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[1])\n",
      "len(dt)\n",
      "69/29:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[4])\n",
      "len(dt)\n",
      "69/30:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = json.loads(f.readlines())\n",
      "len(dt)\n",
      "69/31:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "dt\n",
      "69/32:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "dt[1]\n",
      "69/33:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "dt[0]\n",
      "69/34:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "len(dt)\n",
      "69/35:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "repo=json.loads(dt)\n",
      "repo\n",
      "69/36:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "repo=json.loads(dt)\n",
      "repo\n",
      "dt\n",
      "69/37:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "\n",
      "dt\n",
      "69/38:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "repo = []\n",
      "for dt in dt:\n",
      "    repo.append(json.loads(dt)[\"items\"])\n",
      "repo\n",
      "69/39:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "dt[0]\n",
      "69/40:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "json.loads(dt[0])\n",
      "69/41:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "json.loads(dt[0])['items']\n",
      "69/42:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "repo = []\n",
      "for counter in range(len(dt))\n",
      "    repo.extend(json.loads(dt[counter])['items'])\n",
      "69/43:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "repo = []\n",
      "for counter in range(0,len(dt)+1)\n",
      "    repo.extend(json.loads(dt[counter])['items'])\n",
      "69/44:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "repo = []\n",
      "for dt in dt\n",
      "    repo.extend(json.loads(dt)['items'])\n",
      "69/45:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "repo = []\n",
      "for dt in dt:\n",
      "    repo.extend(json.loads(dt)['items'])\n",
      "72/1:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    # clean file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return file_list #return the list of file names\n",
      "72/2:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    # clean file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return file_list #return the list of file names\n",
      "72/3:\n",
      "#try the function\n",
      "file_list = find_repo(\"education\",2)\n",
      "72/4: file_list\n",
      "72/5: filename\n",
      "72/6:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return filename\n",
      "72/7:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",2)\n",
      "72/8: filename\n",
      "72/9:\n",
      "#run the below \n",
      "#copy paste the json file below\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "73/1:\n",
      "#if there is an error-continue in the next line \n",
      "repo =[]\n",
      "import json \n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list=jsonobj[\"items\"]\n",
      "    \n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "        \n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "        repo.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "73/2: repo\n",
      "73/3:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return filename\n",
      "73/4:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "73/5: filename\n",
      "73/6:\n",
      "#run the below \n",
      "#copy paste the json file below\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "73/7:\n",
      "#if there is an error-continue in the next line \n",
      "repo =[]\n",
      "import json \n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list=jsonobj[\"items\"]\n",
      "    \n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "        \n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "        repo.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "73/8: repo\n",
      "73/9: len(repo)\n",
      "73/10:\n",
      "#Writing data into .csv file and returning the table:\n",
      "#error here\n",
      "import csv\n",
      "import pandas as pd \n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"repo.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\",\"readme\"])\n",
      "    for repo in repo:\n",
      "        writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks'],repo['readme']])\n",
      "data = pd.read_csv(\"repo.csv\", delimiter= \";\")\n",
      "73/11: type(repo)\n",
      "73/12: repo\n",
      "73/13: len(content)\n",
      "73/14:\n",
      "#if there is an error-continue in the next line \n",
      "repo =[]\n",
      "repo_list = []\n",
      "import json \n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "        \n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "        \n",
      "        repo.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "73/15: repo_list\n",
      "73/16: len(repo_list)\n",
      "73/17: repo_list[0]\n",
      "73/18: content\n",
      "73/19: content[1]\n",
      "73/20: print(len(content))\n",
      "73/21: content[0]\n",
      "73/22: content[1]\n",
      "73/23: content[2]\n",
      "73/24: content[3]\n",
      "73/25: content[4]\n",
      "73/26: content[5]\n",
      "73/27:\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "73/28:\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "\n",
      "repo_list\n",
      "73/29:\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "\n",
      "len(repo_list)\n",
      "73/30:\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "\n",
      "repo_list\n",
      "73/31:\n",
      "#if there is an error-continue in the next line \n",
      "repo =[]\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    repo.extend({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "73/32:\n",
      "#if there is an error-continue in the next line \n",
      "repo =[]\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    repo.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "73/33:\n",
      "#taking list of repo\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "73/34:\n",
      "#taking list of repo\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "repo_list\n",
      "73/35:\n",
      "#taking list of repo\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "repo_list[0]\n",
      "73/36:\n",
      "repo =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    repo.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "73/37:\n",
      "repo =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    repo.extend([{\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme}])\n",
      "73/38:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "73/39: dt\n",
      "73/40: len(dt)\n",
      "73/41:\n",
      "#Writing data into .csv file and returning the table:\n",
      "#error here\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\",\"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([dt['id'], dt['name'], dt['url'], dt['language'], dt['created'], dt['stars'], dt['watch'], dt['forks'],dt['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "73/42: dt[0]\n",
      "73/43: dt[0]['id']\n",
      "73/44:\n",
      "#error here\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\",\"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "73/45:\n",
      "#error here\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\",\"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "73/46:\n",
      "#error here\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "73/47: data\n",
      "73/48:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\",\"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "75/1:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return filename\n",
      "75/2:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "75/3: filename\n",
      "75/4:\n",
      "#run the below \n",
      "#copy paste the json file below\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "75/5: len(content)\n",
      "75/6:\n",
      "#taking list of repo\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "75/7:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "75/8: dt[0]\n",
      "75/9: dt[0]\n",
      "75/10: data\n",
      "75/11: data\n",
      "75/12:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "#if we remove the readme, then file can be saved and the following works well\n",
      "\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "75/13: data\n",
      "75/14:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "#if we remove the readme, then file can be saved and the following works well\n",
      "\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "75/15:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text() #.replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "75/16: dt[0]\n",
      "75/17:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "#if we remove the readme, then file can be saved and the following works well\n",
      "\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "75/18:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "#if we remove the readme, then file can be saved and the following works well\n",
      "\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "81/1:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    \n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return filename\n",
      "81/2:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    \n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return filename\n",
      "81/3:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "81/4: filename\n",
      "81/5: file_list\n",
      "81/6:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(fetch_time+day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    \n",
      "    return filename\n",
      "81/7:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "81/8: filename\n",
      "81/9:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(since+day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    \n",
      "    return filename\n",
      "81/10:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "81/11:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(str(since)+'\\t'+day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    \n",
      "    return filename\n",
      "81/12:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "81/13:\n",
      "#run the below \n",
      "#copy paste the json file below\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "81/14: len(content)\n",
      "81/15:\n",
      "#taking list of repo\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "81/16:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "81/17: dt[0]\n",
      "81/18: dt[1]\n",
      "81/19: dt[3]\n",
      "81/20:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "#if we remove the readme, then file can be saved and the following works well\n",
      "\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "81/21:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "#if we remove the readme, then file can be saved and the following works well\n",
      "\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\", encoding='utf-8') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "81/22: data\n",
      "81/23:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "#if we remove the readme, then file can be saved and the following works well\n",
      "\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "81/24: data\n",
      "82/1:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(str(since)+'\\t'+day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "82/2:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "82/3: filename\n",
      "82/4:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "82/5: len(content)\n",
      "82/6:\n",
      "#taking list of repo\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "82/7: content[1]\n",
      "82/8:\n",
      "total_count = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    total_count.extend(jsonobj[\"total_count\"])\n",
      "\n",
      "total_count\n",
      "82/9:\n",
      "no_of_repo = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    no_of_repo.extend(jsonobj[\"total_count\"])\n",
      "\n",
      "no_of_repo\n",
      "82/10:\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "\n",
      "no_of_repo\n",
      "82/11: URL\n",
      "83/1:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=python+created:2021-10-10T20:09:08Z..2021-10-11T00:09:08Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(day_url, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "repo_request\n",
      "83/2:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=python+created:2021-10-10T20:09:08Z..2021-10-11T00:09:08Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "repo_request\n",
      "83/3:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=python+created:2021-10-10T20:09:08Z..2021-10-11T00:09:08Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "len(repo_request['items'])\n",
      "83/4:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        \n",
      "        #pagination:\n",
      "        for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_response = requests.get(page_url, headers=HEADERS)\n",
      "            \n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            \n",
      "            #writing all request in the json file (filename):\n",
      "            repo_request=page_response.json()\n",
      "            converted_to_string=json.dumps(repo_request)\n",
      "            f=open(filename,'a',encoding='utf-8')\n",
      "            f.write(converted_to_string + '\\n')\n",
      "            f.close()\n",
      "        \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/5:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/6:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        if search_response.json().get(\"total_count\") == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/7:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/8:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        if search_response.json().get(\"total_count\") == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/9:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/10:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "len(repo_request['items'])\n",
      "83/11:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "repo_request\n",
      "83/12:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "repo_request['items']\n",
      "83/13:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "len(repo_request['items'])\n",
      "83/14: repo_request\n",
      "83/15:\n",
      "no_page = math.ceil(repo_request.get(\"total_count\")/100)\n",
      "no_page\n",
      "83/16:\n",
      "no_page = math.ceil(repo_request.get(\"total_count\")/100)\n",
      "for i in range(1, no_page + 1):\n",
      "    print(i)\n",
      "83/17:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100&page=1'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "len(repo_request['items'])\n",
      "83/18:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "len(repo_request['items'])\n",
      "83/19:\n",
      "no_page = math.ceil(repo_request.get(\"total_count\")/100)\n",
      "for i in range(1, no_page + 1):\n",
      "    page_url = f'{URL}&page={i}'\n",
      "    page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "page_response\n",
      "83/20: repo_request['total_count']\n",
      "83/21:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        if search_response.json().get(\"total_count\") == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/22:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/23: repo_request.get('total_count')\n",
      "83/24:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "\n",
      "response.json().get('total_count')\n",
      "83/25:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100&page=1'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "repo_request\n",
      "83/26: repo_request.get('total_count')\n",
      "83/27:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(search_response.json().get('total_count'))\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/28:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/29:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No. of repositories created between {since} and {until}:{search_response.json().get('total_count')})\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/30:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No. of repositories created between {since} and {until}:{search_response.json().get('total_count')}'')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/31:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No. of repositories created between {since} and {until}:{search_response.json().get('total_count')}')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/32:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/33:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/34:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "83/35:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            if (page_response.status_code!=200): print(page_response.text)\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/36:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/37:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "83/38:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",3)\n",
      "83/39:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",3)\n",
      "82/12:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(str(since)+'\\t'+day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "            \n",
      "        #writing all request in the json file (filename):\n",
      "        repo_request=response.json()\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "82/13:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "82/14: filename\n",
      "82/15:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "82/16:\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "\n",
      "no_of_repo\n",
      "82/17:\n",
      "#taking list of repo\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "83/40:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            if (page_response.status_code!=200): print(page_response.text)\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/41:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "83/42:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/43:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(page_response.text)\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/44:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/45:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(page_response.text)\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/46:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",3)\n",
      "83/47:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "83/48:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(page_response.text)\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/49:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "83/50:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "83/51:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/52:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "83/53:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",2)\n",
      "83/54:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/55:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "82/18:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "82/19:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "82/20: filename\n",
      "82/21:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "82/22:\n",
      "#getting the number of repositories in each search\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "\n",
      "no_of_repo\n",
      "82/23:\n",
      "#taking list of repo and the number of the repos\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "82/24:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "82/25: filename\n",
      "82/26:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "82/27:\n",
      "#getting the number of repositories in each search\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "\n",
      "no_of_repo\n",
      "82/28:\n",
      "#taking list of repo and the number of the repos\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "82/29:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "83/56:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/57:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/58:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    #import os\n",
      "    import time\n",
      "    \n",
      "    #token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':'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'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/59:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/60:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #token=os.environ['GITHUBTOKEN']\n",
      "    token='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'\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':token}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/61:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/62:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    #HEADERS = {'Authorization':f'{token}'}\n",
      "    HEADERS = {'Authorization':'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'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/63:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    #HEADERS = {'Authorization':f'{token}'}\n",
      "    HEADERS = {'Authorization':'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'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/64:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/65:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    #HEADERS = {'Authorization':f'{token}'}\n",
      "    HEADERS = {'Authorization':'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'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/66:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/67:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/68:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/69:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/70:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'% self.api_token}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/71:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/72:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/73:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/74:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100&access_token={token}'\n",
      "    #HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/75:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/76:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/77:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100&access_token={token}'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/78:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/79:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/80:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/81:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/82:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/83:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/84:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/85:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "87/1:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(str(since)+'\\t'+day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "            \n",
      "        #writing all request in the json file (filename):\n",
      "        repo_request=response.json()\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "87/2:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "87/3: filename\n",
      "87/4:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "87/5:\n",
      "#getting the number of repositories in each search\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "\n",
      "no_of_repo\n",
      "87/6:\n",
      "#getting the number of repositories in each search\n",
      "import json\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "\n",
      "no_of_repo\n",
      "87/7:\n",
      "#taking list of repo and the number of the repos\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "87/8:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "87/9:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "87/10:\n",
      "#taking list of repo and the number of the repositories\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "87/11:\n",
      "#getting No. of repositories in each search\n",
      "import json\n",
      "\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj = json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "    \n",
      "no_of_repo\n",
      "88/1:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "88/2:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "87/12: content[0]\n",
      "87/13: len(content)\n",
      "87/14:\n",
      "#taking list of repo and the number of the repositories\n",
      "import json \n",
      "\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    print(len(jsonobj['items']))\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "88/3:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'} #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\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(2)\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "88/4:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "88/5:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'} #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\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7)\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "88/6:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "88/7:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'} #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\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7)\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "88/8:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "88/9:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'} #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\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7)\n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7)\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "       \n",
      "    return filename\n",
      "88/10:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "88/11:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "88/12: filename\n",
      "88/13:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "88/14: content\n",
      "88/15:\n",
      "import json \n",
      "\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    print(len(jsonobj['items']))\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "88/16:\n",
      "#getting No. of repositories in each search\n",
      "import json\n",
      "\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj = json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "    \n",
      "no_of_repo\n",
      "88/17:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "89/1: filename\n",
      "89/2:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'} #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\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7)\n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7)\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "       \n",
      "    return filename\n",
      "89/3:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",3)\n",
      "89/4: filename\n",
      "89/5:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "89/6:\n",
      "import json \n",
      "\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    print(len(jsonobj['items']))\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "89/7:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "89/8: dt[0]\n",
      "89/9: len(dt)\n",
      "89/10:\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "89/11: data\n",
      "89/12:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "89/13: filename\n",
      "89/14:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "89/15:\n",
      "import json \n",
      "\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    print(len(jsonobj['items']))\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "89/16:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "89/17: len(dt)\n",
      "89/18:\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "89/19: data\n",
      "89/20:\n",
      "#try the function\n",
      "filename = find_repo(\"machine learning\",1)\n",
      "89/21: filename\n",
      "89/22:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "89/23:\n",
      "import json \n",
      "\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    print(len(jsonobj['items']))\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "89/24:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "89/25: len(dt)\n",
      "89/26:\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "89/27: data\n",
      "89/28:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",3)\n",
      "89/29: filename\n",
      "89/30:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "89/31:\n",
      "import json \n",
      "\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    print(len(jsonobj['items']))\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "89/32:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "89/33: len(dt)\n",
      "89/34:\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "89/35: data\n",
      "94/1:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "       \n",
      "    return filename\n",
      "94/2:\n",
      "#try the function\n",
      "filename = find_repo(\"open education\",3)\n",
      "97/1:\n",
      "#Function1 find_repo: Looking for respositories containing a specific \"term\"\n",
      "#created within a specific timeframe of [day] days\n",
      "\n",
      "def find_repo(term, day):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the fetching URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##building URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{datetime.today()}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "       \n",
      "    return filename\n",
      "97/2:\n",
      "#Function 2:  Extracting the list of repositories from the json file\n",
      "\n",
      "def extract_repo_list(filename):\n",
      "    import json \n",
      "    f=open(filename,'r',encoding='utf-8')\n",
      "    content=f.readlines()\n",
      "    repo_list = []\n",
      "\n",
      "    for item in content:\n",
      "        jsonobj=json.loads(item)\n",
      "        print(len(jsonobj['items']))\n",
      "        repo_list.extend(jsonobj[\"items\"])\n",
      "    return repo_list\n",
      "97/3:\n",
      "#Function 3: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    \n",
      "    dt =[]\n",
      "\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "        return dt\n",
      "97/4:\n",
      "#Function 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "97/5:\n",
      "# Search for repositories:\n",
      "filename = find_repo(\"education\",1)\n",
      "# Extract repository lists:\n",
      "repo_list = extract_repo_list(filename)\n",
      "# Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "# Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/6:\n",
      "#Function1 find_repo: Looking for respositories containing a specific \"term\"\n",
      "#created within a specific timeframe of [day] days\n",
      "\n",
      "def find_repo(term, day):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the fetching URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##building URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "       \n",
      "    return filename\n",
      "97/7:\n",
      "#Function 2:  Extracting the list of repositories from the json file\n",
      "\n",
      "def extract_repo_list(filename):\n",
      "    import json \n",
      "    f=open(filename,'r',encoding='utf-8')\n",
      "    content=f.readlines()\n",
      "    repo_list = []\n",
      "\n",
      "    for item in content:\n",
      "        jsonobj=json.loads(item)\n",
      "        print(len(jsonobj['items']))\n",
      "        repo_list.extend(jsonobj[\"items\"])\n",
      "    return repo_list\n",
      "97/8:\n",
      "#Function 3: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    \n",
      "    dt =[]\n",
      "\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "        return dt\n",
      "97/9:\n",
      "#Function 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "97/10:\n",
      "# Search for repositories:\n",
      "filename = find_repo(\"education\",1)\n",
      "# Extract repository lists:\n",
      "repo_list = extract_repo_list(filename)\n",
      "# Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "# Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/11: len(dt)\n",
      "97/12: len(repo_list)\n",
      "97/13:\n",
      "#Function 3: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    \n",
      "    dt =[]\n",
      "\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "    return dt\n",
      "97/14:\n",
      "# Search for repositories:\n",
      "filename = find_repo(\"education\",1)\n",
      "# Extract repository lists:\n",
      "repo_list = extract_repo_list(filename)\n",
      "# Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "# Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/15: len(repo_list)\n",
      "97/16: len(dt)\n",
      "97/17:\n",
      "#Function1 find_repo: Looking for respositories containing a specific \"term\"\n",
      "#created within a specific timeframe of [day] days\n",
      "\n",
      "def find_repo(term, day):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the fetching URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##building URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=12)\n",
      "       \n",
      "    return filename\n",
      "97/18:\n",
      "#Function1 find_repo: Looking for respositories containing a specific \"term\"\n",
      "#created within a specific timeframe of [day] days\n",
      "\n",
      "def find_repo(term, day, h):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the fetching URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##building URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours=h)\n",
      "       \n",
      "    return filename\n",
      "97/19:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"python\",1,10)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/20:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"python\",3,5)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/21:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"python\",3,8)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/22:\n",
      "#Function1 grf - GitHub Repository Finder: \n",
      "#Looking for respositories containing a specific \"term\"\n",
      "#created within a specific timeframe of [day] days\n",
      "#search results are crawled on every h-hour segment\n",
      "\n",
      "def grf(term, day, h):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the API URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##building URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file to monitor error\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file to combine all json files returned from each call:\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until <= datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every h-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours=h)\n",
      "       \n",
      "    return filename\n",
      "97/23:\n",
      "#Function 2:  Extracting the list of repositories from the json file\n",
      "\n",
      "def extract_repo_list(filename):\n",
      "    import json \n",
      "    f=open(filename,'r',encoding='utf-8')\n",
      "    content=f.readlines()\n",
      "    repo_list = []\n",
      "\n",
      "    for item in content:\n",
      "        jsonobj=json.loads(item)\n",
      "        repo_list.extend(jsonobj[\"items\"])\n",
      "    return repo_list\n",
      "97/24:\n",
      "#Function 3: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    \n",
      "    dt =[]\n",
      "\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "    return dt\n",
      "97/25:\n",
      "#Function 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "97/26:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"python\",3,8)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/27:\n",
      "#Function1 grf - GitHub Repository Finder: 3 parameters are defined:\n",
      "#term: specifies the keyword to search for respositories\n",
      "#day: specifies the timeframe to search for repositories. If t is the moment extraction is started then the function will return all repositories created between (t-d) and t.\n",
      "#search results are crawled on every h-hour segment\n",
      "\n",
      "def grf(term, day, h):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the API URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##building URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file to monitor error\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file to combine all json files returned from each call:\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until <= datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every h-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours=h)\n",
      "       \n",
      "    return filename\n",
      "97/28:\n",
      "#Function1 grf - GitHub Repository Finder: 3 parameters are defined:\n",
      "#>term: specifies the keyword to search for respositories. Type: str\n",
      "#>d: specifies the timeframe to search for repositories. Type: int \n",
      "##If [now] is the moment when extraction is started, function will return all repositories \n",
      "##created between (now-d) days and now.\n",
      "#>h: specifies the time duration (hours) of each query. Type: int\n",
      "\n",
      "def grf(term, d, h):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the API URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##building URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days = d)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours = h) # dividing the total No.of repo into segments of h hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file to monitor error\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file to combine all json files returned from each call:\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until <= datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every h-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours=h)\n",
      "       \n",
      "    return filename\n",
      "97/29:\n",
      "#Function 2:  Extracting the list of repositories from the json file\n",
      "\n",
      "def extract_repo_list(filename):\n",
      "    import json \n",
      "    f=open(filename,'r',encoding='utf-8')\n",
      "    content=f.readlines()\n",
      "    repo_list = []\n",
      "\n",
      "    for item in content:\n",
      "        jsonobj=json.loads(item)\n",
      "        repo_list.extend(jsonobj[\"items\"])\n",
      "    return repo_list\n",
      "97/30:\n",
      "#Function 3: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    \n",
      "    dt =[]\n",
      "\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "    return dt\n",
      "97/31:\n",
      "#Function 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "97/32:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"education\",1,8)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/33:\n",
      "#Function1 find_repo - Looking for repositories defined by 3 parameters:\n",
      "#>term: specifies the keyword to search for respositories. Type: str\n",
      "#>d: specifies the timeframe to search for repositories. Type: int \n",
      "##If [now] is the moment when extraction is started, function will return all repositories \n",
      "##created between (now-d) days and now.\n",
      "#>h: specifies the time duration (hours) of each query. Type: int\n",
      "\n",
      "def grf(term, d, h):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the API URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##constructing API URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days = d)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours = h) # dividing the total No.of repo into segments of h hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file to monitor error\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file to combine all json files returned from each call:\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until <= datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every h-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours=h)\n",
      "       \n",
      "    return filename\n",
      "97/34:\n",
      "#Function 2:  Extracting the list of repositories from the json file\n",
      "\n",
      "def extract_repo_list(filename):\n",
      "    import json \n",
      "    f=open(filename,'r',encoding='utf-8')\n",
      "    content=f.readlines()\n",
      "    repo_list = []\n",
      "\n",
      "    for item in content:\n",
      "        jsonobj=json.loads(item)\n",
      "        repo_list.extend(jsonobj[\"items\"])\n",
      "    return repo_list\n",
      "97/35:\n",
      "#Function 3: save_column: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    \n",
      "    dt =[]\n",
      "\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "        \n",
      "        #scraping readme by going through each repository url\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "    return dt\n",
      "97/36:\n",
      "#Function 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "97/37:\n",
      "#Function 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "97/38:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"education\",1,8)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "96/1:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=8) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=8)\n",
      "       \n",
      "    return filename\n",
      "96/2:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "96/3: filename\n",
      "97/39:\n",
      "#Function1 find_repo - Looking for repositories defined by 3 parameters:\n",
      "#>term: specifies the keyword to search for respositories. Type: str\n",
      "#>d: specifies the timeframe to search for repositories. Type: int \n",
      "##If [now] is the moment when extraction is started, function will return all repositories \n",
      "##created between (now-d) days and now.\n",
      "#>h: specifies the time duration (hours) of each query. Type: int\n",
      "\n",
      "def find_repo(term, d, h):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the API URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days = d)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours = h) # dividing the total No.of repo into segments of h hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file to monitor error\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file to combine all json files returned from each call:\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until <= datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every h-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours = h)\n",
      "       \n",
      "    return filename\n",
      "97/40: find_repo(\"education\", 1, 8)\n",
      "97/41:\n",
      "#Function1 find_repo - Looking for repositories defined by 3 parameters:\n",
      "#>term: specifies the keyword to search for respositories. Type: str\n",
      "#>d: specifies the timeframe to search for repositories. Type: int \n",
      "##If [now] is the moment when extraction is started, function will return all repositories \n",
      "##created between (now-d) days and now.\n",
      "#>h: specifies the time duration (hours) of each query. Type: int\n",
      "\n",
      "def find_repo(term, d, h):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the API URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days = d)  # Start fetching repo created {d} days ago\n",
      "    until = since + timedelta(hours = h) # dividing the total No.of repo into segments of h hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{d}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file to monitor error\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file to combine all json files returned from each call:\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until <= datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every h-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request = page_response.json()\n",
      "                converted_to_string = json.dumps(repo_request)\n",
      "                f = open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours = h)\n",
      "       \n",
      "    return filename\n",
      "97/42: find_repo(\"education\", 1, 8)\n",
      "97/43:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"education\",1,8)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/44:\n",
      "#Wrapping up into one single function:\n",
      "def grf(term,d,h):\n",
      "    filename = find_repo(\"education\",1,8)\n",
      "    repo_list = extract_repo_list(filename)\n",
      "    dt = save_column(repo_list)\n",
      "    data = save_dt(dt)\n",
      "    return data\n",
      "97/45: grf(\"education\", 1, 8)\n",
      "98/1:\n",
      "import requests\n",
      "url = 'https://www.reddit.com/r/politics.json'\n",
      "\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "politics_response = response.json()\n",
      "98/2: politics_response\n",
      "98/3: politics_response['data'][1]\n",
      "98/4: politics_response('data')\n",
      "98/5: politics_response['data']\n",
      "98/6: politics_response['data']['children']\n",
      "98/7: len(politics_response['data']['children'])\n",
      "98/8: politics_response['data']['children'][0]\n",
      "98/9: politics_response['data']['children'][0]['data']\n",
      "98/10: politics_response['data']['children'][0]['data']['selftext']\n",
      "98/11: len(politics_response['data']['children'])\n",
      "98/12: politics_response['data']\n",
      "98/13:\n",
      "mod = 'politics'\n",
      "after = None\n",
      "item_type = []\n",
      "\n",
      "for counter in range(5): \n",
      "    url = f'https://www.reddit.com/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['kind'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/14:\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": t3_q9dtmj})\n",
      "json_response = response.json()\n",
      "after = json_response['data']['after']\n",
      "98/15:\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": t3_q9dtmj})\n",
      "json_response = response.json()\n",
      "98/16:\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": 't3_q9dtmj'})\n",
      "json_response = response.json()\n",
      "98/17:\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": 't3_q9dtmj'})\n",
      "json_response = response.json()\n",
      "after = json_response['data']['after']\n",
      "98/18: json_response\n",
      "98/19:\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "after = t3_q9dtmj\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": after})\n",
      "json_response = response.json()\n",
      "after = json_response['data']['after']\n",
      "98/20:\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "after = 't3_q9dtmj'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": after})\n",
      "json_response = response.json()\n",
      "after = json_response['data']['after']\n",
      "98/21:\n",
      "url = f'https://www.reddit.com/{mod}.json?after=t3_1jg35w'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers)\n",
      "json_response = response.json()\n",
      "after = json_response['data']['after']\n",
      "98/22:\n",
      "url = f'https://www.reddit.com/{mod}.json?after=t3_1jg35w'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers)\n",
      "json_response = response.json()\n",
      "98/23: json_response\n",
      "99/1:\n",
      "after = None\n",
      "item_type = []\n",
      "mod = \"sixwaystop313\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/user/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['kind'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/24:\n",
      "import json\n",
      "import requests\n",
      "\n",
      "mod = 'politics'\n",
      "after = None\n",
      "item_type = []\n",
      "\n",
      "for counter in range(5): \n",
      "    url = f'https://www.reddit.com/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['kind'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/25:\n",
      "import json\n",
      "import requests\n",
      "\n",
      "mod = 'politics'\n",
      "after = None\n",
      "item_type = []\n",
      "\n",
      "for counter in range(5): \n",
      "    url = f'https://www.reddit.com/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after']\n",
      "98/26:\n",
      "import json\n",
      "import requests\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "after = 't3_q9dtmj'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": after})\n",
      "json_response = response.json()\n",
      "after = json_response['data']['after']\n",
      "99/2:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "joke_request = response.json() \n",
      "print(joke_request)\n",
      "99/3:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "joke_request = response.json()\n",
      "print(joke_request)\n",
      "99/4: # your answer goes here!\n",
      "99/5:\n",
      "# Question 1 \n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"dog\"})\n",
      "joke_request = response.json()\n",
      "print(f\"The number of dog jokes is: {joke_request['total_jokes']}\")\n",
      "99/6:\n",
      "# Question 2\n",
      "def find_jokes(term):\n",
      "    search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    joke_request = response.json()\n",
      "    num_results = joke_request['total_jokes']\n",
      "    return num_results\n",
      "\n",
      "find_jokes(\"some-searchterm-you-would-like-to-try-out\")\n",
      "99/7:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"})\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = '' # let's remove all jokes, and only look at the other attributes in the JSON response\n",
      "joke_request\n",
      "99/8:\n",
      "for limit in range(10, 31, 10):  # note that range(a, b) runs from a to b-1; so the last value is exclusive (so from 10 to 30 with steps of 10)\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": limit})\n",
      "    joke_request = response.json()\n",
      "    print(f\"Limit {limit} gives {joke_request['total_pages']} pages\")\n",
      "99/9:\n",
      "response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": 5,\n",
      "                                   \"page\": 2})\n",
      "response.json()\n",
      "99/10: # your answer goes here!\n",
      "99/11:\n",
      "def find_jokes(term):\n",
      "    search_url = \"https://icanhazdadjoke.com/search\"\n",
      "    page = 1\n",
      "    jokes = []\n",
      "\n",
      "    while True:  # alternatively you can also use a for-loop that goes from page 1 to total_results / 20 (rounded up)\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,  # optionally you can add \"limit\": 20 but that's already the default so it doesn't change anything\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        jokes.extend(joke_request['results'])\n",
      "        if joke_request['current_page'] <= joke_request['total_pages']:\n",
      "            page += 1\n",
      "        else: \n",
      "            return jokes\n",
      "\n",
      "output = find_jokes(\"rain\") # try running it with \"\", too!\n",
      "99/12: print(f\"You've collected {len(output)} jokes\")\n",
      "99/13:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    total_page = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "99/14:\n",
      "import requests\n",
      "url = 'https://www.reddit.com/r/marketing/about/.json'\n",
      "\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "json_response = response.json()\n",
      "99/15:\n",
      "# Question 2 \n",
      "import time\n",
      "\n",
      "i = 1\n",
      "while i <= 3:\n",
      "    url = 'https://www.reddit.com/r/marketing/about/.json'\n",
      "    headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "    response = requests.get(url, headers=headers)\n",
      "    json_response = response.json()\n",
      "    \n",
      "    print(json_response['data']['active_user_count'])\n",
      "    i += 1\n",
      "    time.sleep(5)\n",
      "99/16:\n",
      "# Question 3\n",
      "def get_usercount(subreddit):\n",
      "    response = response = requests.get(f'https://www.reddit.com/r/{subreddit}/about/.json', headers=headers)\n",
      "    json_response = response.json()\n",
      "    out = {}\n",
      "    out['subreddit'] = subreddit\n",
      "    out['total_users'] = json_response['data']['subscribers']\n",
      "    out['active_users'] = json_response['data']['active_user_count']\n",
      "    return out\n",
      "    \n",
      "get_usercount('science')\n",
      "99/17:\n",
      "mod = \"sixwaystop313\"\n",
      "response = requests.get(f'https://www.reddit.com/user/{mod}.json', headers=headers)\n",
      "json_response = response.json()\n",
      "json_response\n",
      "99/18:\n",
      "import json\n",
      "json_response = json.loads(response.text.replace('null', '\"None\"').replace('True','\"True\"').replace('False','\"False\"'))\n",
      "json_response\n",
      "99/19: json_response['data']['after']\n",
      "99/20:\n",
      "after = json_response['data']['after']\n",
      "url = f'https://www.reddit.com/user/{mod}.json'\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": after})\n",
      "json_response_after = response.json()\n",
      "json_response_after\n",
      "99/21:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "item_type = []\n",
      "mod = \"sixwaystop313\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/user/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['kind'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/27:\n",
      "import json\n",
      "import requests\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "after = 't3_q9dtmj'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": after})\n",
      "json_response = response.json()\n",
      "after = json_response['data']['after']\n",
      "98/28:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "item_type = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['kind'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/29:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "item_type = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['kind'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/30:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "self_text = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['selftext'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/31: politics_response['data']['children']\n",
      "98/32:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "self_text = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['selftext'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/33: politics_response['data']['children']['selftext']\n",
      "98/34: politics_response['data']['children']('selftext')\n",
      "98/35: politics_response['data']['children'][0]('selftext')\n",
      "98/36: politics_response['data']['children'][0]['selftext']\n",
      "98/37: politics_response['data']['children'][0]\n",
      "98/38: politics_response['data']['children'][0]['data']['selftext']\n",
      "98/39:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "self_text = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['data']['selftext'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/40:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "self_text = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        self_text.append(item['data']['selftext'])\n",
      "\n",
      "# Let's view the item types: \n",
      "self_text\n",
      "98/41: politics_response['data']['children'][1]['data']['selftext']\n",
      "98/42: politics_response['data']['children'][3]['data']['selftext']\n",
      "98/43: politics_response['data']['children'][3]['data']\n",
      "98/44: politics_response['data']['children'][3]\n",
      "98/45:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']['data']:\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append(\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups)\n",
      "\n",
      "post_list\n",
      "98/46:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']['data']:\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/47: politics_response['data']['children'][3]['data']\n",
      "98/48:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "    politics = json_response['data']['children']['data']\n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in politics:\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/49: politics_response['data']['children'][3]['data'].get(\"ups\")\n",
      "98/50: politics_response['data']['children'][3]['data'][\"ups\"]\n",
      "98/51:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "    json_response['data']['children']['data']\n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i['data']\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/52:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "    \n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i['data']\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/53:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "selftext = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "    \n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        selftext.append(item['data'][\"selftext\"])\n",
      "    \n",
      "\n",
      "\n",
      "selftext\n",
      "98/54:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "    json_response['data']['children']['data']\n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/55:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i['data']\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/56: len(post_list)\n",
      "98/57:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "selftext = []\n",
      "mod = \"politics\"\n",
      "for counter in range(100): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "    \n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        selftext.append(item['data'][\"selftext\"])\n",
      "    \n",
      "selftext\n",
      "98/58:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "for counter in range(100): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i['data']\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/59: len(post_list)\n",
      "99/22:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "item_type = []\n",
      "mod = \"sixwaystop313\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/user/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['kind'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "99/23: len(item_type)\n",
      "98/60:\n",
      "import json\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        id = item['data'][\"id\"]\n",
      "        selftext = item['data'][\"selftext\"]\n",
      "        downs = item['data']['downs']\n",
      "        ups = item['data']['ups']\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/61:\n",
      "import json\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        id = item['data'][\"id\"]\n",
      "        selftext = item['data'][\"selftext\"]\n",
      "        downs = item['data']['downs']\n",
      "        ups = item['data']['ups']\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/62: len(post_list)\n",
      "98/63:\n",
      "import json\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        id = item['data'][\"id\"]\n",
      "        selftext = item['data'][\"selftext\"]\n",
      "        downs = item['data']['downs']\n",
      "        ups = item['data']['ups']\n",
      "        post_list.append({\"id\":id,\n",
      "                        \"selftext\":selftext,\n",
      "                        \"downs\": downs,\n",
      "                        \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/64: len(post_list)\n",
      "98/65: post_list[2]\n",
      "98/66: post_list[6]\n",
      "98/67: post_list[10]\n",
      "98/68: post_list\n",
      "98/69:\n",
      "for i in post_list:\n",
      "    print(i['selftext'])\n",
      "98/70:\n",
      "import json\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i['data']\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "        subreddit = item.get('subreddit')\n",
      "        post_list.append({\"id\":id,\n",
      "                        \"selftext\":selftext,\n",
      "                        \"downs\": downs,\n",
      "                        \"ups\":ups,\n",
      "                         \"community\":subreddit})\n",
      "\n",
      "post_list\n",
      "98/71:\n",
      "def post_list(mod)\n",
      "    import json\n",
      "    for counter in range(10): \n",
      "        url = f'https://www.reddit.com/r/{mod}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            subreddit = item.get('subreddit')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups,\n",
      "                             \"community\":subreddit})\n",
      "\n",
      "    return post_list\n",
      "98/72:\n",
      "def post_list(mod):\n",
      "    import json\n",
      "    for counter in range(10): \n",
      "        url = f'https://www.reddit.com/r/{mod}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            subreddit = item.get('subreddit')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups,\n",
      "                             \"community\":subreddit})\n",
      "\n",
      "    return post_list\n",
      "98/73:\n",
      "after = None\n",
      "post_list = []\n",
      "post_list = post_list(\"science\")\n",
      "post_list(\"politics\")\n",
      "98/74:\n",
      "after = None\n",
      "post_list = []\n",
      "post_list = export_post(\"science\")\n",
      "export_post(\"politics\")\n",
      "98/75:\n",
      "def export_post(com):\n",
      "    import json\n",
      "    for counter in range(10): \n",
      "        url = f'https://www.reddit.com/r/{com}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            subreddit = item.get('subreddit')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups,\n",
      "                             \"community\":subreddit})\n",
      "\n",
      "    return post_list\n",
      "98/76:\n",
      "after = None\n",
      "post_list = []\n",
      "post_list = export_post(\"science\")\n",
      "export_post(\"politics\")\n",
      "98/77:\n",
      "def export_post(com):\n",
      "    import json\n",
      "    after = None\n",
      "    for counter in range(10): \n",
      "        url = f'https://www.reddit.com/r/{com}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            subreddit = item.get('subreddit')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups,\n",
      "                             \"community\":subreddit})\n",
      "\n",
      "    return post_list\n",
      "98/78:\n",
      "post_list = []\n",
      "post_list = export_post(\"science\")\n",
      "export_post(\"politics\")\n",
      "98/79:\n",
      "post_list = []\n",
      "post_list = export_post(\"science\")\n",
      "data = export_post(\"politics\")\n",
      "98/80: data\n",
      "98/81: len(data)\n",
      "98/82: post_list\n",
      "98/83:\n",
      "post_list = []\n",
      "post_list = export_post(\"science\")\n",
      "post_list = export_post(\"politics\")\n",
      "98/84: len(post_list)\n",
      "98/85: post_list\n",
      "98/86:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i['data']\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "        post_list.append({\"id\":id,\n",
      "                        \"selftext\":selftext,\n",
      "                        \"downs\": downs,\n",
      "                        \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/87:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i['data']\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "        post_list.append({\"id\":id,\n",
      "                        \"selftext\":selftext,\n",
      "                        \"downs\": downs,\n",
      "                        \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/88:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(filename,'w',encoding='utf-8')\n",
      "f.close()\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{mod}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups})\n",
      "98/89:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{mod}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups})\n",
      "97/46:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"python\",3,8)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/47:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"python\",3,8)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "97/48:\n",
      "#Wrapping up into one single function: Github Repository Finder (grf)\n",
      "def grf(term, d, h):\n",
      "    filename = find_repo(term, d, h)\n",
      "    repo_list = extract_repo_list(filename)\n",
      "    dt = save_column(repo_list)\n",
      "    data = save_dt(dt)\n",
      "    return data\n",
      "97/49: grf(\"python\", 3, 8)\n",
      "100/1:\n",
      "#Step 2:  Extracting the list of repositories from the json file\n",
      "\n",
      "def extract_repo_list(filename):\n",
      "    '''\n",
      "    Reads .json file of repository data, returns a list of repositories.\n",
      "\n",
      "            Parameters:\n",
      "                    filename (str): Name of the .json file obtained from find_repo()\n",
      "                    \n",
      "            Returns:\n",
      "                    repo_list (list): List of all repositories found in the search queries \n",
      "    ''' \n",
      "    import json \n",
      "    f=open(filename,'r',encoding='utf-8')\n",
      "    content=f.readlines()\n",
      "    repo_list = []\n",
      "\n",
      "    for item in content:\n",
      "        jsonobj=json.loads(item)\n",
      "        repo_list.extend(jsonobj[\"items\"])\n",
      "    return repo_list\n",
      "100/2:\n",
      "#Step1: Searching for repositories, crawling data, storing responses in a .json file\n",
      "\n",
      "def find_repo(term, d, h):\n",
      "    '''\n",
      "    Searches for repositories on GitHub, returns name of a .json file of original data.\n",
      "\n",
      "            Parameters:\n",
      "                    term (str): A string defining the search term\n",
      "                    d (int): An integer defining the timeframe to search (in days)\n",
      "                    h (int): An integer defining the duration (in hours) of each query\n",
      "\n",
      "            Returns:\n",
      "                    filename (str): Name of a single .json file which stores the original response data\n",
      "                    .json file: A file stored in working directory\n",
      "                    log.txt: Log file recording all API calls and their status\n",
      "    \n",
      "    Check GRF documentation for detailed function explanation. \n",
      "    ''' \n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #constructing URLs\n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    #setting timeframe and search segments\n",
      "    since = datetime.today() - relativedelta(days = d)  \n",
      "    until = since + timedelta(hours = h)\n",
      "    \n",
      "    #setting name for the final json file\n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{d}days_{fetch_time}.json' \n",
      "    \n",
      "    # creating the log file to monitor error\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # creating an empty json file to combine all json files returned from each call:\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7)\n",
      "        \n",
      "        #print out No of repos in every h-hour request\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #If no Total_count found, print out the failed link and additional information\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            \n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) \n",
      "        \n",
      "        #if extraction successful, do pagination:\n",
      "        else:\n",
      "            #calculating the total No. of pages\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) \n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7)\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request = page_response.json()\n",
      "                converted_to_string = json.dumps(repo_request)\n",
      "                f = open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        #update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours = h)\n",
      "       \n",
      "    return filename\n",
      "100/3:\n",
      "#Step 3: save_column: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    '''\n",
      "    Extracts relevant repository information, saves it in a list.\n",
      "\n",
      "            Parameters:\n",
      "                    repo_list (list): List obtained from export_repo_list()\n",
      "                    \n",
      "            Returns:\n",
      "                    dt (list): List of repositories with only desired information \n",
      "    ''' \n",
      "    dt =[]\n",
      "    \n",
      "    #extracting data directly from the list\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "        \n",
      "        #scraping readme by going through each repository url\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        #if readme is empty, recording as blank (NaN)\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "    return dt\n",
      "100/4:\n",
      "#Step 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    '''\n",
      "    Saves desired data in a .csv file, returns a preview of the final dataset.\n",
      "\n",
      "            Parameters:\n",
      "                    dt (list): List obtained from save_column()\n",
      "                    \n",
      "            Returns:\n",
      "                    data.csv: Final dataset stored in working directory\n",
      "    ''' \n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "100/5: ?find_repo\n",
      "100/6: ?export_repo_list\n",
      "100/7:\n",
      "#Step 2:  Extracting the list of repositories from the json file\n",
      "\n",
      "def extract_repo_list(filename):\n",
      "    '''\n",
      "    Reads .json file of repository data, returns a list of repositories.\n",
      "\n",
      "            Parameters:\n",
      "                    filename (str): Name of the .json file obtained from find_repo()\n",
      "                    \n",
      "            Returns:\n",
      "                    repo_list (list): List of all repositories found in the search queries \n",
      "    ''' \n",
      "    import json \n",
      "    f=open(filename,'r',encoding='utf-8')\n",
      "    content=f.readlines()\n",
      "    repo_list = []\n",
      "\n",
      "    for item in content:\n",
      "        jsonobj=json.loads(item)\n",
      "        repo_list.extend(jsonobj[\"items\"])\n",
      "    return repo_list\n",
      "100/8:\n",
      "#Step 3: save_column: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    '''\n",
      "    Extracts relevant repository information, saves it in a list.\n",
      "\n",
      "            Parameters:\n",
      "                    repo_list (list): List obtained from extract_repo_list()\n",
      "                    \n",
      "            Returns:\n",
      "                    dt (list): List of repositories with only desired information \n",
      "    ''' \n",
      "    dt =[]\n",
      "    \n",
      "    #extracting data directly from the list\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "        \n",
      "        #scraping readme by going through each repository url\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        #if readme is empty, recording as blank (NaN)\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "    return dt\n",
      "100/9:\n",
      "#Step 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    '''\n",
      "    Saves desired data in a .csv file, returns a preview of the final dataset.\n",
      "\n",
      "            Parameters:\n",
      "                    dt (list): List obtained from save_column()\n",
      "                    \n",
      "            Returns:\n",
      "                    data.csv: Final dataset stored in working directory\n",
      "    ''' \n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "100/10: ?extract_repo_list\n",
      "100/11: ?save_column\n",
      "100/12: ?save_dt\n",
      "100/13:\n",
      "#Wrapping up into one single function: Github Repository Finder (grf)\n",
      "def grf(term, d, h):\n",
      "    '''\n",
      "    Searches for repositories on GitHub, returns a .csv file of final dataset.\n",
      "\n",
      "            Parameters:\n",
      "                    term (str): A string defining the search term\n",
      "                    d (int): An integer defining the timeframe to search (in days)\n",
      "                    h (int): An integer defining the duration (in hours) of each query\n",
      "\n",
      "            Returns:\n",
      "                    data.csv: Final dataset stored in working directory\n",
      "                    .json file: A file of raw data stored in working directory\n",
      "                    log.txt: Log file recording all API calls and their status\n",
      "    \n",
      "    Check GRF documenation for more details on the function and its components.\n",
      "    ''' \n",
      "    filename = find_repo(term, d, h)\n",
      "    repo_list = extract_repo_list(filename)\n",
      "    dt = save_column(repo_list)\n",
      "    data = save_dt(dt)\n",
      "    return data\n",
      "100/14: ?grf\n",
      "100/15: grf(\"python\", 3, 8)\n",
      "100/16: grf(\"python\", 3, 8)\n",
      "100/17:\n",
      "import pandas as pd\n",
      "pd.read_csv(\"data/dt.csv\", delimiter= \";\",nrows=100)\n",
      "100/18:\n",
      "import pandas as pd\n",
      "pd.read_csv(\"data/dt.csv\", delimiter= \";\",nrows=10)\n",
      "102/1:\n",
      "import pandas as pd\n",
      "pd.read_csv(\"../data/dt.csv\", delimiter= \";\",nrows=10)\n",
      "106/1:\n",
      "# extracts all links (the a tag!), and prints the URL (href) to the screen;\n",
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find_all(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "106/2: soup\n",
      "106/3: res\n",
      "106/4: res.text\n",
      "106/5: soup\n",
      "106/6: res.text\n",
      "106/7: soup.find_all(\"a\")\n",
      "106/8: soup.find_all(\"a\")[0].attrs[\"href\"]\n",
      "106/9: soup.find_all(\"a\")[0]\n",
      "106/10: soup.find_all(\"a\")[0].attrs[\"href\"]\n",
      "106/11: soup.find_all(\"a\")\n",
      "106/12: soup.find_all(\"product_pod\")\n",
      "106/13: soup.find_all(class_ = \"product_pod\")\n",
      "106/14: len(soup.find_all(class_ = \"product_pod\"))\n",
      "106/15: soup.find_all(class_ = \"product_pod\")[0]\n",
      "106/16: soup.find_all(class_ = \"product_pod\")[0].find_all[\"h3\"]\n",
      "106/17: soup.find_all(class_ = \"product_pod\")[0].find_all(\"h3\")\n",
      "106/18: soup.find_all(class_ = \"product_pod\")[0].find_all(\"a\").attrs[\"href\"]\n",
      "106/19: soup.find_all(class_ = \"product_pod\")[0].find_all(\"a\")\n",
      "106/20: soup.find_all(class_ = \"product_pod\")[0].find(\"a\").attrs[\"href\"]\n",
      "106/21:\n",
      "# extracts all links (the a tag!), and prints the URL (href) to the screen;\n",
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "106/22:\n",
      "# extracts all links (the a tag!), and prints the URL (href) to the screen;\n",
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find_all(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "106/23: soup.find(\"a\")\n",
      "106/24: soup.find_all(\"a\")\n",
      "106/25: soup.findl(\"a\").link.attrs[\"href\"]\n",
      "106/26: soup.findl(\"a\").attrs[\"href\"]\n",
      "106/27: soup.findl(\"a\")\n",
      "106/28: soup.find(\"a\").attrs[\"href\"]\n",
      "106/29: soup.find_all(class_ = \"product_pod\")[1].find(\"a\").attrs[\"href\"]\n",
      "106/30:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "book_url = base_url + url_book[6:]\n",
      "book_url\n",
      "106/31: url_book = soup.find_all(class_ = \"product_pod\")[1].find(\"a\").attrs[\"href\"]\n",
      "106/32:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "book_url = base_url + url_book[6:]\n",
      "book_url\n",
      "106/33: url_book.replace(\"../../\",base_url)\n",
      "106/34: soup.find_all(class_ = \"product_pod\")[1]\n",
      "106/35: soup.find_all(class_ = \"product_pod\")\n",
      "106/36:\n",
      "books = soup.find_all(class_ = \"product_pod\")\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url_list = []\n",
      "for book in books:\n",
      "    url_book = book.find(\"a\").attrs[\"href\"]\n",
      "    url = url_book.replace(\"../../\", base_url)\n",
      "    url_list.append(url)\n",
      "url_list\n",
      "106/37:\n",
      "books = soup.find_all(class_ = \"product_pod\")\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url_list = []\n",
      "for book in books:\n",
      "    url_book = book.find(\"a\").attrs[\"href\"]\n",
      "    url = url_book.replace(\"../../\", base_url)\n",
      "    url_list.append(url)\n",
      "url_list\n",
      "106/38: soup.find_all(class_ = \"product_pod\")\n",
      "106/39: soup.find_all(class_ = \"product_pod\").find(\"img\")\n",
      "106/40: soup.find_all(class_ = \"product_pod\")[0].find(\"img\")\n",
      "106/41: soup.find_all(class_ = \"product_pod\")[0].find(\"img\").attrs['alt']\n",
      "106/42:\n",
      "books = soup.find_all(class_ = \"product_pod\")\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url_list = []\n",
      "\n",
      "for book in books:\n",
      "    url_book = book.find(\"a\").attrs[\"href\"]\n",
      "    url = url_book.replace(\"../../\", base_url)\n",
      "    book_title = book.find(\"img\").attrs['alt']\n",
      "    url_list.append({\"title\": book_title,\n",
      "                     \"url\": url})\n",
      "url_list\n",
      "106/43: next((book for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "106/44:\n",
      "books = soup.find_all(class_ = \"product_pod\")\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "book_list = []\n",
      "\n",
      "for book in books:\n",
      "    url_book = book.find(\"a\").attrs[\"href\"]\n",
      "    url = url_book.replace(\"../../\", base_url)\n",
      "    book_title = book.find(\"img\").attrs['alt']\n",
      "    book_list.append({\"title\": book_title,\n",
      "                     \"url\": url})\n",
      "book_list\n",
      "106/45: next((book for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "106/46: next((book[\"url\"] for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "106/47:\n",
      "next((book for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "\n",
      "#or:\n",
      "next((book[\"url\"] for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "106/48:\n",
      "next((book for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "\n",
      "#or:\n",
      "next((book[\"url\"] for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "106/49: ?enumerate\n",
      "106/50: enumerate(book_list)\n",
      "106/51: ?enumerate\n",
      "106/52:\n",
      "# iterate over the dictionaries and update URLs accordingly:\n",
      "\n",
      "for id, book in enumerate(book_list):\n",
      "    book[\"org_url\"] = (book[\"url\"]).replace('https://books.toscrape.com/catalogue/','../../')\n",
      "\n",
      "# show the first five elements\n",
      "book_list[0:5]\n",
      "106/53:\n",
      "# creating a list of page_url:\n",
      "\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = []\n",
      "\n",
      "for counter in range(1, 51):\n",
      "    full_url = f'{base_url}page-{counter}.html' \n",
      "    page_urls.append(full_url)\n",
      "106/54:\n",
      "# creating a list of page_url:\n",
      "\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = []\n",
      "\n",
      "for counter in range(1, 51):\n",
      "    full_url = f'{base_url}page-{counter}.html' \n",
      "    page_urls.append(full_url)\n",
      "\n",
      "page_urls[0:5]\n",
      "106/55:\n",
      "quote_base = \"https://quotes.toscrape.com/page/\"\n",
      "url_list =[]\n",
      "\n",
      "for counter in range(1:11):\n",
      "    quote_url = f'{quote_base}{counter}/'\n",
      "    url_list.append(quote_url)\n",
      "\n",
      "url_list\n",
      "106/56:\n",
      "quote_base = \"https://quotes.toscrape.com/page/\"\n",
      "url_list =[]\n",
      "\n",
      "for counter in range(11):\n",
      "    quote_url = f'{quote_base}{counter}/'\n",
      "    url_list.append(quote_url)\n",
      "\n",
      "url_list\n",
      "106/57:\n",
      "quote_base = \"https://quotes.toscrape.com/page/\"\n",
      "url_list =[]\n",
      "\n",
      "for counter in range(1,11):\n",
      "    quote_url = f'{quote_base}{counter}/'\n",
      "    url_list.append(quote_url)\n",
      "\n",
      "url_list\n",
      "106/58:\n",
      "quote_base = \"https://quotes.toscrape.com/page/\"\n",
      "url_list =[]\n",
      "\n",
      "for counter in range(1,11):\n",
      "    quote_url = f'{quote_base}{counter}/'\n",
      "    url_list.append(quote_url)\n",
      "\n",
      "url_list\n",
      "106/59:\n",
      "# creating a list of page_url:\n",
      "def generate_url_list(base_url, num_page):\n",
      "    \n",
      "    page_urls = []\n",
      "\n",
      "    for counter in range(1, num_page + 1):\n",
      "        full_url = f'{base_url}page-{counter}.html' \n",
      "        page_urls.append(full_url)\n",
      "\n",
      "    return page_urls\n",
      "106/60:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 15\n",
      "generate_url_list(base_url, num_page)\n",
      "106/61:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 5\n",
      "generate_url_list(base_url, num_page)\n",
      "106/62:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 5\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "106/63:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    for page in pages\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #extract book information from the list of books\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"]\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url})\n",
      "    return book_list\n",
      "106/64:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #extract book information from the list of books\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"]\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url})\n",
      "    return book_list\n",
      "106/65:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 5\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "extract_books(pages)\n",
      "106/66:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 5\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "books = extract_books(pages)\n",
      "106/67: len(books)\n",
      "106/68: books[1]\n",
      "106/69:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 2\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "books = extract_books(pages)\n",
      "106/70: books[0:5]\n",
      "106/71:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    # collect all books on page_url\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #for each book on that page look up the title and url and store it in a list\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"] #search for a particular tag and its attribute\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            availability = book.find(\"p\", class_= \"instock availability\") #search for a particular class\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url,\n",
      "                             \"availability\": availability})\n",
      "    return book_list\n",
      "106/72:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 2\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "books = extract_books(pages)\n",
      "106/73: books[0:5]\n",
      "106/74:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    # collect all books on page_url\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #for each book on that page look up the title and url and store it in a list\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"] #search for a particular tag and its attribute\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            availability = book.find(\"p\", class_= \"icon-ok\") #search for a particular class\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url,\n",
      "                             \"availability\": availability})\n",
      "    return book_list\n",
      "106/75:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 2\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "books = extract_books(pages)\n",
      "106/76: books[0:5]\n",
      "106/77:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    # collect all books on page_url\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #for each book on that page look up the title and url and store it in a list\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"] #search for a particular tag and its attribute\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            availability = book.find(\"p\", class_= \"instock availability\") #search for a particular class\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url,\n",
      "                             \"availability\": availability})\n",
      "    return book_list\n",
      "106/78:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "soup\n",
      "106/79:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "soup.find_all(class_ = \"product_pod\")\n",
      "106/80:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "soup.find_all(class_ = \"product_pod\").find(class_= \"instock availability\")\n",
      "106/81:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "book = soup.find_all(class_ = \"product_pod\")\n",
      "106/82: book.find(class_= \"instock availability\")\n",
      "106/83: book\n",
      "106/84: len(book)\n",
      "106/85: book[0]\n",
      "106/86: book[0].find(class_= \"instock availability\")\n",
      "106/87: book[0].find(\"p\", class_= \"instock availability\")\n",
      "106/88: book[0]\n",
      "106/89: book[0]find(\"p\", class_= \"instock availability\").text\n",
      "106/90: book[0]find(\"p\", class_= \"instock availability\")\n",
      "106/91: book[0].find(\"p\", class_= \"instock availability\").text\n",
      "106/92:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    # collect all books on page_url\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #for each book on that page look up the title and url and store it in a list\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"] #search for a particular tag and its attribute\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            #search for a particular class\n",
      "            availability = book.find(\"p\", class_= \"instock availability\").text.replace(\"\\n\", \"\")\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url,\n",
      "                             \"availability\": availability})\n",
      "    return book_list\n",
      "106/93:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 5\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "books = extract_books(pages)\n",
      "106/94: books[0:5]\n",
      "106/95:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    # collect all books on page_url\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #for each book on that page look up the title and url and store it in a list\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"] #search for a particular tag and its attribute\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            #search for a particular class\n",
      "            availability = book.find(\"p\", class_= \"instock availability\").text.replace(\"\\n\", \"\").replace(\" \",\"\")\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url,\n",
      "                             \"availability\": availability})\n",
      "    return book_list\n",
      "106/96:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 5\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "books = extract_books(pages)\n",
      "106/97: books[0:5]\n",
      "107/1:\n",
      "# extracts all links (the a tag!), and prints the URL (href) to the screen;\n",
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find_all(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "107/2: soup.find_all(class_ = \"product_pod\")[0].find(\"a\").attrs[\"href\"]\n",
      "107/3: url_book = soup.find_all(class_ = \"product_pod\")[1].find(\"a\").attrs[\"href\"]\n",
      "107/4:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "book_url = base_url + url_book[6:]\n",
      "book_url\n",
      "107/5: url_book.replace(\"../../\",base_url)\n",
      "107/6:\n",
      "books = soup.find_all(class_ = \"product_pod\")\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url_list = []\n",
      "for book in books:\n",
      "    url_book = book.find(\"a\").attrs[\"href\"]\n",
      "    url = url_book.replace(\"../../\", base_url)\n",
      "    url_list.append(url)\n",
      "url_list\n",
      "107/7:\n",
      "books = soup.find_all(class_ = \"product_pod\")\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "book_list = []\n",
      "\n",
      "for book in books:\n",
      "    url_book = book.find(\"a\").attrs[\"href\"]\n",
      "    url = url_book.replace(\"../../\", base_url)\n",
      "    book_title = book.find(\"img\").attrs['alt']\n",
      "    book_list.append({\"title\": book_title,\n",
      "                     \"url\": url})\n",
      "book_list\n",
      "107/8:\n",
      "next((book for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "#or:\n",
      "next((book[\"url\"] for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "107/9:\n",
      "# iterate over the dictionaries and update URLs accordingly:\n",
      "\n",
      "for id, book in enumerate(book_list):\n",
      "    book[\"org_url\"] = (book[\"url\"]).replace('https://books.toscrape.com/catalogue/','../../')\n",
      "\n",
      "# show the first five elements\n",
      "book_list[0:5]\n",
      "107/10:\n",
      "# creating a list of page_url:\n",
      "def generate_url_list(base_url, num_page):\n",
      "    \n",
      "    page_urls = []\n",
      "\n",
      "    for counter in range(1, num_page + 1):\n",
      "        full_url = f'{base_url}page-{counter}.html' \n",
      "        page_urls.append(full_url)\n",
      "\n",
      "    return page_urls\n",
      "107/11:\n",
      "from time import sleep\n",
      "sleep(5)\n",
      "#sleep (2*60) --> sleep for 2 minutes\n",
      "107/12:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    # collect all books on page_url\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #for each book on that page look up the title and url and store it in a list\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"] #search for a particular tag and its attribute\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            #search for a particular class\n",
      "            availability = book.find(\"p\", class_= \"instock availability\").text.replace(\"\\n\", \"\").replace(\" \",\"\")\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url,\n",
      "                             \"availability\": availability})\n",
      "    return book_list\n",
      "107/13:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 5\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "books = extract_books(pages)\n",
      "107/14: books[0:5]\n",
      "107/15:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "book = soup.find_all(class_ = \"product_pod\")\n",
      "107/16:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1'\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\").attrs[\"href\"].text == \"next\"\n",
      "107/17:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\").attrs[\"href\"].text == \"next\"\n",
      "107/18:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\").attrs[\"href\"]\n",
      "107/19:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup\n",
      "107/20:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup\n",
      "107/21:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\").attrs['href'].text\n",
      "107/22:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\").attrs['href']\n",
      "107/23:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\")\n",
      "107/24:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\").attrs['href']\n",
      "107/25:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\")[0]\n",
      "107/26:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\")[0].attrs['href']\n",
      "107/27:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\")[0].attrs['href'].text\n",
      "107/28:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\")[0].text\n",
      "107/29:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\")[19].text\n",
      "107/30:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_button = soup.find(\"p\", class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "\n",
      "check_next_page(url)\n",
      "107/31:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "\n",
      "check_next_page(url)\n",
      "107/32:\n",
      "url = 'https://books.toscrape.com/catalogue/page-50.html'\n",
      "\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "\n",
      "check_next_page(url)\n",
      "107/33:\n",
      "url = 'https://books.toscrape.com/catalogue/page-50.html'\n",
      "\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else print(\"This is the last page\")\n",
      "\n",
      "check_next_page(url)\n",
      "107/34:\n",
      "def next_page_url(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser)\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    if next_button.find('a').attrs['href']!= None:\n",
      "                         next_page = next_button.find('a').attrs['href']\n",
      "                         return next_url = next_page.replace(\"../../\",base_url)              \n",
      "    else print(\"This is the last page\")\n",
      "107/35:\n",
      "def next_page_url(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    if next_button.find('a').attrs['href']!= None:\n",
      "                         next_page = next_button.find('a').attrs['href']\n",
      "                         return next_url = next_page.replace(\"../../\",base_url)              \n",
      "    else print(\"This is the last page\")\n",
      "107/36:\n",
      "def next_page_url(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    if next_button.find('a').attrs['href']!= None:\n",
      "        next_page = next_button.find('a').attrs['href']\n",
      "        next_url = next_page.replace(\"../../\",base_url)\n",
      "        return next_url\n",
      "    else print(\"This is the last page\")\n",
      "107/37:\n",
      "def next_page_url(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    if next_button.find('a').attrs['href']!= None:\n",
      "        next_page = next_button.find('a').attrs['href']\n",
      "        next_url = next_page.replace(\"../../\",base_url)\n",
      "        return next_url\n",
      "    else:\n",
      "        print(\"This is the last page\")\n",
      "107/38:\n",
      "url = 'https://books.toscrape.com/catalogue/page-10.html'\n",
      "next_page_url(url)\n",
      "107/39:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url = 'https://books.toscrape.com/catalogue/page-10.html'\n",
      "next_page_url(url)\n",
      "107/40:\n",
      "def next_page_url(url):\n",
      "    if url!= None:\n",
      "        next_page = url.replace(\"../../\", base_url)\n",
      "        return next_url\n",
      "    else:\n",
      "        print(\"This is the last page\")\n",
      "107/41:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    \n",
      "    #check next page:\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "\n",
      "check_next_page('https://books.toscrape.com/catalogue/page-10.html')\n",
      "107/42:\n",
      "def next_page_url(url):\n",
      "    if url!= None:\n",
      "        next_page = f'{base_url}{url}'\n",
      "        return next_url\n",
      "    else:\n",
      "        print(\"This is the last page\")\n",
      "107/43:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url = 'https://books.toscrape.com/catalogue/page-10.html'\n",
      "next_page_url(url)\n",
      "107/44:\n",
      "def next_page_url(url):\n",
      "    if url!= None:\n",
      "        next_page = f'{base_url}{url}'\n",
      "        return next_page\n",
      "    else:\n",
      "        print(\"This is the last page\")\n",
      "107/45:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url = 'https://books.toscrape.com/catalogue/page-10.html'\n",
      "next_page_url(url)\n",
      "107/46:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url = check_next_page('https://books.toscrape.com/catalogue/page-10.html')\n",
      "next_page_url(url)\n",
      "107/47:\n",
      "def next_page_url(url):\n",
      "    if url!= None:\n",
      "        next_page = base_url + url\n",
      "        return next_page\n",
      "    else:\n",
      "        print(\"This is the last page\")\n",
      "107/48:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url = check_next_page('https://books.toscrape.com/catalogue/page-10.html')\n",
      "next_page_url(url)\n",
      "107/49:\n",
      "def extract_all_books(page_url):\n",
      "    books = []\n",
      "    while page_url:\n",
      "        print(page_url)\n",
      "        for book in extract_book_urls([page_url]):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(page_url) != None: \n",
      "            page_url = \"https://books.toscrape.com/catalogue/category/books_1/\" + check_next_page(page_url)\n",
      "        else: \n",
      "            break\n",
      "        \n",
      "        # if \"page-4\" in page_url: break # (activate this if you don't want to run the entire loop)\n",
      "    return books\n",
      "107/50: extract_all_books(\"https://books.toscrape.com/catalogue/page-1.html\")\n",
      "107/51:\n",
      "def extract_all_books(page_url):\n",
      "    books = []\n",
      "    while page_url:\n",
      "        print(page_url)\n",
      "        for book in extract_books([page_url]):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(page_url) != None: \n",
      "            page_url = \"https://books.toscrape.com/catalogue/category/books_1/\" + check_next_page(page_url)\n",
      "        else: \n",
      "            break\n",
      "        \n",
      "        # if \"page-4\" in page_url: break # (activate this if you don't want to run the entire loop)\n",
      "    return books\n",
      "107/52: extract_all_books(\"https://books.toscrape.com/catalogue/page-1.html\")\n",
      "107/53: len(book_list)\n",
      "107/54: len(books)\n",
      "107/55: books = extract_all_books(\"https://books.toscrape.com/catalogue/page-1.html\")\n",
      "107/56: len(books)\n",
      "107/57: next((book for book in book_list if book[\"title\"] == 'The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics'), None)\n",
      "107/58:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    # collect all books on page_url\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #for each book on that page look up the title and url and store it in a list\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"] #search for a particular tag and its attribute\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            #search for a particular class\n",
      "            availability = book.find(\"p\", class_= \"instock availability\").text.replace(\"\\n\", \"\").replace(\" \",\"\")\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url,\n",
      "                             \"availability\": availability})\n",
      "    return book_list\n",
      "107/59:\n",
      "def extract_all_books(page_url):\n",
      "    books = []\n",
      "    while page_url:\n",
      "        print(page_url)\n",
      "        for book in extract_books([page_url]):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(page_url) != None: \n",
      "            page_url = \"https://books.toscrape.com/catalogue/category/books_1/\" + check_next_page(page_url)\n",
      "        else: \n",
      "            break\n",
      "        \n",
      "        # if \"page-4\" in page_url: break # (activate this if you don't want to run the entire loop)\n",
      "    return books\n",
      "107/60: books = extract_all_books(\"https://books.toscrape.com/catalogue/page-49.html\")\n",
      "107/61: len(books)\n",
      "107/62: books[0]\n",
      "107/63:\n",
      "books_instock = [book for book in book_list if book[\"Availability\"] == \"Instock\"]\n",
      "len(books_instock)\n",
      "107/64:\n",
      "books_instock = [book for book in book_list if book[\"availability\"] == \"Instock\"]\n",
      "len(books_instock)\n",
      "107/65:\n",
      "books_instock = [book for book in books if book[\"availability\"] == \"Instock\"]\n",
      "len(books_instock)\n",
      "107/66: next((book for book in book_list if book[\"title\"] == 'On the Road (Duluoz Legend)'), None)\n",
      "107/67: next((book for book in books if book[\"title\"] == 'On the Road (Duluoz Legend)'), None)\n",
      "107/68: books[9]\n",
      "107/69:\n",
      "#### Checking if there is a specific word in title:\n",
      "len([book for book in books if \"boat\" in book[\"title\"].lower()])\n",
      "107/70:\n",
      "#### Checking if there is a specific word in title:\n",
      "len([book for book in books if \"road\" in book[\"title\"].lower()])\n",
      "107/71:\n",
      "#### Checking if there is a specific word in title:\n",
      "next[book for book in books if \"road\" in book[\"title\"].lower()])\n",
      "107/72:\n",
      "#### Checking if there is a specific word in title:\n",
      "next((book for book in books if \"road\" in book[\"title\"].lower()]), None)\n",
      "107/73:\n",
      "#### Checking if there is a specific word in title:\n",
      "next((book for book in books if \"road\" in book[\"title\"].lower()), None)\n",
      "107/74: len([book for book in books if \"boat\" in book[\"title\"].lower()])\n",
      "107/75: len([book for book in books if \"love\" in book[\"title\"].lower()])\n",
      "107/76:\n",
      "url = \"https://books.toscrape.com/catalogue/black-dust_976/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "107/77: soup\n",
      "107/78: soup.find_all(\"p\", class_=\"star-rating\")\n",
      "107/79: soup.find(\"p\", class_=\"star-rating\")\n",
      "107/80: soup.find(\"p\", class_=\"star-rating\").text\n",
      "107/81: soup.find(\"p\", class_=\"star-rating\")\n",
      "107/82: soup.find(\"p\", class_=\"star-rating\").find(\"i\")\n",
      "107/83: soup.find(\"p\", class_=\"star-rating\").find_all(\"i\")\n",
      "107/84: len(soup.find(\"p\", class_=\"star-rating\").find_all(\"i\"))\n",
      "107/85: soup.find(id=\"content_inner\")\n",
      "107/86: soup.find(id=\"content_inner\").find(class_=\"product_page\")\n",
      "107/87: soup.find(id=\"content_inner\").find(\"p\", class_=\"product_page\")\n",
      "107/88: soup.find(id=\"content_inner\").find(\"p\", class_=\"product_page\")\n",
      "107/89: soup.find(id=\"content_inner\").find(\"p\", class_=\"star-rating\")\n",
      "107/90: soup.find(id=\"content_inner\").find_all(\"p\", class_=\"star-rating\")\n",
      "107/91: soup.find(id=\"content_inner\").find_all(class_=\"star-rating\")\n",
      "107/92: soup.find(id=\"content_inner\").find(class_=\"star-rating\")\n",
      "107/93: soup.find(id=\"content_inner\").find(\"i\", class_=\"star-rating\")\n",
      "107/94: soup.find(id=\"content_inner\").find_all(\"i\", class_=\"star-rating\")\n",
      "107/95: soup.find(id=\"content_inner\").find_all(\"p\", class_=\"star-rating\")\n",
      "107/96: soup.find(id=\"content_inner\").find(\"p\", class_=\"star-rating\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/97: soup.find(id=\"content_inner\").find_all(\"i\", class_=\"icon-star\")\n",
      "107/98: soup.find(id=\"content_inner\").find(\"i\", class_=\"icon-star\")\n",
      "107/99:\n",
      "url = \"https://books.toscrape.com/catalogue/black-dust_976/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "107/100: soup.find(\"article\")\n",
      "107/101: soup.find(\"article\").find_all(\"p\")\n",
      "107/102: soup.find(id=\"content_inner\")\n",
      "107/103: soup.find(id=\"content_inner\").find_all(\"p\")\n",
      "107/104: soup.find(\"article\").find_all(\"p\")\n",
      "107/105: soup.find(\"article\").find_all(\"p\")[3]\n",
      "107/106: soup.find(\"article\").find_all(\"p\")[3].get_text()\n",
      "107/107:\n",
      "def get_book_description(books):\n",
      "    book_descriptions = []\n",
      "    \n",
      "    for book in books: \n",
      "        page_url = book[\"url\"]\n",
      "\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        # tip: look at the Google Inspector screenshot below \n",
      "        description = soup.find(id=\"content_inner\").find_all(\"p\")[3].get_text()\n",
      "        title = soup.find(id=\"content_inner\").find('img')['alt']\n",
      "        book_descriptions.append({'url': page_url,\n",
      "                                  'title': title,\n",
      "                                  'description': description})\n",
      "    return book_descriptions\n",
      "\n",
      "book_descriptions = get_book_description(book_list[0:2])\n",
      "book_descriptions\n",
      "107/108:\n",
      "def get_book_description(books):\n",
      "    book_descriptions = []\n",
      "    \n",
      "    for book in books: \n",
      "        page_url = book[\"url\"]\n",
      "\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        # tip: look at the Google Inspector screenshot below \n",
      "        description = soup.find(id=\"content_inner\").find_all(\"p\")[3].get_text()\n",
      "        title = soup.find(id=\"content_inner\").find('img')['alt']\n",
      "        book_descriptions.append({'url': page_url,\n",
      "                                  'title': title,\n",
      "                                  'description': description})\n",
      "    return book_descriptions\n",
      "\n",
      "book_descriptions = get_book_description(book_list[0:1])\n",
      "book_descriptions\n",
      "107/109:\n",
      "def get_book_description(books):\n",
      "    book_descriptions = []\n",
      "    \n",
      "    for book in books: \n",
      "        page_url = book[\"url\"]\n",
      "\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        # tip: look at the Google Inspector screenshot below \n",
      "        description = soup.find(id=\"content_inner\").find_all(\"p\")[3].get_text()\n",
      "        title = soup.find(id=\"content_inner\").find('img')['alt']\n",
      "        book_descriptions.append({'url': page_url,\n",
      "                                  'title': title,\n",
      "                                  'description': description})\n",
      "    return book_descriptions\n",
      "\n",
      "book_descriptions = get_book_description(book_list[0:2])\n",
      "book_descriptions\n",
      "107/110:\n",
      "from datetime import datetime\n",
      "now = datetime.now()\n",
      "\n",
      "import csv \n",
      "\n",
      "with open(\"book_descriptions.csv\", \"a\") as csv_file: # <<- this is the line with the \"flag\"l see exercises below\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"description\", \"date_time\"])\n",
      "    now = datetime.now()\n",
      "    for book in book_descriptions: # here we reference the book_descriptions list - make sure it's loaded otherwise you get an error! (Cell > Run All Above)\n",
      "        writer.writerow([book['title'], book['description'], now])\n",
      "print('done!')\n",
      "110/1:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    \n",
      "    #check next page:\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "\n",
      "check_next_page(url)\n",
      "110/2:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "110/3:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    \n",
      "    #check next page:\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "110/4:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "check_next_page(url)\n",
      "110/5:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "check_next_page(url)\n",
      "110/6:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        for book in extract_books([url]):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(page_url) != None: \n",
      "            page_url = \"https://books.toscrape.com/catalogue/category/books/nonfiction_13/\" + check_next_page(page_url)\n",
      "        else: \n",
      "            break\n",
      "110/7:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "extract_all_books(url)\n",
      "110/8:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "books = soup.find_all(\"h3\")\n",
      "110/9:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "books = soup.find_all(\"h3\")\n",
      "110/10:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"h3\")\n",
      "110/11:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"h3\").text\n",
      "110/12:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"h3\")[0]\n",
      "110/13:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"h3\")[0].text\n",
      "110/14: soup.find(\"img\").attrs['alt']\n",
      "110/15: soup.find_all(\"img\").attrs['alt']\n",
      "110/16: soup.find_all(\"img\")\n",
      "110/17: soup.find_all(\"img\")[0].attrs['alt']\n",
      "110/18: soup.find_all(\"img\")\n",
      "110/19: soup.find_all(\"img\")['alt']\n",
      "110/20: soup.find_all(\"img\").text\n",
      "110/21: soup.find_all(\"img\")\n",
      "110/22: soup.find_all(class_=\"product_pod\")\n",
      "110/23: soup.find_all(class_=\"product_pod\")[0]\n",
      "110/24: soup.find_all(class_=\"product_pod\")[0].find(\"a\")['alt']\n",
      "110/25: soup.find_all(class_=\"product_pod\")[0].find(\"img\")['alt']\n",
      "110/26: soup.find_all(class_=\"product_pod\")[0]\n",
      "110/27: soup.find_all(class_=\"product_pod\")[0].find(class_=\"product_price\").text\n",
      "110/28: soup.find_all(class_=\"product_pod\")[0].find(class_=\"product_price\")\n",
      "110/29: soup.find_all(class_=\"product_pod\")[0].find(class_=\"price_color\")\n",
      "110/30: soup.find_all(class_=\"product_pod\")[0].find(class_=\"price_color\").text\n",
      "110/31:\n",
      "def extract_nonfiction_book(url):\n",
      "    nonfic_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for books in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text\n",
      "        nonfic_book.append({'title': title,\n",
      "                            'price': price})\n",
      "110/32:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "nonfic_book = extract_nonfiction_book(url)\n",
      "110/33:\n",
      "def extract_nonfiction_book(url):\n",
      "    nonfic_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text\n",
      "        nonfic_book.append({'title': title,\n",
      "                            'price': price})\n",
      "110/34:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "nonfic_book = extract_nonfiction_book(url)\n",
      "110/35: nonfic_book\n",
      "110/36:\n",
      "def extract_nonfiction_book(url):\n",
      "    nonfic_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text\n",
      "        nonfic_book.append({'title': title,\n",
      "                            'price': price})\n",
      "    return nonfic_book\n",
      "110/37:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "nonfic_book = extract_nonfiction_book(url)\n",
      "110/38: nonfic_book\n",
      "110/39:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        for book in extract_nonfiction_book([url]):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/nonfiction_13/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "110/40:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "extract_all_books(url)\n",
      "110/41:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        for book in extract_nonfiction_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/nonfiction_13/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "110/42:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "extract_all_books(url)\n",
      "110/43:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "nonfic_book_list = extract_all_books(url)\n",
      "110/44: nonfic_book_list\n",
      "110/45: len(nonfic_book_list)\n",
      "110/46:\n",
      "from datetime import datetime\n",
      "now = datetime.now()\n",
      "\n",
      "import csv \n",
      "\n",
      "with open(\"nonfic_book.csv\", \"a\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"date_time\"])\n",
      "    now = datetime.now()\n",
      "    for book in nonfic_book_list:\n",
      "        writer.writerow([book['title'], book['price'], now])\n",
      "        \n",
      "pd.read_csv(\"nonfic_book.csv\", delimiter= \";\")\n",
      "110/47:\n",
      "from datetime import datetime\n",
      "now = datetime.now()\n",
      "\n",
      "import csv \n",
      "\n",
      "with open(\"nonfic_book.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"date_time\"])\n",
      "    now = datetime.now()\n",
      "    for book in nonfic_book_list:\n",
      "        writer.writerow([book['title'], book['price'], now])\n",
      "        \n",
      "pd.read_csv(\"nonfic_book.csv\", delimiter= \";\")\n",
      "110/48:\n",
      "from datetime import datetime\n",
      "now = datetime.now()\n",
      "\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"nonfic_book.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"date_time\"])\n",
      "    now = datetime.now()\n",
      "    for book in nonfic_book_list:\n",
      "        writer.writerow([book['title'], book['price'], now])\n",
      "        \n",
      "pd.read_csv(\"nonfic_book.csv\", delimiter= \";\")\n",
      "110/49:\n",
      "def extract_nonfiction_book(url):\n",
      "    nonfic_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text.replace(\"√Ç\",\"\")\n",
      "        nonfic_book.append({'title': title,\n",
      "                            'price': price})\n",
      "    return nonfic_book\n",
      "110/50:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "nonfic_book_list = extract_all_books(url)\n",
      "110/51: nonfic_book_list[0]\n",
      "110/52:\n",
      "from datetime import datetime\n",
      "now = datetime.now()\n",
      "\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"nonfic_book.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"date_time\"])\n",
      "    now = datetime.now()\n",
      "    for book in nonfic_book_list:\n",
      "        writer.writerow([book['title'], book['price'], now])\n",
      "        \n",
      "pd.read_csv(\"nonfic_book.csv\", delimiter= \";\")\n",
      "110/53: pd.read_csv(\"nonfic_book.csv\", delimiter= \";\")[0:6]\n",
      "110/54: pd.read_csv(\"nonfic_book.csv\", delimiter= \";\")[0:5]\n",
      "110/55:\n",
      "from datetime import datetime\n",
      "now = datetime.now()\n",
      "\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"nonfic_book.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"date_time\"])\n",
      "    now = datetime.now()\n",
      "    for book in nonfic_book_list:\n",
      "        writer.writerow([book['title'], book['price'], now])\n",
      "\n",
      "pd.read_csv(\"nonfic_book.csv\", delimiter= \";\")[0:5]\n",
      "114/1:\n",
      "import requests\n",
      "import json\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{mod}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups})\n",
      "114/2:\n",
      "import requests\n",
      "import json\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups})\n",
      "114/3:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups})\n",
      "114/4: post_list\n",
      "114/5: post_list[10]\n",
      "114/6: post_list[0:5]\n",
      "114/7:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            url = item['url']\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups})\n",
      "114/8:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            url = item['url']\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups,\n",
      "                             \"url\":url})\n",
      "114/9: post_list[0:5]\n",
      "114/10:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            ext_url = item['url_overridden_by_dest']\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups,\n",
      "                             \"ext_url\":ext_url})\n",
      "114/11:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups)\n",
      "114/12:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups})\n",
      "114/13: post_list[0:5]\n",
      "114/14:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"reddit.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"selftext\", \"downs\", \"ups\"])\n",
      "    now = datetime.now()\n",
      "    for post in post_list:\n",
      "        writer.writerow([post['id'], post['selftext'], post['downs'], post['ups'])\n",
      "\n",
      "pd.read_csv(\"reddit.csv\", delimiter= \";\")[0:5]\n",
      "114/15:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"reddit.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"selftext\", \"downs\", \"ups\"])\n",
      "    now = datetime.now()\n",
      "    for post in post_list:\n",
      "        writer.writerow([post['id'], post['selftext'], post['downs'], post['ups']])\n",
      "\n",
      "pd.read_csv(\"reddit.csv\", delimiter= \";\")[0:5]\n",
      "114/16:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"reddit.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"selftext\", \"downs\", \"ups\"])\n",
      "    for post in post_list:\n",
      "        writer.writerow([post['id'], post['selftext'], post['downs'], post['ups']])\n",
      "\n",
      "pd.read_csv(\"reddit.csv\", delimiter= \";\")[0:5]\n",
      "114/17: len(post_list)\n",
      "114/18:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"reddit.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"selftext\", \"downs\", \"ups\"])\n",
      "    for post in post_list:\n",
      "        writer.writerow([post['id'], post['selftext'], post['downs'], post['ups']])\n",
      "\n",
      "pd.read_csv(\"reddit.csv\", delimiter= \";\")[250:300]\n",
      "114/19:\n",
      "url = 'https://www.reddit.com/r/politics.json'\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": after})\n",
      "json_response = response.json()\n",
      "114/20: json_response\n",
      "114/21: len(json_response)\n",
      "114/22:\n",
      "url = 'https://www.reddit.com/r/politics.json'\n",
      "response = requests.get(url, \n",
      "                        headers=headers)\n",
      "json_response = response.json()\n",
      "114/23: json_response\n",
      "114/24: json_response['children']\n",
      "114/25: json_response[0]['children']\n",
      "114/26: json_response[0]\n",
      "114/27: json_response\n",
      "114/28: json_response['data']\n",
      "114/29: json_response['data'][children]\n",
      "114/30: json_response['data']['children']\n",
      "114/31: len(json_response['data']['children'])\n",
      "114/32: json_response['data']['children']\n",
      "114/33: json_response['data']['children']['title']\n",
      "114/34: json_response['data']['children'][0]\n",
      "114/35: json_response['data']['children'][0]['title']\n",
      "114/36: json_response['data']['children'][0]\n",
      "114/37: json_response['data']['children'][0]['data']\n",
      "114/38: json_response['data']['children'][0]['data']['title']\n",
      "114/39:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            title = item['data']['title']\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            post_list.append({\"id\":id,\n",
      "                              \"title\": title,\n",
      "                              \"selftext\":selftext,\n",
      "                              \"downs\": downs,\n",
      "                              \"ups\":ups})\n",
      "114/40:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            item_data = item['data']\n",
      "            title = item_data['title']\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            post_list.append({\"id\":id,\n",
      "                              \"title\": title,\n",
      "                              \"selftext\":selftext,\n",
      "                              \"downs\": downs,\n",
      "                              \"ups\":ups})\n",
      "114/41: json_response['data']['children'][0]['data']\n",
      "114/42: json_response['data']['children']\n",
      "114/43: json_response['data']['children'][0]\n",
      "114/44: json_response['data']['children'][0]['data']\n",
      "114/45: json_response['data']['children'][0]['selftext']\n",
      "114/46:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            title = item['title']\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            post_list.append({\"id\":id,\n",
      "                              \"title\": title,\n",
      "                              \"selftext\":selftext,\n",
      "                              \"downs\": downs,\n",
      "                              \"ups\":ups})\n",
      "114/47:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"reddit.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"selftext\", \"downs\", \"ups\"])\n",
      "    for post in post_list:\n",
      "        writer.writerow([post['id'], post['selftext'], post['downs'], post['ups']])\n",
      "\n",
      "pd.read_csv(\"reddit.csv\", delimiter= \";\")[250:260]\n",
      "114/48: post_list[0]\n",
      "114/49:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"reddit.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\",\"title\", \"selftext\", \"downs\", \"ups\"])\n",
      "    for post in post_list:\n",
      "        writer.writerow([post['id'], post['title'], post['selftext'], post['downs'], post['ups']])\n",
      "\n",
      "pd.read_csv(\"reddit.csv\", delimiter= \";\")[250:260]\n",
      "118/1:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append(item['data']['author'])\n",
      "    return users\n",
      "\n",
      "users = get_users('marketing')\n",
      "users\n",
      "118/2:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append({\"name\": item['data']['author'],\n",
      "                     \"community\": subreddit})\n",
      "    return users\n",
      "118/3: get_users('marketing')\n",
      "118/4:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append({\"name\": item['data']['author'],\n",
      "                     \"subreddit\": subreddit})\n",
      "    return users\n",
      "118/5: get_users('marketing')\n",
      "118/6:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "\n",
      "user_list =[]\n",
      "for sub in subreddits:\n",
      "    user_list.append(get_users(subreddit))\n",
      "118/7:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "\n",
      "user_list =[]\n",
      "for sub in subreddits:\n",
      "    user_list.append(get_users(sub))\n",
      "118/8: user_list.append(get_users('marketing'))\n",
      "118/9: user_list\n",
      "118/10: user_list = get_users('marketing')\n",
      "118/11: user_list\n",
      "118/12:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append({\"name\": item['data']['author'])\n",
      "    return users\n",
      "118/13:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append(item['data']['author'])\n",
      "    return users\n",
      "118/14: get_users('marketing')\n",
      "118/15:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "\n",
      "user_list =[]\n",
      "for sub in subreddits:\n",
      "    user_list.append(get_users(sub))\n",
      "118/16:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "\n",
      "user_list =[]\n",
      "for sub in subreddits:\n",
      "    get_users(sub)\n",
      "118/17: user_list = get_users('surfing')\n",
      "118/18: user_list\n",
      "118/19:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "\n",
      "for subreddit in subreddits:\n",
      "    get_users(subreddit)\n",
      "121/1:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "121/2:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    \n",
      "    #check next page:\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "121/3:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "121/4: soup.find_all(class_=\"product_pod\")\n",
      "121/5: soup.find_all(class_=\"product_pod\").find(\"p\", class_=\"star-rating\")\n",
      "121/6: soup.find_all(class_=\"product_pod\")\n",
      "121/7: soup.find_all(class_=\"product_pod\").find(\"p\")\n",
      "121/8: soup.find_all(class_=\"product_pod\")\n",
      "121/9: soup.find_all(class_=\"product_pod\")[0]\n",
      "121/10: soup.find_all(class_=\"product_pod\")[0].find(\"p\")\n",
      "121/11: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"star-rating\")\n",
      "121/12: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"star-rating\").find_all(\"i\")\n",
      "121/13: len(soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"star-rating\").find_all(\"i\"))\n",
      "121/14: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"star-rating\").find_all(\"i\")\n",
      "121/15: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"star-rating\")\n",
      "121/16:\n",
      "def extract_mystery_book(url):\n",
      "    mystery_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text.replace(\"√Ç\",\"\")\n",
      "        rating = book.find(\"p\", class_=\"star-rating\")\n",
      "        mystery_book.append({'title': title,\n",
      "                            'price': price})\n",
      "    return mistery_book\n",
      "121/17:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "mystery_book_list = extract_all_books(url)\n",
      "121/18:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/19:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "mystery_book_list = extract_all_books(url)\n",
      "121/20:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    \n",
      "    #check next page:\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "121/21:\n",
      "def extract_mystery_book(url):\n",
      "    mystery_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text.replace(\"√Ç\",\"\")\n",
      "        rating = book.find(\"p\", class_=\"star-rating\")\n",
      "        mystery_book.append({'title': title,\n",
      "                            'price': price,\n",
      "                            'rating': rating})\n",
      "    return mistery_book\n",
      "121/22:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/23:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "mystery_book_list = extract_all_books(url)\n",
      "121/24:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "extract_mystery_book(url)\n",
      "121/25:\n",
      "def extract_mystery_book(url):\n",
      "    mystery_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text.replace(\"√Ç\",\"\")\n",
      "        rating = book.find(\"p\", class_=\"star-rating\")\n",
      "        mystery_book.append({'title': title,\n",
      "                            'price': price,\n",
      "                            'rating': rating})\n",
      "    return mystery_book\n",
      "121/26:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/27:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "extract_all_books(url)\n",
      "121/28:\n",
      "def extract_mystery_book(url):\n",
      "    mystery_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text.replace(\"√Ç\",\"\")\n",
      "        rating = len(book.find(\"p\", class_=\"star-rating\"))\n",
      "        mystery_book.append({'title': title,\n",
      "                            'price': price,\n",
      "                            'rating': rating})\n",
      "    return mystery_book\n",
      "121/29: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"star-rating\")\n",
      "121/30: soup.find_all(class_=\"product_pod\")[0].find(class_).\n",
      "121/31: soup.find_all(class_=\"product_pod\")[0].find(class_='').\n",
      "121/32: soup.find_all(class_=\"product_pod\")[0].find(class_='icon-star').\n",
      "121/33: soup.find_all(class_=\"product_pod\")[0].find(class_='icon-star')\n",
      "121/34: soup.find_all(class_=\"product_pod\")[0].find(class_='star-rating')\n",
      "121/35: soup.find_all(class_=\"product_pod\")[6].find(class_='star-rating')\n",
      "121/36: soup.find_all(class_=\"product_pod\")[6].find(class_='star-rating')[0]\n",
      "121/37: soup.find_all(class_=\"product_pod\")[6].find(class_='star-rating')\n",
      "121/38:\n",
      "def extract_mystery_book(url):\n",
      "    mystery_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text.replace(\"√Ç\",\"\")\n",
      "        rating = len(book.find(\"p\", class_=\"star-rating\").find_all(\"i\"))\n",
      "        mystery_book.append({'title': title,\n",
      "                            'price': price,\n",
      "                            'rating': rating})\n",
      "    return mystery_book\n",
      "121/39:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/40:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "mystery_book = extract_all_books(url)\n",
      "121/41: mystery_book[0:5]\n",
      "121/42:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "121/43: soup.find_all(class_=\"product_pod\")\n",
      "121/44: soup.find_all(class_=\"product_pod\")[0]\n",
      "121/45: soup.find_all(class_=\"product_pod\")[0]find(\"p\", class_=\"star-rating\")\n",
      "121/46: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"star-rating\")\n",
      "121/47: soup.find_all(class_=\"product_pod\")[0].find(\"p\")\n",
      "121/48: soup.find_all(class_=\"product_pod\")[0].find(\"p\").get_text()\n",
      "121/49: soup.find_all(class_=\"product_pod\")[0].find(\"p\")\n",
      "121/50: soup.find_all(class_=\"product_pod\")[0].find(\"Four\")\n",
      "121/51: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"Four\")\n",
      "121/52: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"One\")\n",
      "121/53:\n",
      "from datetime import datetime\n",
      "now = datetime.now()\n",
      "\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"books.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"rating\"])\n",
      "    now = datetime.now()\n",
      "    for book in mystery_book:\n",
      "        writer.writerow([book['title'], book['price'], book['rating']])\n",
      "\n",
      "pd.read_csv(\"books.csv\", delimiter= \";\")[0:5]\n",
      "121/54:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"books.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"rating\"])\n",
      "    for book in mystery_book:\n",
      "        writer.writerow([book['title'], book['price'], book['rating']])\n",
      "\n",
      "pd.read_csv(\"books.csv\", delimiter= \";\")[0:5]\n",
      "121/55:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        \n",
      "        with open(\"seeds.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/56:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "mystery_book = extract_all_books(url)\n",
      "121/57:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        \n",
      "        with open(\"seeds.csv\", \"a\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/58:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "mystery_book = extract_all_books(url)\n",
      "121/59:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        \n",
      "        with open(\"seeds.csv\", \"a\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/60:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        \n",
      "        with open(\"seeds.csv\", \"a\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/61:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "mystery_book = extract_all_books(url)\n",
      "121/62:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "121/63:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    \n",
      "    #check next page:\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "121/64:\n",
      "def extract_mystery_book(url):\n",
      "    mystery_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text.replace(\"√Ç\",\"\")\n",
      "        rating = len(book.find(\"p\", class_=\"star-rating\").find_all(\"i\"))\n",
      "        mystery_book.append({'title': title,\n",
      "                            'price': price,\n",
      "                            'rating': rating})\n",
      "    return mystery_book\n",
      "121/65:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        \n",
      "        with open(\"seeds.csv\", \"a\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/66:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "mystery_book = extract_all_books(url)\n",
      "121/67:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"books.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"rating\"])\n",
      "    for book in mystery_book:\n",
      "        writer.writerow([book['title'], book['price'], book['rating']])\n",
      "\n",
      "pd.read_csv(\"books.csv\", delimiter= \";\")[0:5]\n",
      "121/68:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        \n",
      "        with open(\"seeds.csv\", \"a\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/69:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "mystery_book = extract_all_books(url)\n",
      "121/70:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"books.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"rating\"])\n",
      "    for book in mystery_book:\n",
      "        writer.writerow([book['title'], book['price'], book['rating']])\n",
      "\n",
      "pd.read_csv(\"books.csv\", delimiter= \";\")[0:5]\n",
      "121/71: len(mystery_book)\n",
      "121/72:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    with open(\"seeds.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"seeds\"])\n",
      "    while url:\n",
      "        print(url)\n",
      "        with open(\"seeds.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/73:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "mystery_book = extract_all_books(url)\n",
      "121/74:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"books.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"rating\"])\n",
      "    for book in mystery_book:\n",
      "        writer.writerow([book['title'], book['price'], book['rating']])\n",
      "\n",
      "pd.read_csv(\"books.csv\", delimiter= \";\")[0:5]\n",
      "121/75:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        with open(\"seeds.csv\", \"a\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/76:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "mystery_book = extract_all_books(url)\n",
      "121/77:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "121/78:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        with open(\"seeds.csv\", \"a\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/79:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "mystery_book = extract_all_books(url)\n",
      "121/80:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"books.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"rating\"])\n",
      "    for book in mystery_book:\n",
      "        writer.writerow([book['title'], book['price'], book['rating']])\n",
      "\n",
      "pd.read_csv(\"books.csv\", delimiter= \";\")[0:5]\n",
      "118/20:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append(item['data']['author'])\n",
      "    return users\n",
      "\n",
      "users = get_users('marketing')\n",
      "users\n",
      "118/21:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "\n",
      "for subreddit in subreddits:\n",
      "    get_users(subreddit)\n",
      "118/22:\n",
      "users = get_users('skating')\n",
      "users\n",
      "118/23:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "users = []\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append(item['data']['author'])\n",
      "    return users\n",
      "118/24:\n",
      "users = get_users('skating')\n",
      "users\n",
      "118/25:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append(item['data']['author'])\n",
      "    return users\n",
      "118/26:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "users = []\n",
      "\n",
      "for subreddit in subreddits:\n",
      "    users.append = get_users('skating')\n",
      "118/27:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "users = []\n",
      "\n",
      "for subreddit in subreddits:\n",
      "    users.append (get_users('skating'))\n",
      "118/28: users[1]\n",
      "118/29: len(users)\n",
      "118/30:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "\n",
      "for subreddit in subreddits:\n",
      "    users.append(get_users(subreddit))\n",
      "118/31:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "users = []\n",
      "for subreddit in subreddits:\n",
      "    users.append(get_users(subreddit))\n",
      "118/32:\n",
      "url = 'https://www.reddit.com/r/surfing.json'\n",
      "response = requests.get(url,headers=headers)\n",
      "json_response = response.json()\n",
      "118/33: json_response\n",
      "118/34: json_response['data']\n",
      "118/35:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append(item['data']['author'])\n",
      "    return users\n",
      "118/36:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append(item['data']['author'])\n",
      "    return users\n",
      "118/37:\n",
      "users = get_users('skating')\n",
      "users\n",
      "118/38:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "users = []\n",
      "for item in subreddits:\n",
      "    print(get_users(item))\n",
      "118/39:\n",
      "url = 'https://www.reddit.com/r/horseriding.json'\n",
      "response = requests.get(url,headers=headers)\n",
      "json_response = response.json()\n",
      "118/40: json_response['data']\n",
      "118/41: response\n",
      "118/42:\n",
      "subreddits = ['surfing','skating', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "users = []\n",
      "for item in subreddits:\n",
      "    users.append(get_users(item))\n",
      "118/43: users\n",
      "118/44: len(users)\n",
      "118/45:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        user = item['data']['author']\n",
      "        subred = subreddit\n",
      "        users.append({\"user\":user,\n",
      "                      \"subred\":subreddit})\n",
      "    return users\n",
      "118/46:\n",
      "users = get_users('skating')\n",
      "users\n",
      "118/47:\n",
      "subreddits = ['surfing','skating', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "users = []\n",
      "for item in subreddits:\n",
      "    users.append(get_users(item))\n",
      "121/81:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "soup.find_all(class_=\"product_pod\")\n",
      "121/82:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "soup.find_all(class_=\"product_pod\")[0]\n",
      "121/83: soup.find_all(class_=\"product_pod\")[0].find(\"p\").attrs['class']\n",
      "121/84: soup.find_all(class_=\"product_pod\")[0].find(\"p\").attrs['class'].[1]\n",
      "121/85: soup.find_all(class_=\"product_pod\")[0].find(\"p\").attrs['class']\n",
      "121/86: soup.find_all(class_=\"product_pod\")[0].find(\"p\").attrs['class'][0]\n",
      "121/87: soup.find_all(class_=\"product_pod\")[0].find(\"p\").attrs['class'][1]\n",
      "123/1:\n",
      "import requests\n",
      "import lxml.html as lh\n",
      "import pandas as pd\n",
      "123/2:\n",
      "import requests\n",
      "import lxml.html as lh\n",
      "import pandas as pd\n",
      "123/3:\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "#Create a handle, page, to handle the contents of the website\n",
      "page = requests.get(url)\n",
      "#Store the contents of the website under doc\n",
      "doc = lh.fromstring(page.content)\n",
      "#Parse data that are stored between <tr>..</tr> of HTML\n",
      "tr_elements = doc.xpath('//tr')\n",
      "123/4:\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "#Create a handle, page, to handle the contents of the website\n",
      "page = requests.get(url)\n",
      "#Store the contents of the website under doc\n",
      "doc = lh.fromstring(page.content)\n",
      "#Parse data that are stored between <tr>..</tr> of HTML\n",
      "tr_elements = doc.xpath('//tr')\n",
      "123/5:\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "#Create a handle, page, to handle the contents of the website\n",
      "page = requests.get(url)\n",
      "#Store the contents of the website under doc\n",
      "doc = lh.fromstring(page.content)\n",
      "#Parse data that are stored between <tr>..</tr> of HTML\n",
      "tr_elements = doc.xpath('//tr')\n",
      "123/6: tr_elements\n",
      "123/7: page\n",
      "123/8: doc\n",
      "123/9: page.content\n",
      "123/10: page.header\n",
      "123/11:\n",
      "tr_elements = doc.xpath('//tr')\n",
      "#Create empty list\n",
      "col=[]\n",
      "i=0\n",
      "#For each row, store each first element (header) and an empty list\n",
      "for t in tr_elements[0]:\n",
      "    i+=1\n",
      "    name=t.text_content()\n",
      "    print '%d:\"%s\"'%(i,name)\n",
      "    col.append((name,[]))\n",
      "123/12:\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "#Create a handle, page, to handle the contents of the website\n",
      "page = requests.get(url)\n",
      "#Store the contents of the website under doc\n",
      "doc = lh.fromstring(page.content)\n",
      "#Parse data that are stored between <tr>..</tr> of HTML\n",
      "tr_elements = doc.xpath('//tr')\n",
      "123/13: [len(T) for T in tr_elements[:12]]\n",
      "123/14: tr_elements = doc.xpath('//tr')\n",
      "123/15: tr_elements = doc.xpath('//tr')\n",
      "123/16: tr_elements\n",
      "123/17: tr_elements[0]\n",
      "123/18:\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "#Create a handle, page, to handle the contents of the website\n",
      "page = requests.get(url)\n",
      "#Store the contents of the website under doc\n",
      "doc = lh.fromstring(page.content)\n",
      "#Parse data that are stored between <tr>..</tr> of HTML\n",
      "tr_elements = doc.xpath('//tr')\n",
      "123/19: tr_elements\n",
      "123/20: tr_elements[0]\n",
      "123/21: tr_elements[0].content\n",
      "123/22: tr_elements[0].content()\n",
      "123/23: tr_elements[0].text\n",
      "123/24: tr_elements[0].text_content()\n",
      "123/25: tr_elements[0]\n",
      "123/26: tr_elements[0].text_content()\n",
      "123/27: len(tr_elements[0])\n",
      "123/28: tr_elements[0][0]\n",
      "123/29: tr_elements[0][1]\n",
      "123/30:\n",
      "tr_elements = doc.xpath('//tr')\n",
      "#Create empty list\n",
      "col=[]\n",
      "i=0\n",
      "#For each row, store each first element (header) and an empty list\n",
      "for t in tr_elements[0]:\n",
      "    i+=1\n",
      "    name=t.text_content()\n",
      "    print '%d:\"%s\"'%(i,name)\n",
      "    col.append((name,[]))\n",
      "123/31:\n",
      "tr_elements = doc.xpath('//tr')\n",
      "#Create empty list\n",
      "col=[]\n",
      "i=0\n",
      "#For each row, store each first element (header) and an empty list\n",
      "for t in tr_elements[0]:\n",
      "    i+=1\n",
      "    name=t.text_content()\n",
      "    col.append((name,[]))\n",
      "123/32: col\n",
      "123/33: col[0]\n",
      "123/34: col[1]\n",
      "123/35: col\n",
      "123/36:\n",
      "#Since out first row is the header, data is stored on the second row onwards\n",
      "for j in range(1,len(tr_elements)):\n",
      "    #T is our j'th row\n",
      "    T=tr_elements[j]\n",
      "    \n",
      "    #If row is not of size 10, the //tr data is not from our table \n",
      "    if len(T)!=5:\n",
      "        break\n",
      "    \n",
      "    #i is the index of our column\n",
      "    i=0\n",
      "    \n",
      "    #Iterate through each element of the row\n",
      "    for t in T.iterchildren():\n",
      "        data=t.text_content() \n",
      "        #Check if row is empty\n",
      "        if i>0:\n",
      "        #Convert any numerical value to integers\n",
      "            try:\n",
      "                data=int(data)\n",
      "            except:\n",
      "                pass\n",
      "        #Append the data to the empty list of the i'th column\n",
      "        col[i][1].append(data)\n",
      "        #Increment i for the next column\n",
      "        i+=1\n",
      "123/37:\n",
      "Dict={title:column for (title,column) in col}\n",
      "df=pd.DataFrame(Dict)\n",
      "123/38: df.head()\n",
      "123/39: df.head(nrows = 100)\n",
      "123/40: df.head(100)\n",
      "123/41:\n",
      "tr_elements = doc.xpath('//tr')\n",
      "#Create empty list\n",
      "col=[]\n",
      "i=0\n",
      "#For each row, store each first element (header) and an empty list\n",
      "for t in tr_elements[0]:\n",
      "    i+=1\n",
      "    name=t.text_content().replace(\"\\n\",\"\")\n",
      "    col.append((name,[]))\n",
      "123/42: col\n",
      "123/43:\n",
      "#Since out first row is the header, data is stored on the second row onwards\n",
      "for j in range(1,len(tr_elements)):\n",
      "    #T is our j'th row\n",
      "    T=tr_elements[j]\n",
      "    \n",
      "    #If row is not of size 10, the //tr data is not from our table \n",
      "    if len(T)!=5:\n",
      "        break\n",
      "    \n",
      "    #i is the index of our column\n",
      "    i=0\n",
      "    \n",
      "    #Iterate through each element of the row\n",
      "    for t in T.iterchildren():\n",
      "        data=t.text_content() \n",
      "        #Check if row is empty\n",
      "        if i>0:\n",
      "        #Convert any numerical value to integers\n",
      "            try:\n",
      "                data=int(data)\n",
      "            except:\n",
      "                pass\n",
      "        #Append the data to the empty list of the i'th column\n",
      "        col[i][1].append(data)\n",
      "        #Increment i for the next column\n",
      "        i+=1\n",
      "123/44:\n",
      "Dict={title:column for (title,column) in col}\n",
      "df=pd.DataFrame(Dict)\n",
      "123/45: df.head(100)\n",
      "123/46: len(df)\n",
      "123/47:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find_all(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "123/48:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(class_=\"f3-widget-paginator\")[0].find(\"a\").attrs[\"href\"]\n",
      "123/49:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "123/50:\n",
      "url_list = []\n",
      "for i in range(104):\n",
      "    url_list.append(soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"])\n",
      "123/51:\n",
      "url_list = []\n",
      "for i in range(1:105):\n",
      "    url_list.append(soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"])\n",
      "123/52:\n",
      "url_list = []\n",
      "for i in range(1,105):\n",
      "    url_list.append(soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"])\n",
      "123/53:\n",
      "url_list = []\n",
      "for i in range(0,105):\n",
      "    url_list.append(soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"])\n",
      "123/54:\n",
      "url_list = []\n",
      "for i in range(0,104):\n",
      "    url_list.append(soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"])\n",
      "123/55:\n",
      "url_list = []\n",
      "for i in range(0,104):\n",
      "    url = soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"])\n",
      "    url_list.append(url)\n",
      "123/56:\n",
      "url_list = []\n",
      "for i in range(0,104):\n",
      "    url = soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"]\n",
      "    url_list.append(url)\n",
      "123/57:\n",
      "### url_list = []\n",
      "for i in range(0,103):\n",
      "    url = soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"]\n",
      "    url_list.append(url)\n",
      "123/58:\n",
      "### url_list = []\n",
      "for i in range(1,103):\n",
      "    url = soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"]\n",
      "    url_list.append(url)\n",
      "123/59:\n",
      "### url_list = []\n",
      "for i in range(10):\n",
      "    url = soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"]\n",
      "    url_list.append(url)\n",
      "123/60: soup.find_all(class_=\"f3-widget-paginator\")\n",
      "123/61: len(soup.find_all(class_=\"f3-widget-paginator\"))\n",
      "123/62: soup.find_all(class_=\"f3-widget-paginator\").find_all(\"a\").attrs[\"href\"]\n",
      "123/63: soup.find_all(class_=\"f3-widget-paginator\").find(\"a\").attrs[\"href\"]\n",
      "123/64: soup.find_all(class_=\"f3-widget-paginator\")\n",
      "123/65: soup.find_all(class_=\"f3-widget-paginator\")[0]\n",
      "123/66: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\").attrs[\"href\"]\n",
      "123/67: soup.find_all(class_=\"f3-widget-paginator\")[0].find(\"a\").attrs[\"href\"]\n",
      "123/68: soup.find_all(class_=\"f3-widget-paginator\")[1].find(\"a\").attrs[\"href\"]\n",
      "123/69: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\").attrs[\"href\"]\n",
      "123/70: soup.find_all(class_=\"f3-widget-paginator\")[0]\n",
      "123/71: soup.find_all(class_=\"pagerLink\")\n",
      "123/72: len(soup.find_all(class_=\"pagerLink\"))\n",
      "123/73: soup.find_all(class_=\"pagerLink\")\n",
      "125/1:\n",
      "import requests\n",
      "import lxml.html as lh\n",
      "import pandas as pd\n",
      "125/2:\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "#Create a handle, page, to handle the contents of the website\n",
      "page = requests.get(url)\n",
      "#Store the contents of the website under doc\n",
      "doc = lh.fromstring(page.content)\n",
      "#Parse data that are stored between <tr>..</tr> of HTML\n",
      "tr_elements = doc.xpath('//tr')\n",
      "125/3:\n",
      "tr_elements = doc.xpath('//tr')\n",
      "#Create empty list\n",
      "col=[]\n",
      "i=0\n",
      "#For each row, store each first element (header) and an empty list\n",
      "for t in tr_elements[0]:\n",
      "    i+=1\n",
      "    name=t.text_content().replace(\"\\n\",\"\")\n",
      "    col.append((name,[]))\n",
      "125/4: col\n",
      "125/5:\n",
      "#Since out first row is the header, data is stored on the second row onwards\n",
      "for j in range(1,len(tr_elements)):\n",
      "    #T is our j'th row\n",
      "    T=tr_elements[j]\n",
      "    \n",
      "    #If row is not of size 10, the //tr data is not from our table \n",
      "    if len(T)!=5:\n",
      "        break\n",
      "    \n",
      "    #i is the index of our column\n",
      "    i=0\n",
      "    \n",
      "    #Iterate through each element of the row\n",
      "    for t in T.iterchildren():\n",
      "        data=t.text_content() \n",
      "        #Check if row is empty\n",
      "        if i>0:\n",
      "        #Convert any numerical value to integers\n",
      "            try:\n",
      "                data=int(data)\n",
      "            except:\n",
      "                pass\n",
      "        #Append the data to the empty list of the i'th column\n",
      "        col[i][1].append(data)\n",
      "        #Increment i for the next column\n",
      "        i+=1\n",
      "125/6:\n",
      "Dict={title:column for (title,column) in col}\n",
      "df=pd.DataFrame(Dict)\n",
      "125/7: df\n",
      "125/8:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/9: len(soup.find_all(class_=\"pagerLink\"))\n",
      "125/10: soup.find_all(class_=\"pagerLink\")[19]\n",
      "125/11: soup.find_all(class_=\"pagerLink\")[18]\n",
      "125/12: soup.find_all(class_=\"pagerLink\")[19]\n",
      "125/13: soup.find_all(class_=\"pagerLink\")[19].find(\"a\").attrs(\"href\")\n",
      "125/14: soup.find_all(class_=\"pagerLink\")[19].find(\"a\").attrs[\"href\"]\n",
      "125/15: soup.find_all(class_=\"pagerLink\").find(\"a\").attrs[\"href\"]\n",
      "125/16: soup.find_all(class_=\"pagerLink\")\n",
      "125/17: soup.find_all(class_=\"pagerLink\").attrs[\"href\"]\n",
      "125/18: soup.find_all(class_=\"pagerLink\").text\n",
      "125/19: soup.find_all(class_=\"pagerLink\")[0].text\n",
      "125/20: soup.find_all(class_=\"pagerLink\")[0]\n",
      "125/21: soup.find_all(class_=\"pagerLink\")[0].find(\"href\")\n",
      "125/22: soup.find_all(class_=\"pagerLink\")[0].find(\"a\")\n",
      "125/23: soup.find_all(class_=\"a\")\n",
      "125/24: soup.find_all(\"a\")\n",
      "125/25: soup.find_all(class_=\"f3-widget-paginator\")\n",
      "125/26: soup.find_all(class_=\"f3-widget-paginator\").find_all(\"a\")\n",
      "125/27: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "125/28: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\").attrs['href']\n",
      "125/29: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[1].attrs['href']\n",
      "125/30: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[0].attrs['href']\n",
      "125/31: len(soup.find_all(class_=\"f3-widget-paginator\")[0])\n",
      "125/32: len(soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[0])\n",
      "125/33: len(soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\"))\n",
      "125/34:\n",
      "url_list = []\n",
      "for item in soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\"):\n",
      "    url = item.attrs[\"href\"]\n",
      "    url_list.append(url)\n",
      "125/35:\n",
      "url_list = []\n",
      "for item in soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\"):\n",
      "    url = f'https://www.ohnegentechnik.org{item.attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "125/36: url_list\n",
      "125/37:\n",
      "url_list = []\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "for i in range(20):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "125/38: url_list\n",
      "125/39:\n",
      "url_list = []\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "for i in range(19):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "125/40: url_list\n",
      "125/41: len(url_list)\n",
      "125/42: page = url[len(url_list)-1]\n",
      "125/43:\n",
      "page = url[len(url_list)-1]\n",
      "page\n",
      "125/44:\n",
      "page = url_list[len(url_list)-1]\n",
      "page\n",
      "125/45:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/46:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/47: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "125/48: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[20]\n",
      "125/49: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[0]\n",
      "125/50: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[1]\n",
      "125/51: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[2]\n",
      "125/52: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "125/53: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11]\n",
      "125/54: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:18]\n",
      "125/55: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:19]\n",
      "125/56: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "125/57: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:21]\n",
      "125/58: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "125/59:\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "for i in range(10):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "    page = url_list[len(url_list)-1]\n",
      "125/60: all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "125/61:\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "len(all_url)\n",
      "125/62:\n",
      "for i in range(9):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "    page = url_list[len(url_list)-1]\n",
      "125/63: url_list\n",
      "125/64: len(url_list)\n",
      "125/65: url_list\n",
      "125/66:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/67:\n",
      "url_list = []\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "for i in range(19):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "    page = url_list[len(url_list)-1]\n",
      "125/68: url_list\n",
      "125/69:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/70:\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "len(all_url)\n",
      "125/71:\n",
      "for i in range(9):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "    page = url_list[len(url_list)-1]\n",
      "125/72: url_list\n",
      "125/73: len(url_list)\n",
      "125/74: url_list\n",
      "125/75: page\n",
      "125/76:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "all_url\n",
      "125/77:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "all_url\n",
      "125/78:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "all_url\n",
      "125/79:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "for i in range(9):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "    page = url_list[len(url_list)-1]\n",
      "125/80: url_list\n",
      "125/81: len(url_list)\n",
      "125/82:\n",
      "while len(url_list) < 105:\n",
      "    res = requests.get(page)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "    for i in range(9):\n",
      "        url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "        url_list.append(url)\n",
      "        page = url_list[len(url_list)-1]\n",
      "125/83: len(url_list)\n",
      "125/84: url_list\n",
      "125/85:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/86:\n",
      "url_list = []\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "for i in range(19):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "    page = url_list[len(url_list)-1]\n",
      "125/87:\n",
      "while len(url_list) < 100:\n",
      "    res = requests.get(page)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "    for i in range(9):\n",
      "        url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "        url_list.append(url)\n",
      "        page = url_list[len(url_list)-1]\n",
      "125/88: url_list\n",
      "125/89: page\n",
      "125/90:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/91:\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "all_url\n",
      "125/92:\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "len(all_url)\n",
      "125/93: all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[17:19]\n",
      "125/94:\n",
      "\n",
      "all_url\n",
      "125/95:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[16:19]\n",
      "125/96: all_url\n",
      "125/97:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[17:19]\n",
      "125/98: all_url\n",
      "125/99:\n",
      "for i in range(2):\n",
      "        url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "        url_list.append(url)\n",
      "125/100: url_list\n",
      "125/101:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[17:20]\n",
      "for i in range(2):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "125/102: url_list\n",
      "125/103:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/104:\n",
      "url_list = []\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "for i in range(19):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "    page = url_list[len(url_list)-1]\n",
      "125/105:\n",
      "while len(url_list) < 100:\n",
      "    res = requests.get(page)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "    for i in range(9):\n",
      "        url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "        url_list.append(url)\n",
      "        page = url_list[len(url_list)-1]\n",
      "125/106:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "125/107: all_url[17:21]\n",
      "125/108: url_list\n",
      "125/109:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[17:21]\n",
      "125/110:\n",
      "for i in range(3):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "125/111: url_list\n",
      "125/112: len(url_list)\n",
      "125/113: url\n",
      "125/114: page1 = [\"https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche\"]\n",
      "125/115: page1.append(url_list)\n",
      "125/116: page1\n",
      "125/117: urls = [\"https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche\"]\n",
      "125/118: urls.extend(url_list)\n",
      "125/119: urls\n",
      "125/120: len(urls)\n",
      "125/121:\n",
      "#Create a handle, page, to handle the contents of the website\n",
      "url= url_list[0]\n",
      "page = requests.get(url)\n",
      "#Store the contents of the website under doc\n",
      "doc = lh.fromstring(page.content)\n",
      "#Parse data that are stored between <tr>..</tr> of HTML\n",
      "tr_elements = doc.xpath('//tr')\n",
      "\n",
      "#Create empty list\n",
      "col=[]\n",
      "i=0\n",
      "#For each row, store each first element (header) and an empty list\n",
      "for t in tr_elements[0]:\n",
      "    i+=1\n",
      "    name=t.text_content().replace(\"\\n\",\"\")\n",
      "    col.append((name,[]))\n",
      "\n",
      "#Since out first row is the header, data is stored on the second row onwards\n",
      "for url in url_list:\n",
      "    page = requests.get(url)\n",
      "    doc = lh.fromstring(page.content)\n",
      "    tr_elements = doc.xpath('//tr')\n",
      "    for j in range(1,len(tr_elements)):\n",
      "        #T is our j'th row\n",
      "        T=tr_elements[j]\n",
      "\n",
      "        #If row is not of size 10, the //tr data is not from our table \n",
      "        if len(T)!=5:\n",
      "            break\n",
      "\n",
      "        #i is the index of our column\n",
      "        i=0\n",
      "\n",
      "        #Iterate through each element of the row\n",
      "        for t in T.iterchildren():\n",
      "            data=t.text_content() \n",
      "            #Check if row is empty\n",
      "            if i>0:\n",
      "            #Convert any numerical value to integers\n",
      "                try:\n",
      "                    data=int(data)\n",
      "                except:\n",
      "                    pass\n",
      "            #Append the data to the empty list of the i'th column\n",
      "            col[i][1].append(data)\n",
      "            #Increment i for the next column\n",
      "            i+=1\n",
      "125/122:\n",
      "Dict={title:column for (title,column) in col}\n",
      "df=pd.DataFrame(Dict)\n",
      "125/123: df\n",
      "125/124: df.to_csv(\"nonGMO.csv\")\n",
      "129/1:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://info.lidl/en'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "129/2:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://info.lidl/en'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "129/3: soup.find_all(class_=\"lidl-m-svg-map-marker__text\").text\n",
      "129/4: soup.find_all(class_=\"lidl-m-svg-map-marker__text\")\n",
      "129/5: soup.find_all(class_=\"lidl-m-svg-map-marker__text\")[1]\n",
      "129/6: soup.find_all(class_=\"lidl-m-svg-map-marker__text\")[1].text\n",
      "129/7:\n",
      "for i in soup.find_all(class_=\"lidl-m-svg-map-marker__text\"):\n",
      "    print i.text\n",
      "129/8:\n",
      "for i in soup.find_all(class_=\"lidl-m-svg-map-marker__text\"):\n",
      "    print (i.text)\n",
      "130/1:\n",
      "url='https://www.international.tiffany.com/jewelry-stores/store-list/'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "130/2:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "url='https://www.international.tiffany.com/jewelry-stores/store-list/'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "130/3: soup.find_all(class_=\"stores-filter__regions-content-dropdown-list\")\n",
      "131/1:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://storelocator.yves-rocher.com/en/'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "131/2: soup.find_all(\"option\")\n",
      "131/3:\n",
      "for i in soup.find_all(\"option\"):\n",
      "    print(i.text)\n",
      "133/1:\n",
      "import pandas as pd\n",
      "import requests\n",
      "import json\n",
      "133/2:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup\n",
      "import pandas as pd\n",
      "import requests\n",
      "import json\n",
      "133/3:\n",
      "#DE - Woman sportwear\n",
      "url1 = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "de1 = requests.get(url)\n",
      "soup_de1 = BeautifulSoup(res.text, \"html.parser\")\n",
      "133/4:\n",
      "#DE - Woman sportwear\n",
      "url1 = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "de1 = requests.get(url1)\n",
      "soup_de1 = BeautifulSoup(res.text, \"html.parser\")\n",
      "133/5:\n",
      "#DE - Woman sportwear\n",
      "url1 = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "de1 = requests.get(url1)\n",
      "soup_de1 = BeautifulSoup(de1.text, \"html.parser\")\n",
      "133/6:\n",
      "#DE - Woman sportwear\n",
      "url1 = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "de1 = requests.get(url1)\n",
      "soup_de1 = BeautifulSoup(de1.text, \"html.parser\")\n",
      "\n",
      "soup_de1\n",
      "133/7:\n",
      "#DE - Woman sportwear\n",
      "url1 = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "de1 = requests.get(url1)\n",
      "soup_de1 = BeautifulSoup(de1.text, \"html.parser\")\n",
      "\n",
      "de1\n",
      "133/8:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup\n",
      "import pandas as pd\n",
      "import requests\n",
      "from urllib.request import urlopen\n",
      "133/9:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "from urllib.request import urlopen\n",
      "133/10:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "from urllib.request import urlopen\n",
      "133/11:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "def getAndParseURL(url): \n",
      "result = requests.get(url)\n",
      "soup = bts(result.text, 'html.parser')\n",
      "return soup\n",
      "133/12:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url)\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "133/13:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url)\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "\n",
      "getAndParseURL(url)\n",
      "133/14:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "\n",
      "getAndParseURL(url)\n",
      "133/15:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/98.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "\n",
      "getAndParseURL(url)\n",
      "133/16:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "\n",
      "getAndParseURL(url)\n",
      "133/17:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 <- getAndParseURL(url)\n",
      "\n",
      "de1\n",
      "133/18:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "133/19:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 <- getAndParseURL(url)\n",
      "\n",
      "de1\n",
      "133/20:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "from urllib.request import urlopen\n",
      "133/21:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "de1\n",
      "133/22: print(de1.find('product-list')\n",
      "133/23:\n",
      "for i in de1.findAll(\"h2\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "133/24:\n",
      "for i in de1.findAll(\"strong\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "133/25:\n",
      "# Getting product category\n",
      "de1.find(\"h1\")\n",
      "\n",
      "# Getting product name\n",
      "for i in de1.findAll(\"h2\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "    \n",
      "# Getting brand name\n",
      "\n",
      "for i in de1.findAll(\"strong\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "133/26: de1.find(\"h1\")\n",
      "133/27: de1.find(\"h1\").text\n",
      "133/28: print(de1.find(\"h1\").text)\n",
      "133/29: print(de1.find(\"div\", {\"class\": \"plp-bar-info svelte-1uqvrhu\"}).text)\n",
      "133/30: print(de1.find(\"span\", {\"class\": \"svelte-1uqvrhu\"}).text)\n",
      "133/31: 4096/40\n",
      "136/1:\n",
      "# Getting product category\n",
      "print(de1.find(\"h1\").text)\n",
      "\n",
      "# Getting product name\n",
      "for i in de1.findAll(\"h2\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "    \n",
      "# Getting brand name\n",
      "\n",
      "for i in de1.findAll(\"strong\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "    \n",
      "help(findAll)\n",
      "136/2: help(findAll)\n",
      "136/3: ?findAll\n",
      "136/4:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "from urllib.request import urlopen\n",
      "136/5:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "136/6:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/7:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "de1\n",
      "136/8:\n",
      "# Getting product category\n",
      "print(de1.find(\"h1\").text)\n",
      "136/9:\n",
      "# Getting product name\n",
      "for i in de1.findAll(\"h2\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "136/10:\n",
      "# Getting product name\n",
      "for i in de1.find_all(\"h2\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "136/11:\n",
      "# Getting product name\n",
      "for i in de1.find_all(\"h2\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "136/12:\n",
      "# Getting brand name\n",
      "\n",
      "for i in de1.find_All(\"strong\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "136/13:\n",
      "# Getting brand name\n",
      "for i in de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "136/14:\n",
      "# Getting brand name\n",
      "for i in de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    i.text\n",
      "136/15:\n",
      "# Getting brand name\n",
      "for i in de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "136/16:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "136/17:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "brand_name\n",
      "136/18:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "brand_name[1]\n",
      "136/19:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "brand_name[0]\n",
      "136/20:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "brand_name[0].text\n",
      "136/21:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "length(brand_name)\n",
      "136/22:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "len(brand_name)\n",
      "136/23:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "for name in brand_name:\n",
      "    print(brand_name.text)\n",
      "136/24:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "for name in brand_name:\n",
      "    print(name.text)\n",
      "136/25:\n",
      "# Getting product name\n",
      "prod_name = de1.find_all(\"h2\",{\"class\":\"svelte-wmr4s2\"})\n",
      "prod_name_list =[]\n",
      "\n",
      "for name in prod_name:\n",
      "    prod_name_list.append(name.text)\n",
      "136/26:\n",
      "# Getting product name\n",
      "prod_name = de1.find_all(\"h2\",{\"class\":\"svelte-wmr4s2\"})\n",
      "prod_name_list =[]\n",
      "\n",
      "for name in prod_name:\n",
      "    prod_name_list.append(name.text)\n",
      "    \n",
      "prod_name_list\n",
      "136/27:\n",
      "# Getting product name\n",
      "prod_name = de1.find_all(\"h2\",{\"class\":\"svelte-wmr4s2\"})\n",
      "prod_name_list =[]\n",
      "\n",
      "for name in prod_name:\n",
      "    prod_name_list.append(name.text)\n",
      "    \n",
      "len(prod_name_list)\n",
      "136/28:\n",
      "#Getting product name\n",
      "prod_name = de1.find_all(\"h2\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "#creating an empty array of product names\n",
      "prod_name_list =[]\n",
      "\n",
      "for name in prod_name:\n",
      "    prod_name_list.append(name.text)\n",
      "    \n",
      "len(prod_name_list)\n",
      "136/29: prod_name\n",
      "136/30: sport = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "136/31:\n",
      "sport = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "sport\n",
      "136/32:\n",
      "sport = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "len(sport)\n",
      "136/33:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "len(prod)\n",
      "\n",
      "type(prod)\n",
      "136/34:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "len(prod)\n",
      "136/35:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "len(prod)\n",
      "136/36:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[0]\n",
      "136/37:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[0].find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "136/38:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[0].find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "136/39:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[3].find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "136/40:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[3].find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "136/41:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[2].find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "136/42:\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = product.find(\"a\").attrs[\"href\"].text\n",
      "    prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'price': prod_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker})\n",
      "136/43:\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'price': prod_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker})\n",
      "136/44:\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'price': prod_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker})\n",
      "136/45:\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'price': prod_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker})\n",
      "136/46:\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'price': prod_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker})\n",
      "136/47: de1_list\n",
      "136/48: de1[3].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "136/49: prod[3].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "136/50: prod[3].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "136/51: prod[1].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "136/52: prod[1].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "136/53: prod[2].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "136/54: prod[3].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "136/55: prod[3].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text == None\n",
      "136/56:\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = product.find(\"a\").attrs[\"href\"]\n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'regular price': prod_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker})\n",
      "136/57: de1_list\n",
      "136/58:\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = product.find(\"a\").attrs[\"href\"]\n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "136/59: de1_list\n",
      "136/60:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}product.find(\"a\").attrs[\"href\"]'\n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "136/61: de1_list\n",
      "136/62:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{product.find(\"a\").attrs[\"href\"]}'\n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "136/63: de1_list\n",
      "140/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "140/2:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "140/3:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "de1\n",
      "140/4:\n",
      "# Getting product category\n",
      "print(de1.find(\"h1\").text)\n",
      "140/5:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{product.find(\"a\").attrs[\"href\"]}'\n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/6:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[2].find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "140/7:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{product.find(\"a\").attrs[\"href\"]}'\n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/8:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{product.find(\"a\").attrs[\"href\"]}'\n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/9: de1_list\n",
      "140/10: prod[1].find(\"div\").attrs[\"href\"]\n",
      "140/11: text = 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ'\n",
      "140/12:\n",
      "string = 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ'\n",
      "\n",
      "print(string.partition(\"?mc\")[2])\n",
      "140/13:\n",
      "string = 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ'\n",
      "\n",
      "print(string.partition(\"?mc=\")[2])\n",
      "140/14:\n",
      "string = 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ'\n",
      "\n",
      "print(string.partition(\"?mc=\")[2].rpartition('&')[0])\n",
      "140/15:\n",
      "string = 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ'\n",
      "\n",
      "print(string.partition(\"?mc=\")[2].rpartition('&')[0])\n",
      "140/16:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "print(string.partition(\"?mc=\")[2].rpartition('&')[0])\n",
      "140/17:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "string.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "140/18:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "string.partition(\"?mc=\")[2].rpartition('&')[0] == None\n",
      "140/19:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "print(string.partition(\"?mc=\")[2].rpartition('&')[0]) == None\n",
      "140/20:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "print(string.partition(\"?mc=\")[7])\n",
      "140/21:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "print(string.partition(\"?mc=\")[2].rpartition('&')[0]) == None\n",
      "140/22:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "print(string.partition(\"?mc=\")[2].rpartition('&')[0]) == None\n",
      "140/23:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2]\n",
      "140/24:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:6]\n",
      "140/25:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2]\n",
      "140/26:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:7]\n",
      "140/27:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][1:7]\n",
      "140/28:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:6]\n",
      "140/29:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0]\n",
      "140/30:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:1]\n",
      "140/31:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:2]\n",
      "140/32:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:7]\n",
      "140/33:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0]\n",
      "140/34:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][1]\n",
      "140/35:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][2]\n",
      "140/36:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][3]\n",
      "140/37:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][4]\n",
      "140/38:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:7]\n",
      "140/39:\n",
      "string = 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:7]\n",
      "140/40:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{product.find(\"a\").attrs[\"href\"]}'\n",
      "    prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/41: de1_list\n",
      "140/42:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    #prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0]) == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0])\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/43:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    #prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    \n",
      "    if print(link.partition(\"?mc=\")[2].rpartition('&')[0])) == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0])\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/44:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    #prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0]) == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0])\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/45:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    #prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0])\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/46:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    #prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/47: de1_list\n",
      "140/48:\n",
      "string = 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:7]\n",
      "140/49: string.replace('\\n',\" \")\n",
      "140/50: '8380104'.replace('\\n',\" \")\n",
      "140/51: '8380104\\n'.replace('\\n',\" \")\n",
      "140/52: '8380104\\n'.replace('\\n',\"\")\n",
      "140/53:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n',\"\")\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n',\"\")\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n',\"\")\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/54: de1_list\n",
      "140/55: '8380104\\n'.replace('\\n'&'*',\"\")\n",
      "140/56: '8380104\\n'.replace('\\n'|'*',\"\")\n",
      "140/57: '8380104\\n'.replace('\\n',\"\").replace('*',\"\")\n",
      "140/58: '8380104\\n**'.replace('\\n',\"\").replace('*',\"\")\n",
      "140/59:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n',\"\").replace('*',\"\")\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n',\"\").replace('*',\"\")\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n',\"\").replace('*',\"\")\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/60: de1_list\n",
      "140/61:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n',\"\").replace('*',\"\")\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n',\"\").replace('*',\"\")\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n',\"\").replace('*',\"\")\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n',\"\").replace('*',\"\")\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/62: de1_list\n",
      "140/63:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/64: de1_list\n",
      "143/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "143/2:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "143/3:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "de1\n",
      "143/4:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "143/5:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[2].find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "143/6:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "143/7: de1_list\n",
      "143/8: prod[0].find(\"a\").attrs[\"href\"]\n",
      "143/9:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        id1 = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        id1 = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "143/11:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        id1 = link.partition(\"?mc=\")[2]\n",
      "else:\n",
      "    id1 = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "143/12:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        id1 = link.partition(\"?mc=\")[2]\n",
      "else:\n",
      "    id1 = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "id1\n",
      "143/13:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        id1 = link.partition(\"?mc=\")[2]\n",
      "else:\n",
      "    id1 = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "143/14:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        id1 = link.partition(\"?mc=\")[2]\n",
      "else:\n",
      "    id1 = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "link\n",
      "143/15:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        id1 = link.partition(\"?mc=\")[2]\n",
      "else:\n",
      "    id1 = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "link.partition(\"?mc=\")[2].rpartition('&')[0] == None\n",
      "\n",
      "link\n",
      "143/16:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        id1 = link.partition(\"?mc=\")[2]\n",
      "else:\n",
      "    id1 = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "link.partition(\"?mc=\")[2].rpartition('&')[0] == None\n",
      "143/17:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "143/18:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "143/19:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "link.partition(\"?mc=\")[2]\n",
      "143/20:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if print(link.partition(\"?mc=\")[2].rpartition('&')[0]) == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "143/21:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if print(link.partition(\"?mc=\")[2].rpartition('&')[0]) == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "143/22: de1_list\n",
      "143/23:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "143/24: de1_list\n",
      "143/25:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"span\",{\"class\":\"svelte-1uqvrhu\"})\n",
      "143/26:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"span\",{\"class\":\"svelte-1uqvrhu\"})\n",
      "\n",
      "total_prod\n",
      "143/27:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"span\",{\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "total_prod\n",
      "143/28:\n",
      "#Getting total products in the category\n",
      "de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"})\n",
      "143/29:\n",
      "#Getting total products in the category\n",
      "de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "143/30:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "total_prod/40\n",
      "143/31:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "int(total_prod)/40\n",
      "143/32:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "# No of pages\n",
      "round(int(total_prod)/40)\n",
      "143/33:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "# No of pages\n",
      "math_ceil(int(total_prod)/40)\n",
      "143/34:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "143/35:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "# No of pages\n",
      "math_ceil(int(total_prod)/40)\n",
      "143/36:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "# No of pages\n",
      "math.ceil(int(total_prod)/40)\n",
      "143/37:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "# No of pages\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "math.ceil(1.2)\n",
      "143/38:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "# No of pages\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "math.ceil(1.1)\n",
      "143/39:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "# No of pages\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "math.ceil(1.0)\n",
      "143/40:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "[1:10]\n",
      "143/41:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "print([1:10])\n",
      "143/42:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "print(1:10)\n",
      "143/43:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in 1:10:\n",
      "        print(i)\n",
      "143/44:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in (1:10):\n",
      "        print(i)\n",
      "143/45:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1:10):\n",
      "        print(i)\n",
      "143/46:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1,10):\n",
      "        print(i)\n",
      "143/47:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1, math.ceil(int(total_prod)/40)):\n",
      "        print(i)\n",
      "143/48:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1, math.ceil(int(total_prod)/40)):\n",
      "        page = f'{page1}?from={40 * i}&size=40'\n",
      "        print(f'page{i + 1}: {page}')\n",
      "143/49:\n",
      "#Pagination\n",
      "page1 = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1, math.ceil(int(total_prod)/40)):\n",
      "        page = f'{page1}?from={40 * i}&size=40'\n",
      "        print(f'page{i + 1}: {page}')\n",
      "144/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "144/2:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "144/3:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "de1\n",
      "144/4:\n",
      "#Pagination\n",
      "page1 = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(0, math.ceil(int(total_prod)/40)):\n",
      "        page = f'{page1}?from={40 * i}&size=40'\n",
      "        print(f'page{i + 1}: {page}')\n",
      "144/5:\n",
      "#Pagination\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "url_list = [url]\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(0, math.ceil(int(total_prod)/40)):\n",
      "        url = f'{url}?from={40 * i}&size=40'\n",
      "        url_list.append(url)\n",
      "144/6:\n",
      "#Pagination\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "url_list = [url]\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(0, math.ceil(int(total_prod)/40)):\n",
      "        url = f'{url}?from={40 * i}&size=40'\n",
      "        url_list.append(url)\n",
      "        \n",
      "url_list\n",
      "144/7:\n",
      "#Pagination\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "url_list = [url]\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1, math.ceil(int(total_prod)/40)):\n",
      "        url = f'{url}?from={40 * i}&size=40'\n",
      "        url_list.append(url)\n",
      "        \n",
      "url_list\n",
      "144/8:\n",
      "#Pagination\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "url_list = [url]\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1, math.ceil(int(total_prod)/40)):\n",
      "        page = f'{url}?from={40 * i}&size=40'\n",
      "        url_list.append(page)\n",
      "        \n",
      "url_list\n",
      "144/9:\n",
      "#Pagination\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "url_list = [url]\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1, math.ceil(int(total_prod)/40)):\n",
      "        page = f'{url}?from={40 * i}&size=40'\n",
      "        url_list.append(page)\n",
      "144/10:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "145/1: %history -g\n",
      "145/2:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "145/3:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "145/4:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "145/5:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "145/6:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "145/7:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "145/8:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "145/9:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "145/10: de1_list\n",
      "144/11:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "144/12:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "144/13:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "144/14:\n",
      "#Pagination\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "url_list = [url]\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1, math.ceil(int(total_prod)/40)):\n",
      "        page = f'{url}?from={40 * i}&size=40'\n",
      "        url_list.append(page)\n",
      "144/15:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for url in url_list:\n",
      "    de1 = getAndParseURL(url)\n",
      "    prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "    for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "144/16:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for url in url_list:\n",
      "    de1 = getAndParseURL(url)\n",
      "    prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "    for product in prod:\n",
      "        link = product.find(\"a\").attrs[\"href\"]\n",
      "        prod_cat = de1.find(\"h1\").text\n",
      "        prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "        brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "        prod_url = f'{de_base}{link}'\n",
      "\n",
      "        #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "        #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "        if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "            prod_id = link.partition(\"?mc=\")[2]\n",
      "        else:\n",
      "            prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "        if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "            reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "            act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "        else:\n",
      "            reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "            act_price = None\n",
      "\n",
      "        if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "            prod_sticker = None\n",
      "        else:\n",
      "            prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "        de1_list.append({'title': prod_title,\n",
      "                         'sku': prod_id,\n",
      "                         'regular price': reg_price,\n",
      "                         'actual price' : act_price,\n",
      "                         'brand': brand_name,\n",
      "                         'url' : prod_url,\n",
      "                         'sticker' : prod_sticker,\n",
      "                         'category' : prod_cat})\n",
      "144/17: de1_list\n",
      "144/18: len(de1_list)\n",
      "144/19:\n",
      "len(de1_list)\n",
      "\n",
      "total_prod\n",
      "144/20:\n",
      "len(de1_list)\n",
      "\n",
      "total_prod\n",
      "144/21:\n",
      "len(de1_list)\n",
      "\n",
      "total_prod\n",
      "\n",
      "prod_cat\n",
      "144/22:\n",
      "#Saving into csv:\n",
      "\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import time\n",
      "\n",
      "fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "filename = f'{prod_cat}_{total_prod}_{fetch_time}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "    for item in de1_list:\n",
      "        writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "\n",
      "pd.read_csv(filename, delimiter= \";\")\n",
      "144/23:\n",
      "#Saving into csv:\n",
      "\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import time\n",
      "\n",
      "fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "filename = f'{prod_cat}_{total_prod}_{fetch_time}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "    for item in de1_list:\n",
      "        writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "\n",
      "pd.read_csv(filename, delimiter= \";\", 10)\n",
      "144/24:\n",
      "#Saving into csv:\n",
      "\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import time\n",
      "\n",
      "fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "filename = f'{prod_cat}_{total_prod}_{fetch_time}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "    for item in de1_list:\n",
      "        writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "\n",
      "pd.read_csv(filename, delimiter= \";\", nrows=10)\n",
      "144/25:\n",
      "#Saving into csv:\n",
      "\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import time\n",
      "\n",
      "fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "filename = f'{prod_cat}_{total_prod}_{fetch_time}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "    for item in de1_list:\n",
      "        writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "\n",
      "pd.read_csv(filename, delimiter= \";\", nrows=20)\n",
      "144/26:\n",
      "len(de1_list)\n",
      "\n",
      "total_prod\n",
      "\n",
      "de1_list[17]\n",
      "144/27:\n",
      "#Saving into csv:\n",
      "\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import time\n",
      "\n",
      "country = 'DE'\n",
      "fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "filename = f'{country}_{prod_cat}_{total_prod}_{fetch_time}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "    for item in de1_list:\n",
      "        writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "\n",
      "pd.read_csv(filename, delimiter= \";\", nrows=20)\n",
      "144/28:\n",
      "#Saving into csv:\n",
      "\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import time\n",
      "\n",
      "country = 'DE'\n",
      "fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "filename = f'{country}_{prod_cat}_{total_prod}_{fetch_time}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "    for item in de1_list:\n",
      "        writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "\n",
      "pd.read_csv(filename, delimiter= \",\", nrows=20)\n",
      "144/29:\n",
      "#Saving into csv:\n",
      "\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import time\n",
      "\n",
      "country = 'DE'\n",
      "fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "filename = f'{country}_{prod_cat}_{total_prod}_{fetch_time}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "    for item in de1_list:\n",
      "        writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "\n",
      "pd.read_csv(filename, delimiter= \",\", nrows=20)\n",
      "147/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "147/2:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "147/3:\n",
      "# Function to get main data\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "\n",
      "def getDecathlonData(country_url, url_list):\n",
      "    prod_list = []\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. abse)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat})\n",
      "            \n",
      "    print(f'{len(prod_list)} products in the {prod_cat} category have been saved!)\n",
      "        \n",
      "    return prod_list\n",
      "147/4:\n",
      "# Function to get main data\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "\n",
      "def getDecathlonData(country_url, url_list):\n",
      "    prod_list = []\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. abse)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat})\n",
      "            \n",
      "    print(f'{len(prod_list)} products in the {prod_cat} category have been saved!')\n",
      "        \n",
      "    return prod_list\n",
      "147/5: datetime.date()\n",
      "147/6:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "import csv\n",
      "#import time\n",
      "147/7: datetime.date()\n",
      "147/8:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "#from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import time\n",
      "147/9: datetime.date()\n",
      "147/10: datetime.today()\n",
      "147/11: datetime.today().date\n",
      "147/12: datetime.date(datetime.today())\n",
      "147/13:\n",
      "datetime.date(datetime.today())\n",
      "\n",
      "fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "147/14:\n",
      "fetch_date=f'{datetime.date(datetime.today())}'.replace(\":\",\"_\")\n",
      "fetch_date\n",
      "147/15:\n",
      "fetch_date=f'{datetime.date(datetime.today())}'\n",
      "fetch_date\n",
      "147/16:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "#from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import time\n",
      "147/17:\n",
      "#Function for parsing the URLs\n",
      "def cookSoup(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "147/18:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(cat_url, soup):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "    #No of pages (rounding up all numbers) = math.ceil(int(total_prod)/40)\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/40\n",
      "    for i in range(1, total_page)):\n",
      "            page = f'{url}?from={40 * i}&size=40'\n",
      "            url_list.append(page)\n",
      "    print (f'There are {total_prod} products ({total_page} pages) in the category')\n",
      "    return url_list\n",
      "147/19:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(cat_url, soup):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "    #No of pages (rounding up all numbers) = math.ceil(int(total_prod)/40)\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/40\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{url}?from={40 * i}&size=40'\n",
      "            url_list.append(page)\n",
      "    print (f'There are {total_prod} products ({total_page} pages) in the category')\n",
      "    return url_list\n",
      "147/20:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(cat_url, soup):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "    #No of pages (rounding up all numbers) = math.ceil(int(total_prod)/40)\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/40)\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{url}?from={40 * i}&size=40'\n",
      "            url_list.append(page)\n",
      "    print (f'There are {total_prod} products ({total_page} pages) in the category')\n",
      "    return url_list\n",
      "147/21:\n",
      "# Function to get main data\n",
      "def getDecathlonData(country_url, url_list):\n",
      "    prod_list = []\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. abse)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat})\n",
      "            \n",
      "    print(f'{len(prod_list)} products in the {prod_cat} category have been saved!')\n",
      "        \n",
      "    return prod_list\n",
      "147/22:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, prod_list)\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{prod_cat}_{total_prod}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{filename} saved')\n",
      "147/23:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{prod_cat}_{total_prod}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{filename} saved')\n",
      "147/24:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(cat_url, soup)\n",
      "prod_list = getDecathlonData(country_url, url_list)\n",
      "saveDecathlonData(country, prod_list)\n",
      "147/25:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(cat_url, soup):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "    #No of pages (rounding up all numbers) = math.ceil(int(total_prod)/40)\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/40)\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{cat_url}?from={40 * i}&size=40'\n",
      "            url_list.append(page)\n",
      "    print (f'There are {total_prod} products ({total_page} pages) in the category')\n",
      "    return url_list\n",
      "147/26:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(cat_url, soup)\n",
      "prod_list = getDecathlonData(country_url, url_list)\n",
      "saveDecathlonData(country, prod_list)\n",
      "147/27:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(cat_url, soup)\n",
      "prod_list = getDecathlonData(country_url, url_list)\n",
      "\n",
      "filename = f'{country}_{prod_cat}_{total_prod}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "filename\n",
      "#saveDecathlonData(country, prod_list)\n",
      "147/28:\n",
      "# Function to get main data\n",
      "def getDecathlonData(country_url, url_list):\n",
      "    prod_list = []\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. abse)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat})\n",
      "            \n",
      "    print(f'{len(prod_list)} products in the {prod_cat} category have been saved!')\n",
      "        \n",
      "    return prod_list, prod_cat\n",
      "147/29:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(cat_url, soup)\n",
      "prod_list = getDecathlonData(country_url, url_list)\n",
      "saveDecathlonData(country, prod_list, prod_cat)\n",
      "\n",
      "#saveDecathlonData(country, prod_list)\n",
      "147/30:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, prod_list, prod_cat):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{prod_cat}_{total_prod}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{filename} saved')\n",
      "147/31:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(cat_url, soup)\n",
      "prod_list = getDecathlonData(country_url, url_list)\n",
      "saveDecathlonData(country, prod_list, prod_cat)\n",
      "\n",
      "#saveDecathlonData(country, prod_list)\n",
      "147/32:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "cat = 'Sportbekleidung Damen'\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(cat_url, soup)\n",
      "prod_list = getDecathlonData(country_url, url_list)\n",
      "saveDecathlonData(country, prod_list)\n",
      "147/33:\n",
      "# Function to get main data\n",
      "def getDecathlonData(country_url, url_list):\n",
      "    prod_list = []\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. abse)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat})\n",
      "            \n",
      "    print(f'{len(prod_list)} products in the {prod_cat} category have been saved!')\n",
      "        \n",
      "    return prod_list\n",
      "147/34:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{cat}_{total_prod}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{filename} saved')\n",
      "147/35:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "cat = 'Sportbekleidung Damen'\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(cat_url, soup)\n",
      "prod_list = getDecathlonData(country_url, url_list)\n",
      "saveDecathlonData(country, prod_list)\n",
      "147/36:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{cat}_{total_prod}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{filename} saved')\n",
      "147/37:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "prod_cat = 'Sportbekleidung Damen'\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(cat_url, soup)\n",
      "prod_list = getDecathlonData(country_url, url_list)\n",
      "saveDecathlonData(country, prod_list)\n",
      "147/38:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "cat = 'Sportbekleidung Damen'\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(cat_url, soup)\n",
      "prod_list = getDecathlonData(country_url, url_list)\n",
      "saveDecathlonData(country, prod_list)\n",
      "147/39:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, prod_list, total_prod):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{cat}_{total_prod}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{filename} saved')\n",
      "147/40:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "cat = 'Sportbekleidung Damen'\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(cat_url, soup)\n",
      "prod_list = getDecathlonData(country_url, url_list)\n",
      "saveDecathlonData(country, prod_list, total_prod)\n",
      "147/41:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{cat}_{len(prod_list)}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{filename} saved')\n",
      "147/42:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, cat, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{cat}_{len(prod_list)}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{filename} saved')\n",
      "147/43:\n",
      "# Function to get main data\n",
      "def getDecathlonData(country_url, url_list):\n",
      "    prod_list = []\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. abse)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat})\n",
      "            \n",
      "    print(f'{len(prod_list)} products in the {prod_cat} category have been saved!')\n",
      "        \n",
      "    return prod_list\n",
      "147/44:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, cat, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{cat}_{len(prod_list)}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{filename} saved')\n",
      "147/45:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "cat = 'Sportbekleidung Damen'\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(cat_url, soup)\n",
      "prod_list = getDecathlonData(country_url, url_list)\n",
      "saveDecathlonData(country, cat, prod_list)\n",
      "147/46:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "cat = 'Sportbekleidung Damen'\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(soup, cat_url, country, cat)\n",
      "prod_list = getDecathlonData(country_url, url_list, country, cat)\n",
      "saveDecathlonData(country, cat, prod_list)\n",
      "147/47:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "#from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import time\n",
      "147/48:\n",
      "#Function for parsing the URLs\n",
      "def cookSoup(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "147/49:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(soup, cat_url, country, cat):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "    #No of pages (rounding up all numbers) = math.ceil(int(total_prod)/40)\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/40)\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{cat_url}?from={40 * i}&size=40'\n",
      "            url_list.append(page)\n",
      "    print (f'{country}_{cat}: There are {total_prod} products ({total_page} pages) in the category')\n",
      "    return url_list\n",
      "147/50:\n",
      "# Function to get main data\n",
      "def getDecathlonData(country_url, url_list, country, cat):\n",
      "    prod_list = []\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. abse)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat})\n",
      "            \n",
      "    print(f'{country}_{cat}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      "147/51:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, cat, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{cat}_{len(prod_list)}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{country}_{cat}: Data has been saved in {filename}')\n",
      "147/52:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "cat = 'Sportbekleidung Damen'\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(soup, cat_url, country, cat)\n",
      "prod_list = getDecathlonData(country_url, url_list, country, cat)\n",
      "saveDecathlonData(country, cat, prod_list)\n",
      "147/53:\n",
      "# Importing country - category data\n",
      "de = pd.read_csv('cat_list.csv', header=None, index_col=0, squeeze=True).to_dict()\n",
      "\n",
      "print(de)\n",
      "147/54:\n",
      "# Importing country - category data\n",
      "de = pd.read_csv('cat_list.csv', header= TRUE, index_col=0, squeeze=True).to_dict()\n",
      "147/55:\n",
      "# Importing country - category data\n",
      "de = pd.read_csv('cat_list.csv', header=True, index_col=0, squeeze=True).to_dict()\n",
      "147/56:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open(cat_list.csv, 'r') as data:\n",
      "    for line in csv.DictReader(data):\n",
      "    print(line)\n",
      "147/57:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open(cat_list.csv, 'r') as data:\n",
      "    for line in csv.DictReader(data):\n",
      "        print(line)\n",
      "147/58:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as data:\n",
      "    for line in csv.DictReader(data):\n",
      "        print(line)\n",
      "147/59:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as data:\n",
      "    for line in csv.DictReader(data):\n",
      "        print(line)\n",
      "\n",
      "line\n",
      "147/60:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as data:\n",
      "    for line in csv.DictReader(data):\n",
      "        print(line)\n",
      "\n",
      "type(line)\n",
      "147/61:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as data:\n",
      "    for line in csv.DictReader(data):\n",
      "        print(line)\n",
      "\n",
      "types(line)\n",
      "147/62:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as data:\n",
      "    for line in csv.DictReader(data):\n",
      "        print(line)\n",
      "\n",
      "len(line)\n",
      "147/63:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as data:\n",
      "    for line in csv.DictReader(data):\n",
      "        print(line)\n",
      "\n",
      "type(line)\n",
      "147/64:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as data:\n",
      "    for line in csv.DictReader(data):\n",
      "        print(line)\n",
      "\n",
      "len(line)\n",
      "147/65:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        print(cat)\n",
      "147/66:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        print(cat)\n",
      "147/67: cat\n",
      "147/68: cat_data\n",
      "147/69:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "cat_list\n",
      "147/70: len(cat_list)\n",
      "147/71:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "cat_list\n",
      "147/72:\n",
      "for category in cat_list:\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat)\n",
      "    saveDecathlonData(country, cat, prod_list)\n",
      "147/73:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "import csv\n",
      "import time\n",
      "147/74:\n",
      "#Function for parsing the URLs\n",
      "def cookSoup(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "147/75:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(soup, cat_url, country, cat):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "    #No of pages (rounding up all numbers) = math.ceil(int(total_prod)/40)\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/40)\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{cat_url}?from={40 * i}&size=40'\n",
      "            url_list.append(page)\n",
      "    print (f'{country}_{cat}: There are {total_prod} products ({total_page} pages) in the category')\n",
      "    return url_list\n",
      "147/76:\n",
      "# Function to get main data\n",
      "def getDecathlonData(country_url, url_list, country, cat):\n",
      "    prod_list = []\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. abse)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat})\n",
      "            \n",
      "    print(f'{country}_{cat}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      "147/77:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, cat, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{cat}_{len(prod_list)}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{country}_{cat}: Data has been saved in {filename}')\n",
      "147/78:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "cat_list\n",
      "147/79:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "147/80:\n",
      "for category in cat_list:\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat)\n",
      "    saveDecathlonData(country, cat, prod_list)\n",
      "147/81:\n",
      "for category in cat_list:\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat)\n",
      "    saveDecathlonData(country, cat, prod_list)\n",
      "147/82: cat_list[1]\n",
      "147/83: cat_list[1].country\n",
      "147/84: cat_list[1](country)\n",
      "147/85: cat_list[1]['country']\n",
      "147/86: cat_list[1]\n",
      "147/87:\n",
      "for category in cat_list:\n",
      "    country = category['country']\n",
      "    country_url = category['country url']\n",
      "    cat = category['cat']\n",
      "    cat_url = category['cat_url']\n",
      "        soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat)\n",
      "    saveDecathlonData(country, cat, prod_list)\n",
      "147/88:\n",
      "for category in cat_list:\n",
      "    country = category['country']\n",
      "    country_url = category['country url']\n",
      "    cat = category['cat']\n",
      "    cat_url = category['cat_url']\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat)\n",
      "    saveDecathlonData(country, cat, prod_list)\n",
      "147/89:\n",
      "for category in cat_list:\n",
      "    country = category['country']\n",
      "    country_url = category['country_url']\n",
      "    cat = category['cat']\n",
      "    cat_url = category['cat_url']\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat)\n",
      "    saveDecathlonData(country, cat, prod_list)\n",
      "147/90:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, cat, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{cat}_{len(prod_list)}_{fetch_date}.csv'.replace(\" \",\"_\").replace(\",\",\"\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{country}_{cat}: Data has been saved in {filename}')\n",
      "148/1:\n",
      "# Opening the html file\n",
      "HTMLFile = open(\"Factiva.html\", \"r\")\n",
      "  \n",
      "# Reading the file\n",
      "index = HTMLFile.read()\n",
      "  \n",
      "# Creating a BeautifulSoup object and specifying the parser\n",
      "soup = bts(index, 'lxml')\n",
      "148/2:\n",
      "# Opening the html file\n",
      "HTMLFile = open(\"amz.html\", \"r\")\n",
      "  \n",
      "# Reading the file\n",
      "index = HTMLFile.read()\n",
      "  \n",
      "# Creating a BeautifulSoup object and specifying the parser\n",
      "soup = bts(index, 'lxml')\n",
      "148/3:\n",
      "# Opening the html file\n",
      "HTMLFile = open(\"amz.html\", \"r\", encoding='Latin1')\n",
      "  \n",
      "# Reading the file\n",
      "index = HTMLFile.read()\n",
      "  \n",
      "# Creating a BeautifulSoup object and specifying the parser\n",
      "soup = bts(index, 'lxml')\n",
      "148/4:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "148/5:\n",
      "# Opening the html file\n",
      "HTMLFile = open(\"amz.html\", \"r\", encoding='Latin1')\n",
      "  \n",
      "# Reading the file\n",
      "index = HTMLFile.read()\n",
      "  \n",
      "# Creating a BeautifulSoup object and specifying the parser\n",
      "soup = bts(index, 'lxml')\n",
      "148/6:\n",
      "# Opening the html file\n",
      "HTMLFile = open(\"amz.html\", \"r\", encoding='Latin1')\n",
      "  \n",
      "# Reading the file\n",
      "index = HTMLFile.read()\n",
      "  \n",
      "# Creating a BeautifulSoup object and specifying the parser\n",
      "soup = bts(index, 'lxml')\n",
      "\n",
      "soup\n",
      "148/7:\n",
      "# Opening the html file\n",
      "HTMLFile = open(\"amz.html\", \"r\", encoding='Latin1')\n",
      "  \n",
      "# Reading the file\n",
      "index = HTMLFile.read()\n",
      "  \n",
      "# Creating a BeautifulSoup object and specifying the parser\n",
      "soup = bts(index, 'lxml')\n",
      "148/8: factiva.find_all(class_=\"clsSplitter\")\n",
      "148/9:\n",
      "# Opening the html file\n",
      "HTMLFile = open(\"amz.html\", \"r\", encoding='Latin1')\n",
      "  \n",
      "# Reading the file\n",
      "index = HTMLFile.read()\n",
      "  \n",
      "# Creating a BeautifulSoup object and specifying the parser\n",
      "factiva = bts(index, 'lxml')\n",
      "148/10: factiva.find_all(class_=\"clsSplitter\")\n",
      "148/11:\n",
      "art <- factiva.find_all(class_=\"clsSplitter\")\n",
      "len(art)\n",
      "148/12:\n",
      "art <- factiva.find_all(class_=\"clsSplitter\")\n",
      "len(art)\n",
      "148/13:\n",
      "art = factiva.find_all(class_=\"clsSplitter\")\n",
      "len(art)\n",
      "148/14:\n",
      "art = factiva.find_all(class_=\"headline\")\n",
      "len(art)\n",
      "148/15:\n",
      "art = factiva.find(\"tbody\")\n",
      "len(art)\n",
      "148/16:\n",
      "art = factiva.find_all(\"tbody\")\n",
      "len(art)\n",
      "148/17:\n",
      "art = factiva.find(\"tbody\")\n",
      "len(art)\n",
      "148/18:\n",
      "art = factiva.find(\"tbody\")\n",
      "art\n",
      "148/19: art = factiva.find(\"tbody\")\n",
      "148/20: art = factiva.find(\"div\",{\"class\":\"headlines\"})\n",
      "148/21:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\"})\n",
      "len(art)\n",
      "148/22:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "len(art)\n",
      "148/23:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "art\n",
      "148/24:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", style:\"height: 416px;\"})\n",
      "art\n",
      "148/25:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", 'style':\"height: 416px;\"})\n",
      "art\n",
      "148/26:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", 'style':\"height: 416px;\"})\n",
      "len(art)\n",
      "148/27: art = factiva.find(\"div\",{\"class\":\"headlines\", \"style\":\"height: 416px;\"})\n",
      "148/28:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"style\":\"height: 416px;\"})\n",
      "art\n",
      "148/29:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"style\":\"height: 416px\"})\n",
      "art\n",
      "148/30:\n",
      "art = factiva.find_all(\"div\",{\"class\":\"headlines\", \"style\":\"height: 416px\"})\n",
      "art\n",
      "148/31:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "len(art)\n",
      "148/32:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "art[1]\n",
      "148/33:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "art[0]\n",
      "148/34:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "art\n",
      "148/35:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "art.find_all(\"tr\", {\"class\":'headline'})\n",
      "148/36:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "len(art.find_all(\"tr\", {\"class\":'headline'}))\n",
      "148/37:\n",
      "for article in art:\n",
      "    if article.find('div', {'class':'dedupHeadlines'}) != None:\n",
      "        art.remove(article)\n",
      "    else:\n",
      "        print(len(art))\n",
      "148/38:\n",
      "for article in art:\n",
      "    if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "        print(len(art))\n",
      "    else:\n",
      "        art.remove(article)\n",
      "148/39:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "len(art.find_all(\"tr\", {\"class\":'headline'}))\n",
      "\n",
      "art[1]\n",
      "148/40:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "len(art.find_all(\"tr\", {\"class\":'headline'}))\n",
      "\n",
      "art\n",
      "148/41:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "len(art.find_all(\"tr\", {\"class\":'headline'}))\n",
      "148/42:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "articles = art.find_all(\"tr\", {\"class\":'headline'})\n",
      "\n",
      "articles\n",
      "148/43:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "        print(len(art))\n",
      "    else:\n",
      "        art.remove(article)\n",
      "148/44:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "        print(len(art))\n",
      "    else:\n",
      "        article.remove(article)\n",
      "148/45:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "        print(len(art))\n",
      "    else:\n",
      "        articles.remove(article)\n",
      "148/46:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "        print(len(articles))\n",
      "    else:\n",
      "        articles.remove(article)\n",
      "148/47:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "articles = art.find_all(\"tr\", {\"class\":'headline'})\n",
      "\n",
      "len(articles)\n",
      "148/48:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "        print(len(articles))\n",
      "    else:\n",
      "        articles.remove(article)\n",
      "\n",
      "len(articles)\n",
      "148/49:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHeadlines'}) != None:\n",
      "        articles.remove(article)\n",
      "    else: \n",
      "        print(len(articles))\n",
      "148/50:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHeadlines'}) != None:\n",
      "        articles.remove(article)\n",
      "    else: \n",
      "        print(len(articles))\n",
      "\n",
      "len(articles)\n",
      "148/51:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "articles = art.find_all(\"tr\", {\"class\":'headline'})\n",
      "\n",
      "len(articles)\n",
      "148/52:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHeadlines'}) != None:\n",
      "        articles.remove(article)\n",
      "    else: \n",
      "        print(len(articles))\n",
      "\n",
      "len(articles)\n",
      "148/53: articles\n",
      "148/54: articles[1]\n",
      "148/55: articles[1].find(\"tr\", {\"class\":\"headline\"})\n",
      "148/56: articles[1]\n",
      "148/57: articles[1].find(\"a\").attrs[\"href\"]\n",
      "148/58: articles[1].find(\"a\").text\n",
      "148/59:\n",
      "#title\n",
      "for article in articles:\n",
      "    print(article.find(\"a\").text)\n",
      "148/60:\n",
      "#title\n",
      "for article in articles:\n",
      "    print(article: article.find(\"a\").text)\n",
      "148/61:\n",
      "#title\n",
      "for article in articles:\n",
      "    print(f'{article}. {article.find(\"a\").text'})\n",
      "148/62:\n",
      "#title\n",
      "for article in articles:\n",
      "    print(f'{article}. {article.find(\"a\").text}')\n",
      "148/63:\n",
      "#title\n",
      "for article in articles:\n",
      "    i=1\n",
      "    print(f'{i}. {article.find(\"a\").text}')\n",
      "    i = i+1\n",
      "148/64:\n",
      "#title\n",
      "i=1\n",
      "for article in articles:\n",
      "    print(f'{i}. {article.find(\"a\").text}')\n",
      "    i = i+1\n",
      "148/65:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "articles = art.find_all(\"tr\", {\"class\":'headline'})\n",
      "\n",
      "len(articles)\n",
      "148/66:\n",
      "#title\n",
      "i=1\n",
      "for article in articles:\n",
      "    print(f'{i}. {article.find(\"a\").text}')\n",
      "    i = i+1\n",
      "148/67:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHd'}) != None:\n",
      "        articles.remove(article)\n",
      "    else: \n",
      "        print(len(articles))\n",
      "\n",
      "len(articles)\n",
      "148/68:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHd'}) == None:\n",
      "        articles.remove(article)\n",
      "    else: \n",
      "        print(len(articles))\n",
      "\n",
      "len(articles)\n",
      "148/69:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHd'}) != None:\n",
      "        articles.remove(article)\n",
      "    else: \n",
      "        print(len(articles))\n",
      "\n",
      "len(articles)\n",
      "148/70:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "articles = art.find_all(\"tr\", {\"class\":'headline'})\n",
      "\n",
      "len(articles)\n",
      "148/71:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHd'}) != None:\n",
      "        articles.remove(article)\n",
      "    else: \n",
      "        print(len(articles))\n",
      "\n",
      "len(articles)\n",
      "148/72:\n",
      "#title\n",
      "i=1\n",
      "for article in articles:\n",
      "    print(f'{i}. {article.find(\"a\").text}')\n",
      "    i = i+1\n",
      "148/73: articles[0].find(\"td\")\n",
      "148/74: articles[0].find_all(\"td\")\n",
      "148/75: articles[0].find_all(\"td\")[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148/76: articles[0].find_all(\"td\")[2]\n",
      "148/77: articles[0].find_all(\"td\")[2].text\n",
      "148/78: articles[0].find_all(\"td\")[2]\n",
      "148/79: articles[0].find_all(\"td\")[2].find_all(\"a\")\n",
      "148/80: articles[0].find_all(\"td\")[2]\n",
      "148/81: articles[0].find_all(\"td\")[2].find(\"div\")\n",
      "148/82: articles[0].find_all(\"td\")[2].find(\"div\").text\n",
      "148/83: articles[0].find_all(\"td\")[2].find(\"div\").text\n",
      "148/84: articles[0].find_all(\"td\")[1].find(\"div\").text\n",
      "148/85: articles[0].find_all(\"td\")[2].find(\"div\").text\n",
      "148/86: articles[0].find_all(\"td\")\n",
      "148/87: articles[0]\n",
      "148/88: articles[0].find('td')\n",
      "148/89: articles[0].find('tr')\n",
      "148/90: articles[0]\n",
      "148/91:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "articles = art.find_all(\"tr\", {\"class\":'headline'})\n",
      "\n",
      "len(articles)\n",
      "148/92:\n",
      "#title\n",
      "amz =[]\n",
      "for article in articles:\n",
      "    title = article.find(\"a\").text\n",
      "    date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "148/93: amz\n",
      "148/94: articles[0].find(\"a\").text\n",
      "148/95: articles[0]\n",
      "148/96: articles[1].\n",
      "148/97: articles[1]\n",
      "148/98: articles[0]\n",
      "148/99: articles[1]\n",
      "148/100: articles[0:1]\n",
      "148/101: articles[0:2]\n",
      "148/102: articles[0]\n",
      "148/103: articles[0].find('td', {'class':'count'})\n",
      "148/104:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "articles = art.find_all(\"td\", {\"class\":'count'})\n",
      "\n",
      "len(articles)\n",
      "148/105: articles[0]\n",
      "148/106:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "articles = art.find_all(\"tr\", {\"class\":'headline'})\n",
      "\n",
      "len(articles)\n",
      "148/107: articles[0]\n",
      "148/108: articles[0]\n",
      "148/109: articles[1]\n",
      "148/110: len(articles)\n",
      "148/111: articles[0]\n",
      "148/112: articles[0].find(\"a\").text\n",
      "148/113: articles[0].find_all(\"td\")[2].find(\"div\").text\n",
      "148/114: articles[0]\n",
      "148/115: articles[0].find('div', {'class':'dedupHeadlines'}).text\n",
      "148/116:\n",
      "#title\n",
      "factiva_list =[]\n",
      "for article in articles:\n",
      "    title = article.find(\"a\").text\n",
      "    date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "    \n",
      "    if articles.find('div', {'class':'dedupHeadlines'}).text == None:\n",
      "        dup = None\n",
      "    else:\n",
      "        dup = articles.find('div', {'class':'dedupHeadlines'}).text\n",
      "        \n",
      "    factiva_list.append({'title': title,\n",
      "                 'date': date,\n",
      "                 'dup': dup})\n",
      "148/117:\n",
      "#title\n",
      "factiva_list =[]\n",
      "for article in articles:\n",
      "    title = article.find(\"a\").text\n",
      "    date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "    \n",
      "    if articles.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "        dup = None\n",
      "    else:\n",
      "        dup = articles.find('div', {'class':'dedupHeadlines'}).text\n",
      "        \n",
      "    factiva_list.append({'title': title,\n",
      "                 'date': date,\n",
      "                 'dup': dup})\n",
      "148/118:\n",
      "#title\n",
      "factiva_list =[]\n",
      "for article in articles:\n",
      "    title = article.find(\"a\").text\n",
      "    date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "    \n",
      "    if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "        dup = None\n",
      "    else:\n",
      "        dup = articles.find('div', {'class':'dedupHeadlines'}).text\n",
      "        \n",
      "    factiva_list.append({'title': title,\n",
      "                 'date': date,\n",
      "                 'dup': dup})\n",
      "148/119:\n",
      "#title\n",
      "factiva_list =[]\n",
      "for article in articles:\n",
      "    title = article.find(\"a\").text\n",
      "    date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "    \n",
      "    if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "        dup = None\n",
      "    else:\n",
      "        dup = article.find('div', {'class':'dedupHeadlines'}).text\n",
      "        \n",
      "    factiva_list.append({'title': title,\n",
      "                 'date': date,\n",
      "                 'dup': dup})\n",
      "148/120: factiva_list\n",
      "148/121: len(factiva_list)\n",
      "148/122:\n",
      "comp = 'amz'\n",
      "fetch_date=f'{datetime.date(datetime.today())}'\n",
      "filename = f'{comp}_{len(art_list)}_{fetch_date}.csv'\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"title\", \"date\", \"dup\"])\n",
      "\n",
      "    for item in art_list:\n",
      "        writer.writerow([item['title'], item['date'], item['dup'])\n",
      "                         \n",
      "return print(f'Articles for {comp} has been saved in {filename}')\n",
      "148/123:\n",
      "comp = 'amz'\n",
      "fetch_date=f'{datetime.date(datetime.today())}'\n",
      "filename = f'{comp}_{len(art_list)}_{fetch_date}.csv'\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"title\", \"date\", \"dup\"])\n",
      "\n",
      "    for item in art_list:\n",
      "        writer.writerow([item['title'], item['date'], item['dup'])\n",
      "                         \n",
      "print(f'Articles for {comp} has been saved in {filename}')\n",
      "148/124:\n",
      "comp = 'amz'\n",
      "fetch_date=f'{datetime.date(datetime.today())}'\n",
      "filename = f'{comp}_{len(art_list)}_{fetch_date}.csv'\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"title\", \"date\", \"dup\"])\n",
      "\n",
      "    for item in art_list:\n",
      "        writer.writerow([item['title'], item['date'], item['dup']])\n",
      "                         \n",
      "print(f'Articles for {comp} has been saved in {filename}')\n",
      "148/125:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "#from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import time\n",
      "148/126:\n",
      "comp = 'amz'\n",
      "fetch_date=f'{datetime.date(datetime.today())}'\n",
      "filename = f'{comp}_{len(art_list)}_{fetch_date}.csv'\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"title\", \"date\", \"dup\"])\n",
      "\n",
      "    for item in art_list:\n",
      "        writer.writerow([item['title'], item['date'], item['dup']])\n",
      "                         \n",
      "print(f'Articles for {comp} has been saved in {filename}')\n",
      "148/127:\n",
      "#title\n",
      "art_list =[]\n",
      "\n",
      "for article in articles:\n",
      "    title = article.find(\"a\").text\n",
      "    date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "    \n",
      "    #identify duplicates\n",
      "    if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "        dup = None\n",
      "    else:\n",
      "        dup = article.find('div', {'class':'dedupHeadlines'}).text\n",
      "        \n",
      "    art_list.append({'title': title,\n",
      "                 'date': date,\n",
      "                 'dup': dup})\n",
      "148/128:\n",
      "comp = 'amz'\n",
      "fetch_date=f'{datetime.date(datetime.today())}'\n",
      "filename = f'{comp}_{len(art_list)}_{fetch_date}.csv'\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"title\", \"date\", \"dup\"])\n",
      "\n",
      "    for item in art_list:\n",
      "        writer.writerow([item['title'], item['date'], item['dup']])\n",
      "                         \n",
      "print(f'Articles for {comp} has been saved in {filename}')\n",
      "152/1:\n",
      "# Importing company list\n",
      "comp_list = []  \n",
      "\n",
      "# opening the file using \"with\" statement\n",
      "with open('comp-usa.csv', 'r') as comp_data:\n",
      "    for comp in csv.DictReader(comp_data):\n",
      "        comp_list.append(comp)\n",
      "        \n",
      "comp_list\n",
      "152/2:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "import csv\n",
      "import time\n",
      "152/3:\n",
      "# Importing company list\n",
      "comp_list = []  \n",
      "\n",
      "# opening the file using \"with\" statement\n",
      "with open('comp-usa.csv', 'r') as comp_data:\n",
      "    for comp in csv.DictReader(comp_data):\n",
      "        comp_list.append(comp)\n",
      "        \n",
      "comp_list\n",
      "152/4:\n",
      "# Importing company list\n",
      "comp_list = []  \n",
      "\n",
      "# opening the file using \"with\" statement\n",
      "with open('comp-usa.csv', 'r') as comp_data:\n",
      "    for comp in csv.DictReader(comp_data):\n",
      "        comp_list.append(comp)\n",
      "len(comp_list)\n",
      "152/5:\n",
      "# Importing company list\n",
      "comp_list = []  \n",
      "\n",
      "# opening the file using \"with\" statement\n",
      "with open('comp-usa.csv', 'r') as comp_data:\n",
      "    for comp in csv.DictReader(comp_data):\n",
      "        comp_list.append(comp)\n",
      "\n",
      "len(comp_list)\n",
      "152/6:\n",
      "def getArticles(filename, comp):\n",
      "\n",
      "    # Opening the html file\n",
      "    HTMLFile = open(filename, \"r\", encoding='Latin1')\n",
      "\n",
      "    # Reading the file\n",
      "    index = HTMLFile.read()\n",
      "\n",
      "    # Creating a BeautifulSoup object and specifying the parser\n",
      "    factiva = bts(index, 'lxml')\n",
      "    \n",
      "    # Get articles from soup\n",
      "    art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "    articles = art.find_all(\"tr\", {\"class\":'headline'})\n",
      "    \n",
      "    print(f'{comp}: {len(articles)} articles have been scraped')\n",
      "\n",
      "    return articles\n",
      "152/7:\n",
      "# Function to get all articles of a company:\n",
      "\n",
      "def getArticles(filename, comp):\n",
      "\n",
      "    # Opening the html file\n",
      "    HTMLFile = open(filename, \"r\", encoding='Latin1')\n",
      "\n",
      "    # Reading the file\n",
      "    index = HTMLFile.read()\n",
      "\n",
      "    # Creating a BeautifulSoup object and specifying the parser\n",
      "    factiva = bts(index, 'lxml')\n",
      "    \n",
      "    # Get articles from soup\n",
      "    art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "    articles = art.find_all(\"tr\", {\"class\":'headline'})\n",
      "    \n",
      "    print(f'{comp}: {len(articles)} articles have been scraped')\n",
      "\n",
      "    return articles\n",
      "152/8: comp_list[0]\n",
      "152/9:\n",
      "filename = 'AMZN.html'\n",
      "comp = 'AMZN'\n",
      "\n",
      "articles = getArticles(filename, comp)\n",
      "152/10:\n",
      "filename = 'AMZN.html'\n",
      "comp = 'AMZN'\n",
      "\n",
      "articles = getArticles(filename, comp)\n",
      "152/11:\n",
      "# Function saving articles into dictionary\n",
      "def saveArticles(articles, comp):\n",
      "    art_list =[]\n",
      "    \n",
      "    for article in articles:\n",
      "        title = article.find(\"a\").text\n",
      "        date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "\n",
      "        #identify duplicates\n",
      "        if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "            dup = None\n",
      "        else:\n",
      "            dup = article.find('div', {'class':'dedupHeadlines'}).text\n",
      "\n",
      "        art_list.append({'comp': comp,\n",
      "                         'title': title,\n",
      "                         'date': date,\n",
      "                         'dup': dup})\n",
      "152/12:\n",
      "# Function saving articles into dictionary\n",
      "def saveArticles(articles, comp):\n",
      "    art_list =[]\n",
      "    \n",
      "    for article in articles:\n",
      "        title = article.find(\"a\").text\n",
      "        date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "\n",
      "        #identify duplicates\n",
      "        if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "            dup = None\n",
      "        else:\n",
      "            dup = article.find('div', {'class':'dedupHeadlines'}).text\n",
      "\n",
      "        art_list.append({'comp': comp,\n",
      "                         'title': title,\n",
      "                         'date': date,\n",
      "                         'dup': dup})\n",
      "    return art_list\n",
      "152/13:\n",
      "# Function saving articles into dictionary\n",
      "def saveArticles(articles, comp):\n",
      "    art_list =[]\n",
      "    \n",
      "    for article in articles:\n",
      "        title = article.find(\"a\").text\n",
      "        date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "\n",
      "        #identify duplicates\n",
      "        if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "            dup = None\n",
      "        else:\n",
      "            dup = article.find('div', {'class':'dedupHeadlines'}).text\n",
      "\n",
      "        art_list.append({'comp': comp,\n",
      "                         'title': title,\n",
      "                         'date': date,\n",
      "                         'dup': dup})\n",
      "    return art_list\n",
      "152/14:\n",
      "def saveFactiva (art_list, comp):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{comp}_{len(art_list)}_{fetch_date}.csv'\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"date\", \"dup\"])\n",
      "\n",
      "        for item in art_list:\n",
      "            writer.writerow([item['title'], item['date'], item['dup']])\n",
      "\n",
      "    return print(f'Articles for {comp} has been saved in {filename}')\n",
      "152/15:\n",
      "filename = 'AMZN.html'\n",
      "comp = 'AMZN'\n",
      "\n",
      "articles = getArticles(filename, comp)\n",
      "\n",
      "art_list = saveArticles(articles, comp)\n",
      "\n",
      "saveFactiva (art_list, comp)\n",
      "152/16:\n",
      "def saveFactiva (art_list, comp):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{comp}_{len(art_list)}_{fetch_date}.csv'\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"comp\", \"title\", \"date\", \"dup\"])\n",
      "\n",
      "        for item in art_list:\n",
      "            writer.writerow([item['comp'], item['title'], item['date'], item['dup']])\n",
      "\n",
      "    return print(f'Articles for {comp} has been saved in {filename}')\n",
      "152/17:\n",
      "filename = 'AMZN.html'\n",
      "comp = 'AMZN'\n",
      "\n",
      "articles = getArticles(filename, comp)\n",
      "\n",
      "art_list = saveArticles(articles, comp)\n",
      "\n",
      "saveFactiva (art_list, comp)\n",
      "152/18:\n",
      "filename = 'AMZN.html'\n",
      "comp = 'AMZN'\n",
      "\n",
      "articles = getArticles(filename, comp)\n",
      "\n",
      "art_list = saveArticles(articles, comp)\n",
      "\n",
      "saveFactiva (art_list, comp)\n",
      "152/19:\n",
      "# Function saving articles into dictionary\n",
      "def saveArticles(articles, art_list, comp):\n",
      "      \n",
      "    for article in articles:\n",
      "        title = article.find(\"a\").text\n",
      "        date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "\n",
      "        #identify duplicates\n",
      "        if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "            dup = None\n",
      "        else:\n",
      "            dup = article.find('div', {'class':'dedupHeadlines'}).text\n",
      "\n",
      "        art_list.append({'comp': comp,\n",
      "                         'title': title,\n",
      "                         'date': date,\n",
      "                         'dup': dup})\n",
      "        \n",
      "        print(f'{comp} articles has been saved. Total articles: {len(art_list)}')\n",
      "        \n",
      "    return art_list\n",
      "152/20:\n",
      "def saveFactiva (art_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    csv_name = f'factiva_{len(art_list)}_{fetch_date}.csv'\n",
      "\n",
      "    with open(csv_name, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"comp\", \"title\", \"date\", \"dup\"])\n",
      "\n",
      "        for item in art_list:\n",
      "            writer.writerow([item['comp'], item['title'], item['date'], item['dup']])\n",
      "\n",
      "    return print(f'{len(art_list)} articles has been saved in {csv_name}')\n",
      "152/21:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "import csv\n",
      "import time\n",
      "152/22:\n",
      "# Function to get all articles of a company:\n",
      "\n",
      "def getArticles(filename, comp):\n",
      "\n",
      "    # Opening the html file\n",
      "    HTMLFile = open(filename, \"r\", encoding='Latin1')\n",
      "\n",
      "    # Reading the file\n",
      "    index = HTMLFile.read()\n",
      "\n",
      "    # Creating a BeautifulSoup object and specifying the parser\n",
      "    factiva = bts(index, 'lxml')\n",
      "    \n",
      "    # Get articles from soup\n",
      "    art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "    articles = art.find_all(\"tr\", {\"class\":'headline'})\n",
      "    \n",
      "    print(f'{comp}: {len(articles)} articles have been scraped')\n",
      "\n",
      "    return articles\n",
      "152/23:\n",
      "# Function saving articles into dictionary\n",
      "def saveArticles(articles, art_list, comp):\n",
      "      \n",
      "    for article in articles:\n",
      "        title = article.find(\"a\").text\n",
      "        date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "\n",
      "        #identify duplicates\n",
      "        if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "            dup = None\n",
      "        else:\n",
      "            dup = article.find('div', {'class':'dedupHeadlines'}).text\n",
      "\n",
      "        art_list.append({'comp': comp,\n",
      "                         'title': title,\n",
      "                         'date': date,\n",
      "                         'dup': dup})\n",
      "        \n",
      "        print(f'{comp} articles has been saved. Total articles: {len(art_list)}')\n",
      "        \n",
      "    return art_list\n",
      "152/24:\n",
      "def saveFactiva (art_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    csv_name = f'factiva_{len(art_list)}_{fetch_date}.csv'\n",
      "\n",
      "    with open(csv_name, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"comp\", \"title\", \"date\", \"dup\"])\n",
      "\n",
      "        for item in art_list:\n",
      "            writer.writerow([item['comp'], item['title'], item['date'], item['dup']])\n",
      "\n",
      "    return print(f'{len(art_list)} articles has been saved in {csv_name}')\n",
      "152/25:\n",
      "# Importing company list\n",
      "comp_list = []  \n",
      "\n",
      "# opening the file using \"with\" statement\n",
      "with open('comp-usa.csv', 'r') as comp_data:\n",
      "    for comp in csv.DictReader(comp_data):\n",
      "        comp_list.append(comp)\n",
      "\n",
      "len(comp_list)\n",
      "152/26:\n",
      "art_list =[]\n",
      "\n",
      "for company in comp_list:\n",
      "    articles = getArticles(filename, comp)\n",
      "    art_list = saveArticles(articles, comp)\n",
      "    saveFactiva (art_list)\n",
      "152/27:\n",
      "art_list =[]\n",
      "\n",
      "for company in comp_list:\n",
      "    articles = getArticles(filename, comp)\n",
      "    art_list = saveArticles(articles, art_list, comp)\n",
      "    saveFactiva (art_list)\n",
      "152/28:\n",
      "# Function saving articles into dictionary\n",
      "def saveArticles(articles, art_list, comp):\n",
      "      \n",
      "    for article in articles:\n",
      "        comp = comp_list['comp']\n",
      "        title = article.find(\"a\").text\n",
      "        date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "\n",
      "        #identify duplicates\n",
      "        if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "            dup = None\n",
      "        else:\n",
      "            dup = article.find('div', {'class':'dedupHeadlines'}).text\n",
      "\n",
      "        art_list.append({'comp': comp,\n",
      "                         'title': title,\n",
      "                         'date': date,\n",
      "                         'dup': dup})\n",
      "        \n",
      "        print(f'{comp} articles has been saved. Total articles: {len(art_list)}')\n",
      "        \n",
      "    return art_list\n",
      "152/29:\n",
      "def saveFactiva (art_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    csv_name = f'factiva_{len(art_list)}_{fetch_date}.csv'\n",
      "\n",
      "    with open(csv_name, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"comp\", \"title\", \"date\", \"dup\"])\n",
      "\n",
      "        for item in art_list:\n",
      "            writer.writerow([item['comp'], item['title'], item['date'], item['dup']])\n",
      "\n",
      "    return print(f'{len(art_list)} articles has been saved in {csv_name}')\n",
      "152/30:\n",
      "# Importing company list\n",
      "comp_list = []  \n",
      "\n",
      "# opening the file using \"with\" statement\n",
      "with open('comp-usa.csv', 'r') as comp_data:\n",
      "    for comp in csv.DictReader(comp_data):\n",
      "        comp_list.append(comp)\n",
      "\n",
      "len(comp_list)\n",
      "152/31:\n",
      "art_list =[]\n",
      "\n",
      "for company in comp_list:\n",
      "    articles = getArticles(filename, comp)\n",
      "    art_list = saveArticles(articles, art_list, comp)\n",
      "    saveFactiva (art_list)\n",
      "152/32:\n",
      "art_list =[]\n",
      "\n",
      "for company in comp_list:\n",
      "    articles = getArticles(filename, comp)\n",
      "152/33:\n",
      "# Importing company list\n",
      "comp_list = []  \n",
      "\n",
      "# opening the file using \"with\" statement\n",
      "with open('comp-usa.csv', 'r') as comp_data:\n",
      "    for comp in csv.DictReader(comp_data):\n",
      "        comp_list.append(comp)\n",
      "\n",
      "len(comp_list)\n",
      "\n",
      "comp_list\n",
      "152/34:\n",
      "art_list =[]\n",
      "\n",
      "for company in comp_list:\n",
      "    filename = comp_list['filename']\n",
      "    comp = comp_list['comp']\n",
      "    articles = getArticles(filename, comp)\n",
      "152/35:\n",
      "# Importing company list\n",
      "comp_list = []  \n",
      "\n",
      "# opening the file using \"with\" statement\n",
      "with open('comp-usa.csv', 'r') as comp_data:\n",
      "    for comp in csv.DictReader(comp_data):\n",
      "        comp_list.append(comp)\n",
      "\n",
      "len(comp_list)\n",
      "\n",
      "type(comp_list)\n",
      "152/36:\n",
      "art_list =[]\n",
      "\n",
      "for company in comp_list:\n",
      "    filename = company['filename']\n",
      "    comp = company['comp']\n",
      "    articles = getArticles(filename, comp)\n",
      "152/37:\n",
      "art_list =[]\n",
      "\n",
      "for company in comp_list:\n",
      "    filename = company['filename']\n",
      "    comp = company['comp']\n",
      "    articles = getArticles(filename, comp)\n",
      "    art_list = saveArticles(articles, art_list, comp)\n",
      "    saveFactiva (art_list)\n",
      "152/38:\n",
      "# Function saving articles into dictionary\n",
      "def saveArticles(articles, art_list, comp):\n",
      "      \n",
      "    for article in articles:\n",
      "        title = article.find(\"a\").text\n",
      "        date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "\n",
      "        #identify duplicates\n",
      "        if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "            dup = None\n",
      "        else:\n",
      "            dup = article.find('div', {'class':'dedupHeadlines'}).text\n",
      "\n",
      "        art_list.append({'comp': comp,\n",
      "                         'title': title,\n",
      "                         'date': date,\n",
      "                         'dup': dup})\n",
      "        \n",
      "        print(f'{comp} articles has been saved. Total articles: {len(art_list)}')\n",
      "        \n",
      "    return art_list\n",
      "152/39:\n",
      "# Importing company list\n",
      "comp_list = []  \n",
      "\n",
      "# opening the file using \"with\" statement\n",
      "with open('comp-usa.csv', 'r') as comp_data:\n",
      "    for comp in csv.DictReader(comp_data):\n",
      "        comp_list.append(comp)\n",
      "\n",
      "len(comp_list)\n",
      "\n",
      "type(comp_list)\n",
      "152/40:\n",
      "art_list =[]\n",
      "\n",
      "for company in comp_list:\n",
      "    filename = company['filename']\n",
      "    comp = company['comp']\n",
      "    articles = getArticles(filename, comp)\n",
      "    art_list = saveArticles(articles, art_list, comp)\n",
      "    saveFactiva (art_list)\n",
      "152/41:\n",
      "art_list =[]\n",
      "\n",
      "for company in comp_list:\n",
      "    filename = company['filename']\n",
      "    comp = company['comp']\n",
      "    articles = getArticles(filename, comp)\n",
      "    art_list = saveArticles(articles, art_list, comp)\n",
      "    print(f'{comp} articles has been saved. Total articles: {len(art_list)}')\n",
      "    saveFactiva (art_list)\n",
      "152/42:\n",
      "# Function saving articles into dictionary\n",
      "def saveArticles(articles, art_list, comp):\n",
      "      \n",
      "    for article in articles:\n",
      "        title = article.find(\"a\").text\n",
      "        date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "\n",
      "        #identify duplicates\n",
      "        if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "            dup = None\n",
      "        else:\n",
      "            dup = article.find('div', {'class':'dedupHeadlines'}).text\n",
      "\n",
      "        art_list.append({'comp': comp,\n",
      "                         'title': title,\n",
      "                         'date': date,\n",
      "                         'dup': dup})  \n",
      "         \n",
      "    return art_list\n",
      "152/43:\n",
      "def saveFactiva (art_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    csv_name = f'factiva_{len(art_list)}_{fetch_date}.csv'\n",
      "\n",
      "    with open(csv_name, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"comp\", \"title\", \"date\", \"dup\"])\n",
      "\n",
      "        for item in art_list:\n",
      "            writer.writerow([item['comp'], item['title'], item['date'], item['dup']])\n",
      "\n",
      "    return print(f'{len(art_list)} articles has been saved in {csv_name}')\n",
      "152/44:\n",
      "# Importing company list\n",
      "comp_list = []  \n",
      "\n",
      "# opening the file using \"with\" statement\n",
      "with open('comp-usa.csv', 'r') as comp_data:\n",
      "    for comp in csv.DictReader(comp_data):\n",
      "        comp_list.append(comp)\n",
      "\n",
      "len(comp_list)\n",
      "\n",
      "type(comp_list)\n",
      "152/45:\n",
      "art_list =[]\n",
      "\n",
      "for company in comp_list:\n",
      "    filename = company['filename']\n",
      "    comp = company['comp']\n",
      "    articles = getArticles(filename, comp)\n",
      "    art_list = saveArticles(articles, art_list, comp)\n",
      "    print(f'{comp} articles has been saved. Total articles: {len(art_list)}')\n",
      "    saveFactiva (art_list)\n",
      "152/46:\n",
      "art_list =[]\n",
      "\n",
      "for company in comp_list:\n",
      "    filename = company['filename']\n",
      "    comp = company['comp']\n",
      "    articles = getArticles(filename, comp)\n",
      "    art_list = saveArticles(articles, art_list, comp)\n",
      "    print(f'{comp} articles has been saved. Total articles: {len(art_list)}')\n",
      "\n",
      "saveFactiva (art_list)\n",
      "153/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "import csv\n",
      "import time\n",
      "153/2:\n",
      "#Function for parsing the URLs\n",
      "def cookSoup(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "153/3:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(soup, cat_url, country, cat):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "    #No of pages (rounding up all numbers) = math.ceil(int(total_prod)/40)\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/40)\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{cat_url}?from={40 * i}&size=40'\n",
      "            url_list.append(page)\n",
      "    print (f'{country}_{cat}: There are {total_prod} products ({total_page} pages)')\n",
      "    return url_list\n",
      "153/4:\n",
      "# Function to get main data\n",
      "def getDecathlonData(country_url, url_list, country, cat, prod_list):\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. absent)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat,\n",
      "                             'EN_cat': cat})\n",
      "            \n",
      "    print(f'{country}_{cat}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      "153/5:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{len(prod_list)}_{fetch_date}.csv'.replace(\" \",\"_\").replace(\",\",\"\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"EN_cat\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['EN_cat'], item['url']])\n",
      "    \n",
      "    return print(f'{country}: {len(prod_list)} products have been saved in {filename}')\n",
      "153/6:\n",
      "# Function to get main data\n",
      "def getDecathlonData(country_url, url_list, country, cat, prod_list):\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. absent)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat,\n",
      "                             'EN_cat': cat})\n",
      "            \n",
      "    print(f'{country}_{cat}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      "153/7:\n",
      "# Importing country - category data\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "153/8: cat_list[1]\n",
      "153/9:\n",
      "#Looping over several categories\n",
      "\n",
      "prod_list = []\n",
      "\n",
      "for category in cat_list:\n",
      "    country = category['country']\n",
      "    country_url = category['country_url']\n",
      "    cat = category['cat']\n",
      "    cat_url = category['cat_url']\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat, prod_list)\n",
      "\n",
      "saveDecathlonData(country, prod_list)\n",
      "154/1:\n",
      "import pandas as pd\n",
      "import yfinance as yf\n",
      "from yahoofinancials import YahooFinancials\n",
      "154/2:\n",
      "import pandas as pd\n",
      "import yfinance as yf\n",
      "from yahoofinancials import YahooFinancials\n",
      "154/3:\n",
      "import pandas as pd\n",
      "import yfinance as yf\n",
      "from yahoofinancials import YahooFinancials\n",
      "154/4:\n",
      "import pandas as pd\n",
      "import yfinance as yf\n",
      "from yahoofinancials import YahooFinancials\n",
      "154/5:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "tsla_df.head()\n",
      "154/6:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df.head()\n",
      "154/7:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df.head()\n",
      "\n",
      "len(amzn_df.head)\n",
      "154/8:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df.head()\n",
      "\n",
      "len(amzn_df)\n",
      "154/9:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df\n",
      "154/10:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "type(amzn_df)\n",
      "154/11:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "head(amzn_df)\n",
      "154/12:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df.head\n",
      "154/13:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df.head()\n",
      "154/14:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "df = to_dict(amzn_df)\n",
      "154/15:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "df.to_dict('dict')\n",
      "154/16:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df.to_dict('dict')\n",
      "154/17:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df.to_dict('records')\n",
      "154/18:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False).to_dict('records')\n",
      "len(amzn_df)\n",
      "154/19:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False).to_dict('records')\n",
      "amzn_df[1]\n",
      "154/20:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False).to_dict('records')\n",
      "amzn_df[1]['Open']\n",
      "154/21:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False).to_dict('records')\n",
      "amzn_df[1]\n",
      "154/22:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False).to_dict('records')\n",
      "amzn_df[1]\n",
      "154/23: amzn_df[1]['Open']\n",
      "154/24: amzn_df[1]['comp'] = 'AMZN'\n",
      "154/25: amzn_df[1]['comp'] = 'AMZN'\n",
      "154/26: amzn_df[1]\n",
      "154/27: amzn_df[1]\n",
      "154/28:\n",
      "def stockPrice(comp):\n",
      "    stock = yf.download(comp, \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False).to_dict('records')\n",
      "    for item in df:\n",
      "        item['comp'] = comp\n",
      "    return stock\n",
      "154/29:\n",
      "def stockPrice(comp):\n",
      "    stock = yf.download(comp, \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False).to_dict('records')\n",
      "    for item in df:\n",
      "        item['comp'] = comp\n",
      "    return stock\n",
      "154/30:\n",
      "comp = 'CVS'\n",
      "stockPrice(comp)\n",
      "154/31:\n",
      "def stockPrice(comp):\n",
      "    stock = yf.download(comp, \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False).to_dict('records')\n",
      "    for item in stock:\n",
      "        item['comp'] = comp\n",
      "    return stock\n",
      "154/32:\n",
      "comp = 'CVS'\n",
      "stockPrice(comp)\n",
      "154/33:\n",
      "comp = 'CVS'\n",
      "stockPrice(comp)\n",
      "154/34:\n",
      "comp = 'CVS'\n",
      "cvs = stockPrice(comp)\n",
      "154/35: len(cvs)\n",
      "154/36:\n",
      "comp_list=list(ACI, AMZN, ANF, ASO, BBBY, BNED, BURL, CHS, CONN, COST, \n",
      "               CVS, CWH, DDS, DG, DLTR, DXLG, FL, GCO, GES, GME, GPS, GRWG, HD, \n",
      "               IFMK, JWN, KR, LOW, M, ODP, QRTEA, RAD, ROST, SCVL, SFM, SPTN, TCS, \n",
      "               TDUP, TGT, TSCO, ULTA, W, WBA, WINA, WMK, WMT, WOOF)\n",
      "\n",
      "type(comp_list)\n",
      "154/37:\n",
      "comp_list=list(\"ACI\", \"AMZN\", \"ANF\", \"ASO\", \"BBBY\", \"BNED\", \"BURL\", \"CHS\", \"CONN\", \n",
      "               \"COST\", \"CVS\", \"CWH\", \"DDS\", \"DG\", \"DLTR\", \"DXLG\", \"FL\", \"GCO\", \"GES\", \"GME\", \n",
      "               \"GPS\", \"GRWG\", \"HD\", \"IFMK\", \"JWN\", \"KR\", \"LOW\", \"M\", \"ODP\", \"QRTEA\", \"RAD\", \n",
      "               \"ROST\", \"SCVL\", \"SFM\", \"SPTN\", \"TCS\", \"TDUP\", \"TGT\", \"TSCO\", \"ULTA\", \"W\", \"WBA\", \n",
      "               \"WINA\", \"WMK\", \"WMT\", \"WOOF\")\n",
      "\n",
      "type(comp_list)\n",
      "154/38:\n",
      "comp_list=list[\"ACI\", \"AMZN\", \"ANF\", \"ASO\", \"BBBY\", \"BNED\", \"BURL\", \"CHS\", \"CONN\", \n",
      "               \"COST\", \"CVS\", \"CWH\", \"DDS\", \"DG\", \"DLTR\", \"DXLG\", \"FL\", \"GCO\", \"GES\", \"GME\", \n",
      "               \"GPS\", \"GRWG\", \"HD\", \"IFMK\", \"JWN\", \"KR\", \"LOW\", \"M\", \"ODP\", \"QRTEA\", \"RAD\", \n",
      "               \"ROST\", \"SCVL\", \"SFM\", \"SPTN\", \"TCS\", \"TDUP\", \"TGT\", \"TSCO\", \"ULTA\", \"W\", \"WBA\", \n",
      "               \"WINA\", \"WMK\", \"WMT\", \"WOOF\"]\n",
      "\n",
      "type(comp_list)\n",
      "154/39:\n",
      "comp_list=list[\"ACI\", \"AMZN\", \"ANF\", \"ASO\", \"BBBY\", \"BNED\", \"BURL\", \"CHS\", \"CONN\", \n",
      "               \"COST\", \"CVS\", \"CWH\", \"DDS\", \"DG\", \"DLTR\", \"DXLG\", \"FL\", \"GCO\", \"GES\", \"GME\", \n",
      "               \"GPS\", \"GRWG\", \"HD\", \"IFMK\", \"JWN\", \"KR\", \"LOW\", \"M\", \"ODP\", \"QRTEA\", \"RAD\", \n",
      "               \"ROST\", \"SCVL\", \"SFM\", \"SPTN\", \"TCS\", \"TDUP\", \"TGT\", \"TSCO\", \"ULTA\", \"W\", \"WBA\", \n",
      "               \"WINA\", \"WMK\", \"WMT\", \"WOOF\"]\n",
      "\n",
      "len(comp_list)\n",
      "154/40:\n",
      "comp_list=[\"ACI\", \"AMZN\", \"ANF\", \"ASO\", \"BBBY\", \"BNED\", \"BURL\", \"CHS\", \"CONN\", \n",
      "               \"COST\", \"CVS\", \"CWH\", \"DDS\", \"DG\", \"DLTR\", \"DXLG\", \"FL\", \"GCO\", \"GES\", \"GME\", \n",
      "               \"GPS\", \"GRWG\", \"HD\", \"IFMK\", \"JWN\", \"KR\", \"LOW\", \"M\", \"ODP\", \"QRTEA\", \"RAD\", \n",
      "               \"ROST\", \"SCVL\", \"SFM\", \"SPTN\", \"TCS\", \"TDUP\", \"TGT\", \"TSCO\", \"ULTA\", \"W\", \"WBA\", \n",
      "               \"WINA\", \"WMK\", \"WMT\", \"WOOF\"]\n",
      "\n",
      "len(comp_list)\n",
      "154/41:\n",
      "comp_list=[\"ACI\", \"AMZN\", \"ANF\", \"ASO\", \"BBBY\", \"BNED\", \"BURL\", \"CHS\", \"CONN\", \n",
      "               \"COST\", \"CVS\", \"CWH\", \"DDS\", \"DG\", \"DLTR\", \"DXLG\", \"FL\", \"GCO\", \"GES\", \"GME\", \n",
      "               \"GPS\", \"GRWG\", \"HD\", \"IFMK\", \"JWN\", \"KR\", \"LOW\", \"M\", \"ODP\", \"QRTEA\", \"RAD\", \n",
      "               \"ROST\", \"SCVL\", \"SFM\", \"SPTN\", \"TCS\", \"TDUP\", \"TGT\", \"TSCO\", \"ULTA\", \"W\", \"WBA\", \n",
      "               \"WINA\", \"WMK\", \"WMT\", \"WOOF\"]\n",
      "\n",
      "type(comp_list)\n",
      "154/42:\n",
      "comp_list=[\"ACI\", \"AMZN\", \"ANF\", \"ASO\", \"BBBY\", \"BNED\", \"BURL\", \"CHS\", \"CONN\", \n",
      "               \"COST\", \"CVS\", \"CWH\", \"DDS\", \"DG\", \"DLTR\", \"DXLG\", \"FL\", \"GCO\", \"GES\", \"GME\", \n",
      "               \"GPS\", \"GRWG\", \"HD\", \"IFMK\", \"JWN\", \"KR\", \"LOW\", \"M\", \"ODP\", \"QRTEA\", \"RAD\", \n",
      "               \"ROST\", \"SCVL\", \"SFM\", \"SPTN\", \"TCS\", \"TDUP\", \"TGT\", \"TSCO\", \"ULTA\", \"W\", \"WBA\", \n",
      "               \"WINA\", \"WMK\", \"WMT\", \"WOOF\"]\n",
      "154/43:\n",
      "stock = []\n",
      "for comp in comp_list:\n",
      "    stock.append(stockPrice(comp))\n",
      "    \n",
      "len(stock)\n",
      "154/44:\n",
      "stock = []\n",
      "for comp in comp_list:\n",
      "    stock.append(stockPrice(comp))\n",
      "    \n",
      "stock[1]\n",
      "154/45: stock[46]\n",
      "154/46: stock[45]\n",
      "154/47:\n",
      "stock = []\n",
      "for comp in comp_list:\n",
      "    stock_comp = stockPrice(comp)\n",
      "    for date_comp in stock_comp:\n",
      "        stock.append(date_comp)\n",
      "154/48: len(stock)\n",
      "154/49: len(stock)\n",
      "154/50: len(stock)\n",
      "154/51: 46*757\n",
      "154/52: 45*757\n",
      "154/53: 42*757\n",
      "154/54: len(comp_list)*757\n",
      "154/55: stock[1]\n",
      "154/56: csv\n",
      "154/57:\n",
      "comp = 'CVS'\n",
      "cvs = stockPrice(comp)\n",
      "154/58: csv\n",
      "154/59: cvs\n",
      "154/60:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df\n",
      "154/61:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df.to_dict('records')\n",
      "154/62:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df\n",
      "154/63:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "type(amzn_df)\n",
      "154/64:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "len(amzn_df)\n",
      "154/65:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df[1]\n",
      "154/66:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "154/67:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False).to_dict('records', index=True)\n",
      "amzn_df\n",
      "154/68:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df['Date']\n",
      "154/69:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df\n",
      "154/70:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df\n",
      "154/71: amzn_df.to_csv(\"amzn.csv\", index=True)\n",
      "154/72:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "amzn_df.to_csv(\"amzn.csv\", index=True)\n",
      "154/73:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "amzn_df.to_csv(\"amzn.csv\", index=False)\n",
      "154/74:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "amzn_df.to_csv(\"amzn.csv\", index=True)\n",
      "154/75:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "amzn_df.to_dict(\"amzn\", index=True)\n",
      "154/76:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "amzn_df.to_dict(\"amzn\")\n",
      "154/77:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "amzn_df.to_dict(\"records\")\n",
      "154/78:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "#amzn_df.to_dict(\"records\")\n",
      "\n",
      "amzn_df.reset_index(inplace=True)\n",
      "154/79:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "#amzn_df.to_dict(\"records\")\n",
      "\n",
      "amzn_df.reset_index(inplace=True)\n",
      "154/80:\n",
      "def stockPrice(comp):\n",
      "    stock = yf.download(comp, \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False).to_dict('records')\n",
      "    for item in stock:\n",
      "        item['comp'] = comp\n",
      "    return stock\n",
      "154/81:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "#amzn_df.to_dict(\"records\")\n",
      "\n",
      "amzn_df.reset_index(inplace=True)\n",
      "\n",
      "amzn_df\n",
      "154/82:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False).reset_index(inplace=True).to_dict(\"records\")\n",
      "154/83:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False).reset_index(inplace=True)\n",
      "154/84:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False).reset_index(inplace=True)\n",
      "154/85:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False).reset_index(inplace=True)\n",
      "\n",
      "type(amzn_df)\n",
      "154/86:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False).reset_index(inplace=True)\n",
      "\n",
      "amzn_df\n",
      "154/87:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "\n",
      "amzn = amzn_df.reset_index(inplace=True).to_dict(\"records\")\n",
      "154/88:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "#amzn_df.to_dict(\"records\")\n",
      "\n",
      "\n",
      "\n",
      "amzn_df\n",
      "154/89:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "amzn_df.reset_index(inplace = True).to_dict(\"records\")\n",
      "154/90:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "amzn_df.reset_index(inplace = True)\n",
      "154/91:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "amzn_df.reset_index(inplace = True)\n",
      "154/92:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "amzn_df.reset_index(inplace = True)\n",
      "154/93: amzn_df\n",
      "154/94: amzn_df.to_dict(\"records\")\n",
      "154/95:\n",
      "def stockPrice(comp):\n",
      "    stock_df = yf.download(comp, \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "    stock_reset = stock_df.reset_index(inplace = True)\n",
      "    stock = stock_df.to_dict('records')\n",
      "    for item in stock:\n",
      "        item['comp'] = comp\n",
      "    return stock\n",
      "154/96:\n",
      "comp = 'CVS'\n",
      "cvs = stockPrice(comp)\n",
      "154/97:\n",
      "comp = 'CVS'\n",
      "cvs = stockPrice(comp)\n",
      "154/98: cvs[1]\n",
      "154/99:\n",
      "stock = []\n",
      "for comp in comp_list:\n",
      "    stock_comp = stockPrice(comp)\n",
      "    stock.extend(stock_comp)\n",
      "\n",
      "len(stock)\n",
      "154/100:\n",
      "stock = []\n",
      "for comp in comp_list:\n",
      "    stock_comp = stockPrice(comp)\n",
      "    stock.extend(stock_comp)\n",
      "\n",
      "len(stock)\n",
      "154/101: stock[0]\n",
      "154/102: stock[3]\n",
      "154/103: stock[32900]\n",
      "154/104:\n",
      "with open('stock.csv', \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"date\", \"open\", \"close\", \"adj_close\", \"comp\"])\n",
      "\n",
      "        for item in stock:\n",
      "            writer.writerow([item['Date'], item['Open'], item['Close'], item['Adj Close'], item['comp']])\n",
      "154/105:\n",
      "import csv\n",
      "with open('stock.csv', \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"date\", \"open\", \"close\", \"adj_close\", \"comp\"])\n",
      "\n",
      "        for item in stock:\n",
      "            writer.writerow([item['Date'], item['Open'], item['Close'], item['Adj Close'], item['comp']])\n",
      "158/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "import csv\n",
      "import time\n",
      "158/2:\n",
      "#Function for parsing the URLs\n",
      "def cookSoup(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "158/3:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(soup, cat_url, country, cat):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/40)\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{cat_url}?from={40 * i}&size=40'\n",
      "            url_list.append(page)\n",
      "    print (f'{country}_{cat}: There are {total_prod} products ({total_page} pages)')\n",
      "    return url_list\n",
      "158/4:\n",
      "# Function to get main data\n",
      "def getDecathlonData(country_url, url_list, country, cat, prod_list):\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. absent)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat,\n",
      "                             'EN_cat': cat})\n",
      "            \n",
      "    print(f'{country}_{cat}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      "158/5:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{len(prod_list)}_{fetch_date}.csv'.replace(\" \",\"_\").replace(\",\",\"\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"EN_cat\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['EN_cat'], item['url']])\n",
      "    \n",
      "    return print(f'{country}: {len(prod_list)} products have been saved in {filename}')\n",
      "158/6:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "import csv\n",
      "import time\n",
      "158/7:\n",
      "#Function for parsing the URLs\n",
      "def cookSoup(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "158/8:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(soup, cat_url, country, cat):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/40)\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{cat_url}?from={40 * i}&size=40'\n",
      "            url_list.append(page)\n",
      "    print (f'{country}_{cat}: There are {total_prod} products ({total_page} pages)')\n",
      "    return url_list\n",
      "158/9:\n",
      "# Function to get main data\n",
      "def getDecathlonData(country_url, url_list, country, cat, prod_list):\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. absent)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat,\n",
      "                             'EN_cat': cat})\n",
      "            \n",
      "    print(f'{country}_{cat}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      "158/10:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{len(prod_list)}_{fetch_date}.csv'.replace(\" \",\"_\").replace(\",\",\"\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"EN_cat\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['EN_cat'], item['url']])\n",
      "    \n",
      "    return print(f'{country}: {len(prod_list)} products have been saved in {filename}')\n",
      "158/11:\n",
      "# Importing country - category data\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list_NL.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "158/12: cat_list[1]\n",
      "158/13:\n",
      "#Looping over several categories\n",
      "\n",
      "prod_list = []\n",
      "\n",
      "for category in cat_list:\n",
      "    country = category['country']\n",
      "    country_url = category['country_url']\n",
      "    cat = category['cat']\n",
      "    cat_url = category['cat_url']\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat, prod_list)\n",
      "\n",
      "saveDecathlonData(country, prod_list)\n",
      "158/14:\n",
      "# Importing country - category data\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list_NL.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "158/15: cat_list[1]\n",
      "158/16:\n",
      "#Looping over several categories\n",
      "\n",
      "prod_list = []\n",
      "\n",
      "for category in cat_list:\n",
      "    country = category['country']\n",
      "    country_url = category['country_url']\n",
      "    cat = category['cat']\n",
      "    cat_url = category['cat_url']\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat, prod_list)\n",
      "\n",
      "saveDecathlonData(country, prod_list)\n",
      "158/17:\n",
      "# Importing country - category data\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list_NL.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "158/18: cat_list[1]\n",
      "158/19:\n",
      "#Looping over several categories\n",
      "\n",
      "prod_list = []\n",
      "\n",
      "for category in cat_list:\n",
      "    country = category['country']\n",
      "    country_url = category['country_url']\n",
      "    cat = category['cat']\n",
      "    cat_url = category['cat_url']\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat, prod_list)\n",
      "\n",
      "saveDecathlonData(country, prod_list)\n",
      "158/20:\n",
      "# Importing country - category data\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list_NL.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "158/21: cat_list[1]\n",
      "158/22:\n",
      "# Importing country - category data\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list_NL.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "158/23: cat_list[0]\n",
      "158/24:\n",
      "#Looping over several categories\n",
      "\n",
      "prod_list = []\n",
      "\n",
      "for category in cat_list:\n",
      "    country = category['country']\n",
      "    country_url = category['country_url']\n",
      "    cat = category['cat']\n",
      "    cat_url = category['cat_url']\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat, prod_list)\n",
      "\n",
      "saveDecathlonData(country, prod_list)\n",
      "158/25:\n",
      "# Importing country - category data\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list_NL.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "158/26: cat_list[0]\n",
      "158/27:\n",
      "#Looping over several categories\n",
      "\n",
      "prod_list = []\n",
      "\n",
      "for category in cat_list:\n",
      "    country = category['country']\n",
      "    country_url = category['country_url']\n",
      "    cat = category['cat']\n",
      "    cat_url = category['cat_url']\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat, prod_list)\n",
      "\n",
      "saveDecathlonData(country, prod_list)\n",
      "160/1:\n",
      "#NL - Woman sportwear\n",
      "url = 'https://www.decathlon.nl/browse/c0-dames-sportkleding/_/N-1fj8z6z'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "162/1:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "162/2:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "162/3:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "162/4:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "162/5:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "162/6:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "162/7: prod\n",
      "162/8:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "162/9: de1_list\n",
      "159/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "import csv\n",
      "import time\n",
      "159/2:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "import csv\n",
      "import time\n",
      "159/3:\n",
      "#Function for parsing the URLs\n",
      "def cookSoup(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "159/4:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(soup, cat_url, country, cat):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/40)\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{cat_url}?from={40 * i}&size=40'\n",
      "            url_list.append(page)\n",
      "    print (f'{country}_{cat}: There are {total_prod} products ({total_page} pages)')\n",
      "    return url_list\n",
      "159/5:\n",
      "# Function to get main data\n",
      "def getDecathlonData(country_url, url_list, country, cat, prod_list):\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. absent)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat,\n",
      "                             'EN_cat': cat})\n",
      "            \n",
      "    print(f'{country}_{cat}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      "159/6:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{len(prod_list)}_{fetch_date}.csv'.replace(\" \",\"_\").replace(\",\",\"\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"EN_cat\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['EN_cat'], item['url']])\n",
      "    \n",
      "    return print(f'{country}: {len(prod_list)} products have been saved in {filename}')\n",
      "159/7:\n",
      "# Importing country - category data\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "159/8: cat_list[0]\n",
      "159/9:\n",
      "#Looping over several categories\n",
      "\n",
      "prod_list = []\n",
      "\n",
      "for category in cat_list:\n",
      "    country = category['country']\n",
      "    country_url = category['country_url']\n",
      "    cat = category['cat']\n",
      "    cat_url = category['cat_url']\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat, prod_list)\n",
      "\n",
      "saveDecathlonData(country, prod_list)\n",
      "162/10:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "162/11:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "162/12:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-d5pqmr\")\n",
      "162/13: prod\n",
      "162/14: len(prod)\n",
      "162/15:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "162/16:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-d5pqmr\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-d5pqmr\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "162/17: de1_list\n",
      "162/18: len(de1_list)\n",
      "160/2:\n",
      "#NL - Woman sportwear\n",
      "url = 'https://www.decathlon.nl/browse/c0-dames-sportkleding/_/N-1fj8z6z'\n",
      "\n",
      "nl1 = getAndParseURL(url)\n",
      "\n",
      "prod = nl1.find_all(class_=\"dpb-holder loaded svelte-1e4me6r\")\n",
      "160/3:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "160/4:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "160/5:\n",
      "#NL - Woman sportwear\n",
      "url = 'https://www.decathlon.nl/browse/c0-dames-sportkleding/_/N-1fj8z6z'\n",
      "\n",
      "nl1 = getAndParseURL(url)\n",
      "\n",
      "prod = nl1.find_all(class_=\"dpb-holder loaded svelte-1e4me6r\")\n",
      "160/6:\n",
      "nl_base = \"https://www.decathlon.nl\"\n",
      "nl1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1e4me6r\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1e4me6r\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/7:\n",
      "nl_base = \"https://www.decathlon.nl\"\n",
      "nl1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = nl1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1e4me6r\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1e4me6r\"}).text\n",
      "    prod_url = f'{nl_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/8:\n",
      "#NL - Woman sportwear\n",
      "url = 'https://www.decathlon.nl/browse/c0-dames-sportkleding/_/N-1fj8z6z'\n",
      "\n",
      "nl1 = getAndParseURL(url)\n",
      "\n",
      "prod = nl1.find_all(class_=\"dpb-holder loaded svelte-1e4me6r\")\n",
      "160/9: prod[1]\n",
      "160/10: prod[1].find(\"a\").attrs[\"href\"]\n",
      "160/11: prod[1].find(\"a\").attrs[\"href\"]\n",
      "160/12: nl1.find(\"h1\").text\n",
      "160/13: nl1.find(\"h1\").text\n",
      "160/14: prod[1].find(\"h2\",{\"class\":\"svelte-1e4me6r\"}).text\n",
      "160/15: prod[1].find(\"h2\",{\"class\":\"svelte-1e4me6r\"}).text\n",
      "160/16: prod[1].find(\"strong\",{\"class\":\"svelte-1e4me6r\"}).text\n",
      "160/17: prod[1].find(\"strong\",{\"class\":\"svelte-1e4me6r\"}).text\n",
      "160/18: prod[1].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "160/19: a <- prod[1].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "160/20: a = prod[1].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "160/21:\n",
      "a = prod[1].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "a\n",
      "160/22: a = prod[1].find(\"a\").attrs[\"href\"]\n",
      "160/23:\n",
      "a = prod[1].find(\"a\").attrs[\"href\"]\n",
      "a.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "160/24:\n",
      "a = prod[1].find(\"a\").attrs[\"href\"]\n",
      "a.partition(\"?mc=\")\n",
      "160/25:\n",
      "a = prod[1].find(\"a\").attrs[\"href\"]\n",
      "prod_id = link.partition(\"?mc=\")[2]\n",
      "160/26:\n",
      "a = prod[1].find(\"a\").attrs[\"href\"]\n",
      "a.partition(\"?mc=\")[2]\n",
      "160/27: prod[1].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "160/28: prod[1].find(\"div\", {\"class\":\"prc__active-price\"})\n",
      "160/29: prod[1].find(\"div\", {\"class\":\"prc__active-price\"})\n",
      "160/30: prod[1].find(\"div\", {\"class\":\"prc__active-price\"}).text\n",
      "160/31: prod[0].find(\"div\", {\"class\":\"prc__active-price\"}).text\n",
      "160/32: prod[0].find(\"div\", {\"class\":\"prc__previous\"}).text\n",
      "160/33: prod[1].find(\"div\", {\"class\":\"prc__previous\"}).text\n",
      "160/34: prod[1].find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "160/35: prod[0].find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "160/36: prod[0].find(\"span\", {\"class\":\"prc__info-addon\"}).text\n",
      "160/37: prod[1].find(\"div\", {\"class\":\"prc__active-price\"})\n",
      "160/38: prod[0].find(\"span\", {\"class\":\"prc__info-addon\"})\n",
      "160/39: prod[1].find(\"span\", {\"class\":\"prc__info-addon\"})\n",
      "160/40: prod[0].find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "160/41: prod[0].find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "160/42: prod[1].find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "160/43: prod[1].find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "160/44: prod[1].find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "160/45:\n",
      "#NL - Woman sportwear\n",
      "url = 'https://www.decathlon.nl/browse/c0-dames-sportkleding/_/N-1fj8z6z'\n",
      "\n",
      "nl1 = getAndParseURL(url)\n",
      "\n",
      "prod = nl1.find_all(class_=\"dpb-holder loaded svelte-1e4me6r\")\n",
      "160/46:\n",
      "nl_base = \"https://www.decathlon.nl\"\n",
      "nl1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = nl1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1e4me6r\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1e4me6r\"}).text\n",
      "    prod_url = f'{nl_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    nl1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/47: nl1_list\n",
      "160/48:\n",
      "#NL - Woman sportwear\n",
      "url = 'https://www.decathlon.fr/browse/c0-femme/_/N-ry4jwt'\n",
      "\n",
      "nl1 = getAndParseURL(url)\n",
      "\n",
      "prod = nl1.find_all(class_=\"dpb-holder loaded svelte-1e4me6r\")\n",
      "160/49:\n",
      "a = prod[1].find(\"a\").attrs[\"href\"]\n",
      "a.partition(\"?mc=\")[2]\n",
      "160/50: prod\n",
      "160/51:\n",
      "#NL - Woman sportwear\n",
      "url = 'https://www.decathlon.fr/browse/c0-femme/_/N-ry4jwt'\n",
      "\n",
      "nl1 = getAndParseURL(url)\n",
      "\n",
      "prod = nl1.find_all(class_=\"dpb-holder loaded svelte-aim4wv\")\n",
      "160/52: prod\n",
      "160/53: len(prod)\n",
      "160/54:\n",
      "nl_base = \"https://www.decathlon.fr\"\n",
      "nl1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = nl1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-aim4wv\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-aim4wv\"}).text\n",
      "    prod_url = f'{nl_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    nl1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/55: nl1_list\n",
      "160/56:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "160/57:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "160/58:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "160/59:\n",
      "#NL - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjj'\n",
      "\n",
      "doc = getAndParseURL(url)\n",
      "\n",
      "prod = doc.find_all(class_=\"dpb-holder loaded svelte-d5pqmr\")\n",
      "160/60: len(prod)\n",
      "160/61: doc.find(\"h1\").text\n",
      "160/62: prod[1].find(\"h2\",{\"class\":\"svelte-1e4me6r\"}).text\n",
      "160/63: prod[1].find(\"h2\",{\"class\":\"svelte-d5pqmr\"}).text\n",
      "160/64: prod[1].find(\"strong\",{\"class\":\"svelte-d5pqmr\"}).text\n",
      "160/65: prod[0].find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "160/66: prod[1].find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "160/67: prod[15].find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "160/68: prod[15].find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "160/69:\n",
      "url_base = \"https://www.decathlon.de\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-d5pqmr\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-d5pqmr\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/70: prod_list\n",
      "160/71:\n",
      "#NL - Woman sportwear\n",
      "url = 'https://www.decathlon.co.uk/browse/c0-women/_/N-1an7dur'\n",
      "\n",
      "doc = getAndParseURL(url)\n",
      "\n",
      "prod = doc.find_all(class_=\"dpb-holder loaded svelte-1ls8j3w\")\n",
      "160/72: len(prod)\n",
      "160/73: prod[1].find(\"strong\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "160/74: prod[1].find(\"h2\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "160/75:\n",
      "url_base = \"https://www.decathlon.co.uk\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/76: prod_list\n",
      "160/77:\n",
      "url_base = \"https://www.decathlon.co.uk\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/78: prod_list\n",
      "160/79:\n",
      "url_base = \"https://www.decathlon.co.uk\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}).replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/80:\n",
      "url_base = \"https://www.decathlon.co.uk\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}).replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/81:\n",
      "url_base = \"https://www.decathlon.co.uk\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/82:\n",
      "url_base = \"https://www.decathlon.co.uk\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}).replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/83:\n",
      "url_base = \"https://www.decathlon.co.uk\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}).text.replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/84: prod_list\n",
      "160/85:\n",
      "url_base = \"https://www.decathlon.co.uk\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/86:\n",
      "#Woman sportwear\n",
      "url = 'https://www.decathlon.ch/de/browse/c0-damen/c1-damenbekleidung/_/N-1ezwtdc'\n",
      "\n",
      "doc = getAndParseURL(url)\n",
      "\n",
      "prod = doc.find_all(class_=\"dpb-holder loaded svelte-aim4wv\")\n",
      "160/87: len(prod)\n",
      "160/88:\n",
      "#Woman sportwear\n",
      "url = 'https://www.decathlon.ch/de/browse/c0-damen/c1-damenbekleidung/_/N-1ezwtdc'\n",
      "\n",
      "doc = getAndParseURL(url)\n",
      "\n",
      "prod = doc.find_all(class_=\"dpb-holder loaded svelte-aim4wv\")\n",
      "\n",
      "per_page = 20\n",
      "160/89: doc.find(\"h1\").text\n",
      "160/90: prod[1].find(\"h2\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "160/91: prod[1].find(\"h2\",{\"class\":\"svelte-aim4wv\"}).text\n",
      "160/92: prod[1].find(\"strong\",{\"class\":\"svelte-aim4wv\"}).text\n",
      "160/93: prod[0].find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "160/94:\n",
      "url_base = \"https://www.decathlon.ch/de/\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/95:\n",
      "url_base = \"https://www.decathlon.ch/de/\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-Z19lq375\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-Z19lq375\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/96:\n",
      "url_base = \"https://www.decathlon.ch/de/\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-aim4wv\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-aim4wv\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/97: prod_list\n",
      "160/98:\n",
      "#Woman sportwear\n",
      "url = 'https://www.decathlon.ch/de/browse/c0-damen/c1-damenbekleidung/_/N-1ezwtdc'\n",
      "code = 'aim4wv'\n",
      "doc = getAndParseURL(url)\n",
      "\n",
      "prod = doc.find_all(class_=f\"dpb-holder loaded svelte-{code}\")\n",
      "\n",
      "#per_page = 20\n",
      "160/99:\n",
      "#Woman sportwear\n",
      "url = 'https://www.decathlon.co.uk/browse/c0-women/c1-clothing/_/N-7reina'\n",
      "code = '1ls8j3w'\n",
      "stk_code = '1bh0x5d'\n",
      "doc = getAndParseURL(url)\n",
      "\n",
      "prod = doc.find_all(class_=f\"dpb-holder loaded svelte-{code}\")\n",
      "\n",
      "#per_page = 20\n",
      "160/100: len(prod)\n",
      "160/101:\n",
      "url_base = \"https://www.decathlon.co.uk\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":f\"svelte-{code}\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":f\"svelte-{code}\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": f\"sticker svelte-{stk_code}\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{stk_code}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/102: prod_list\n",
      "163/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "163/2:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "163/3: sport = doc.find_all(class_=\"filter-content-input-group\")\n",
      "163/4:\n",
      "#DE\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "\n",
      "doc = getAndParseURL(url)\n",
      "163/5:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "163/6:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "163/7:\n",
      "#DE\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "\n",
      "doc = getAndParseURL(url)\n",
      "163/8:\n",
      "#DE\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "\n",
      "doc = getAndParseURL(filter_link)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/9: sport = doc.find_all(class_=\"filter-content-input-group\")\n",
      "163/10:\n",
      "sport = doc.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "len(sport)\n",
      "163/11: sport\n",
      "163/12: sport[4]\n",
      "163/13: sport[0]\n",
      "163/14: $ chromedriver\n",
      "163/15:\n",
      "options = webdriver.ChromeOptions()\n",
      "options.add_argument('--ignore-certificate-errors')\n",
      "options.add_argument(\"--test-type\")\n",
      "options.binary_location = \"/usr/bin/chromium\"\n",
      "driver = webdriver.Chrome(chrome_options=options)\n",
      "driver.get('https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd')\n",
      "163/16:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "163/17:\n",
      "options = webdriver.ChromeOptions()\n",
      "options.add_argument('--ignore-certificate-errors')\n",
      "options.add_argument(\"--test-type\")\n",
      "options.binary_location = \"/usr/bin/chromium\"\n",
      "driver = webdriver.Chrome(chrome_options=options)\n",
      "driver.get('https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd')\n",
      "164/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "164/2:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(browser,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters js svelte-11vrp39\")))\n",
      "\n",
      "buttons = driver.find_elements_by_class_name(\"cta cta--outline cta--2nd cta--small\")\n",
      "for button in buttons:\n",
      "    button.click()\n",
      "164/3:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters js svelte-11vrp39\")))\n",
      "\n",
      "buttons = driver.find_elements_by_class_name(\"cta cta--outline cta--2nd cta--small\")\n",
      "for button in buttons:\n",
      "    button.click()\n",
      "164/4:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters js svelte-11vrp39\")))\n",
      "\n",
      "buttons = driver.find_elements_by_class_name(\"cta cta--outline cta--2nd cta--small\")\n",
      "for button in buttons:\n",
      "    button.click()\n",
      "164/5:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "#buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters js svelte-11vrp39\")))\n",
      "\n",
      "buttons = driver.find_elements_by_class_name(\"cta cta--outline cta--2nd cta--small\")\n",
      "for button in buttons:\n",
      "    button.click()\n",
      "164/6:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "#buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters js svelte-11vrp39\")))\n",
      "\n",
      "buttons = driver.find_elements(by = By.CLASS_NAME, value = \"cta cta--outline cta--2nd cta--small\")\n",
      "for button in buttons:\n",
      "    button.click()\n",
      "164/7: buttons\n",
      "164/8:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters js svelte-11vrp39\")))\n",
      "\n",
      "buttons = driver.find_elements(by = By.CLASS_NAME, value = \"cta cta--outline cta--2nd cta--small\")\n",
      "for button in buttons:\n",
      "    button.click()\n",
      "164/9:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "#buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters js svelte-11vrp39\")))\n",
      "\n",
      "buttons = driver.find_elements(By.CLASS_NAME, \"cta cta--outline cta--2nd cta--small\")\n",
      "\n",
      "for button in buttons:\n",
      "    button.click()\n",
      "164/10: buttons\n",
      "164/11:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "#buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters js svelte-11vrp39\")))\n",
      "\n",
      "buttons = driver.find_elements(By.CLASS_NAME, \"cta cta--outline cta--2nd cta--small\")\n",
      "164/12: buttons\n",
      "164/13:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "#buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters js svelte-11vrp39\")))\n",
      "\n",
      "buttons = driver.find_elements(By.CLASS_NAME, \"cta--outline\")\n",
      "164/14: buttons\n",
      "164/15:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "#buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters js svelte-11vrp39\")))\n",
      "\n",
      "buttons = driver.find_elements(By.CLASS_NAME, \"cta--outline\")\n",
      "\n",
      "for button in buttons:\n",
      "    button.click()\n",
      "164/16: buttons[0]\n",
      "164/17:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "#buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters js svelte-11vrp39\")))\n",
      "\n",
      "buttons = driver.find_elements(By.CLASS_NAME, \"cta--outline\")\n",
      "\n",
      "for button in buttons:\n",
      "    print(button)\n",
      "164/18:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "buttons = driver.find_elements(By.CLASS_NAME, \"cta--outline\")\n",
      "\n",
      "for button in buttons:\n",
      "    print(button)\n",
      "164/19:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "buttons = driver.find_elements(By.CLASS_NAME, \"cta--outline\")\n",
      "\n",
      "for button in buttons:\n",
      "    button.click()\n",
      "164/20:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "buttons = driver.find_elements(By.XPATH, '//button[@class='cta']')\n",
      "\n",
      "#for button in buttons:\n",
      "#    button.click()\n",
      "164/21:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "buttons = driver.find_elements(By.XPATH, '//button[@class=\"cta\"]')\n",
      "\n",
      "#for button in buttons:\n",
      "#    button.click()\n",
      "164/22: buttons\n",
      "164/23:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "#buttons = driver.find_elements(By.XPATH, '//button[@class=\"cta\"]')\n",
      "\n",
      "#for button in buttons:\n",
      "#    button.click()\n",
      "164/24: buttons\n",
      "165/1:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "#buttons = driver.find_elements(By.XPATH, '//button[@class=\"cta\"]')\n",
      "\n",
      "#for button in buttons:\n",
      "#    button.click()\n",
      "165/2:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "165/3:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "#buttons = driver.find_elements(By.XPATH, '//button[@class=\"cta\"]')\n",
      "\n",
      "#for button in buttons:\n",
      "#    button.click()\n",
      "165/4:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "buttons = driver.find_elements(By.XPATH, '//button[@type=\"button\"]')\n",
      "\n",
      "buttons[1]\n",
      "\n",
      "#for button in buttons:\n",
      "#    button.click()\n",
      "165/5:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "buttons = driver.find_elements(By.XPATH, '//button[@type=\"button\"]')\n",
      "\n",
      "for button in buttons:\n",
      "    button.click()\n",
      "165/6:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "for button in buttons:\n",
      "    button.click()\n",
      "165/7: buttons\n",
      "165/8: len(buttons)\n",
      "165/9:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "button[5].click()\n",
      "165/10:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "buttons[5].click()\n",
      "165/11:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "buttons[5].click()\n",
      "165/12:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "buttons[5].click()\n",
      "165/13:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "\n",
      "driver.execute_script(\"arguments[0].click();\", buttons[5])\n",
      "#buttons[5].click()\n",
      "165/14:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "165/15: html_source = driver.page_source\n",
      "165/16:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    soup = bts(driver.page_source)\n",
      "165/17: soup\n",
      "165/18:\n",
      "sport = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "len(sport)\n",
      "165/19:\n",
      "sport = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "len(sport)\n",
      "165/20: sport[0]\n",
      "165/21:\n",
      "sport = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "sport[len(sport)-1]\n",
      "165/22:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "sport <- [len(filter)-1]\n",
      "165/23:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "sport = [len(filter)-1]\n",
      "165/24: sport[0]\n",
      "165/25:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "sport = [len(filter)-1]\n",
      "165/26: sport\n",
      "165/27:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "sport = filter[len(filter)-1]\n",
      "165/28: sport\n",
      "165/29: len(sport)\n",
      "165/30: sport[1]\n",
      "165/31: sport[0]\n",
      "165/32: sport\n",
      "165/33: type(sport)\n",
      "165/34: sport.find_all(\"a\").attrs[\"href\"]\n",
      "165/35:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "sport = filter[len(filter)-1]\n",
      "165/36: sport.find_all(\"a\")\n",
      "165/37:\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = sport.text\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                     'sport_link': sport_link})\n",
      "165/38:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "sports = filter[len(filter)-1]\n",
      "165/39:\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = sport.text\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                     'sport_link': sport_link})\n",
      "165/40: sport_list\n",
      "165/41: sport_list[0]\n",
      "165/42:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = sport.text\n",
      "    re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", sport_type)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                     'sport_link': sport_link})\n",
      "165/43: sport_list[0]\n",
      "165/44:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = sport.text\n",
      "    re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", sport_type)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                     'sport_link': sport_link})\n",
      "165/45:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = sport.text\n",
      "    re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", sport_type)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/46: sport_list[0]\n",
      "165/47:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = sport.text\n",
      "    re.sub(\"\\(\\.*?\\)\\\", \"\", sport_type)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/48:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = sport.text\n",
      "    re.sub(\"(.*?)\", \"\", sport_type)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/49: sport_list[0]\n",
      "165/50:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = re.sub(\"(.*?)\",\"\", sport.text)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/51: sport_list[0]\n",
      "165/52:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = sport.text\n",
      "    re.sub(\"(.*?)\",\"\", sport.text)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/53: sport_list[0]\n",
      "165/54:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", sport.text)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/55: sport_list[0]\n",
      "165/56:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = re.sub(\"\\(\\.*?\\)\\\", \"\", sport.text)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/57:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = re.sub(\"[\\(\\.*?\\)\\]\", \"\", sport.text)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/58:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = re.sub(\"[\\(\\.*?\\)\\]\", \"\", sport.text)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/59:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = re.sub(\"[\\(].*?[\\)]\", \"\", sport.text)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/60: sport_list[0]\n",
      "165/61:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport.text)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/62: sport_list[0]\n",
      "165/63: sport_list\n",
      "165/64:\n",
      "a = '/browse/c0-damen/_/N-1wmfzjjZhpsxkdZ4505jj'\n",
      "\n",
      "a - '/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "165/65:\n",
      "a = '/browse/c0-damen/_/N-1wmfzjjZhpsxkdZ4505jj'\n",
      "\n",
      "b = '/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "\n",
      "a.replace(b,\"\")\n",
      "165/66:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import re\n",
      "165/67:\n",
      "driver = webdriver.Chrome()\n",
      "\n",
      "base_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_code = 'Zhpsxkd'\n",
      "\n",
      "filter_link = f{base_link}{base_code}\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/68:\n",
      "driver = webdriver.Chrome()\n",
      "\n",
      "base_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_code = 'Zhpsxkd'\n",
      "\n",
      "filter_link = f'{base_link}{base_code}''\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/69:\n",
      "driver = webdriver.Chrome()\n",
      "\n",
      "base_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_code = 'Zhpsxkd'\n",
      "\n",
      "filter_link = f'{base_link}{base_code}'\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/70:\n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "165/71:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_link': sport_link})\n",
      "\n",
      "for i in range(1, len(sport_url) + 1):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport.text) #remove all numbers\n",
      "    sport_code = sport.attrs[\"href\"].replace(filter_link,\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/72: sport_list\n",
      "165/73: sport_url\n",
      "165/74: sport_list[0]\n",
      "165/75:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url) + 1):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport.text) #remove all numbers\n",
      "    sport_code = sport.attrs[\"href\"].replace(filter_link,\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/76:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url) + 1):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport.text) #remove all numbers\n",
      "    sport_code = sport.attrs[\"href\"].replace(filter_link,\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/77:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url) + 1):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport.text) #remove all numbers\n",
      "    sport_code = sport.attrs[\"href\"].replace(filter_link,\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/78: sport_list[0]\n",
      "165/79: sport_list\n",
      "165/80: filter_link\n",
      "165/81: sport_list[1]\n",
      "165/82:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url) + 1):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport.text) #remove all numbers\n",
      "    sport_code = sport.attrs[\"href\"].text.replace(filter_link,\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/83:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url) + 1):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport.text) #remove all numbers\n",
      "    sport_code = sport.attrs[\"href\"].replace(filter_link,\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/84:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url) + 1):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(filter_link,\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/85: sport_url[1].text\n",
      "165/86: len(sport_url) + 1\n",
      "165/87: print(range(1,135))\n",
      "165/88: range(1,135)\n",
      "165/89:\n",
      "for i in range(1,135):\n",
      "    print(i)\n",
      "165/90: len(sport_url)\n",
      "165/91:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url) - 1):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(filter_link,\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/92: sport_list\n",
      "165/93:\n",
      "driver = webdriver.Chrome()\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "\n",
      "filter_link = f'{base_index}{base_link}{base_code}'\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/94:\n",
      "driver = webdriver.Chrome()\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "\n",
      "filter_link = f'{base_index}{base_link}{base_code}'\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/95:\n",
      "driver = webdriver.Chrome()\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{base_code}'\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/96:\n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "165/97:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url) - 1):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(base_cat,\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/98: sport_list\n",
      "165/99:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url) - 1):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{base_code}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/100: sport_list\n",
      "165/101: len(sport_list)\n",
      "165/102:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{base_code}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/103: len(sport_list)\n",
      "165/104:\n",
      "driver = webdriver.Chrome()\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-herren/_/N-1m7gkpt'\n",
      "base_sport = 'Zhpsxkd'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{base_code}'\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/105:\n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "165/106:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{base_code}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/107: len(sport_list)\n",
      "165/108:\n",
      "driver = webdriver.Chrome()\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-kinder/_/N-pavoh3'\n",
      "base_sport = 'Zhpsxkd'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{base_code}'\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/109:\n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "165/110:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{base_code}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/111: len(sport_list)\n",
      "165/112:\n",
      "driver = webdriver.Chrome()\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-accessoires/_/N-xx62mz'\n",
      "base_sport = 'Zhpsxkd'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{base_code}'\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/113:\n",
      "driver = webdriver.Chrome()\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-kinder/_/N-pavoh3'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_code}'\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/114:\n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "165/115:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_code}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/116: len(sport_list)\n",
      "165/117:\n",
      "driver = webdriver.Chrome()\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-kinder/_/N-pavoh3'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/118:\n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "165/119:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/120: len(sport_list)\n",
      "165/121:\n",
      "driver = webdriver.Chrome()\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/122:\n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "165/123:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/124: len(sport_list)\n",
      "165/125:\n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "165/126:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/127: len(sport_list)\n",
      "165/128:\n",
      "def getSportList(base_index, base_cat, sold_by, base_sport):\n",
      "    filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "    driver.get(filter_link)\n",
      "\n",
      "    #accept cookie popup\n",
      "    WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "    #get all buttons and click on them all (show more) by \n",
      "    buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "    buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "    #click all buttons\n",
      "    for button in buttons:\n",
      "        driver.execute_script(\"arguments[0].click();\", button)\n",
      "        #getting the soup\n",
      "        soup = bts(driver.page_source)\n",
      "    \n",
      "    # access all filters:\n",
      "    filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "    # access sport filter at the last position:\n",
      "    sports = filter[len(filter)-1]\n",
      "    \n",
      "    sport_list = []\n",
      "    sport_url = sports.find_all(\"a\")\n",
      "\n",
      "    #first item\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "    sport_code = base_sport\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "\n",
      "    for i in range(1, len(sport_url)):\n",
      "        sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "        sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "        sport_list.append({'sport_type': sport_type,\n",
      "                           'sport_code': sport_code})\n",
      "    return (sport_list)\n",
      "165/129:\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "sport_list = getSportList(base_index, base_cat, sold_by, base_sport)\n",
      "165/130:\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "sport_list = getSportList(base_index, base_cat, sold_by, base_sport)\n",
      "165/131:\n",
      "def getSportSoup(base_index, base_cat, sold_by, base_sport):\n",
      "    filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "    driver.get(filter_link)\n",
      "\n",
      "    #accept cookie popup\n",
      "    WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "    #get all buttons and click on them all (show more) by \n",
      "    buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "    buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "    #click all buttons\n",
      "    for button in buttons:\n",
      "        driver.execute_script(\"arguments[0].click();\", button)\n",
      "        #getting the soup\n",
      "        soup = bts(driver.page_source)\n",
      "    return soup\n",
      "165/132:\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "soup = getSportSoup(base_index, base_cat, sold_by, base_sport)\n",
      "165/133:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import re\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "165/134:\n",
      "def getSportSoup(base_index, base_cat, sold_by, base_sport):\n",
      "    filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "    driver.get(filter_link)\n",
      "\n",
      "    #accept cookie popup\n",
      "    WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "    #get all buttons and click on them all (show more) by \n",
      "    buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "    buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "    #click all buttons\n",
      "    for button in buttons:\n",
      "        driver.execute_script(\"arguments[0].click();\", button)\n",
      "        #getting the soup\n",
      "        soup = bts(driver.page_source)\n",
      "    return soup\n",
      "165/135:\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "soup = getSportSoup(base_index, base_cat, sold_by, base_sport)\n",
      "165/136:\n",
      "def getSportList(base_index, base_cat, sold_by, base_sport):\n",
      "    filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "    driver.get(filter_link)\n",
      "\n",
      "    #accept cookie popup\n",
      "    WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "    #get all buttons and click on them all (show more) by \n",
      "    buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "    buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "    #click all buttons\n",
      "    for button in buttons:\n",
      "        driver.execute_script(\"arguments[0].click();\", button)\n",
      "        #getting the soup\n",
      "        soup = bts(driver.page_source)\n",
      "    \n",
      "    # access all filters:\n",
      "    filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "    # access sport filter at the last position:\n",
      "    sports = filter[len(filter)-1]\n",
      "\n",
      "    sport_list = []\n",
      "    sport_url = sports.find_all(\"a\")\n",
      "\n",
      "    #first item\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "    sport_code = base_sport\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "\n",
      "    for i in range(1, len(sport_url)):\n",
      "        sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "        sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "        sport_list.append({'sport_type': sport_type,\n",
      "                           'sport_code': sport_code})\n",
      "    return (sport_list)\n",
      "165/137:\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "sport_list = getSportList(base_index, base_cat, sold_by, base_sport)\n",
      "165/138:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import re\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "165/139:\n",
      "def getSportSoup(base_index, base_cat, sold_by, base_sport):\n",
      "    filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "    driver.get(filter_link)\n",
      "\n",
      "    #accept cookie popup\n",
      "    WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "    #get all buttons and click on them all (show more) by \n",
      "    buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "    buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "    #click all buttons\n",
      "    for button in buttons:\n",
      "        driver.execute_script(\"arguments[0].click();\", button)\n",
      "        #getting the soup\n",
      "        soup = bts(driver.page_source)\n",
      "    \n",
      "    return soup\n",
      "165/140:\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "sport_soup = getSportSoup(base_index, base_cat, sold_by, base_sport)\n",
      "165/141:\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "soup = getSportSoup(base_index, base_cat, sold_by, base_sport)\n",
      "165/142:\n",
      "#input\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "    \n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "\n",
      "sport_list = []\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/143:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import re\n",
      "165/144:\n",
      "#input\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "driver = webdriver.Chrome()\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "    \n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "\n",
      "sport_list = []\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/145:\n",
      "#input\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "driver = webdriver.Chrome()\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "    \n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "\n",
      "sport_list = []\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/146: len(sport_list)\n",
      "165/147:\n",
      "# get category links\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "165/148:\n",
      "# get category links\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "\n",
      "categories[0]\n",
      "165/149:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "\n",
      "for cat in categories:\n",
      "    level_1 = cat.find(\"h1\").text\n",
      "    cat_name = cat.find(\"a\").text\n",
      "    cat_link = cat.find(\"a\").attrs[\"href\"]\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'cat_name': cat_name,\n",
      "                     'cat_link': cat_link})\n",
      "\n",
      "cat_list\n",
      "165/150:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "\n",
      "categories\n",
      "165/151:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "\n",
      "categories\n",
      "165/152:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "\n",
      "categories[1]\n",
      "165/153:\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    cat_name = cat.find(\"a\").text\n",
      "    cat_link = cat.find(\"a\").attrs[\"href\"]\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'cat_name': cat_name,\n",
      "                     'cat_link': cat_link})\n",
      "\n",
      "cat_list\n",
      "165/154:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "\n",
      "categories[1].find(\"a\")\n",
      "165/155:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "165/156: categories[0].find(\"a\")\n",
      "165/157: categories[0]\n",
      "165/158: categories[0].text\n",
      "165/159: categories[0].find('img')\n",
      "165/160: categories[0].find('img').attrs['alt']\n",
      "165/161: categories[0].find('a').attrs['href']\n",
      "165/162: categories[0]\n",
      "165/163: categories[0].find(\"href\")\n",
      "165/164: categories[0].find(\"a\")\n",
      "165/165: categories[0]\n",
      "165/166: categories[0].find(\"a\")\n",
      "165/167: categories[0].find(\"img\")\n",
      "165/168: categories[0].attrs[\"href\"]\n",
      "165/169:\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    cat_name = cat.find('img').attrs['alt']\n",
      "    cat_link = cat.attrs[\"href\"]\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'cat_name': cat_name,\n",
      "                     'cat_link': cat_link})\n",
      "\n",
      "cat_list\n",
      "165/170: categories[0].find('img').attrs['alt']\n",
      "165/171:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "165/172:\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    cat_name = cat.find('img').attrs['alt']\n",
      "    cat_link = cat.attrs[\"href\"]\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'cat_name': cat_name,\n",
      "                     'cat_link': cat_link})\n",
      "\n",
      "cat_list\n",
      "165/173: categories[7].find('img').attrs['alt']\n",
      "165/174: categories[6].find('img').attrs['alt']\n",
      "165/175: categories[5].find('img').attrs['alt']\n",
      "165/176: categories[4].find('img').attrs['alt']\n",
      "165/177: categories[3].find('img').attrs['alt']\n",
      "165/178: categories[2].find('img').attrs['alt']\n",
      "165/179: categories[1].find('img').attrs['alt']\n",
      "165/180: categories[2].find('img').attrs['alt']\n",
      "165/181: categories[2].text\n",
      "165/182: categories[0].text\n",
      "165/183: categories[1].text\n",
      "165/184: categories[2].text\n",
      "165/185: categories\n",
      "166/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import re\n",
      "166/2:\n",
      "#input\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "driver = webdriver.Chrome()\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "    \n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "\n",
      "sport_list = []\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "166/3:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import re\n",
      "166/4:\n",
      "#input\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "driver = webdriver.Chrome()\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "    \n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "\n",
      "sport_list = []\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "166/5:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "166/6: categories\n",
      "166/7: categories[0].attrs['data-help']\n",
      "166/8:\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    cat_name = cat.attrs['data-help']\n",
      "    cat_link = cat.attrs[\"href\"]\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'cat_name': cat_name,\n",
      "                     'cat_link': cat_link})\n",
      "\n",
      "cat_list\n",
      "166/9:\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    level_2 = cat.attrs['data-help']\n",
      "    cat_link = cat.attrs[\"href\"].replace[base_sport,\"\"]\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'cat_name': cat_name,\n",
      "                     'cat_link': cat_link})\n",
      "\n",
      "cat_list\n",
      "166/10:\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    level_2 = cat.attrs['data-help']\n",
      "    cat_link = cat.attrs[\"href\"].replace(base_sport,\"\")\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'cat_name': cat_name,\n",
      "                     'cat_link': cat_link})\n",
      "\n",
      "cat_list\n",
      "166/11:\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    level_2 = cat.attrs['data-help']\n",
      "    cat_link = cat.attrs[\"href\"].replace(f'{base_sport}',\"\")\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'cat_name': cat_name,\n",
      "                     'cat_link': cat_link})\n",
      "\n",
      "cat_list\n",
      "166/12:\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    level_2 = cat.attrs['data-help']\n",
      "    cat_link = cat.attrs[\"href\"].replace(f'{base_sport}',\"\")\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'cat_name': cat_name,\n",
      "                     'cat_link': cat_link})\n",
      "\n",
      "cat_list\n",
      "166/13:\n",
      "a = '/browse/c0-damen/c1-sportbekleidung/_/N-1peigq4Z1da2q0eZhpsxkd'\n",
      "\n",
      "a.replace(base_sport,\"\")\n",
      "166/14:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "166/15:\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    level_2 = cat.attrs['data-help']\n",
      "    cat_link = cat.attrs[\"href\"].replace(base_sport,\"\")\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'cat_name': cat_name,\n",
      "                     'cat_link': cat_link})\n",
      "\n",
      "cat_list\n",
      "166/16:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    level_2 = cat.attrs['data-help']\n",
      "    cat_link = cat.attrs[\"href\"].replace(base_sport,\"\")\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'cat_name': cat_name,\n",
      "                     'cat_link': cat_link})\n",
      "166/17: cat_list\n",
      "166/18:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    level_2 = cat.attrs['data-help']\n",
      "    cat_link = cat.attrs[\"href\"].replace(base_sport,\"\")\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'level_2': level_2,\n",
      "                     'cat_link': cat_link})\n",
      "166/19: cat_list\n",
      "166/20:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    level_2 = cat.attrs['data-help']\n",
      "    cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}''\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'level_2': level_2,\n",
      "                     'cat_link': cat_link})\n",
      "166/21:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    level_2 = cat.attrs['data-help']\n",
      "    cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'level_2': level_2,\n",
      "                     'cat_link': cat_link})\n",
      "166/22: cat_list\n",
      "166/23: len(cat_list)\n",
      "166/24:\n",
      "len(cat_list)\n",
      "len(sport_list)\n",
      "166/25:\n",
      "len(cat_list)\n",
      "len(sport_list)\n",
      "166/26: print(len(cat_list), len(sport_list))\n",
      "166/27:\n",
      "print(len(cat_list), len(sport_list))\n",
      "\n",
      "cat_list[0]\n",
      "166/28:\n",
      "print(len(cat_list), len(sport_list))\n",
      "\n",
      "cat_list[0]\n",
      "sport_list[0]\n",
      "166/29: cat_list[0]\n",
      "166/30: cat_list[0][level_1]\n",
      "166/31: cat_list[0](level_1)\n",
      "166/32: cat_list[0](\"level_1\")\n",
      "166/33: cat_list[0][\"level_1\"]\n",
      "166/34:\n",
      "#constructing links for categories level_2 + filter\n",
      "cat_level3=[]\n",
      "for cat in cat_list:\n",
      "    for sport in sport_list:\n",
      "        level_1 = cat[\"level_1\"]\n",
      "        level_2 = cat[\"level_2\"]\n",
      "        sport_type = sport[\"sport_type\"]\n",
      "        url = f'{cat['cat_link']}{sport['sport_code']}'\n",
      "        cat_level3.append({'level_1': level_1,\n",
      "                           'level_2': level_2,\n",
      "                           'sport_type': sport_type,\n",
      "                           'url': url})\n",
      "166/35: cat_list[0]\n",
      "166/36:\n",
      "#constructing links for categories level_2 + filter\n",
      "cat_level3=[]\n",
      "for cat in cat_list:\n",
      "    for sport in sport_list:\n",
      "        level_1 = cat[\"level_1\"]\n",
      "        level_2 = cat[\"level_2\"]\n",
      "        sport_type = sport[\"sport_type\"]\n",
      "        url = f'{cat[\"cat_link\"]}{sport[\"sport_code\"]}'\n",
      "        cat_level3.append({'level_1': level_1,\n",
      "                           'level_2': level_2,\n",
      "                           'sport_type': sport_type,\n",
      "                           'url': url})\n",
      "166/37:\n",
      "#constructing links for categories level_2 + filter\n",
      "cat_level3=[]\n",
      "for cat in cat_list:\n",
      "    for sport in sport_list:\n",
      "        level_1 = cat[\"level_1\"]\n",
      "        level_2 = cat[\"level_2\"]\n",
      "        sport_type = sport[\"sport_type\"]\n",
      "        url = f'{cat[\"cat_link\"]}{sport[\"sport_code\"]}'\n",
      "        cat_level3.append({'level_1': level_1,\n",
      "                           'level_2': level_2,\n",
      "                           'sport_type': sport_type,\n",
      "                           'url': url})\n",
      "166/38: cat_level3[0]\n",
      "166/39: len(cat_level3)\n",
      "166/40:\n",
      "cat_list= []\n",
      "\n",
      "cat_list = getCatlink(soup, cat_list)\n",
      "166/41:\n",
      "# Function to get category links\n",
      "def getCatLink(soup, cat_list):\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    return cat_list\n",
      "166/42:\n",
      "cat_list= []\n",
      "\n",
      "cat_list = getCatlink(soup, cat_list)\n",
      "166/43:\n",
      "# Function to get category links\n",
      "def getCatLink(soup, cat_list):\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    return cat_list\n",
      "166/44:\n",
      "# Function to get category links\n",
      "def getCatLink(soup, cat_list):\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    return cat_list\n",
      "166/45:\n",
      "cat_list= []\n",
      "\n",
      "cat_list = getCatLink(soup, cat_list)\n",
      "166/46:\n",
      "print(len(cat_list), len(sport_list))\n",
      "\n",
      "cat_list[0]\n",
      "166/47:\n",
      "# Function to get category links\n",
      "def getCatLink(base_index, base_cat, cat_list):\n",
      "    url = f'{base_index}{base_cat}'\n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    return cat_list\n",
      "166/48:\n",
      "# Importing category_url data\n",
      "cat_url = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_url.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_url.append(cat)\n",
      "166/49:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import re\n",
      "import csv\n",
      "166/50:\n",
      "# Importing category_url data\n",
      "cat_url = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_url.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_url.append(cat)\n",
      "166/51: cat_url[0]\n",
      "166/52:\n",
      "#getting category links for all genders:\n",
      "cat_list =[]\n",
      "for i in cat_url:\n",
      "    base_index = i[\"base_index\"]\n",
      "    base_cat = i[\"base_cat\"]\n",
      "    cat_list = getCatLink(base_index, base_cat, cat_list)\n",
      "166/53:\n",
      "# Function to get category links\n",
      "def getCatLink(base_index, base_cat, cat_list):\n",
      "    url = f'{base_index}{base_cat}'\n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    return cat_list\n",
      "166/54:\n",
      "#getting category links for all genders:\n",
      "cat_list =[]\n",
      "for i in cat_url:\n",
      "    base_index = i[\"base_index\"]\n",
      "    base_cat = i[\"base_cat\"]\n",
      "    cat_list = getCatLink(base_index, base_cat, cat_list)\n",
      "166/55: cat_list[0]\n",
      "166/56: len(cat_list)\n",
      "166/57: cat_list\n",
      "166/58:\n",
      "#combine link for category level 2 and filtered by sport\n",
      "\n",
      "cat_level3=[]\n",
      "\n",
      "for cat in cat_list:\n",
      "    for sport in sport_list:\n",
      "        level_1 = cat[\"level_1\"]\n",
      "        level_2 = cat[\"level_2\"]\n",
      "        sport_type = sport[\"sport_type\"]\n",
      "        url = f'{cat[\"cat_link\"]}{sport[\"sport_code\"]}'\n",
      "        cat_level3.append({'level_1': level_1,\n",
      "                           'level_2': level_2,\n",
      "                           'sport_type': sport_type,\n",
      "                           'url': url})\n",
      "166/59: len(cat_level3)\n",
      "166/60: cat_level3[]\n",
      "166/61: cat_level3[0]\n",
      "166/62:\n",
      "#saving cat_level3\n",
      "\n",
      "with open(\"DE_cat_level3.csv\", \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"cat_l1\", \"cat_l2\", \"sport_type\", \"url\"])\n",
      "\n",
      "    for item in cat_level3:\n",
      "        writer.writerow([item['level_1'], item['level_2'], item['sport_type'], item['url']])\n",
      "166/63:\n",
      "# Function to get category links\n",
      "def getCatLink(base_index, base_cat, sold_by, cat_list):\n",
      "    url = f'{base_index}{base_cat}{sold_by}'\n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    return cat_list\n",
      "166/64:\n",
      "# Importing category_url data\n",
      "cat_url = []  \n",
      "# opening the file using \"with\" statement\n",
      "with open('cat_url.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_url.append(cat)\n",
      "166/65:\n",
      "#getting category links for all genders:\n",
      "cat_list =[]\n",
      "for i in cat_url:\n",
      "    base_index = i[\"base_index\"]\n",
      "    base_cat = i[\"base_cat\"]\n",
      "    cat_list = getCatLink(base_index, base_cat, sold_by, cat_list)\n",
      "166/66:\n",
      "# Importing category_url data\n",
      "cat_url = []  \n",
      "# opening the file using \"with\" statement\n",
      "with open('cat_url.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_url.append(cat)\n",
      "166/67:\n",
      "#getting category links for all genders:\n",
      "cat_list =[]\n",
      "for i in cat_url:\n",
      "    base_index = i[\"base_index\"]\n",
      "    base_cat = i[\"base_cat\"]\n",
      "    sold_by = i[\"sold_by\"]\n",
      "    cat_list = getCatLink(base_index, base_cat, sold_by, cat_list)\n",
      "166/68: cat_list\n",
      "166/69:\n",
      "#combine link for category level 2 and filtered by sport\n",
      "\n",
      "cat_level3=[]\n",
      "\n",
      "for cat in cat_list:\n",
      "    for sport in sport_list:\n",
      "        level_1 = cat[\"level_1\"]\n",
      "        level_2 = cat[\"level_2\"]\n",
      "        sport_type = sport[\"sport_type\"]\n",
      "        url = f'{cat[\"cat_link\"]}{sport[\"sport_code\"]}'\n",
      "        cat_level3.append({'level_1': level_1,\n",
      "                           'level_2': level_2,\n",
      "                           'sport_type': sport_type,\n",
      "                           'url': url})\n",
      "166/70: cat_level3[0]\n",
      "166/71:\n",
      "#saving cat_level3\n",
      "\n",
      "with open(\"DE_cat_level3.csv\", \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"cat_l1\", \"cat_l2\", \"sport_type\", \"url\"])\n",
      "\n",
      "    for item in cat_level3:\n",
      "        writer.writerow([item['level_1'], item['level_2'], item['sport_type'], item['url']])\n",
      "166/72:\n",
      "#saving cat_level3\n",
      "\n",
      "with open(\"DE_cat_level3.csv\", \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"cat_l1\", \"cat_l2\", \"sport_type\", \"url\"])\n",
      "\n",
      "    for item in cat_level3:\n",
      "        writer.writerow([item['level_1'], item['level_2'], item['sport_type'], item['url']])\n",
      "   1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import re\n",
      "import csv\n",
      "   2:\n",
      "#input\n",
      "country = 'DE'\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "driver = webdriver.Chrome()\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "    \n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "\n",
      "sport_list = []\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "   3:\n",
      "# Function to get category links\n",
      "def getCatLink(base_index, base_cat, sold_by, cat_list):\n",
      "    url = f'{base_index}{base_cat}{sold_by}'\n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    return cat_list\n",
      "   4:\n",
      "# Importing category_url data\n",
      "cat_url = []  \n",
      "# opening the file using \"with\" statement\n",
      "with open('cat_url.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_url.append(cat)\n",
      "   5:\n",
      "#getting category links for all genders:\n",
      "cat_list =[]\n",
      "for i in cat_url:\n",
      "    base_index = i[\"base_index\"]\n",
      "    base_cat = i[\"base_cat\"]\n",
      "    sold_by = i[\"sold_by\"]\n",
      "    cat_list = getCatLink(base_index, base_cat, sold_by, cat_list)\n",
      "   6: cat_list\n",
      "   7:\n",
      "#combine link for category level 2 and filtered by sport\n",
      "\n",
      "cat_level3=[]\n",
      "\n",
      "for cat in cat_list:\n",
      "    for sport in sport_list:\n",
      "        country = country\n",
      "        level_1 = cat[\"level_1\"]\n",
      "        level_2 = cat[\"level_2\"]\n",
      "        sport_type = sport[\"sport_type\"]\n",
      "        url = f'{cat[\"cat_link\"]}{sport[\"sport_code\"]}'\n",
      "        cat_level3.append({'country': country,\n",
      "                           'level_1': level_1,\n",
      "                           'level_2': level_2,\n",
      "                           'sport_type': sport_type,\n",
      "                           'url': url})\n",
      "   8: cat_level3[0]\n",
      "   9:\n",
      "#saving cat_level3\n",
      "\n",
      "with open(\"DE_cat_level3.csv\", \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"country\",\"cat_l1\", \"cat_l2\", \"sport_type\", \"url\"])\n",
      "\n",
      "    for item in cat_level3:\n",
      "        writer.writerow([item['country'], item['level_1'], item['level_2'], item['sport_type'], item['url']])\n",
      "  10: len(cat_level3)\n",
      "  11:\n",
      "#saving cat_level3\n",
      "\n",
      "with open(\"DE_cat_level3.csv\", \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"country\",\"cat_level1\", \"cat_level2\", \"sport_type\", \"cat_url\"])\n",
      "\n",
      "    for item in cat_level3:\n",
      "        writer.writerow([item['country'], item['level_1'], item['level_2'], item['sport_type'], item['url']])\n",
      "  12:\n",
      "#saving cat_level3\n",
      "\n",
      "with open(f\"{country}_cat_level3.csv\", \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"country\",\"cat_level1\", \"cat_level2\", \"sport_type\", \"cat_url\"])\n",
      "\n",
      "    for item in cat_level3:\n",
      "        writer.writerow([item['country'], item['level_1'], item['level_2'], item['sport_type'], item['url']])\n",
      "  13:\n",
      "#Function for parsing the URLs\n",
      "def cookSoup(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "  14:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(soup, cat_url, country, l1, l2, l3, total_page_src, per_page):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":f\"plp-bar-info svelte-{total_page_src}\"}).find(\"span\", {\"class\":f\"svelte-{total_page_src}\"}).text\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/per_page)\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{cat_url}?from={per_page * i}&size={per_page}'\n",
      "            url_list.append(page)\n",
      "    print (f'{country}_{l1}_{l2}_{l3}: There are {total_prod} products ({total_page} pages)')\n",
      "    return url_list\n",
      "  15:\n",
      "# Function to get main data\n",
      "def getDecathlonData(base_index, url_list, country, l1, l2, l3, prod_list, prod_list_src, sticker_src):\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "        \n",
      "        for product in prod:\n",
      "            cat1 = l1\n",
      "            cat2 = l2\n",
      "            sport = l3\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "            prod_url = f'{base_index}{link}'\n",
      "\n",
      "            #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #Prices:\n",
      "            #for product without discount\n",
      "            if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "\n",
      "            #for product with discount\n",
      "            else:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "            #label:\n",
      "            if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'cat_1' : cat1,\n",
      "                             'cat_2' : cat2,\n",
      "                             'cat_3' : cat3})\n",
      "\n",
      "    print(f'{country}_{l1}_{l2}_{l3}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      "  16:\n",
      "# country input\n",
      "country = 'DE'\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "prod_list_src = 'd5pqm'\n",
      "sticker_src = '15lojui'\n",
      "total_page_src = '1uqvrhu'\n",
      "per_page = 40\n",
      "  17:\n",
      "#combine link for category level 2 and filtered by sport\n",
      "\n",
      "cat_level3=[]\n",
      "\n",
      "for cat in cat_list:\n",
      "    for sport in sport_list:\n",
      "        country = country\n",
      "        level_1 = cat[\"level_1\"]\n",
      "        level_2 = cat[\"level_2\"]\n",
      "        sport_type = sport[\"sport_type\"]\n",
      "        url = f'{cat[\"cat_link\"]}{sport[\"sport_code\"]}'\n",
      "        cat_level3.append({'country': country,\n",
      "                           'cat1': level_1,\n",
      "                           'cat2': level_2,\n",
      "                           'cat3': sport_type,\n",
      "                           'cat_url': url})\n",
      "  18: len(cat_level3)\n",
      "  19:\n",
      "#saving cat_level3\n",
      "\n",
      "with open(f\"{country}_cat_level3.csv\", \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"country\",\"cat1\", \"cat2\", \"cat3\", \"cat_url\"])\n",
      "\n",
      "    for item in cat_level3:\n",
      "        writer.writerow([item['country'], item['cat1'], item['cat2'], item['cat3'], item['cat_url']])\n",
      "  20: cat_level3[1]\n",
      "  21:\n",
      "prod_list = []\n",
      "\n",
      "for item in cat_level3:\n",
      "    country = item[\"country\"]\n",
      "    cat_url = item[\"cat_url\"]\n",
      "    cat1 = item[\"cat1\"]\n",
      "    cat2 = item[\"cat2\"]\n",
      "    cat3 = item[\"cat3\"]\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page)\n",
      "    prod_list = getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src)\n",
      "  22:\n",
      "country = cat_level3[0][\"country\"]\n",
      "cat_url = cat_level3[0][\"cat_url\"]\n",
      "cat1 = cat_level3[0][\"cat1\"]\n",
      "cat2 = cat_level3[0][\"cat2\"]\n",
      "cat3 = cat_level3[0][\"cat3\"]\n",
      "soup = cookSoup(cat_url)\n",
      "  23: soup\n",
      "  24: url_list = pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page)\n",
      "  25: prod_list = getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src)\n",
      "  26: url_list = pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page)\n",
      "  27:\n",
      "for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "        \n",
      "        for product in prod:\n",
      "            cat1 = cat1\n",
      "            cat2 = cat2\n",
      "            sport = cat3\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "            prod_url = f'{base_index}{link}'\n",
      "\n",
      "            #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #Prices:\n",
      "            #for product without discount\n",
      "            if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "\n",
      "            #for product with discount\n",
      "            else:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "            #label:\n",
      "            if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'cat_1' : cat1,\n",
      "                             'cat_2' : cat2,\n",
      "                             'cat_3' : cat3})\n",
      "\n",
      "    print(f'{country}_{l1}_{l2}_{l3}: {len(prod_list)} products have been scraped!')\n",
      "  29: url_list\n",
      "  30:\n",
      "for url in url_list:\n",
      "    page_soup = cookSoup(url)\n",
      "    prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "\n",
      "    for product in prod:\n",
      "        cat1 = cat1\n",
      "        cat2 = cat2\n",
      "        sport = cat3\n",
      "        link = product.find(\"a\").attrs[\"href\"]\n",
      "        prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "        brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "        prod_url = f'{base_index}{link}'\n",
      "\n",
      "        #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "        #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "        if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "            prod_id = link.partition(\"?mc=\")[2]\n",
      "        else:\n",
      "            prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "        #Prices:\n",
      "        #for product without discount\n",
      "        if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "            reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "            act_price = None\n",
      "\n",
      "        #for product with discount\n",
      "        else:\n",
      "            reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "            act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "        #label:\n",
      "        if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "            prod_sticker = None\n",
      "        else:\n",
      "            prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "        prod_list.append({'title': prod_title,\n",
      "                         'sku': prod_id,\n",
      "                         'regular price': reg_price,\n",
      "                         'actual price' : act_price,\n",
      "                         'brand': brand_name,\n",
      "                         'url' : prod_url,\n",
      "                         'sticker' : prod_sticker,\n",
      "                         'cat_1' : cat1,\n",
      "                         'cat_2' : cat2,\n",
      "                         'cat_3' : cat3})\n",
      "\n",
      "    print(f'{country}_{l1}_{l2}_{l3}: {len(prod_list)} products have been scraped!')\n",
      "  31:\n",
      "for url in url_list:\n",
      "    page_soup = cookSoup(url)\n",
      "    prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "\n",
      "    for product in prod:\n",
      "        cat1 = cat1\n",
      "        cat2 = cat2\n",
      "        sport = cat3\n",
      "        link = product.find(\"a\").attrs[\"href\"]\n",
      "        prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "        brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "        prod_url = f'{base_index}{link}'\n",
      "\n",
      "        #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "        #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "        if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "            prod_id = link.partition(\"?mc=\")[2]\n",
      "        else:\n",
      "            prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "        #Prices:\n",
      "        #for product without discount\n",
      "        if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "            reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "            act_price = None\n",
      "\n",
      "        #for product with discount\n",
      "        else:\n",
      "            reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "            act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "        #label:\n",
      "        if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "            prod_sticker = None\n",
      "        else:\n",
      "            prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "        prod_list.append({'title': prod_title,\n",
      "                         'sku': prod_id,\n",
      "                         'regular price': reg_price,\n",
      "                         'actual price' : act_price,\n",
      "                         'brand': brand_name,\n",
      "                         'url' : prod_url,\n",
      "                         'sticker' : prod_sticker,\n",
      "                         'cat_1' : cat1,\n",
      "                         'cat_2' : cat2,\n",
      "                         'cat_3' : cat3})\n",
      "\n",
      "    print(f'{country}_{cat1}_{cat2}_{cat3}: {len(prod_list)} products have been scraped!')\n",
      "  32:\n",
      "page_soup = cookSoup(url_list[0])\n",
      "prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "  33:\n",
      "page_soup = cookSoup(url_list[0])\n",
      "prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "prod\n",
      "  34:\n",
      "# country input\n",
      "country = 'DE'\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "prod_list_src = 'd5pqmr'\n",
      "sticker_src = '15lojui'\n",
      "total_page_src = '1uqvrhu'\n",
      "per_page = 40\n",
      "  35:\n",
      "page_soup = cookSoup(url_list[0])\n",
      "prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "prod\n",
      "  36:\n",
      "prod_list = []\n",
      "\n",
      "for item in cat_level3:\n",
      "    country = item[\"country\"]\n",
      "    cat_url = item[\"cat_url\"]\n",
      "    cat1 = item[\"cat1\"]\n",
      "    cat2 = item[\"cat2\"]\n",
      "    cat3 = item[\"cat3\"]\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page)\n",
      "    prod_list = getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src)\n",
      "  37:\n",
      "prod_list = []\n",
      "\n",
      "for item in cat_level3:\n",
      "    country = item[\"country\"]\n",
      "    cat_url = item[\"cat_url\"]\n",
      "    cat1 = item[\"cat1\"]\n",
      "    cat2 = item[\"cat2\"]\n",
      "    cat3 = item[\"cat3\"]\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page)\n",
      "    prod_list = getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src)\n",
      "  38:\n",
      "page_soup = cookSoup('https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung/_/N-1peigq4Z1da2q0eZexlast')\n",
      "prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "  39: prod\n",
      "  40: prod[0].find(\"span\", {\"class\":\"prc__previous\"})\n",
      "  41: prod[0].find(\"span\", {\"class\":\"prc__info-addon\"})\n",
      "  42: prod[0]\n",
      "  43: len(prod)\n",
      "  44: prod[0]\n",
      "  45: prod[1]\n",
      "  46:\n",
      "page_soup = cookSoup('https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung/_/N-1peigq4Z1da2q0eZexlast')\n",
      "prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-d5pqmr\")\n",
      "  47: prod[0]\n",
      "  48: prod[0].find(\"span\", {\"class\":\"prc__info-addon\"})\n",
      "  49: prod[0].find(\"div\", {\"class\":\"prc__active-price\"})\n",
      "  50: prod[1].find(\"div\", {\"class\":\"prc__active-price\"})\n",
      "  51: prod[113].find(\"div\", {\"class\":\"prc__active-price\"})\n",
      "  52: prod[112].find(\"div\", {\"class\":\"prc__active-price\"})\n",
      "  53: prod[111]\n",
      "  54: len(prod)\n",
      "  55: prod[40]\n",
      "  56: prod[39]\n",
      "  57: prod[39].find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "  58: prod[0].find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "  59:\n",
      "for product in prod:\n",
      "    print(product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text)\n",
      "  60:\n",
      "for product in prod:\n",
      "    print(product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text)\n",
      "    print(product.find(\"div\", {\"class\":\"prc__active-price\"}))\n",
      "  61:\n",
      "for product in prod:\n",
      "    print(product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text)\n",
      "    print(product.find(\"div\", {\"class\":\"prc__active-price\"}).text)\n",
      "  62:\n",
      "for product in prod:\n",
      "    print(product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text)\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        print(f'no discount {product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')}')\n",
      "    else:\n",
      "        print(f'with discount {product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')}')\n",
      "  63:\n",
      "for product in prod:\n",
      "    print(product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text)\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        print(f\"no discount: {product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')}\")\n",
      "    else:\n",
      "        print(f\"with discount: {product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')}\")\n",
      "  64:\n",
      "for product in prod:\n",
      "    print(product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text)\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        print(\"no discount\")\n",
      "        print(product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*',''))\n",
      "    else:\n",
      "        print(\"with discount\")\n",
      "        print(product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')})\n",
      "  65:\n",
      "for product in prod:\n",
      "    print(product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text)\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        print(\"no discount\")\n",
      "        print(product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*',''))\n",
      "    else:\n",
      "        print(\"with discount\")\n",
      "        print(product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*',''))\n",
      "  66:\n",
      "for product in prod:\n",
      "    cat1 = cat1\n",
      "    cat2 = cat2\n",
      "    sport = cat3\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "    prod_url = f'{base_index}{link}'\n",
      "\n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "\n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'cat_1' : cat1,\n",
      "                     'cat_2' : cat2,\n",
      "                     'cat_3' : cat3})\n",
      "  67: len(prod_list)\n",
      "  68:\n",
      "page_soup = cookSoup('https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung/_/N-1peigq4Z1da2q0eZexlast')\n",
      "prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "  69:\n",
      "page_soup = cookSoup('https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung/_/N-1peigq4Z1da2q0eZexlast')\n",
      "prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "  70: len(prod)\n",
      "  71:\n",
      "for product in prod:\n",
      "    print(product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text)\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        print(\"no discount\")\n",
      "        print(product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*',''))\n",
      "    else:\n",
      "        print(\"with discount\")\n",
      "        print(product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*',''))\n",
      "  72:\n",
      "for product in prod:\n",
      "    cat1 = cat1\n",
      "    cat2 = cat2\n",
      "    sport = cat3\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "    prod_url = f'{base_index}{link}'\n",
      "\n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "\n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'cat_1' : cat1,\n",
      "                     'cat_2' : cat2,\n",
      "                     'cat_3' : cat3})\n",
      "  73: len(prod_list)\n",
      "  74:\n",
      "prod_list = []\n",
      "for product in prod:\n",
      "    cat1 = cat1\n",
      "    cat2 = cat2\n",
      "    sport = cat3\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "    prod_url = f'{base_index}{link}'\n",
      "\n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "\n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'cat_1' : cat1,\n",
      "                     'cat_2' : cat2,\n",
      "                     'cat_3' : cat3})\n",
      "  75: len(prod_list)\n",
      "  76: len(prod_list)\n",
      "  77: prod_list[0]\n",
      "  78:\n",
      "prod_list = []\n",
      "\n",
      "for item in cat_level3:\n",
      "    country = item[\"country\"]\n",
      "    cat_url = item[\"cat_url\"]\n",
      "    cat1 = item[\"cat1\"]\n",
      "    cat2 = item[\"cat2\"]\n",
      "    cat3 = item[\"cat3\"]\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page)\n",
      "    prod_list = getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src)\n",
      "  79:\n",
      "page_soup = cookSoup('https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung/_/N-1peigq4Z1da2q0eZexlast?from=40&size=40')\n",
      "prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "  80: len(prod)\n",
      "  81:\n",
      "prod_list = []\n",
      "for product in prod:\n",
      "    cat1 = cat1\n",
      "    cat2 = cat2\n",
      "    sport = cat3\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "    prod_url = f'{base_index}{link}'\n",
      "\n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "\n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'cat_1' : cat1,\n",
      "                     'cat_2' : cat2,\n",
      "                     'cat_3' : cat3})\n",
      "  82: len(prod_list)\n",
      "  83: prod_list[0]\n",
      "  84:\n",
      "page_soup = cookSoup('https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung/_/N-1peigq4Z1da2q0eZexlast?from=80&size=40')\n",
      "prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "  85: len(prod)\n",
      "  86:\n",
      "prod_list = []\n",
      "for product in prod:\n",
      "    cat1 = cat1\n",
      "    cat2 = cat2\n",
      "    sport = cat3\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "    prod_url = f'{base_index}{link}'\n",
      "\n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "\n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'cat_1' : cat1,\n",
      "                     'cat_2' : cat2,\n",
      "                     'cat_3' : cat3})\n",
      "  87: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
