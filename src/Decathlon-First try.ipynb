{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "472e20ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up\n",
    "from bs4 import BeautifulSoup as bts\n",
    "import pandas as pd\n",
    "import requests\n",
    "import math #for rounding numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3374fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for parsing the URLs\n",
    "def getAndParseURL(url): \n",
    "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
    "    soup = bts(result.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3472edbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DE - Woman sportwear\n",
    "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
    "\n",
    "de1 = getAndParseURL(url)\n",
    "\n",
    "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc496971",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_base = \"https://www.decathlon.de\"\n",
    "de1_list = []\n",
    "\n",
    "for product in prod:\n",
    "    link = product.find(\"a\").attrs[\"href\"]\n",
    "    prod_cat = de1.find(\"h1\").text\n",
    "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
    "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
    "    prod_url = f'{de_base}{link}'\n",
    "    \n",
    "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
    "    #taking sku's even in case of more than 7 character id's:\n",
    "    \n",
    "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
    "        prod_id = link.partition(\"?mc=\")[2]\n",
    "    else:\n",
    "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
    "        \n",
    "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
    "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
    "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
    "    else:\n",
    "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
    "        act_price = None\n",
    "    \n",
    "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
    "        prod_sticker = None\n",
    "    else:\n",
    "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
    "    \n",
    "    de1_list.append({'title': prod_title,\n",
    "                     'sku': prod_id,\n",
    "                     'regular price': reg_price,\n",
    "                     'actual price' : act_price,\n",
    "                     'brand': brand_name,\n",
    "                     'url' : prod_url,\n",
    "                     'sticker' : prod_sticker,\n",
    "                     'category' : prod_cat})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc5d2af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'T-Shirt Fitness Adidas Linear 100\\xa0% Baumwolle Rundhals Damen schwarz ',\n",
       "  'sku': '8624220',\n",
       "  'regular price': '19,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'ADIDAS',\n",
       "  'url': 'https://www.decathlon.de/p/t-shirt-fitness-adidas-linear-100-baumwolle-rundhals-damen-schwarz/_/R-p-X8624220?mc=8624220',\n",
       "  'sticker': None,\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Strumpfhose Tanzleggings ohne Fuß Mädchen schwarz ',\n",
       "  'sku': '8608538',\n",
       "  'regular price': '7,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'STAREVER',\n",
       "  'url': 'https://www.decathlon.de/p/strumpfhose-tanzleggings-ohne-fuß-madchen/_/R-p-329525?mc=8608538&c=SCHWARZ',\n",
       "  'sticker': 'NEU',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Wanderhose Bergwandern MH500 Damen schwarz',\n",
       "  'sku': '8493683',\n",
       "  'regular price': '29,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'QUECHUA',\n",
       "  'url': 'https://www.decathlon.de/p/wanderhose-bergwandern-mh500-damen/_/R-p-192393?mc=8493683&c=SCHWARZ',\n",
       "  'sticker': None,\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Wandershorts Naturwandern NH500 Regular Damen khaki',\n",
       "  'sku': '8582433',\n",
       "  'regular price': '14,99€',\n",
       "  'actual price': '9,99€',\n",
       "  'brand': 'QUECHUA',\n",
       "  'url': 'https://www.decathlon.de/p/wandershorts-naturwandern-nh500-regular-damen/_/R-p-324589?mc=8582433&c=GRÜN_KHAKI',\n",
       "  'sticker': 'ECODESIGN',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Laufshirt Run Dry kurzarm atmungsaktiv Damen weiss',\n",
       "  'sku': '8543970',\n",
       "  'regular price': '4,99€',\n",
       "  'actual price': '3,99€',\n",
       "  'brand': 'KALENJI',\n",
       "  'url': 'https://www.decathlon.de/p/laufshirt-kurzarm-run-dry-damen/_/R-p-302707?mc=8543970&c=WEIß',\n",
       "  'sticker': 'ECODESIGN',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Sport-BH Bustier Basic starker Halt',\n",
       "  'sku': '8563080',\n",
       "  'regular price': '4,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'KALENJI',\n",
       "  'url': 'https://www.decathlon.de/p/sport-bh-bustier-basic-starker-halt/_/R-p-310705?mc=8563080&c=WEIß',\n",
       "  'sticker': None,\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Lauftop atmungsaktiv Kiprun Care Damen grau/blau',\n",
       "  'sku': '8604399',\n",
       "  'regular price': '9,99€',\n",
       "  'actual price': '4,99€',\n",
       "  'brand': 'KIPRUN',\n",
       "  'url': 'https://www.decathlon.de/p/lauftop-kiprun-care-damen/_/R-p-301240?mc=8604399&c=GRAU_BLAU',\n",
       "  'sticker': '%Endspurt%',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'T-Shirt Slim Fitness',\n",
       "  'sku': '8380104',\n",
       "  'regular price': '4,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'DOMYOS',\n",
       "  'url': 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ',\n",
       "  'sticker': None,\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Fleeceweste Bergwandern MH120 Damen schwarz',\n",
       "  'sku': '8072128',\n",
       "  'regular price': '11,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'QUECHUA',\n",
       "  'url': 'https://www.decathlon.de/p/fleeceweste-bergwandern-mh120-damen/_/R-p-12490?mc=8072128&c=SCHWARZ',\n",
       "  'sticker': 'ECODESIGN',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Laufshirt langarm atmungsaktiv Kiprun Care Damen schwarz',\n",
       "  'sku': '8665686',\n",
       "  'regular price': '19,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'KIPRUN',\n",
       "  'url': 'https://www.decathlon.de/p/laufshirt-langarm-kiprun-care-damen/_/R-p-311796?mc=8665686&c=SCHWARZ',\n",
       "  'sticker': None,\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Wanderhose Bergwandern MH100 Damen dunkelgrau',\n",
       "  'sku': '8493678',\n",
       "  'regular price': '17,99€',\n",
       "  'actual price': '12,99€',\n",
       "  'brand': 'QUECHUA',\n",
       "  'url': 'https://www.decathlon.de/p/wanderhose-bergwandern-mh100-damen/_/R-p-192391?mc=8493678&c=GRAU',\n",
       "  'sticker': '%Endspurt%',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Wandertop Bergwandern MH500 Damen koralle',\n",
       "  'sku': '8612446',\n",
       "  'regular price': '7,99€',\n",
       "  'actual price': '4,99€',\n",
       "  'brand': 'QUECHUA',\n",
       "  'url': 'https://www.decathlon.de/p/wandertop-bergwandern-mh500-damen/_/R-p-308269?mc=8612446&c=GRÜN',\n",
       "  'sticker': 'ECODESIGN',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Radtrikot kurzarm MTB ST 100 Damen marineblau',\n",
       "  'sku': '8555872',\n",
       "  'regular price': '12,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'ROCKRIDER',\n",
       "  'url': 'https://www.decathlon.de/p/radtrikot-kurzarm-mtb-st-100-damen/_/R-p-305571?mc=8555872&c=BLAU',\n",
       "  'sticker': 'ECODESIGN',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Reit-Poloshirt Kurzam 500 Turnier Damen weiß',\n",
       "  'sku': '8485650',\n",
       "  'regular price': '19,99€',\n",
       "  'actual price': '11,99€',\n",
       "  'brand': 'FOUGANZA',\n",
       "  'url': 'https://www.decathlon.de/p/reit-poloshirt-kurzam-500-turnier-damen-weiß/_/R-p-177591?mc=8485650&c=WEIß',\n",
       "  'sticker': '%Endspurt%',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Tanz-Leggings Modern Dance nahtlos Damen schwarz',\n",
       "  'sku': '8357254',\n",
       "  'regular price': '11,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'STAREVER',\n",
       "  'url': 'https://www.decathlon.de/p/tanz-leggings-modern-dance-nahtlos-damen-schwarz/_/R-p-155997?mc=8357254&c=SCHWARZ',\n",
       "  'sticker': None,\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Wandershorts Naturwandern NH100 Damen marineblau ',\n",
       "  'sku': '8493780',\n",
       "  'regular price': '14,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'QUECHUA',\n",
       "  'url': 'https://www.decathlon.de/p/wandershorts-naturwandern-nh100-damen/_/R-p-193173?mc=8493780&c=BLAU',\n",
       "  'sticker': None,\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Langarmshirt Pullover Wandern SH100 warm Damen schwarz',\n",
       "  'sku': '8370112',\n",
       "  'regular price': '9,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'QUECHUA',\n",
       "  'url': 'https://www.decathlon.de/p/langarmshirt-pullover-wandern-sh100-warm-damen/_/R-p-11941?mc=8370112&c=SCHWARZ',\n",
       "  'sticker': None,\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Workout Ready High-Rise Shorts',\n",
       "  'sku': '5a0098a8-00f9-4b96-82c1-b56df5145310_c1',\n",
       "  'regular price': '24,95€',\n",
       "  'actual price': None,\n",
       "  'brand': 'REEBOK',\n",
       "  'url': 'https://www.decathlon.de/p/mp/reebok/workout-ready-high-rise-shorts/_/R-p-5a0098a8-00f9-4b96-82c1-b56df5145310?mc=5a0098a8-00f9-4b96-82c1-b56df5145310_c1&c=SCHWARZ',\n",
       "  'sticker': None,\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Fleecejacke Segeln Sailing 100 warm Ecodesign Damen schwarz',\n",
       "  'sku': '8643501',\n",
       "  'regular price': '19,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'TRIBORD',\n",
       "  'url': 'https://www.decathlon.de/p/fleecejacke-segeln-sailing-100-warm-ecodesign-damen/_/R-p-332449?mc=8643501',\n",
       "  'sticker': 'Ecodesign',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Tennis-Shorts Damen DRY 500 weiss',\n",
       "  'sku': '8547788',\n",
       "  'regular price': '14,99€',\n",
       "  'actual price': '6,99€',\n",
       "  'brand': 'ARTENGO',\n",
       "  'url': 'https://www.decathlon.de/p/tennis-shorts-damen-dry-500/_/R-p-305971?mc=8547788&c=WEIß',\n",
       "  'sticker': '%Endspurt%',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Top Puma Fitness Baumwolle Damen weiss ',\n",
       "  'sku': '8742702',\n",
       "  'regular price': '19,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'PUMA',\n",
       "  'url': 'https://www.decathlon.de/p/top-puma-fitness-baumwolle-damen-weiss/_/R-p-X8742702?mc=8742702',\n",
       "  'sticker': 'NEU',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Golf Daunenweste MW500 Damen grau',\n",
       "  'sku': '8605593',\n",
       "  'regular price': '44,99€',\n",
       "  'actual price': '29,99€',\n",
       "  'brand': 'INESIS',\n",
       "  'url': 'https://www.decathlon.de/p/golf-daunenweste-mw500-damen/_/R-p-328754?mc=8605593&c=GRAU',\n",
       "  'sticker': '%Endspurt%',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Lauftop Run Dry Damen pastellgrün',\n",
       "  'sku': '8520985',\n",
       "  'regular price': '3,99€',\n",
       "  'actual price': '2,99€',\n",
       "  'brand': 'KALENJI',\n",
       "  'url': 'https://www.decathlon.de/p/lauftop-run-dry-damen-pastellgrun/_/R-p-302699?mc=8520985&c=GRÜN',\n",
       "  'sticker': 'ECODESIGN',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Kurze Radhose Rennrad RC 500 Damen schwarz/pink',\n",
       "  'sku': '8380689',\n",
       "  'regular price': '19,99€',\n",
       "  'actual price': '9,99€',\n",
       "  'brand': 'TRIBAN',\n",
       "  'url': 'https://www.decathlon.de/p/kurze-radhose-rennrad-rc-500-damen/_/R-p-120018?mc=8380689&c=SCHWARZ',\n",
       "  'sticker': '%Endspurt%',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Wandershirt Naturwandern NH500 kurzarm Damen rosa ',\n",
       "  'sku': '8582429',\n",
       "  'regular price': '9,99€',\n",
       "  'actual price': '6,99€',\n",
       "  'brand': 'QUECHUA',\n",
       "  'url': 'https://www.decathlon.de/p/wandershirt-naturwandern-nh500-kurzarm-damen/_/R-p-308122?mc=8582429&c=VIOLETT',\n",
       "  'sticker': 'Ecodesign',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'T-Shirt Loose beige ',\n",
       "  'sku': '8665571',\n",
       "  'regular price': '7,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'DOMYOS',\n",
       "  'url': 'https://www.decathlon.de/p/t-shirt-loose/_/R-p-333161?mc=8665571&c=BEIGE',\n",
       "  'sticker': 'NEU',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Top Schwimmerrücken Fitness My Top',\n",
       "  'sku': '8125767',\n",
       "  'regular price': '4,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'DOMYOS',\n",
       "  'url': 'https://www.decathlon.de/p/top-schwimmerrucken-fitness-my-top/_/R-p-8262?mc=8125767&c=SCHWARZ',\n",
       "  'sticker': None,\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Merino Shirt Trekking Travel 100 kurzarm Damen blau',\n",
       "  'sku': '8493314',\n",
       "  'regular price': '19,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'FORCLAZ',\n",
       "  'url': 'https://www.decathlon.de/p/merino-shirt-trekking-travel-100-kurzarm-damen/_/R-p-6455?mc=8493314&c=GRAU_BLAU',\n",
       "  'sticker': None,\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Wanderhose Naturwandern NH100 Damen blau',\n",
       "  'sku': '8554842',\n",
       "  'regular price': '11,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'QUECHUA',\n",
       "  'url': 'https://www.decathlon.de/p/wanderhose-naturwandern-nh100-damen/_/R-p-308204?mc=8554842&c=GRÜN_KHAKI',\n",
       "  'sticker': 'ECODESIGN',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Hybrid-Sweatshirtjacke Naturwandern NH100 Damen grau',\n",
       "  'sku': '8503183',\n",
       "  'regular price': '24,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'QUECHUA',\n",
       "  'url': 'https://www.decathlon.de/p/hybrid-sweatshirtjacke-naturwandern-nh100-damen/_/R-p-12089?mc=8503183&c=GRAU',\n",
       "  'sticker': 'ECODESIGN',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Wanderjacke Bergwandern MH100 Wasserdicht Damen grau meliert',\n",
       "  'sku': '8492378',\n",
       "  'regular price': '39,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'QUECHUA',\n",
       "  'url': 'https://www.decathlon.de/p/wanderjacke-mh100-damen/_/R-p-172374?mc=8492378&c=GRAU',\n",
       "  'sticker': 'Ecodesign',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Golf Poloshirt kurzarm MW500 Damen marineblau',\n",
       "  'sku': '8354074',\n",
       "  'regular price': '9,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'INESIS',\n",
       "  'url': 'https://www.decathlon.de/p/golf-poloshirt-damen/_/R-p-143646?mc=8354074&c=BLAU',\n",
       "  'sticker': None,\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Leggings Fit+ Fitness Baumwolle Damen grau ',\n",
       "  'sku': '8612630',\n",
       "  'regular price': '11,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'DOMYOS',\n",
       "  'url': 'https://www.decathlon.de/p/leggings-fit-fitness-baumwolle-damen/_/R-p-330052?mc=8612630&c=GRAU',\n",
       "  'sticker': None,\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Leggings 7/8 Slim Fitness Damen grün ',\n",
       "  'sku': '8665610',\n",
       "  'regular price': '14,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'DOMYOS',\n",
       "  'url': 'https://www.decathlon.de/p/7-8-leggings-520-slim-gym-pilates-damen-mit-print/_/R-p-311168?mc=8665610&c=GRÜN',\n",
       "  'sticker': 'NEU',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Basketballshorts SH500 Damen schwarz/rosa',\n",
       "  'sku': '8520011',\n",
       "  'regular price': '11,99€',\n",
       "  'actual price': '5,99€',\n",
       "  'brand': 'TARMAK',\n",
       "  'url': 'https://www.decathlon.de/p/basketballshorts-sh500-damen/_/R-p-196139?mc=8520011&c=SCHWARZ',\n",
       "  'sticker': '%Endspurt%',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Golf Poloshirt kurzarm WW500 Damen marineblau',\n",
       "  'sku': '8517204',\n",
       "  'regular price': '19,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'INESIS',\n",
       "  'url': 'https://www.decathlon.de/p/golf-poloshirt-ww500-damen/_/R-p-303993?mc=8517204&c=BLAU',\n",
       "  'sticker': None,\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Top Slim 500 Fitness X-Rücken Synthetik Damen schwarz ',\n",
       "  'sku': '8645599',\n",
       "  'regular price': '5,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'DOMYOS',\n",
       "  'url': 'https://www.decathlon.de/p/top-slim-500-fitness-x-rucken-synthetik-damen/_/R-p-333131?mc=8645599&c=SCHWARZ',\n",
       "  'sticker': 'NEU',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Merino Shirt Trekking MT900 langarm hoher Kragen Zip Damen ',\n",
       "  'sku': '8544817',\n",
       "  'regular price': '59,99€',\n",
       "  'actual price': '39,99€',\n",
       "  'brand': 'FORCLAZ',\n",
       "  'url': 'https://www.decathlon.de/p/merino-shirt-trekking-mt900-langarm-hoher-kragen-zip-damen/_/R-p-306986?mc=8544817&c=ROT_BORDEAUX',\n",
       "  'sticker': '%Endspurt%',\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'T-Shirt Fitness 100\\xa0% Baumwolle Damen weiss',\n",
       "  'sku': '8280228',\n",
       "  'regular price': '3,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'DOMYOS',\n",
       "  'url': 'https://www.decathlon.de/p/t-shirt-fitness-100-baumwolle-damen/_/R-p-11454?mc=8280228&c=WEIß',\n",
       "  'sticker': None,\n",
       "  'category': 'Sportbekleidung Damen'},\n",
       " {'title': 'Wandershirt Kurzarm Bergwandern MH500 Damen hellgrün',\n",
       "  'sku': '8544136',\n",
       "  'regular price': '9,99€',\n",
       "  'actual price': None,\n",
       "  'brand': 'QUECHUA',\n",
       "  'url': 'https://www.decathlon.de/p/wandershirt-kurzarm-bergwandern-mh500-damen/_/R-p-307315?mc=8544136&c=GRAU_BLAU',\n",
       "  'sticker': 'ECODESIGN',\n",
       "  'category': 'Sportbekleidung Damen'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de1_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0904ce46",
   "metadata": {},
   "source": [
    "## To-do\n",
    "1. DONE: Remove the space AND * in price, SPACE in Sticker\n",
    "2. DONE: Saving URL as a list --> Scrape SKU in each URL OR: trying to extract SKU from the URL\n",
    "3. Pagination\n",
    "4. Saving as csv. with country_category as file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4356a4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8624220'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = prod[0].find(\"a\").attrs[\"href\"]\n",
    "\n",
    "link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
    "\n",
    "link.partition(\"?mc=\")[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80481f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
    "print(string.partition(\"?mc=\")[2].rpartition('&')[0]) == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f748c1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8380104'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ'\n",
    "\n",
    "string.partition(\"?mc=\")[2][0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc2e78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/1:\n",
      "a = 1 \n",
      "b = 1\n",
      "print(a > b)\n",
      "print(a < b)\n",
      "print(a >= b)\n",
      "print(a <= b)\n",
      " 2/2: a>b\n",
      " 2/3:\n",
      "a > b\n",
      "a < b\n",
      "a >= b\n",
      "a <=b\n",
      " 2/4:\n",
      "a > b\n",
      "a < b\n",
      "a >= b\n",
      "a <=b\n",
      " 2/5:\n",
      "a > b\n",
      "a < b\n",
      "a >= b\n",
      "a <=b\n",
      " 2/6: url = \"https://www.airbnb.com/s/Amsterdam/homes?place_id=ChIJVXealLU_xkcRja_At0z9AGY&refinement_paths%5B%5D=%2Fhomes&search_type=section_navigation\"\n",
      " 2/7:\n",
      "import requests\n",
      "content = requests.get(url)\n",
      " 2/8:\n",
      "import requests\n",
      "content = requests.get(url)\n",
      " 2/9: content.text\n",
      "2/10: ! ahaha\n",
      "2/11: captured_content = \"Page 1\"\n",
      "2/12: captured_content = \"Page 1\"\n",
      "2/13: numeric_content = int(captured_content.replace( 'Page ', ''))\n",
      "2/14: urls = ['https://www.airbnb.com/rooms/30042149?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba', 'https://www.airbnb.com/rooms/14173621?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba', 'https://www.airbnb.com/rooms/13040996?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba']\n",
      "2/15: urls = ['https://www.airbnb.com/rooms/30042149?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba', 'https://www.airbnb.com/rooms/14173621?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba', 'https://www.airbnb.com/rooms/13040996?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba']\n",
      "2/16:\n",
      "#length of URLs\n",
      "len(urls)\n",
      "2/17: import pandas\n",
      "2/18: import this\n",
      "2/19:\n",
      "def scrape(url):\n",
      "    print('Im scraping' + url )\n",
      "2/20:\n",
      "def scrape(url):\n",
      "    print('Im scraping' + url )\n",
      "2/21: scrape ('airbnb.com')\n",
      " 3/1:\n",
      "# your code goes here!\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "    elif study == \"Research Master\":\n",
      "        print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "        else:\n",
      "            print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      " 3/2:\n",
      "# solution\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course.\")\n",
      " 3/3:\n",
      "# your code goes here!\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "    elif study == \"Research Master\":\n",
      "        print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "        else:\n",
      "            print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      " 3/4:\n",
      "# your code goes here!\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      " 3/5:\n",
      "# your code goes here!\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "    elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      "2/22: urls\n",
      "2/23:\n",
      "club1 = {'url':'https://www.airbnb.com/rooms/30042149?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba', \n",
      "             'country': 'england', \n",
      "             'champions_league': True}\n",
      "club2 = {'url':'https://www.airbnb.com/rooms/30042149?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba', \n",
      "             'country': 'france', \n",
      "             'champions_league': True}\n",
      "club3 = {'url':'https://www.airbnb.com/rooms/30042149?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba', \n",
      "             'country': 'england', \n",
      "             'champions_league': True}\n",
      "2/24: list_of_dics = [club1, club2, club3]\n",
      "2/25: list_of_dics\n",
      "2/26:\n",
      "# Functions can or cannot have Arguemnts\n",
      "\n",
      "def scrape3(url):\n",
      "    print(url)\n",
      "    return(True)\n",
      "2/27: scrape3(urls[0])\n",
      "2/28: urls\n",
      "2/29: print(urls)\n",
      " 5/1:\n",
      "import os\n",
      "os.getcwd()\n",
      " 5/2:\n",
      "import os\n",
      "os.getcwd()\n",
      " 5/3: os.chdir('D:\\OneDrive\\00_Tilburg\\00_RM2021\\Unit 1\\oDCM\\W1\\python-bootcamp')\n",
      " 5/4: os.chdir('D:\\\\OneDrive\\\\00_Tilburg\\\\00_RM2021\\\\Unit 1\\\\oDCM\\\\W1\\\\python-bootcamp')\n",
      " 5/5: os.getcwd()\n",
      " 6/1:\n",
      "a = 1 \n",
      "b = 1\n",
      "print(a > b)\n",
      "print(a < b)\n",
      "print(a >= b)\n",
      "print(a <= b)\n",
      " 6/2: a > b\n",
      " 6/3:\n",
      "balance = 100\n",
      "if balance < 0: \n",
      "    print(\"You should top up your checking account to avoid paying interest\")\n",
      "elif balance == 0: \n",
      "    print(\"Your checking account balance is exactly €0.00, be careful when making new payments!\")\n",
      "else: \n",
      "    print(\"You have a positive balance\")\n",
      " 6/4:\n",
      "balance = 100\n",
      "if balance < 0: \n",
      "    \"You should top up your checking account to avoid paying interest\"\n",
      "elif balance == 0: \n",
      "    \"Your checking account balance is exactly €0.00, be careful when making new payments!\"\n",
      "else: \n",
      "    \"You have a positive balance\"\n",
      " 6/5:\n",
      "balance = 100\n",
      "if balance < 0: \n",
      "    \"You should top up your checking account to avoid paying interest\"\n",
      "elif balance == 0: \n",
      "    \"Your checking account balance is exactly €0.00, be careful when making new payments!\"\n",
      "else: \n",
      "    \"You have a positive balance\"\n",
      " 6/6:\n",
      "balance = 100\n",
      "if balance < 0: \n",
      "    print(\"You should top up your checking account to avoid paying interest\")\n",
      "elif balance == 0: \n",
      "    print(\"Your checking account balance is exactly €0.00, be careful when making new payments!\")\n",
      "else: \n",
      "    print(\"You have a positive balance\")\n",
      " 6/7:\n",
      "balance = -10\n",
      "if balance < 0: \n",
      "    print(\"You should top up your checking account to avoid paying interest\")\n",
      "elif balance == 0: \n",
      "    print(\"Your checking account balance is exactly €0.00, be careful when making new payments!\")\n",
      "else: \n",
      "    print(\"You have a positive balance\")\n",
      " 6/8:\n",
      "# your code goes here!\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      " 6/9:\n",
      "workday = False\n",
      "no_holiday = True\n",
      "\n",
      "if workday and no_holiday:\n",
      "    print(\"Go to work!\")\n",
      "6/10:\n",
      "workday = True\n",
      "no_holiday = True\n",
      "\n",
      "if workday and no_holiday:\n",
      "    print(\"Go to work!\")\n",
      "6/11:\n",
      "workday = False\n",
      "no_holiday = True\n",
      "\n",
      "if workday and no_holiday:\n",
      "    print(\"Go to work!\")\n",
      "else:\n",
      "    print(\"Back to sleep!\")\n",
      "6/12:\n",
      "workday = False\n",
      "no_holiday = False\n",
      "\n",
      "if workday and no_holiday:\n",
      "    print(\"Go to work!\")\n",
      "else:\n",
      "    print(\"Back to sleep!\")\n",
      "6/13:\n",
      "workday = True\n",
      "no_holiday = False\n",
      "\n",
      "if workday and no_holiday:\n",
      "    print(\"Go to work!\")\n",
      "else:\n",
      "    print(\"Back to sleep!\")\n",
      "6/14:\n",
      "# your code goes here!\n",
      "prior_knowledge == True\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\" and prior_knowledge:\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\" and prior_knowledge:\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      "6/15:\n",
      "# your code goes here!\n",
      "prior_knowledge = True\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\" and prior_knowledge:\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\" and prior_knowledge:\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      "6/16:\n",
      "student_dict = student_dict + {\"age\": 23}\n",
      "print(strudent_dict)\n",
      "6/17:\n",
      "student_dict = {\"name\": \"Lotte\", \n",
      "                \"study\": \"Marketing Analytics\", \n",
      "                \"prior_knowledge\": True}\n",
      "\n",
      "print(student_dict)\n",
      "6/18:\n",
      "student_dict = student_dict + {\"age\": 23}\n",
      "print(strudent_dict)\n",
      "6/19:\n",
      "student_dict = {student_dict, \"age\": 23}\n",
      "print(strudent_dict)\n",
      "6/20:\n",
      "student_dict[2]\n",
      "print(strudent_dict)\n",
      "6/21: student_dict[2]\n",
      "6/22: student_dict[\"name\"]\n",
      "6/23: student_dict[\"age\"] = 23\n",
      "6/24:\n",
      "student_dict[\"age\"] = 23\n",
      "print(student_dict)\n",
      "6/25:\n",
      "print(student_dict(\"email\"))\n",
      "print(student_dict.get(\"name\"))\n",
      "print(student_dict.get(\"email\"))\n",
      "6/26:\n",
      "print(student_dict.get(\"email\"))\n",
      "print(student_dict.get(\"name\"))\n",
      "print(student_dict.get(\"email\"))\n",
      "6/27:\n",
      "# your code goes here!\n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "6/28:\n",
      "enrollments = [\n",
      "    {\n",
      "        \"course\": \"Online Data Collection & Management\",\n",
      "        \"instructor\": \"Hannes Datta\",\n",
      "        \"students\": [\n",
      "            {\n",
      "                \"name\": \"Lotte\",\n",
      "                \"study\": \"Marketing Analytics\",\n",
      "                \"prior_knowledge\": True\n",
      "                \n",
      "            }, \n",
      "            {\n",
      "                \"name\": \"Joep\",\n",
      "                \"study\": \"Research Master\",\n",
      "                \"prior_knowledge\": True,\n",
      "                \"honors\": \"The Societal Challenge of Migration\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "6/29:\n",
      "# your code goes here!\n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "6/30:\n",
      "# your code goes here!\n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "6/31: print(enrollments[0][\"students\"][,].get[\"honors\"])\n",
      "6/32: print(enrollments[0][\"students\"][].get[\"honors\"])\n",
      "6/33: print(enrollments[0][\"students\"][0:1].get[\"honors\"])\n",
      "6/34: print(enrollments[0][\"students\"][0].get[\"honors\"])\n",
      "6/35: print(enrollments[0][\"students\"][].get(\"honors\"))\n",
      "6/36: print(enrollments[0][\"students\"][0:1].get(\"honors\"))\n",
      "6/37: print(enrollments[0][\"students\"][0].get(\"honors\"))\n",
      "6/38:\n",
      "# Question 2\n",
      "print(enrollments[0][\"students\"][0].get(\"honors\"))\n",
      "6/39: enrollments\n",
      "6/40: enrollments\n",
      "6/41:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "print(enrollments)\n",
      "6/42:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "enrollments\n",
      "6/43:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@uvt.edu\"\n",
      "enrollments\n",
      "6/44:\n",
      "enrollments = enrollments + [\n",
      "    {\"course\": \"Data Preparation and Workflow Management\",\n",
      "     \"instructor\": \"Hannes Datta\",\n",
      "     \"students\":[{\"name\": \"Sanne\",\n",
      "                  \"study\": \"Marketing Analytics\",\n",
      "                  \"prior_knowledge\": True\n",
      "         \n",
      "     }]\n",
      "        \n",
      "    }\n",
      "]\n",
      "print(enrollments)\n",
      "6/45:\n",
      "enrollments = enrollments + [\n",
      "    {\"course\": \"Data Preparation and Workflow Management\",\n",
      "     \"instructor\": \"Hannes Datta\",\n",
      "     \"students\":[{\"name\": \"Sanne\",\n",
      "                  \"study\": \"Marketing Analytics\",\n",
      "                  \"prior_knowledge\": True\n",
      "         \n",
      "     }]\n",
      "        \n",
      "    }\n",
      "]\n",
      "enrollments\n",
      "6/46:\n",
      "enrollments = enrollments + [,\n",
      "    {\"course\": \"Data Preparation and Workflow Management\",\n",
      "     \"instructor\": \"Hannes Datta\",\n",
      "     \"students\":[{\"name\": \"Sanne\",\n",
      "                  \"study\": \"Marketing Analytics\",\n",
      "                  \"prior_knowledge\": True\n",
      "         \n",
      "     }]\n",
      "        \n",
      "    }\n",
      "]\n",
      "enrollments\n",
      "6/47:\n",
      "enrollments.append([\n",
      "    {\"course\": \"Data Preparation and Workflow Management\",\n",
      "     \"instructor\": \"Hannes Datta\",\n",
      "     \"students\":[{\"name\": \"Sanne\",\n",
      "                  \"study\": \"Marketing Analytics\",\n",
      "                  \"prior_knowledge\": True\n",
      "         \n",
      "     }]\n",
      "        \n",
      "    }\n",
      "])\n",
      "enrollments\n",
      "6/48:\n",
      "enrollments.append({\"course\": \"Data Preparation and Workflow Management\",\n",
      "     \"instructor\": \"Hannes Datta\",\n",
      "     \"students\":[{\"name\": \"Sanne\",\n",
      "                  \"study\": \"Marketing Analytics\",\n",
      "                  \"prior_knowledge\": True\n",
      "         \n",
      "     })\n",
      "enrollments\n",
      "6/49:\n",
      "enrollments.append({\"course\": \"Data Preparation and Workflow Management\",\n",
      "     \"instructor\": \"Hannes Datta\",\n",
      "     \"students\":[{\"name\": \"Sanne\",\n",
      "                  \"study\": \"Marketing Analytics\",\n",
      "                  \"prior_knowledge\": True \n",
      "     })\n",
      "enrollments\n",
      "6/50:\n",
      "enrollments.append({\"course\": \"Data Preparation and Workflow Management\",\n",
      "     \"instructor\": \"Hannes Datta\",\n",
      "     \"students\":[{\"name\": \"Sanne\",\n",
      "                  \"study\": \"Marketing Analytics\",\n",
      "                  \"prior_knowledge\": True \n",
      "    }\n",
      "    }\n",
      "    )\n",
      "enrollments\n",
      "6/51:\n",
      "enrollments.append({\n",
      "    \"course\": \"Data Preparation and Workflow Management\",\n",
      "    \"instructor\": \"Hannes Datta\",\n",
      "    \"students\":[{\n",
      "        \"name\": \"Sanne\",\n",
      "        \"study\": \"Marketing Analytics\",\n",
      "        \"prior_knowledge\": True \n",
      "    }]\n",
      "    })\n",
      "enrollments\n",
      "6/52:\n",
      "enrollments = [\n",
      "    {\n",
      "        \"course\": \"Online Data Collection & Management\",\n",
      "        \"instructor\": \"Hannes Datta\",\n",
      "        \"students\": [\n",
      "            {\n",
      "                \"name\": \"Lotte\",\n",
      "                \"study\": \"Marketing Analytics\",\n",
      "                \"prior_knowledge\": True\n",
      "                \n",
      "            }, \n",
      "            {\n",
      "                \"name\": \"Joep\",\n",
      "                \"study\": \"Research Master\",\n",
      "                \"prior_knowledge\": True,\n",
      "                \"honors\": \"The Societal Challenge of Migration\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "6/53:\n",
      "# your code goes here!\n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "6/54: print(enrollments[0][\"students\"][0].get(\"honors\"))\n",
      "6/55: student_dict[\"email\"] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "6/56:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "enrollments\n",
      "6/57:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@uvt.edu\"\n",
      "enrollments\n",
      "6/58:\n",
      "enrollments.append({\n",
      "    \"course\": \"Data Preparation and Workflow Management\",\n",
      "    \"instructor\": \"Hannes Datta\",\n",
      "    \"students\":[{\n",
      "        \"name\": \"Sanne\",\n",
      "        \"study\": \"Marketing Analytics\",\n",
      "        \"prior_knowledge\": True \n",
      "    }]\n",
      "    })\n",
      "enrollments\n",
      "6/59:\n",
      "prices = [9.99, 3.95, 24.95]\n",
      "\n",
      "for price in prices:\n",
      "    print(price)\n",
      "6/60:\n",
      "prices = [9.99, 3.95, 24.95]\n",
      "\n",
      "for i in prices:\n",
      "    print(i)\n",
      "6/61:\n",
      "prices = [9.99, 3.95, 24.95, 8.95]\n",
      "\n",
      "for price in prices:\n",
      "    print(price)\n",
      "6/62:\n",
      "price_discount = []\n",
      "\n",
      "for price in prices:\n",
      "    price_dis = price * 0.85\n",
      "    price_discount.append(price_dis)\n",
      "    \n",
      "print(price_discount)\n",
      "6/63:\n",
      "price_discount = []\n",
      "\n",
      "for price in prices:\n",
      "    price_dis = price * 0.85\n",
      "    price_discount.append(price_dis)\n",
      "    \n",
      "print(round(price_discount))\n",
      "6/64:\n",
      "price_discount = []\n",
      "\n",
      "for price in prices:\n",
      "    price_dis = round(price * 0.85)\n",
      "    price_discount.append(price_dis)\n",
      "    \n",
      "print(round(price_discount))\n",
      "6/65:\n",
      "price_discount = []\n",
      "\n",
      "for price in prices:\n",
      "    price_dis = round(price * 0.85)\n",
      "    price_discount.append(price_dis)\n",
      "    \n",
      "print(price_discount)\n",
      "6/66:\n",
      "price_discount = []\n",
      "\n",
      "for price in prices:\n",
      "    price_dis = round((price * 0.85),2)\n",
      "    price_discount.append(price_dis)\n",
      "    \n",
      "print(price_discount)\n",
      "6/67: len(prices)\n",
      "6/68:\n",
      "# your code goes here!\n",
      "prices = [9.99, 3.95, 24.95, 8.95]\n",
      "sorted(prices, reverse=True)\n",
      "6/69:\n",
      "# your code goes here!\n",
      "prices = [9.99, 3.95, 24.95, 8.95]\n",
      "sorted(prices, reverse=True) \n",
      "\n",
      "total_prices = 3.95 * 60% + 8.95 * 70% + 9.99*80% + 24.95 *90%\n",
      "6/70:\n",
      "# your code goes here!\n",
      "\n",
      "total_prices = 3.95 * 60% + 8.95 * 70% + 9.99*80% + 24.95 *90%\n",
      "print(total_prices)\n",
      "6/71:\n",
      "# your code goes here!\n",
      "total_prices = 3.95 * 60% + 8.95 * 70% + 9.99 * 80% + 24.95 * 90%\n",
      "print(total_prices)\n",
      "6/72:\n",
      "# your code goes here!\n",
      "total_prices = 3.95 * 0.60 + 8.95 * 0.70 + 9.99 * 0.80 + 24.95 * 0.90\n",
      "print(total_prices)\n",
      "6/73:\n",
      "# solution (it's a bit more difficult, so no worries if you don't get it right away!)\n",
      "total_price = 0\n",
      "discount_rate = 0.10\n",
      "\n",
      "prices = sorted(prices, reverse=True)\n",
      "\n",
      "for price in prices: \n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += 0.10\n",
      "\n",
      "print(total_price)\n",
      " 7/1: prices\n",
      " 7/2:\n",
      "a = 1 \n",
      "b = 1\n",
      "print(a > b)\n",
      "print(a < b)\n",
      "print(a >= b)\n",
      "print(a <= b)\n",
      " 7/3: a > b\n",
      " 7/4:\n",
      "savings = 100 # variable assignment\n",
      "print(savings == 100) # comparison \n",
      "print(savings != 100) # comparison (False because it is 100!)\n",
      " 7/5:\n",
      "balance = -10\n",
      "if balance < 0: \n",
      "    print(\"You should top up your checking account to avoid paying interest\")\n",
      "elif balance == 0: \n",
      "    print(\"Your checking account balance is exactly €0.00, be careful when making new payments!\")\n",
      "else: \n",
      "    print(\"You have a positive balance\")\n",
      " 7/6:\n",
      "# your code goes here!\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      " 7/7:\n",
      "# solution\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course.\")\n",
      " 7/8:\n",
      "workday = True\n",
      "no_holiday = False\n",
      "\n",
      "if workday and no_holiday:\n",
      "    print(\"Go to work!\")\n",
      "else:\n",
      "    print(\"Back to sleep!\")\n",
      " 7/9:\n",
      "tired = True\n",
      "bed_time = False\n",
      "\n",
      "if tired or bed_time:\n",
      "    print(\"Go to sleep!\")\n",
      "7/10:\n",
      "weekend = False\n",
      "\n",
      "if not weekend:\n",
      "    print(\"Go to work!\")\n",
      "7/11:\n",
      "# your code goes here!\n",
      "prior_knowledge = True\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\" and prior_knowledge:\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\" and prior_knowledge:\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      "7/12:\n",
      "# solution (it's not necessary to check whether prior_knowledge == True)\n",
      "if study == \"Marketing Analytics\" and prior_knowledge:\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\" and prior_knowledge:\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course.\")\n",
      "7/13:\n",
      "final_grade = 5.4\n",
      "exam_grade = 4.3\n",
      "resit_grade = 5.6\n",
      "\n",
      "final_grade > 5.5 and exam_grade > 5.5 or resit_grade > 5.5\n",
      "7/14: final_grade > 5.5 and (exam_grade > 5.5 or resit_grade > 5.5)\n",
      "7/15:\n",
      "driver_license = False\n",
      "age = 17\n",
      "coach = True\n",
      "\n",
      "if driver_license and age >= 18 or age == 17 and coach: \n",
      "    print(\"You're allowed to drive!\")    \n",
      "else: \n",
      "    print(\"You're not allowed to drive!\")\n",
      "7/16: # your code goes here!\n",
      "7/17:\n",
      "# solution\n",
      "if driver_license and (age >= 18 or age == 17 and coach): \n",
      "    print(\"You're allowed to drive!\")    \n",
      "else: \n",
      "    print(\"You're not allowed to drive!\")\n",
      "7/18:\n",
      "students_list = [[\"Lotte\", \"Marketing Analytics\", True], \n",
      "            [\"Joep\", \"Research Master\", True]]\n",
      "7/19:\n",
      "student_dict = {\"name\": \"Lotte\", \n",
      "                \"study\": \"Marketing Analytics\", \n",
      "                \"prior_knowledge\": True}\n",
      "\n",
      "print(student_dict)\n",
      "7/20:\n",
      "student_dict[\"age\"] = 23\n",
      "print(student_dict)\n",
      "7/21: print(student_dict[\"name\"])\n",
      "7/22:\n",
      "print(student_dict.get(\"name\"))\n",
      "print(student_dict.get(\"email\"))\n",
      "7/23:\n",
      "enrollments = [\n",
      "    {\n",
      "        \"course\": \"Online Data Collection & Management\",\n",
      "        \"instructor\": \"Hannes Datta\",\n",
      "        \"students\": [\n",
      "            {\n",
      "                \"name\": \"Lotte\",\n",
      "                \"study\": \"Marketing Analytics\",\n",
      "                \"prior_knowledge\": True\n",
      "                \n",
      "            }, \n",
      "            {\n",
      "                \"name\": \"Joep\",\n",
      "                \"study\": \"Research Master\",\n",
      "                \"prior_knowledge\": True,\n",
      "                \"honors\": \"The Societal Challenge of Migration\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "7/24:\n",
      "# your code goes here!\n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "7/25: print(enrollments[0][\"students\"][0].get(\"honors\"))\n",
      "7/26:\n",
      "# Question 1 \n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "7/27:\n",
      "# Question 2\n",
      "print(enrollments[0][\"students\"][0].get(\"honors\"))\n",
      "7/28: student_dict[\"email\"] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "7/29:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "enrollments\n",
      "7/30:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@uvt.edu\"\n",
      "enrollments\n",
      "7/31:\n",
      "enrollments.append({\n",
      "    \"course\": \"Data Preparation and Workflow Management\",\n",
      "    \"instructor\": \"Hannes Datta\",\n",
      "    \"students\":[{\n",
      "        \"name\": \"Sanne\",\n",
      "        \"study\": \"Marketing Analytics\",\n",
      "        \"prior_knowledge\": True \n",
      "    }]\n",
      "    })\n",
      "enrollments\n",
      "7/32: # your code goes here!\n",
      "7/33:\n",
      "# solution\n",
      "enrollments.append({\n",
      "    \"course\": \"Data Preparation and Workflow Management\",\n",
      "    \"instructor\": \"Hannes Datta\",\n",
      "    \"students\": [{\n",
      "        \"name\": \"Sanne\",\n",
      "        \"study\": \"Marketing Analytics\", \n",
      "        \"prior_knowledge\": True\n",
      "    }]\n",
      "})\n",
      "7/34:\n",
      "prices = [9.99, 3.95, 24.95, 8.95]\n",
      "\n",
      "for price in prices:\n",
      "    print(price)\n",
      "7/35:\n",
      "price_discount = []\n",
      "\n",
      "for price in prices:\n",
      "    price_dis = round((price * 0.85),2)\n",
      "    price_discount.append(price_dis)\n",
      "    \n",
      "print(price_discount)\n",
      "7/36:\n",
      "prices_discount = []\n",
      "\n",
      "for price in prices: \n",
      "    price_discount = price * 0.85\n",
      "    prices_discount.append(price_discount)\n",
      "\n",
      "print(prices_discount) # in a future lessson we learn you how to round figures!\n",
      "7/37: prices\n",
      "7/38:\n",
      "# your code goes here!\n",
      "sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "7/39:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "7/40:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "7/41:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "\n",
      "\n",
      "a = 24.95 * 0.9 + 9.99 * 0.8 + 8.95 * 0.7 + 3.95 * 0.6\n",
      "7/42:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "\n",
      "\n",
      "a = 24.95 * 0.9 + 9.99 * 0.8 + 8.95 * 0.7 + 3.95 * 0.6\n",
      "print(a)\n",
      "7/43:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "7/44:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "7/45:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "7/46:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "7/47:\n",
      "# first name, study, prior knowledge\n",
      "students = [[\"Lotte\", \"Marketing Analytics\", True], \n",
      "            [\"Joep\", \"Research Master\", True], \n",
      "            [\"Mirte\", \"Marketing Analytics\", False], \n",
      "            [\"Dirk\", \"Economics\", True], \n",
      "            [\"Sanne\", \"Marketing Analytics\", True], \n",
      "            [\"Roy\", \"Research Master\", False]]\n",
      "\n",
      "# your code goes here!\n",
      "names =[]\n",
      "\n",
      "for student in students:\n",
      "    if student[1] == \"Marketing Analytics\" or student[1] ==\"Research Master\" and student[2] == True:\n",
      "        names.append(student[0])\n",
      "\n",
      "print(names)\n",
      "7/48:\n",
      "# first name, study, prior knowledge\n",
      "students = [[\"Lotte\", \"Marketing Analytics\", True], \n",
      "            [\"Joep\", \"Research Master\", True], \n",
      "            [\"Mirte\", \"Marketing Analytics\", False], \n",
      "            [\"Dirk\", \"Economics\", True], \n",
      "            [\"Sanne\", \"Marketing Analytics\", True], \n",
      "            [\"Roy\", \"Research Master\", False]]\n",
      "\n",
      "# your code goes here!\n",
      "names =[]\n",
      "\n",
      "for student in students:\n",
      "    if (student[1] == \"Marketing Analytics\" or student[1] ==\"Research Master\") and student[2] == True:\n",
      "        names.append(student[0])\n",
      "\n",
      "print(names)\n",
      "7/49:\n",
      "# solution (alternative 2 - counter)\n",
      "for counter in range(len(students)): \n",
      "    if (students[counter][1] == \"Marketing Analytics\" or students[counter][1] == \"Research Master\") and students[counter][2]:\n",
      "        print(students[counter][0])\n",
      "7/50: ?range\n",
      "7/51: range(len(students))\n",
      "7/52:\n",
      "print(list(range(-10,10))) # from 1 to 9\n",
      "print(list(range(20,10))) # from 0 to 9\n",
      "7/53:\n",
      "print(list(range(1,10))) # from 1 to 9\n",
      "print(list(range(10))) # from 0 to 9\n",
      "7/54:\n",
      "print(list(range(3)))\n",
      "print(list(range(len(prices))))\n",
      "7/55:\n",
      "prices.append(10)\n",
      "print(prices)\n",
      "7/56:\n",
      "prices.append(10)\n",
      "print(list(range(len(prices))))\n",
      "7/57:\n",
      "prices.append(10)\n",
      "print(price)\n",
      "print(list(range(len(prices))))\n",
      "7/58:\n",
      "prices.append(10)\n",
      "print(prices)\n",
      "print(list(range(len(prices))))\n",
      "7/59:\n",
      "prices = [9.99, 3.95, 24.95]\n",
      "print(list(range(3)))\n",
      "print(list(range(len(prices))))\n",
      "7/60:\n",
      "prices.append(10)\n",
      "print(prices)\n",
      "print(list(range(len(prices))))\n",
      "7/61:\n",
      "for counter in range(len(prices)):\n",
      "    print(prices[counter]*.21)\n",
      "7/62:\n",
      "for counter in range(len(prices)):\n",
      "    print(prices[counter])\n",
      "7/63:\n",
      "prices = [9.99, 3.95, 24.95]\n",
      "print(list(range(3)))\n",
      "print(list(range(len(prices))))\n",
      "7/64:\n",
      "for counter in range(len(prices)):\n",
      "    print(prices[counter])\n",
      "7/65:\n",
      "for counter in range(len(prices)):\n",
      "    print(prices[counter]*.21)\n",
      "7/66:\n",
      "counter = 0\n",
      "while counter < len(prices):\n",
      "    print(prices[counter])\n",
      " 8/1:\n",
      "a = 1 \n",
      "b = 1\n",
      "print(a > b)\n",
      "print(a < b)\n",
      "print(a >= b)\n",
      "print(a <= b)\n",
      " 8/2: a > b\n",
      " 8/3:\n",
      "savings = 100 # variable assignment\n",
      "print(savings == 100) # comparison \n",
      "print(savings != 100) # comparison (False because it is 100!)\n",
      " 8/4:\n",
      "balance = -10\n",
      "if balance < 0: \n",
      "    print(\"You should top up your checking account to avoid paying interest\")\n",
      "elif balance == 0: \n",
      "    print(\"Your checking account balance is exactly €0.00, be careful when making new payments!\")\n",
      "else: \n",
      "    print(\"You have a positive balance\")\n",
      " 8/5:\n",
      "# your code goes here!\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      " 8/6:\n",
      "# solution\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course.\")\n",
      " 8/7:\n",
      "workday = True\n",
      "no_holiday = False\n",
      "\n",
      "if workday and no_holiday:\n",
      "    print(\"Go to work!\")\n",
      "else:\n",
      "    print(\"Back to sleep!\")\n",
      " 8/8:\n",
      "tired = True\n",
      "bed_time = False\n",
      "\n",
      "if tired or bed_time:\n",
      "    print(\"Go to sleep!\")\n",
      " 8/9:\n",
      "weekend = False\n",
      "\n",
      "if not weekend:\n",
      "    print(\"Go to work!\")\n",
      "8/10:\n",
      "# your code goes here!\n",
      "prior_knowledge = True\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\" and prior_knowledge:\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\" and prior_knowledge:\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      "8/11:\n",
      "# solution (it's not necessary to check whether prior_knowledge == True)\n",
      "if study == \"Marketing Analytics\" and prior_knowledge:\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\" and prior_knowledge:\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course.\")\n",
      "8/12:\n",
      "final_grade = 5.4\n",
      "exam_grade = 4.3\n",
      "resit_grade = 5.6\n",
      "\n",
      "final_grade > 5.5 and exam_grade > 5.5 or resit_grade > 5.5\n",
      "8/13: final_grade > 5.5 and (exam_grade > 5.5 or resit_grade > 5.5)\n",
      "8/14:\n",
      "driver_license = False\n",
      "age = 17\n",
      "coach = True\n",
      "\n",
      "if driver_license and age >= 18 or age == 17 and coach: \n",
      "    print(\"You're allowed to drive!\")    \n",
      "else: \n",
      "    print(\"You're not allowed to drive!\")\n",
      "8/15: # your code goes here!\n",
      "8/16:\n",
      "# solution\n",
      "if driver_license and (age >= 18 or age == 17 and coach): \n",
      "    print(\"You're allowed to drive!\")    \n",
      "else: \n",
      "    print(\"You're not allowed to drive!\")\n",
      "8/17:\n",
      "students_list = [[\"Lotte\", \"Marketing Analytics\", True], \n",
      "            [\"Joep\", \"Research Master\", True]]\n",
      "8/18:\n",
      "student_dict = {\"name\": \"Lotte\", \n",
      "                \"study\": \"Marketing Analytics\", \n",
      "                \"prior_knowledge\": True}\n",
      "\n",
      "print(student_dict)\n",
      "8/19:\n",
      "student_dict[\"age\"] = 23\n",
      "print(student_dict)\n",
      "8/20: print(student_dict[\"name\"])\n",
      "8/21:\n",
      "print(student_dict.get(\"name\"))\n",
      "print(student_dict.get(\"email\"))\n",
      "8/22:\n",
      "enrollments = [\n",
      "    {\n",
      "        \"course\": \"Online Data Collection & Management\",\n",
      "        \"instructor\": \"Hannes Datta\",\n",
      "        \"students\": [\n",
      "            {\n",
      "                \"name\": \"Lotte\",\n",
      "                \"study\": \"Marketing Analytics\",\n",
      "                \"prior_knowledge\": True\n",
      "                \n",
      "            }, \n",
      "            {\n",
      "                \"name\": \"Joep\",\n",
      "                \"study\": \"Research Master\",\n",
      "                \"prior_knowledge\": True,\n",
      "                \"honors\": \"The Societal Challenge of Migration\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "8/23:\n",
      "# your code goes here!\n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "8/24: print(enrollments[0][\"students\"][0].get(\"honors\"))\n",
      "8/25:\n",
      "# Question 1 \n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "8/26:\n",
      "# Question 2\n",
      "print(enrollments[0][\"students\"][0].get(\"honors\"))\n",
      "8/27: student_dict[\"email\"] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "8/28:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "enrollments\n",
      "8/29:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@uvt.edu\"\n",
      "enrollments\n",
      "8/30:\n",
      "enrollments.append({\n",
      "    \"course\": \"Data Preparation and Workflow Management\",\n",
      "    \"instructor\": \"Hannes Datta\",\n",
      "    \"students\":[{\n",
      "        \"name\": \"Sanne\",\n",
      "        \"study\": \"Marketing Analytics\",\n",
      "        \"prior_knowledge\": True \n",
      "    }]\n",
      "    })\n",
      "enrollments\n",
      "8/31: # your code goes here!\n",
      "8/32:\n",
      "# solution\n",
      "enrollments.append({\n",
      "    \"course\": \"Data Preparation and Workflow Management\",\n",
      "    \"instructor\": \"Hannes Datta\",\n",
      "    \"students\": [{\n",
      "        \"name\": \"Sanne\",\n",
      "        \"study\": \"Marketing Analytics\", \n",
      "        \"prior_knowledge\": True\n",
      "    }]\n",
      "})\n",
      "8/33:\n",
      "prices = [9.99, 3.95, 24.95, 8.95]\n",
      "\n",
      "for price in prices:\n",
      "    print(price)\n",
      "8/34:\n",
      "price_discount = []\n",
      "\n",
      "for price in prices:\n",
      "    price_dis = round((price * 0.85),2)\n",
      "    price_discount.append(price_dis)\n",
      "    \n",
      "print(price_discount)\n",
      "8/35:\n",
      "prices_discount = []\n",
      "\n",
      "for price in prices: \n",
      "    price_discount = price * 0.85\n",
      "    prices_discount.append(price_discount)\n",
      "\n",
      "print(prices_discount) # in a future lessson we learn you how to round figures!\n",
      "8/36: prices\n",
      "8/37:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "8/38:\n",
      "# solution (it's a bit more difficult, so no worries if you don't get it right away!)\n",
      "total_price = 0\n",
      "discount_rate = 0.10\n",
      "\n",
      "prices = sorted(prices, reverse=True)\n",
      "\n",
      "for price in prices: \n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += 0.10\n",
      "\n",
      "print(total_price)\n",
      "8/39:\n",
      "# first name, study, prior knowledge\n",
      "students = [[\"Lotte\", \"Marketing Analytics\", True], \n",
      "            [\"Joep\", \"Research Master\", True], \n",
      "            [\"Mirte\", \"Marketing Analytics\", False], \n",
      "            [\"Dirk\", \"Economics\", True], \n",
      "            [\"Sanne\", \"Marketing Analytics\", True], \n",
      "            [\"Roy\", \"Research Master\", False]]\n",
      "\n",
      "# your code goes here!\n",
      "names =[]\n",
      "\n",
      "for student in students:\n",
      "    if (student[1] == \"Marketing Analytics\" or student[1] ==\"Research Master\") and student[2]:\n",
      "        names.append(student[0])\n",
      "\n",
      "print(names)\n",
      "8/40:\n",
      "# solution (most elegant)\n",
      "for student in students: \n",
      "    if student[1] in [\"Marketing Analytics\", \"Research Master\"] and student[2]:\n",
      "        print(student[0])\n",
      "8/41:\n",
      "# solution (alternative 1 - iterator)\n",
      "for student in students: \n",
      "    if (student[1] == \"Marketing Analytics\" or student[1] == \"Research Master\") and student[2]: # mind the brackets!\n",
      "        print(student[0])\n",
      "8/42:\n",
      "# solution (alternative 2 - counter)\n",
      "for counter in range(len(students)): \n",
      "    if (students[counter][1] == \"Marketing Analytics\" or students[counter][1] == \"Research Master\") and students[counter][2]:\n",
      "        print(students[counter][0])\n",
      "8/43:\n",
      "print(list(range(1,10))) # from 1 to 9\n",
      "print(list(range(10))) # from 0 to 9\n",
      "8/44:\n",
      "prices = [9.99, 3.95, 24.95]\n",
      "print(list(range(3)))\n",
      "print(list(range(len(prices))))\n",
      "8/45:\n",
      "prices.append(10)\n",
      "print(prices)\n",
      "print(list(range(len(prices))))\n",
      "8/46:\n",
      "for counter in range(len(prices)):\n",
      "    print(prices[counter])\n",
      "8/47:\n",
      "for counter in range(len(prices)):\n",
      "    print(prices[counter])\n",
      "8/48: print(prices)\n",
      "8/49:\n",
      "prices = prices[0:2]\n",
      "prices\n",
      "8/50:\n",
      "prices = prices[0:3]\n",
      "prices\n",
      "8/51:\n",
      "prices = prices[0:3]\n",
      "prices\n",
      "8/52:\n",
      "prices = [9.99, 3.95, 24.95]\n",
      "print(list(range(3)))\n",
      "print(list(range(len(prices))))\n",
      "8/53: prices\n",
      "8/54:\n",
      "prices.append(10)\n",
      "for counter in range(len(prices)):\n",
      "    print(prices[counter]*.21)\n",
      "8/55:\n",
      "while counter < 100:\n",
      "    print(\\U0001f600)\n",
      "8/56:\n",
      "while counter < 100:\n",
      "    print(\"\\U0001f600\")\n",
      "8/57:\n",
      "while counter in range(10):\n",
      "    print(\"\\U0001f600\" * counter)\n",
      "8/58:\n",
      "for counter in range(10):\n",
      "    print(\"\\U0001f600\" * counter)\n",
      "8/59:\n",
      "# solution\n",
      "for counter in range(10):\n",
      "    print(\"\\U0001f600\" * 10)\n",
      "8/60:\n",
      "for counter in range(10):\n",
      "    print(\"\\U0001f600\" * 10)\n",
      "8/61: student_dict\n",
      "8/62:\n",
      "for key in student_dict.keys(): \n",
      "    print(key)\n",
      "8/63: student_dict\n",
      "8/64: student_dict.keys()\n",
      "8/65:\n",
      "student_dict.keys()\n",
      "student_dict.value()\n",
      "8/66:\n",
      "student_dict.keys()\n",
      "student_dict.values()\n",
      "8/67:\n",
      "student_dict.keys()\n",
      "print(student_dict.values())\n",
      "8/68:\n",
      "student_dict.keys()\n",
      "\n",
      "for value in student_dict.values():\n",
      "    print(student_dict.values())\n",
      "8/69:\n",
      "student_dict.keys()\n",
      "\n",
      "for value in student_dict.values():\n",
      "    print(value)\n",
      "8/70:\n",
      "student_dict.keys()\n",
      "student_dict.values()\n",
      "8/71:\n",
      "student_dict.keys()\n",
      "student_dict.values()\n",
      "student_dict.items()\n",
      "8/72:\n",
      "print(student_dict.keys())\n",
      "print(student_dict.values())\n",
      "print(student_dict.items())\n",
      "8/73:\n",
      "print(student_dict.keys())\n",
      "print(student_dict.values())\n",
      "print(student_dict.items())\n",
      "\n",
      "for item in student_dict.items():\n",
      "    print (item)\n",
      "8/74:\n",
      "print(student_dict.keys())\n",
      "print(student_dict.values())\n",
      "8/75:\n",
      "print(student_dict.items())\n",
      "\n",
      "for item in student_dict.items():\n",
      "    print (item)\n",
      "    \n",
      "for key, value in student_dict.items():\n",
      "    print (key, value)\n",
      "8/76:\n",
      "print(student_dict.items())\n",
      "\n",
      "for item in student_dict.items():\n",
      "    print (item)\n",
      "8/77:\n",
      "for key, value in student_dict.items():\n",
      "    print (key, value)\n",
      "8/78:\n",
      "students_grades = {\"Lotte\": 6.7, \"Joep\": 7.2, \"Mirte\": 9.3, \"Dirk\": 5.2, \"Sanne\": 7.5, \"Roy\": 6.9}\n",
      "# your code goes here\n",
      "8/79:\n",
      "avg = 0\n",
      "\n",
      "for counter in range(len(students_grades))\n",
      "    total += students_grades.values(counter)\n",
      "    avg = total/(counter+1)\n",
      "    counter += 1\n",
      "    print(avg)\n",
      "8/80:\n",
      "students_grades = {\"Lotte\": 6.7, \"Joep\": 7.2, \"Mirte\": 9.3, \"Dirk\": 5.2, \"Sanne\": 7.5, \"Roy\": 6.9}\n",
      "# your code goes here\n",
      "8/81:\n",
      "students_grades = {\"Lotte\": 6.7, \"Joep\": 7.2, \"Mirte\": 9.3, \"Dirk\": 5.2, \"Sanne\": 7.5, \"Roy\": 6.9}\n",
      "# your code goes here\n",
      "8/82: range(len(students_grades))\n",
      "8/83: print(range(len(students_grades)))\n",
      "8/84:\n",
      "avg = 0\n",
      "for counter in range(len(students_grades))\n",
      "    total += students_grades.values(counter)\n",
      "    avg = total/(counter+1)\n",
      "    counter += 1\n",
      "    print(avg)\n",
      "8/85:\n",
      "avg = 0\n",
      "for counter in range(len(students_grades)):\n",
      "    total += students_grades.values(counter)\n",
      "    avg = total/(counter+1)\n",
      "    counter += 1\n",
      "    print(avg)\n",
      "8/86:\n",
      "avg = 0\n",
      "total = 0\n",
      "for counter in range(len(students_grades)):\n",
      "    total += students_grades.values(counter)\n",
      "    avg = total/(counter+1)\n",
      "    counter += 1\n",
      "    print(avg)\n",
      "8/87:\n",
      "avg = 0\n",
      "total = 0\n",
      "for counter in range(len(students_grades)):\n",
      "    total += students_grades.values(counter)\n",
      "    avg = total/(counter+1)\n",
      "    counter += 1\n",
      "    print(avg)\n",
      "8/88:\n",
      "avg = 0\n",
      "total = 0\n",
      "students_grades.values(1)\n",
      "for counter in range(len(students_grades)):\n",
      "    total += students_grades.values(counter)\n",
      "    avg = total/(counter+1)\n",
      "    counter += 1\n",
      "    print(avg)\n",
      "8/89:\n",
      "avg = 0\n",
      "total = 0\n",
      "students_grades.values()\n",
      "for counter in range(len(students_grades)):\n",
      "    total += students_grades.values(counter)\n",
      "    avg = total/(counter+1)\n",
      "    counter += 1\n",
      "    print(avg)\n",
      "8/90: range(len(students_grades)))\n",
      "8/91:\n",
      "students_grades = {\"Lotte\": 6.7, \"Joep\": 7.2, \"Mirte\": 9.3, \"Dirk\": 5.2, \"Sanne\": 7.5, \"Roy\": 6.9}\n",
      "# your code goes here\n",
      "8/92: range(len(students_grades))\n",
      "8/93: students_grades.values[0]\n",
      "8/94: students_grades.values()\n",
      "8/95: students_grades.values()[0]\n",
      "8/96:\n",
      "students_grades.values()\n",
      "dict_values[0]\n",
      "8/97: students_grades.values()\n",
      "8/98: type(students_grades.values())\n",
      "8/99:\n",
      "total = 0\n",
      "\n",
      "for value in students_grades.values():\n",
      "    print(value)\n",
      "8/100:\n",
      "total = 0\n",
      "\n",
      "for value in students_grades.values():\n",
      "    total += value\n",
      "    \n",
      "avg = total/len(students_grades)\n",
      "8/101:\n",
      "total = 0\n",
      "\n",
      "for value in students_grades.values():\n",
      "    total += value\n",
      "    \n",
      "avg = total/len(students_grades)\n",
      "\n",
      "print(avg)\n",
      "8/102:\n",
      "total = 0\n",
      "\n",
      "for value in students_grades.values():\n",
      "    total += value\n",
      "    \n",
      "total/len(students_grades)\n",
      "8/103: print(enrollments)\n",
      "8/104: enrollments\n",
      "8/105: enrollments <- enrollments[0:3]\n",
      "8/106: enrollments <- enrollments[0,3]\n",
      "8/107: enrollments <- enrollments[0]\n",
      "8/108: enrollments[0]\n",
      "8/109: enrollments[1]\n",
      "8/110: enrollments[0:1]\n",
      "8/111: enrollments[0:3]\n",
      "8/112: enrollments[0:2]\n",
      "8/113: enrollments = enrollments[0:2]\n",
      "8/114:\n",
      "enrollments = enrollments[0:2]\n",
      "print(enrollments)\n",
      "8/115:\n",
      "enrollments = enrollments[0:2]\n",
      "enrollments\n",
      "8/116:\n",
      "name = []\n",
      "\n",
      "for course in enrollments:\n",
      "    for student in enrollments[course][\"students\"]:\n",
      "        name.append(enrollments[course][\"students\"][\"name\"])\n",
      "8/117:\n",
      "name = []\n",
      "\n",
      "for course in enrollments:\n",
      "    for student in course[\"students\"]:\n",
      "        name.append(student[\"name\"])\n",
      "8/118:\n",
      "name = []\n",
      "for course in enrollments:\n",
      "    for student in course[\"students\"]:\n",
      "        name.append(student[\"name\"])\n",
      "8/119:\n",
      "name = []\n",
      "for course in enrollments:\n",
      "    for student in course[\"students\"]:\n",
      "        name.append(student[\"name\"])\n",
      "8/120:\n",
      "name = []\n",
      "for course in enrollments:\n",
      "    for student in course[\"students\"]:\n",
      "        name.append(student[\"name\"])\n",
      "print(name)\n",
      "8/121:\n",
      "prices_dict = {\"beer\": 9.99, \"meat\": 3.95, \"toaster\": 24.95}\n",
      "prices_dict_discount = {}\n",
      "\n",
      "for product, price in prices_dict.items():\n",
      "    price_discount = price * 0.85\n",
      "    prices_dict_discount[product] = price_discount\n",
      "    \n",
      "prices_dict_discount\n",
      "8/122:\n",
      "prices_dict = {\"beer\": 9.99, \"meat\": 3.95, \"toaster\": 24.95}\n",
      "\n",
      "price_list = {}\n",
      "for product, price in prices_dict:\n",
      "    price_dis = price * 0.85\n",
      "    price_list[product] = price_dis\n",
      "\n",
      "price_list\n",
      "8/123:\n",
      "prices_dict = {\"beer\": 9.99, \"meat\": 3.95, \"toaster\": 24.95}\n",
      "price_list = {}\n",
      "for product, price in prices_dict.items():\n",
      "    price_dis = price * 0.85\n",
      "    price_list[product] = price_dis\n",
      "\n",
      "price_list\n",
      "8/124:\n",
      "# solution\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "    for student in course[\"students\"]:\n",
      "        students_enrolled.append(student[\"name\"])\n",
      "    \n",
      "print(students_enrolled)\n",
      "8/125: [students_enrolled.append(student[\"name\"]) for course in enrollments, student in course[\"students\"]]\n",
      "8/126: [students_enrolled.append(student[\"name\"]) for course in enrollments: for student in course[\"students\"]]\n",
      "8/127:\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "    [students_enrolled.append(student[\"name\"]) for student in course[\"students\"]]\n",
      "8/128:\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "    print[students_enrolled.append(student[\"name\"]) for student in course[\"students\"]]\n",
      "8/129:\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "    print(students_enrolled.append(student[\"name\"]) for student in course[\"students\"]])\n",
      "8/130:\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "    print(students_enrolled.append(student[\"name\"]) for student in course[\"students\"])\n",
      "8/131:\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "   students_enrolled.append(student[\"name\"]) for student in course[\"students\"]\n",
      "8/132:\n",
      "students_enrolled = []\n",
      "for course in enrollments: \n",
      "   [students_enrolled.append(student[\"name\"])for student in course[\"students\"]]\n",
      "8/133:\n",
      "students_enrolled = []\n",
      "for course in enrollments: \n",
      "   print[students_enrolled.append(student[\"name\"])for student in course[\"students\"]]\n",
      "8/134:\n",
      "students_enrolled = []\n",
      "for course in enrollments: \n",
      "   [students_enrolled.append(student[\"name\"])for student in course[\"students\"]]\n",
      "print(students_enrolled)\n",
      "8/135:\n",
      "students_enrolled = []\n",
      "[students_enrolled.append(student[\"name\"])for student in course[\"students\"] for course in enrollments]\n",
      "print(students_enrolled)\n",
      "8/136:\n",
      "enrollments = enrollments[0:2]\n",
      "enrollments\n",
      "8/137:\n",
      "students_enrolled = []\n",
      "[students_enrolled.append(student[\"name\"])for student in course[\"students\"] for course in enrollments]\n",
      "print(students_enrolled)\n",
      "8/138:\n",
      "# solution\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "    for student in course[\"students\"]:\n",
      "        students_enrolled.append(student[\"name\"])\n",
      "    \n",
      "print(students_enrolled)\n",
      "8/139: enrollments\n",
      "8/140:\n",
      "students_enrolled = []\n",
      "[students_enrolled.append(student[\"name\"])for student in course[\"students\"] for course in enrollments]\n",
      "print(students_enrolled)\n",
      "8/141:\n",
      "students_enrolled = []\n",
      "[students_enrolled.append(student[\"name\"])for student in course[\"students\"] for course in enrollments]\n",
      "8/142: enrollments\n",
      "8/143:\n",
      "students_enrolled = []\n",
      "[students_enrolled.append(student[\"name\"])for student in course[\"students\"] for course in enrollments]\n",
      "8/144:\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "   [student[\"name\"]for student in course[\"students\"]]\n",
      "8/145:\n",
      "for course in enrollments: \n",
      "   print([student[\"name\"]for student in course[\"students\"]])\n",
      "8/146: print([student[\"name\"]for student in course[\"students\"] for course in enrollments]\n",
      "8/147: [student[\"name\"]for student in course[\"students\"] for course in enrollments]\n",
      "8/148:\n",
      "for course in enrollments:\n",
      "    [student[\"name\"]for student in course[\"students\"]]\n",
      "8/149:\n",
      "for course in enrollments:\n",
      "    print([student[\"name\"]for student in course[\"students\"]])\n",
      "8/150:\n",
      "for course in enrollments:\n",
      "    name =[student[\"name\"]for student in course[\"students\"]]\n",
      "8/151:\n",
      "for course in enrollments:\n",
      "    name =[student[\"name\"]for student in course[\"students\"]]\n",
      "    \n",
      "name\n",
      "8/152:\n",
      "for course in enrollments:\n",
      "    name +=[student[\"name\"]for student in course[\"students\"]]\n",
      "    \n",
      "name\n",
      "8/153:\n",
      "name = []\n",
      "for course in enrollments:\n",
      "    name +=[student[\"name\"]for student in course[\"students\"]]\n",
      "    \n",
      "name\n",
      "8/154:\n",
      "# solution\n",
      "total = 0\n",
      "for value in students_grades.values(): \n",
      "    total += value\n",
      "total/len(students_grades)\n",
      "8/155:\n",
      "total += value for value in students_grades.values()\n",
      "avg = total/len(students_grades)\n",
      "8/156:\n",
      "total = 0\n",
      "total+= value for value in students_grades.values()\n",
      "avg = total/len(students_grades)\n",
      "8/157:\n",
      "total = 0\n",
      "[total+= value for value in students_grades.values()]\n",
      "avg = total/len(students_grades)\n",
      "8/158:\n",
      "total = 0\n",
      "(total+= value for value in students_grades.values())\n",
      "avg = total/len(students_grades)\n",
      "8/159:\n",
      "total = 0\n",
      "total+= value for value in students_grades.values()\n",
      "avg = total/len(students_grades)\n",
      "8/160:\n",
      "total = 0\n",
      "total += [value for value in students_grades.values()]\n",
      "avg = total/len(students_grades)\n",
      "8/161:\n",
      "total = 0\n",
      "total += (value for value in students_grades.values())\n",
      "avg = total/len(students_grades)\n",
      "8/162:\n",
      "total = 0\n",
      "total += value for value in students_grades.values()\n",
      "avg = total/len(students_grades)\n",
      "8/163:\n",
      "total = 0\n",
      "[value for value in students_grades.values()]\n",
      "8/164:\n",
      "total = 0\n",
      "[value for value in students_grades.values()][0]\n",
      "8/165: total += [value for value in students_grades.values()]\n",
      "8/166: total += value for value in students_grades.values()\n",
      "8/167: value for value in students_grades.values()\n",
      "8/168: [value for value in students_grades.values()]\n",
      "8/169: [students_grades.values()]\n",
      "8/170: [value for value in students_grades.values()]\n",
      "8/171: total += [value for value in students_grades.values()]\n",
      "8/172: total += [value for value in students_grades.values()][value]\n",
      "8/173:\n",
      "name = []\n",
      "name +=[student[\"name\"]for student in course[\"students\"]] for course in enrollments\n",
      "    \n",
      "name\n",
      "8/174:\n",
      "name = []\n",
      "name +=[student[\"name\"] for course in enrollments for student in course[\"students\"]] \n",
      "    \n",
      "name\n",
      "8/175:\n",
      "name =[student[\"name\"] for course in enrollments for student in course[\"students\"]] \n",
      "    \n",
      "name\n",
      "8/176: name =[student[\"name\"] for course in enrollments for student in course[\"students\"]]\n",
      "8/177:\n",
      "name =[student[\"name\"] for course in enrollments for student in course[\"students\"]] \n",
      "name\n",
      "8/178:\n",
      "book_price_usd = 19\n",
      "book_price_eur = book_price_usd * .82 \n",
      "print(f\"The price in euros is {book_price_eur}\")\n",
      "\n",
      "tablet_price_usd = 349\n",
      "tablet_price_eur = tablet_price_usd * .82 \n",
      "print(f\"The price in euros is {tablet_price_eur}\")\n",
      "\n",
      "laptop_price_usd = 699\n",
      "laptop_price_eur = laptop_price_usd * .82 \n",
      "print(f\"The price in euros is {laptop_price_eur}\")\n",
      "8/179:\n",
      "book_price_usd = 19\n",
      "book_price_eur = book_price_usd * .82 \n",
      "print(\"The price in euros is {book_price_eur}\")\n",
      "\n",
      "tablet_price_usd = 349\n",
      "tablet_price_eur = tablet_price_usd * .82 \n",
      "print(f\"The price in euros is {tablet_price_eur}\")\n",
      "\n",
      "laptop_price_usd = 699\n",
      "laptop_price_eur = laptop_price_usd * .82 \n",
      "print(f\"The price in euros is {laptop_price_eur}\")\n",
      "8/180:\n",
      "book_price_usd = 19\n",
      "book_price_eur = book_price_usd * .82 \n",
      "print(f\"The price in euros is {book_price_eur}\")\n",
      "\n",
      "tablet_price_usd = 349\n",
      "tablet_price_eur = tablet_price_usd * .82 \n",
      "print(f\"The price in euros is {tablet_price_eur}\")\n",
      "\n",
      "laptop_price_usd = 699\n",
      "laptop_price_eur = laptop_price_usd * .82 \n",
      "print(f\"The price in euros is {laptop_price_eur}\")\n",
      "8/181:\n",
      "def sing_happy_birthday(): \n",
      "    print(\"Happy Birthday To You\")\n",
      "    print(\"Happy Birthday To You\")\n",
      "    print(\"Happy Birthday Dear You\")\n",
      "    print(\"Happy Birthday To You\")\n",
      "\n",
      "sing_happy_birthday()\n",
      "8/182:\n",
      "sing_happy_birthday()\n",
      "sing_happy_birthday()\n",
      "sing_happy_birthday()\n",
      "8/183:\n",
      "instructor = \"Hannes\"\n",
      "\n",
      "def say_hello():\n",
      "    print(\"Hello\" + instructor)\n",
      "    \n",
      "print(instructor)\n",
      "8/184:\n",
      "instructor = \"Hannes\"\n",
      "\n",
      "def say_hello():\n",
      "    print(\"Hello\" + instructor)\n",
      "8/185:\n",
      "instructor = \"Hannes\"\n",
      "\n",
      "def say_hello():\n",
      "    print(\"Hello\" + instructor)\n",
      "    \n",
      "say_hello()\n",
      "print(instructor)\n",
      "8/186:\n",
      "instructor = \"Hannes\"\n",
      "\n",
      "def say_hello():\n",
      "    print(\"Hello \" + instructor)\n",
      "    \n",
      "say_hello()\n",
      "print(instructor)\n",
      "8/187:\n",
      "def say_hello(first_name):\n",
      "    print(\"Hello \" + first_name)\n",
      "    \n",
      "say_hello(\"Hannes\")\n",
      "8/188:\n",
      "def say_hello(first_name, last_name):\n",
      "    print(\"Hello \" + first_name + last_name)\n",
      "\n",
      "say_hello(Trang, Bui)\n",
      "8/189:\n",
      "def say_hello(first_name, last_name):\n",
      "    print(\"Hello \" + first_name + last_name)\n",
      "\n",
      "say_hello(\"Trang\", \"Bui\")\n",
      "8/190:\n",
      "def say_hello(first_name, last_name):\n",
      "    print(\"Hello \" + first_name +\" \"+ last_name)\n",
      "\n",
      "say_hello(\"Trang\", \"Bui\")\n",
      "8/191:\n",
      "def add(a,b):\n",
      "    print(result = a + b)\n",
      "\n",
      "add(3,4)\n",
      "8/192:\n",
      "def add(a,b):\n",
      "    print(a + b)\n",
      "\n",
      "add(3,4)\n",
      "8/193:\n",
      "def add(a,b):\n",
      "    print(a + b)\n",
      "\n",
      "add(5,-2)\n",
      "8/194:\n",
      "def add(a,b):\n",
      "    return a + b\n",
      "\n",
      "add(5,-2)\n",
      "8/195:\n",
      "def add(a,b):\n",
      "    return a + b\n",
      "\n",
      "add(add(5,-2),4)\n",
      "8/196:\n",
      "def add(a,b):\n",
      "    return a + b\n",
      "\n",
      "add(add(add(5,-2),4),3)\n",
      "8/197:\n",
      "# your code goes here!\n",
      "day_of_the_week = [Mon, Tue, Wed, Thu, Fri, Sat, Sun]\n",
      "def return_day(x):\n",
      "    if x > 0 and x < 8:\n",
      "        return day_of_the_week[x-1]\n",
      "    else:\n",
      "        return print(\"None\")\n",
      "\n",
      "return_day(1)\n",
      "8/198:\n",
      "# your code goes here!\n",
      "day_of_the_week = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
      "def return_day(x):\n",
      "    if x > 0 and x < 8:\n",
      "        return day_of_the_week[x-1]\n",
      "    else:\n",
      "        return print(\"None\")\n",
      "\n",
      "return_day(1)\n",
      "8/199:\n",
      "# your code goes here!\n",
      "day_of_the_week = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
      "def return_day(x):\n",
      "    if x > 0 and x < 8:\n",
      "        return day_of_the_week[x-1]\n",
      "    else:\n",
      "        return print(\"None\")\n",
      "\n",
      "return_day(8)\n",
      "8/200:\n",
      "# your code goes here!\n",
      "day_of_the_week = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
      "def return_day(x):\n",
      "    if x > 0 and x < 8:\n",
      "        return day_of_the_week[x-1]\n",
      "    else:\n",
      "        return print(\"None\")\n",
      "\n",
      "return_day(7)\n",
      "8/201:\n",
      "# your code goes here!\n",
      "day_of_the_week = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
      "def return_day(x):\n",
      "    if x > 0 and x < 8:\n",
      "        return day_of_the_week[x-1]\n",
      "    else:\n",
      "        return print(\"None\")\n",
      "\n",
      "return_day(1)\n",
      "8/202:\n",
      "# your code goes here!\n",
      "day_of_the_week = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
      "def return_day(x):\n",
      "    if x > 0 and x < 8:\n",
      "        return day_of_the_week[x-1]\n",
      "    else:\n",
      "        return print(\"None\")\n",
      "\n",
      "return_day(8)\n",
      "8/203:\n",
      "# your code goes here!\n",
      "day_of_the_week = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
      "def return_day(x):\n",
      "    if x > 0 and x < 8:\n",
      "        return day_of_the_week[x-1]\n",
      "    else:\n",
      "        return print(\"None\")\n",
      "\n",
      "return_day(8)\n",
      "8/204:\n",
      "def return_day(x):\n",
      "    days={1 : \"Mon\", 2: \"Tue\", 3: \"Wed\", 4 : \"Thu\", 5 : \"Fri\", 6: \"Sat\", 7: \"Sun\"}\n",
      "    return days.get(num)\n",
      "return_day(1)\n",
      "8/205:\n",
      "def return_day(x):\n",
      "    days={1 : \"Mon\", 2: \"Tue\", 3: \"Wed\", 4 : \"Thu\", 5 : \"Fri\", 6: \"Sat\", 7: \"Sun\"}\n",
      "    return days.get(x)\n",
      "return_day(1)\n",
      "8/206:\n",
      "def return_day(x):\n",
      "    days={1 : \"Mon\", 2: \"Tue\", 3: \"Wed\", 4 : \"Thu\", 5 : \"Fri\", 6: \"Sat\", 7: \"Sun\"}\n",
      "    return days.get(x)\n",
      "return_day(8)\n",
      "8/207:\n",
      "def return_day(x):\n",
      "    days={1 : \"Mon\", 2: \"Tue\", 3: \"Wed\", 4 : \"Thu\", 5 : \"Fri\", 6: \"Sat\", 7: \"Sun\"}\n",
      "    return days.get(x)\n",
      "print(return_day(8))\n",
      "8/208:\n",
      "def convert_usd_eur(usd_amounts, currency_rate):\n",
      "    eur_amounts = []\n",
      "    for usd_amount in usd_amounts: \n",
      "        eur_amounts.append(usd_amount * currency_rate)\n",
      "        return eur_amounts\n",
      "        \n",
      "convert_usd_eur([10, 100, 1000], .82)\n",
      "8/209:\n",
      "def convert2(usd, rate):\n",
      "    eur = []\n",
      "    [eur = usd * rate for usd in usd]\n",
      "    return eur\n",
      "\n",
      "covert2([10, 100, 1000], .82)\n",
      "8/210:\n",
      "def convert2(usd, rate):\n",
      "    eur = [usd * rate for usd in usd]\n",
      "    return eur\n",
      "\n",
      "covert2([10, 100, 1000], .82)\n",
      "8/211:\n",
      "def convert2(usd, rate):\n",
      "    return [usd * rate for usd in usd]\n",
      "    \n",
      "covert2([10, 100, 1000], .82)\n",
      "8/212:\n",
      "def convert2(usd, rate):\n",
      "    return [usd * rate for usd in usd]\n",
      "    \n",
      "convert2([10, 100, 1000], .82)\n",
      "8/213:\n",
      "def convert_usd_eur(usd_amounts, currency_rate):\n",
      "    eur_amounts = []\n",
      "    for usd_amount in usd_amounts: \n",
      "        eur_amounts.append(usd_amount * currency_rate)\n",
      "    return eur_amounts\n",
      "        \n",
      "convert_usd_eur([10, 100, 1000], .82)\n",
      "8/214:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        return total += num_list[num] \n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/215:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        return total = total + num_list[num]\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/216:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        return total = total + num_list[num]\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/217:\n",
      "num_list = [3, -4, 1, 2, 0]\n",
      "num_list[0]\n",
      "8/218:\n",
      "num_list = [3, -4, 1, 2, 0]\n",
      "num_list[0] + num_list[1]\n",
      "8/219:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        return total + num_list[num]\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/220:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        total += num_list[num]\n",
      "    return total\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/221:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        total += num_list[num]\n",
      "        return total\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/222:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        return total + num\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/223:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        total += num\n",
      "        return total\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/224:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        total += num\n",
      "    return total\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/225:\n",
      "import os\n",
      "os.getcwd()\n",
      "8/226:\n",
      "# solution\n",
      "def add_list(num_list):\n",
      "    \"\"\"adding up all the numbers in the list\"\"\"\n",
      "    total = 0\n",
      "    for num in num_list: \n",
      "        total += num\n",
      "    return total\n",
      "\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "10/1: import requests\n",
      "10/2: import requests\n",
      "10/3: url = \"https://www.twitch.tv/directory/game/The%20Sims%204\"\n",
      "10/4: url = \"https://www.twitch.tv/directory/game/The%20Sims%204\"\n",
      "10/5:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "10/6:\n",
      "url = \"https://www.twitch.tv/directory/game/The%20Sims%204\"\n",
      "request = request.get(url)\n",
      "10/7:\n",
      "url = \"https://www.twitch.tv/directory/game/The%20Sims%204\"\n",
      "request = requests.get(url)\n",
      "10/8: source.code = request.test\n",
      "10/9: source.code = request.test\n",
      "10/10: source.code = request.text\n",
      "10/11:\n",
      "url = \"https://www.twitch.tv/directory/game/The%20Sims%204\"\n",
      "request = requests.get(url)\n",
      "10/12: source.code = request.text\n",
      "10/13: src = request.text\n",
      "10/14:\n",
      "src = request.text\n",
      "\n",
      "soup = BeautifulSoup(src)\n",
      "10/15:\n",
      "src = request.text\n",
      "\n",
      "soup = BeautifulSoup(src)\n",
      "print(soup.find('h1'))\n",
      "10/16:\n",
      "src = request.text\n",
      "\n",
      "soup = BeautifulSoup(src)\n",
      "print(soup.find('h3'))\n",
      "10/17:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "10/18:\n",
      "url = \"https://www.twitch.tv/directory/game/The%20Sims%204\"\n",
      "request = requests.get(url)\n",
      "10/19:\n",
      "src = request.text\n",
      "\n",
      "soup = BeautifulSoup(src)\n",
      "print(soup.find('h3'))\n",
      "10/20:\n",
      "src = request.text\n",
      "\n",
      "soup = BeautifulSoup(src)\n",
      "print(soup.find('h1 class=\"CoreText-sc-cpl358-0 ScTitleText-sc-1gsen4-0 gAYdrP tw-title'))\n",
      "10/21:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "10/22:\n",
      "url = \"https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=sustainability&oq=\"\n",
      "request = requests.get(url)\n",
      "10/23:\n",
      "src = request.text\n",
      "\n",
      "soup = BeautifulSoup(src)\n",
      "print(soup.find('h3'))\n",
      "10/24:\n",
      "src = request.text\n",
      "\n",
      "soup = BeautifulSoup(src)\n",
      "print(soup.find_all('h3'))\n",
      "12/1: import requests\n",
      "12/2: import requests\n",
      "12/3: import requests\n",
      "12/4:\n",
      "url = \"https://www.hornbach.nl/shop/Planten/Tuinplanten/Vaste-planten/S5120/artikeloverzicht.html\"\n",
      "plant_request = request.get(url)\n",
      "12/5:\n",
      "url = \"https://www.hornbach.nl/shop/Planten/Tuinplanten/Vaste-planten/S5120/artikeloverzicht.html\"\n",
      "plant_request = requests.get(url)\n",
      "12/6: plant_src = plant_request.text\n",
      "12/7:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src)\n",
      "12/8:\n",
      "plant_src = plant_request.text\n",
      "type(plant_request)\n",
      "12/9: ?type\n",
      "12/10: print(plant_src[1:10])\n",
      "12/11:\n",
      "plant_src = plant_request.text\n",
      "print(plant_request)\n",
      "12/12:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src)\n",
      "12/13: print(plant_src[5000:10000])\n",
      "12/14:\n",
      "a = \"<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n",
      "<!DOCTYPE html>\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"nl\" id=\"ng-app\">\n",
      "\n",
      "\n",
      "<head>\n",
      "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=EDGE\" />\n",
      "    <meta name=\"csrf-token\" content=\"V4W7-KLVS-P9PL-WFST-LKYY-WGO6-K83E-WUQC\" />\n",
      "    <title>\"\n",
      "len(a)\n",
      "12/15:\n",
      "a = '<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n",
      "<!DOCTYPE html>\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"nl\" id=\"ng-app\">\n",
      "\n",
      "\n",
      "<head>\n",
      "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=EDGE\" />\n",
      "    <meta name=\"csrf-token\" content=\"V4W7-KLVS-P9PL-WFST-LKYY-WGO6-K83E-WUQC\" />\n",
      "    <title>'\n",
      "len(a)\n",
      "12/16:\n",
      "a = \"<?xml version=\"1.0\" encoding=\"utf-8\" ?><!DOCTYPE html><html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"nl\" id=\"ng-app\">\n",
      "<head>  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=EDGE\" />\n",
      "    <meta name=\"csrf-token\" content=\"V4W7-KLVS-P9PL-WFST-LKYY-WGO6-K83E-WUQC\" />\n",
      "    <title>\"\n",
      "len(a)\n",
      "12/17:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [2000:2500])\n",
      "12/18:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [1000:1100])\n",
      "12/19:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [500:1100])\n",
      "12/20:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:500])\n",
      "12/21:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:300])\n",
      "12/22:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:350])\n",
      "12/23:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:340])\n",
      "12/24:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:330])\n",
      "12/25:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:335])\n",
      "12/26:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:332])\n",
      "12/27:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [200:332])\n",
      "12/28:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [280:332])\n",
      "12/29:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [270:332])\n",
      "12/30:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [278:332])\n",
      "12/31:\n",
      "#html src from the retrieved webpage\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:332])\n",
      "12/32:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(soup.find('title'))\n",
      "12/33:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(soup.find('title')[0:60])\n",
      "12/34:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print((soup.find('title'))[0:60]))\n",
      "12/35:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print((soup.find('title'))[0:60])\n",
      "12/36:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "type(soup.find('title')\n",
      "12/37:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "type(soup.find('title'))\n",
      "12/38:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "soup.find('title')\n",
      "12/39:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "soup.find('title')[1]\n",
      "12/40:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "string(soup.find('title'))\n",
      "12/41:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "str(soup.find('title'))\n",
      "12/42:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "str(soup.find('title'))[0:60]\n",
      "12/43:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(str(soup.find('title'))[0:60])\n",
      "12/44:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(str(soup.find('title'))[8:60])\n",
      "12/45:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(str(soup.find('title'))[7:60])\n",
      "12/46:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(str(soup.find('title'))[7:61])\n",
      "12/47:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(str(soup.find('title'))[7:62])\n",
      "12/48:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(str(soup.find('title'))[7:61])\n",
      "12/49:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(str(soup.find('title'))[7:61])\n",
      "12/50: soup.find_all('h1')\n",
      "12/51: soup.find_all('h2')\n",
      "12/52: soup.find_all('meta')\n",
      "12/53: soup.find_all('price')\n",
      "12/54: soup.find_all('prijs')\n",
      "12/55: soup.find_all('rating')\n",
      "12/56: soup.find_all('vaste')\n",
      "12/57: soup.find_all('meta')\n",
      "12/58: soup.find_all('meta')[1]\n",
      "12/59: soup.find_all('title')[1]\n",
      "12/60: soup.find_all('title')\n",
      "12/61: soup.find_all('title').get_text()\n",
      "12/62: soup.find_all('title').[1]get_text()\n",
      "12/63: soup.find_all('title')[1].get_text()\n",
      "12/64: soup.find_all('title')[0].get_text()\n",
      "12/65: type(soup.find_all('title')[0].get_text())\n",
      "12/66: print(soup.find_all('title')[0].get_text())\n",
      "12/67:\n",
      "url2 = \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n",
      "book = requests.get(url)\n",
      "12/68:\n",
      "url2 = \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n",
      "book = requests.get(url)\n",
      "12/69:\n",
      "url2 = \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n",
      "book = requests.get(url)\n",
      "book_src = book.text\n",
      "soup2 = BeautifulSoup(book_src)\n",
      "12/70:\n",
      "url2 = \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n",
      "book = requests.get(url)\n",
      "book_src = book.text\n",
      "soup2 = BeautifulSoup(book_src)\n",
      "book\n",
      "12/71: soup2.find(table)\n",
      "12/72: soup2.find(\"table\")\n",
      "12/73: print(soup2.find(\"table\"))\n",
      "12/74: print(soup2.find('table'))\n",
      "12/75: print(soup2.find('h1'))\n",
      "12/76:\n",
      "url2 = \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n",
      "book = requests.get(url)\n",
      "book_src = book.text\n",
      "soup2 = BeautifulSoup(book_src)\n",
      "12/77: print(soup2.find('h1'))\n",
      "12/78:\n",
      "url2 = \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n",
      "book = requests.get(url2)\n",
      "book_src = book.text\n",
      "soup2 = BeautifulSoup(book_src)\n",
      "12/79: print(soup2.find('table'))\n",
      "12/80: print(soup2.find.all('table'))\n",
      "12/81: print(soup2.find_all('table'))\n",
      "12/82: print(soup2.find('table'))\n",
      "12/83: print(soup2.find('table').get_text())\n",
      "12/84: soup2.find('table').get_text()\n",
      "12/85: print(soup2.find('table'))\n",
      "12/86: print(soup2.find('table'))\n",
      "12/87: print(soup2.find('table').find_all('td'))\n",
      "12/88: print(soup2.find('table').find_all('td')[4])\n",
      "12/89: print(soup2.find('table').find_all('td')[4].get_text())\n",
      "12/90: print(soup2.find('table').find_all('td')[4].get_text())\n",
      "12/91:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find_all(\"h2\").get_text())\n",
      "12/92:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find_all(\"h2\").get_text())\n",
      "coursera\n",
      "12/93:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "coursera\n",
      "12/94:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find_all('h2').get_text())\n",
      "12/95:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find_all('h2'))\n",
      "12/96:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find_all('h2 class=\"color-primary-text card-title headline-1-text\"'))\n",
      "12/97:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find_all('color-primary-text card-title headline-1-text'))\n",
      "12/98:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find_all('color-primary-text'))\n",
      "12/99:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find('color-primary-text'))\n",
      "12/100:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find('color-primary-text'))\n",
      "12/101:\n",
      "url4 = \"https://icanhazdadjoke.com/\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "joke = response.json()\n",
      "print(joke)\n",
      "11/1:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "joke_request = response.json() \n",
      "print(joke_request)\n",
      "11/2:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "joke_request = response.json() \n",
      "print(joke_request)\n",
      "12/102:\n",
      "url4 = \"https://icanhazdadjoke.com/\"\n",
      "response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "joke = response.json()\n",
      "print(joke)\n",
      "12/103:\n",
      "url4 = \"https://icanhazdadjoke.com/\"\n",
      "response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "joke = response.json()\n",
      "print(joke)\n",
      "12/104:\n",
      "url4 = \"https://icanhazdadjoke.com/\"\n",
      "response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "joke = response.json()\n",
      "print(joke)\n",
      "12/105:\n",
      "url4 = \"https://icanhazdadjoke.com/\"\n",
      "response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "joke = response.json()\n",
      "print(joke)\n",
      "12/106:\n",
      "url4 = \"https://icanhazdadjoke.com/\"\n",
      "response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "joke = response.json()\n",
      "print(joke['joke'])\n",
      "12/107:\n",
      "\n",
      "for i in range(9):\n",
      "    response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "    joke = response.json()\n",
      "    print(joke['joke'])\n",
      "12/108:\n",
      "\n",
      "for i in range(10):\n",
      "    response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "    joke = response.json()\n",
      "    print(joke['joke'])\n",
      "11/3:\n",
      "# Question 3\n",
      "jokes = [] \n",
      "\n",
      "for counter in range(10):\n",
      "    url = \"https://icanhazdadjoke.com\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "    joke_request = response.json() \n",
      "    jokes.append(joke_request)\n",
      "11/4:\n",
      "# Question 3\n",
      "jokes = [] \n",
      "\n",
      "for counter in range(10):\n",
      "    url = \"https://icanhazdadjoke.com\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "    joke_request = response.json() \n",
      "    jokes.append(joke_request)\n",
      "11/5:\n",
      "# Question 3\n",
      "jokes = [] \n",
      "\n",
      "for counter in range(10):\n",
      "    url = \"https://icanhazdadjoke.com\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "    joke_request = response.json() \n",
      "    jokes.append(joke_request)\n",
      "    \n",
      "print(jokes)\n",
      "11/6:\n",
      "# Question 3\n",
      "jokes = [] \n",
      "\n",
      "for counter in range(10):\n",
      "    url = \"https://icanhazdadjoke.com\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "    joke_request = response.json() \n",
      "    jokes.append(joke_request)\n",
      "    \n",
      "print(jokes['joke'])\n",
      "11/7:\n",
      "# Question 3\n",
      "jokes = [] \n",
      "\n",
      "for counter in range(10):\n",
      "    url = \"https://icanhazdadjoke.com\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "    joke_request = response.json() \n",
      "    jokes.append(joke_request)\n",
      "    \n",
      "print(jokes_request)\n",
      "11/8:\n",
      "# Question 3\n",
      "jokes = [] \n",
      "\n",
      "for counter in range(10):\n",
      "    url = \"https://icanhazdadjoke.com\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "    joke_request = response.json() \n",
      "    jokes.append(joke_request)\n",
      "    \n",
      "print(jokes)\n",
      "12/109:\n",
      "jokes = []\n",
      "for i in range(10):\n",
      "    response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "    joke = response.json()\n",
      "    jokes.append = joke\n",
      "12/110:\n",
      "jokes = []\n",
      "for i in range(10):\n",
      "    response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "    joke = response.json()\n",
      "    jokes.append (joke)\n",
      "12/111:\n",
      "jokes = []\n",
      "for i in range(10):\n",
      "    response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "    joke = response.json()\n",
      "    jokes.append(joke)\n",
      "    print(jokes)\n",
      "12/112:\n",
      "jokes = []\n",
      "for i in range(10):\n",
      "    response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "    joke = response.json()\n",
      "    jokes.append(joke['joke'])\n",
      "    print(jokes)\n",
      "12/113:\n",
      "jokes = []\n",
      "for i in range(10):\n",
      "    response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "    joke = response.json()\n",
      "    jokes.append(joke['joke'])\n",
      "print(jokes)\n",
      "17/1:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text)\n",
      "17/2: ?BeautifulSoup\n",
      "17/3:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text)\n",
      "print(soup)\n",
      "17/4:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "print(soup)\n",
      "17/5:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text)\n",
      "print(soup)\n",
      "17/6:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "print(soup)\n",
      "17/7:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "print(soup)\n",
      "17/8:\n",
      "soup2 = BeautifulSoup(res.text)\n",
      "print(soup2)\n",
      "17/9:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "print(soup)\n",
      "17/10:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "type(soup)\n",
      "17/11: soup.find_all(\"a\")\n",
      "15/1:\n",
      "# Run this code now\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find_all(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "\n",
      "soup.find(\"a\", href=True)[\"href\"]\n",
      "15/2:\n",
      "# Run this code now\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find_all(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "17/12:\n",
      "for link in soup.find_all(\"a\"):\n",
      "    print(link.attrs[\"href\"])\n",
      "17/13:\n",
      "for link in soup.find_all(\"a\"):\n",
      "    print(link.attrs[\"href\"])\n",
      "17/14:\n",
      "for link in soup.find_all(\"a\"):\n",
      "    print(link.attrs[\"href\"])\n",
      "17/15: soup.find(\"a\", href=True)[\"href\"]\n",
      "17/16: soup.find_all(\"a\", href=True)[\"href\"]\n",
      "17/17: soup.find(\"a\", href=True)\n",
      "17/18: soup.find(\"a\", href=True)[\"href\"]\n",
      "17/19: type(soup.find(\"a\", href=True))\n",
      "17/20: ?.find_all\n",
      "17/21: ?BeautifulSoup.find_all\n",
      "17/22: doc(BeautifulSoup.find_all)\n",
      "17/23: doc(BeautifulSoup)\n",
      "17/24: help(BeautifulSoup)\n",
      "17/25: help(BeautifulSoup.find_all)\n",
      "17/26:\n",
      "for link in soup.find_all(\"a\"):\n",
      "    print(link.attrs[\"href\"])\n",
      "17/27:\n",
      "for link in soup.find_all('a'):\n",
      "    print(link.attrs)\n",
      "17/28:\n",
      "for link in soup.find_all('a'):\n",
      "    print(link.attrs)\n",
      "17/29: soup.find_all('a')\n",
      "17/30: soup.find_all('a')\n",
      "17/31:\n",
      "for link in soup.find_all('a'):\n",
      "    print(link.attrs)\n",
      "17/32: soup.find_all(\"a\", class_=\"product_pod\")\n",
      "17/33: soup.find_all(class_= 'product_pod')\n",
      "17/34: soup.find_all(class_= 'product_pod')[0]\n",
      "17/35: soup.find_all(class_= 'product_pod')[0].find('a').attrs['href']\n",
      "17/36: soup.find_all(class_= 'product_pod')[0].find('a').attrs['href']\n",
      "17/37: soup.find_all(class_='product_pod')[1].find('a').attrs['href']\n",
      "17/38: soup.find_all(class_='product_pod')[1].find('a').attrs['href']\n",
      "17/39: ?slicing\n",
      "17/40: help(slicing)\n",
      "17/41: link = soup.find_all(class_='product_pod')[1].find('a').attrs['href']\n",
      "17/42: type(link)\n",
      "17/43:\n",
      "link = soup.find_all(class_='product_pod')[1].find('a').attrs['href']\n",
      "print(link)\n",
      "17/44: link[5:]\n",
      "17/45: book_url = 'https://books.toscrape.com/catalogue/' + link[5:]\n",
      "17/46:\n",
      "book_url = 'https://books.toscrape.com/catalogue/' + link[5:]\n",
      "\n",
      "print(book_url)\n",
      "17/47:\n",
      "book_url = 'https://books.toscrape.com/catalogue/' + link[6:]\n",
      "\n",
      "print(book_url)\n",
      "17/48:\n",
      "book_url = 'https://books.toscrape.com/catalogue/' + link[6:]\n",
      "\n",
      "print(book_url)\n",
      "17/49:\n",
      "book_url2 = link.replace('../../', 'https://books.toscrape.com/catalogue/')\n",
      "print(book_url2)\n",
      "17/50:\n",
      "book_url2 = link.replace('../../', 'https://books.toscrape.com/catalogue/')\n",
      "print(book_url2)\n",
      "17/51: soup.find_all(class_='product_pod')[0:5]\n",
      "17/52:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    for i in book:\n",
      "        link = book[i].find('a').attrs['href']\n",
      "        book_links.append(link)\n",
      "        print(book_links)\n",
      "17/53:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    for i in book:\n",
      "        link = book[i].find(\"a\").attrs[\"href\"]\n",
      "        book_links.append(link)\n",
      "        print(book_links)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/54:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    book_links.append(link)\n",
      "print(book_links)\n",
      "17/55: soup.find_all(class_='product_pod')[0]\n",
      "17/56:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    book_links.append(link)\n",
      "print(book_links\n",
      "17/57:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    book_links.append(link)\n",
      "print(book_links)\n",
      "17/58:\n",
      "book_links ={}\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links = {title : link}\n",
      "print(book_links)\n",
      "17/59:\n",
      "book_links ={}\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links += {title : link}\n",
      "print(book_links)\n",
      "17/60:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "print(book_links)\n",
      "17/61:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "print(book_links)[0]\n",
      "17/62:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "print(book_links)[0]\n",
      "17/63:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "print(book_links)\n",
      "17/64:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    book_links.append(link)\n",
      "print(book_links)[1]\n",
      "17/65:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    book_links.append(link)\n",
      "book_links[1]\n",
      "17/66:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "book_links[1]\n",
      "17/67:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "book_links[0]\n",
      "17/68:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "print(book_links[0])\n",
      "17/69:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "\n",
      "book_links.keys\n",
      "17/70:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "\n",
      "book_links[0:5]\n",
      "17/71:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "\n",
      "book_links[0:5]\n",
      "17/72:\n",
      "for book in book_links:\n",
      "    print(book.keys)\n",
      "17/73:\n",
      "for book in book_links:\n",
      "    if book.keys(\"title\")=\"A Light in the Attic\":\n",
      "        print book.values(\"link\")\n",
      "17/74:\n",
      "for book in book_links:\n",
      "    if book[\"title\"]=\"A Light in the Attic\":\n",
      "        print book(\"link\")\n",
      "17/75:\n",
      "for book in book_links:\n",
      "    if book[\"title\"]=\"A Light in the Attic\":\n",
      "        print book[\"link\"]\n",
      "17/76:\n",
      "for book in book_links:\n",
      "    if book.get[\"title\"]=\"A Light in the Attic\":\n",
      "        print book.get[\"link\"]\n",
      "17/77:\n",
      "for book in book_links:\n",
      "    if book.get[\"title\"]==\"A Light in the Attic\":\n",
      "        print book.get[\"link\"]\n",
      "17/78:\n",
      "for book in book_links:\n",
      "    if book[\"title\"] == \"A Light in the Attic\":\n",
      "        print book\n",
      "17/79:\n",
      "for book in book_links:\n",
      "    if book[\"title\"] == \"A Light in the Attic\":\n",
      "        print (book)\n",
      "17/80:\n",
      "for book in book_links:\n",
      "    if book[\"title\"] == \"A Light in the Attic\":\n",
      "        print (book)\n",
      "17/81: ?next\n",
      "15/3: next((book for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "17/82: next(book for book in book_links if book[\"title\"] == \"A Light in the Attic\")\n",
      "17/83: next(book for book in book_links if book[\"title\"] == \"A Light in the Attic\", None)\n",
      "17/84: next((book for book in book_links if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "17/85: next((book for book in book_links if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "17/86:\n",
      "full_url = []\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"].replace('../../','https://books.toscrape.com/catalogue/')\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "17/87:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "\n",
      "book_links[0:5]\n",
      "17/88:\n",
      "full_url = []\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"].replace('../../','https://books.toscrape.com/catalogue/')\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    full_url.append({'title': title, 'link': link})\n",
      "prink()\n",
      "17/89:\n",
      "full_url = []\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"].replace('../../','https://books.toscrape.com/catalogue/')\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    full_url.append({'title': title, 'link': link})\n",
      "prink(full_url)\n",
      "17/90:\n",
      "full_url = []\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"].replace('../../','https://books.toscrape.com/catalogue/')\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    full_url.append({'title': title, 'link': link})\n",
      "print(full_url)\n",
      "17/91:\n",
      "full_url = []\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"].replace('../../','https://books.toscrape.com/catalogue/')\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    full_url.append({'title': title, 'link': link})\n",
      "print(full_url)\n",
      "17/92: book_links[0:2]\n",
      "17/93: book_links[0:2]\n",
      "17/94: ?enumerate\n",
      "17/95:\n",
      "counter = 1\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/page-\"\n",
      "full_url = base_url + str(counter) + \".html\"\n",
      "print(full_url)\n",
      "17/96:\n",
      "counter = 1\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/page-\"\n",
      "full_url = base_url + str(counter) + \".html\"\n",
      "print(full_url)\n",
      "17/97: counter = range[50]\n",
      "17/98: counter = range(50)\n",
      "17/99: print(range(50))\n",
      "17/100: print(range(1,50))\n",
      "17/101:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/page-\"\n",
      "full_urls = []\n",
      "for counter in range(1,50):\n",
      "    full_urls.append(base_url + str(counter) + \".html\")\n",
      "\n",
      "print(full_urls)\n",
      "17/102:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/page-\"\n",
      "full_urls = []\n",
      "for counter in range(1,51):\n",
      "    full_urls.append(base_url + str(counter) + \".html\")\n",
      "\n",
      "print(full_urls)\n",
      "17/103:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/page-\"\n",
      "page = []\n",
      "for counter in range(1,51):\n",
      "    page.append(base_url + str(counter) + \".html\")\n",
      "\n",
      "print(pages)\n",
      "17/104:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/page-\"\n",
      "pages = []\n",
      "for counter in range(1,51):\n",
      "    pages.append(base_url + str(counter) + \".html\")\n",
      "\n",
      "print(pages)\n",
      "17/105: print(range(1, 50))\n",
      "17/106:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in rang(1, 101):\n",
      "    quote_page_urls.append (base + str(counter) + \"/\")\n",
      "print(quote_page_urls)(0:2)\n",
      "17/107:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in rang(1, 101):\n",
      "    quote_page_urls.append (base + str(counter) + \"/\")\n",
      "print(quote_page_urls)[0:2]\n",
      "17/108:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append (base + str(counter) + \"/\")\n",
      "print(quote_page_urls)[0:2]\n",
      "17/109:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "print(quote_page_urls)(0,2)\n",
      "17/110:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "print(quote_page_urls)(0:2)\n",
      "17/111:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "type(quote_page_urls)\n",
      "17/112:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "print(quote_page_urls [0,2])\n",
      "17/113:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "print(quote_page_urls [0:2])\n",
      "17/114:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "print(quote_page_urls [0:3])\n",
      "17/115:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "print(quote_page_urls [0:3])\n",
      "17/116:\n",
      "from time import sleep\n",
      "sleep(3)\n",
      "print(\"Done\")\n",
      "17/117:\n",
      "from time import sleep\n",
      "sleep(3)\n",
      "print(\"Done\")\n",
      "17/118:\n",
      "def gen_page_urls(base, page_num):\n",
      "    page_urls =[]\n",
      "    \n",
      "    for counter in range(1, page_num + 1):\n",
      "        full_url = base + \"page-\" + str(counter) + \".html\"\n",
      "        page_urls.append(full_url)\n",
      "    \n",
      "    return page_urls\n",
      "17/119:\n",
      "def gen_page_urls(base, page_num):\n",
      "    page_urls =[]\n",
      "    \n",
      "    for counter in range(1, page_num + 1):\n",
      "        full_url = base + \"page-\" + str(counter) + \".html\"\n",
      "        page_urls.append(full_url)\n",
      "    \n",
      "    return page_urls\n",
      "\n",
      "gen_page_urls(\"https://books.toscrape.com/catalogue/category/books_1/\", 3)\n",
      "17/120:\n",
      "def gen_page_urls(base, page_num):\n",
      "    page_urls =[]\n",
      "    \n",
      "    for counter in range(1, page_num + 1):\n",
      "        full_url = base + \"page-\" + str(counter) + \".html\"\n",
      "        page_urls.append(full_url)\n",
      "    \n",
      "    return page_urls\n",
      "\n",
      "gen_page_urls(\"https://books.toscrape.com/catalogue/category/books_1/\", 3)\n",
      "17/121:\n",
      "def extr_book_urls(page_urls):\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = Beautiful.Soup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class =\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "17/122:\n",
      "def extr_book_urls(page_urls):\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = Beautiful.Soup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "17/123:\n",
      "def extr_book_urls(page_urls):\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = Beautiful.Soup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "17/124:\n",
      "def extr_book_urls(page_urls):\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = Beautiful.Soup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "17/125:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = generate_page_urls(base_url, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extract_book_urls(page_urls)\n",
      "17/126:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extr_book_urls(page_urls)\n",
      "17/127:\n",
      "def extr_book_urls(page_urls):\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "17/128:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extr_book_urls(page_urls)\n",
      "17/129:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extr_book_urls(page_urls)\n",
      "book_list[0:2]\n",
      "20/1:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extr_book_urls(page_urls)\n",
      "book_list[0:2]\n",
      "20/2:\n",
      "def gen_page_urls(base, page_num):\n",
      "    page_urls =[]\n",
      "    \n",
      "    for counter in range(1, page_num + 1):\n",
      "        full_url = base + \"page-\" + str(counter) + \".html\"\n",
      "        page_urls.append(full_url)\n",
      "    \n",
      "    return page_urls\n",
      "\n",
      "gen_page_urls(\"https://books.toscrape.com/catalogue/category/books_1/\", 3)\n",
      "20/3:\n",
      "def extr_book_urls(page_urls):\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "20/4:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extr_book_urls(page_urls)\n",
      "book_list[0:2]\n",
      "20/5:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "20/6:\n",
      "for link in soup.find_all(\"a\"):\n",
      "    print(link.attrs[\"href\"])\n",
      "20/7:\n",
      "for link in soup.find_all('a'):\n",
      "    print(link.attrs)\n",
      "20/8: soup.find_all('a')\n",
      "20/9: soup.find_all(class_= 'product_pod')[0].find('a').attrs['href']\n",
      "20/10:\n",
      "link = soup.find_all(class_='product_pod')[1].find('a').attrs['href']\n",
      "print(link)\n",
      "20/11:\n",
      "book_url = 'https://books.toscrape.com/catalogue/' + link[6:]\n",
      "\n",
      "print(book_url)\n",
      "20/12:\n",
      "book_url2 = link.replace('../../', 'https://books.toscrape.com/catalogue/')\n",
      "print(book_url2)\n",
      "20/13: soup.find_all(class_='product_pod')[0]\n",
      "20/14:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    book_links.append(link)\n",
      "book_links[1]\n",
      "20/15:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "\n",
      "book_links[0:5]\n",
      "20/16:\n",
      "for book in book_links:\n",
      "    if book[\"title\"] == \"A Light in the Attic\":\n",
      "        print (book)\n",
      "20/17: next((book for book in book_links if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "20/18:\n",
      "full_url = []\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"].replace('../../','https://books.toscrape.com/catalogue/')\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    full_url.append({'title': title, 'link': link})\n",
      "print(full_url)\n",
      "20/19: book_links[0:2]\n",
      "20/20:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/page-\"\n",
      "pages = []\n",
      "for counter in range(1,51):\n",
      "    pages.append(base_url + str(counter) + \".html\")\n",
      "\n",
      "print(pages)\n",
      "20/21: print(range(1, 50))\n",
      "20/22:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "print(quote_page_urls [0:3])\n",
      "20/23:\n",
      "from time import sleep\n",
      "sleep(3)\n",
      "print(\"Done\")\n",
      "20/24:\n",
      "def gen_page_urls(base, page_num):\n",
      "    page_urls =[]\n",
      "    \n",
      "    for counter in range(1, page_num + 1):\n",
      "        full_url = base + \"page-\" + str(counter) + \".html\"\n",
      "        page_urls.append(full_url)\n",
      "    \n",
      "    return page_urls\n",
      "\n",
      "gen_page_urls(\"https://books.toscrape.com/catalogue/category/books_1/\", 3)\n",
      "20/25:\n",
      "def extr_book_urls(page_urls):\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "20/26:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extr_book_urls(page_urls)\n",
      "book_list[0:2]\n",
      "20/27:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book_urls =[]\n",
      "20/28:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book_urls =[]\n",
      "\n",
      "books[0]\n",
      "20/29:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base,5)\n",
      "20/30:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base,5)\n",
      "page_urls\n",
      "20/31:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base,5)\n",
      "book_list = extr_book_urls(page_urls)\n",
      "20/32: book_list[0]\n",
      "20/33: books = soup.find_all(class_=\"product_pod\")\n",
      "20/34: books = soup.find_all(class_=\"product_pod\")\n",
      "20/35:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "books[0]\n",
      "20/36:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book.find(\"p\", class_=\"instock availability\")\n",
      "20/37:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book.find(\"a\")\n",
      "20/38:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book.find(\"p\", class_=\"instock availability\")\n",
      "20/39:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book.find(\"p\", class_=\"instock availability\")==\"In stock\"\n",
      "20/40:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book.find(\"p\", class_=\"instock availability\")== \"In stock\"\n",
      "20/41:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book.find(\"p\", class_=\"instock availability\")\n",
      "20/42:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book.find(\"p\", class_=\"instock availability\").text\n",
      "20/43:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url})\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "        sleep(1)\n",
      "    return book_list\n",
      "20/44:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url})\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "        sleep(1)\n",
      "    return book_list\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list = extr_book_urls2(page_urls)\n",
      "book_list[0:2]\n",
      "20/45:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list = extr_book_urls2(page_urls)\n",
      "book_list[0:2]\n",
      "20/46:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url, \"avail\": book_instock})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list = extr_book_urls2(page_urls)\n",
      "book_list[0:2]\n",
      "20/47:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "            book_list2.append({\"title\": book_title,\n",
      "                             \"url\": book_url, \n",
      "                              \"avail\": book_instock})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list = extr_book_urls2(page_urls)\n",
      "book_list[0:2]\n",
      "20/48:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "            book_list2.append({\"title\": book_title,\n",
      "                             \"url\": book_url, \n",
      "                              \"avail\": book_instock})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list2 = extr_book_urls2(page_urls)\n",
      "book_list2[0:2]\n",
      "20/49:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "            book_list2.append({\"title\": book_title,\n",
      "                             \"url\": book_url, \n",
      "                              \"avail\": book_instock})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "extr_book_urls2(page_urls)\n",
      "book_list2[0:2]\n",
      "20/50:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "            book_list2.append({\"title\": book_title,\n",
      "                             \"url\": book_url, \n",
      "                              \"avail\": book_instock})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list2\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list2 = extr_book_urls2(page_urls)\n",
      "book_list2[0:2]\n",
      "20/51:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text.replace(\"\\n\",\"\").replace(\"  \",\"\")\n",
      "            book_list2.append({\"title\": book_title,\n",
      "                             \"url\": book_url, \n",
      "                              \"avail\": book_instock})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list2\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list2 = extr_book_urls2(page_urls)\n",
      "book_list2[0:2]\n",
      "20/52:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text.replace(\"\\n\",\"\").replace(\"  \",\"\")\n",
      "            book_list2.append({\"title\": book_title,\n",
      "                             \"url\": book_url, \n",
      "                              \"avail\": book_instock})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list2\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list2 = extr_book_urls2(page_urls)\n",
      "book_list2[0:2]\n",
      "20/53:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, 'html.parser')\n",
      "    next_btn = soup.find(class_=\"next\")\n",
      "    return next_btn.find('a').attrs['href'] if next_btn else None\n",
      "\n",
      "page_1 = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "print(\"The next page is: \" + str(check_next_page(page_1)))\n",
      "20/54:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, 'html.parser')\n",
      "    next_btn = soup.find(class_=\"next\")\n",
      "    return next_btn.find('a').attrs['href'] if next_btn else None\n",
      "\n",
      "page_1 = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "print(\"The next page is: \" + str(check_next_page(page_1)))\n",
      "\n",
      "type(check_next_page(page_1))\n",
      "20/55:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, 'html.parser')\n",
      "    next_btn = soup.find(class_=\"next\")\n",
      "    return next_btn.find('a').attrs['href'] if next_btn else None\n",
      "\n",
      "page_1 = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "print(\"The next page is: \" + check_next_page(page_1))\n",
      "\n",
      "type(check_next_page(page_1))\n",
      "20/56: check_next_page(page_50)\n",
      "20/57: check_next_page(page_50)\n",
      "20/58:\n",
      "check_next_page(page_50)\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "def next_page_url(next_btn):\n",
      "    if next_btn != None:\n",
      "        page_url = base + next_btn\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "20/59:\n",
      "check_next_page(https://books.toscrape.com/catalogue/page-1.html)\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "def next_page_url(next_btn):\n",
      "    if next_btn != None:\n",
      "        page_url = base + next_btn\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "  \n",
      "next_page_url\n",
      "20/60:\n",
      "check_next_page(\"https://books.toscrape.com/catalogue/page-1.html\")\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "def next_page_url(next_btn):\n",
      "    if next_btn != None:\n",
      "        page_url = base + next_btn\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "  \n",
      "next_page_url()\n",
      "20/61:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "def next_page_url(check_next_page(url)):\n",
      "    if next_btn != None:\n",
      "        page_url = base + next_btn\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "  \n",
      "next_page_url()\n",
      "20/62:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "check_next_page(url)\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "def next_page_url(check_next_page(url)):\n",
      "    if next_btn != None:\n",
      "        page_url = base + next_btn\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "20/63:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "check_next_page(url)\n",
      "20/64:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "link = check_next_page(url)\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "\n",
      "def next_page_url(link):\n",
      "    if link != None:\n",
      "        page_url = base + link\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "        \n",
      "next_page_url(link)\n",
      "20/65:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "link = check_next_page(url)\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "\n",
      "def next_page_url(link):\n",
      "    if link != None:\n",
      "        return page_url = base + link\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "        \n",
      "next_page_url(link)\n",
      "20/66:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "link = check_next_page(url)\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "\n",
      "def next_page_url(link):\n",
      "    if link != None:\n",
      "        page_url = base + link\n",
      "        print(page_url)\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "        \n",
      "next_page_url(link)\n",
      "20/67:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "link = check_next_page(url)\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "\n",
      "def next_page_url(link):\n",
      "    if link != None:\n",
      "        page_url = base + link\n",
      "        print(\"Next page link:\" + page_url)\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "        \n",
      "next_page_url(link)\n",
      "20/68:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "link = check_next_page(url)\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "\n",
      "def next_page_url(link):\n",
      "    if link != None:\n",
      "        page_url = base + link\n",
      "        print(\"Next page link: \" + page_url)\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "        \n",
      "next_page_url(link)\n",
      "23/1:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "joke_request = response.json() \n",
      "print(joke_request)\n",
      "23/2:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "#get the resquests using the url using the header\n",
      "joke_request = response.json() #convert data into json file\n",
      "print(joke_request)\n",
      "23/3:\n",
      "import requests\n",
      "import time\n",
      "\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "\n",
      "i = 0\n",
      "while i<10:\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "    #get the resquests using the url using the header\n",
      "    joke_request = response.json() #convert data into json file\n",
      "    print(i)\n",
      "    print(joke_request [\"id\"])\n",
      "    print(joke_request [\"joke\"])\n",
      "    i = i+1\n",
      "    time.sleep(2)\n",
      "23/4:\n",
      "# Question 2 \n",
      "import time\n",
      "\n",
      "i = 1\n",
      "while i <= 3:\n",
      "    url = 'https://www.reddit.com/r/marketing/about/.json'\n",
      "    headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "    response = requests.get(url, headers=headers)\n",
      "    json_response = response.json()\n",
      "    \n",
      "    print(json_response['data']['active_user_count'])\n",
      "    i += 1\n",
      "    time.sleep(5)\n",
      "24/1:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "#get the resquests using the url using the header\n",
      "joke_request = response.json() #convert data into json file\n",
      "print(joke_request)\n",
      "24/2:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "#get the resquests using the url using the header\n",
      "joke_request = response.json() #convert data into json file\n",
      "print(joke_request)\n",
      "24/3:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "#get the resquests using the url using the header\n",
      "joke_request = response.json() #convert data into json file\n",
      "print(joke_request)\n",
      "24/4:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "\n",
      "print(response)\n",
      "24/5:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "\n",
      "print(response.url)\n",
      "24/6:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "\n",
      "response\n",
      "24/7:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "\n",
      "type(response)\n",
      "24/8:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "\n",
      "view(response)\n",
      "24/9:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "\n",
      "dir(response)\n",
      "24/10:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "response.url\n",
      "24/11:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "response.text\n",
      "24/12:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "response.text\n",
      "24/13:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "BeautifulSoup(response.text)\n",
      "24/14:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "BeautifulSoup(response)\n",
      "24/15:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "dir(response)\n",
      "24/16:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "response.json\n",
      "24/17:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "dir(response.json)\n",
      "24/18:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "response.json()\n",
      "24/19:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "response.text()\n",
      "24/20:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "response.json()\n",
      "24/21:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "24/22: dir(response)\n",
      "24/23: response.links()\n",
      "24/24: response.links\n",
      "24/25: print(response.links)\n",
      "24/26: print(response.raw)\n",
      "24/27: dir(response.raw)\n",
      "24/28: dir(response.raw)\n",
      "24/29: a = ['head', 'tail']\n",
      "24/30:\n",
      "a = ['head', 'tail']\n",
      "a.head\n",
      "24/31: dir(response.__attrs__)\n",
      "24/32: type(response)\n",
      "24/33: ?requests.models.Response\n",
      "24/34: help(requests.models.Response)\n",
      "24/35: dir(response)\n",
      "24/36:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "response.url\n",
      "24/37:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "24/38:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request\n",
      "24/39:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "print(joke_request)\n",
      "24/40:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request.results\n",
      "24/41:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request[results]\n",
      "24/42:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "24/43:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "type(joke_request)\n",
      "24/44:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "dir(joke_request)\n",
      "24/45: ?dir\n",
      "24/46: help(dir)\n",
      "24/47:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "dir(joke_request)\n",
      "joke_request.class\n",
      "24/48:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "dir(joke_request)\n",
      "24/49:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "joke_request.get()\n",
      "24/50:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "joke_request.get\n",
      "24/51:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "joke_request.get('results')\n",
      "24/52:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in joke_request.get('results'):\n",
      "    print joke.get['joke']\n",
      "24/53:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in joke_request.get('results'):\n",
      "    print (joke.get['joke'])\n",
      "24/54:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in joke_request.get('results'):\n",
      "    print (joke.get('joke')\n",
      "24/55:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in joke_request.get('results'):\n",
      "    print (joke.get('joke'))\n",
      "24/56:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in joke_request.get('results'):\n",
      "    print (joke['joke'])\n",
      "24/57:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in joke_request['results']:\n",
      "    print (joke['joke'])\n",
      "24/58:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in range(5):\n",
      "    for joke in joke_request['results']:\n",
      "    print (joke['joke'])\n",
      "24/59:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in range(5):\n",
      "    for joke in joke_request['results']:\n",
      "    print (joke['joke'])\n",
      "24/60:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in range(5):\n",
      "    for joke in joke_request['results']:\n",
      "        print (joke['joke'])\n",
      "24/61:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in joke_request['results']:\n",
      "        if joke < 5:\n",
      "            print (joke['joke'])\n",
      "24/62:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "24/63:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results'](0:4)\n",
      "24/64:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results'][0:4]\n",
      "24/65:\n",
      "##### import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results'][0:5]\n",
      "24/66:\n",
      "##### import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results'][0:5]\n",
      "for joke in joke_request['results'] [0:5]:\n",
      "        print (joke['joke'])\n",
      "24/67:\n",
      "##### import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"dog\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results'][0:5]\n",
      "for joke in joke_request['results'] [0:5]:\n",
      "        print (joke['joke'])\n",
      "24/68:\n",
      "##### import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"dog\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results'][0:5]\n",
      "for joke in joke_request['results'] [0:5]:\n",
      "        print (joke['joke'])\n",
      "24/69:\n",
      "##### import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"dog\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "24/70:\n",
      "##### import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"dog\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request\n",
      "24/71:\n",
      "def find_joke(q):\n",
      "    response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": q})\n",
      "    joke_request = response.json()\n",
      "    print (\"There are\" + str(joke_request['total_jokes']) + \"jokes\")\n",
      "q = \"dog\"\n",
      "find_joke(q)\n",
      "24/72:\n",
      "def find_joke(q):\n",
      "    response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": q})\n",
      "    joke_request = response.json()\n",
      "    print (\"There are \" + str(joke_request['total_jokes']) + \" jokes\")\n",
      "q = \"dog\"\n",
      "find_joke(q)\n",
      "24/73:\n",
      "def find_joke(q):\n",
      "    response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": q})\n",
      "    joke_request = response.json()\n",
      "    print (\"There are \" + str(joke_request['total_jokes']) + \" jokes about\" + q)\n",
      "q = \"dog\"\n",
      "find_joke(q)\n",
      "24/74:\n",
      "def find_joke(q):\n",
      "    response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": q})\n",
      "    joke_request = response.json()\n",
      "    print (\"There are \" + str(joke_request['total_jokes']) + \" jokes about \" + q)\n",
      "q = \"dog\"\n",
      "find_joke(q)\n",
      "24/75:\n",
      "def find_joke(q):\n",
      "    response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": q})\n",
      "    joke_request = response.json()\n",
      "    print (\"There are \" + str(joke_request['total_jokes']) + \" jokes containing \" + q)\n",
      "q = \"dog\"\n",
      "find_joke(q)\n",
      "24/76:\n",
      "def find_joke(q):\n",
      "    response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": q})\n",
      "    joke_request = response.json()\n",
      "    print (f\"There are \" + str(joke_request['total_jokes']) + \" jokes containing \" + q)\n",
      "q = \"dog\"\n",
      "find_joke(q)\n",
      "24/77:\n",
      "def find_joke(q):\n",
      "    response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": q})\n",
      "    joke_request = response.json()\n",
      "    print (\"There are \" + str(joke_request['total_jokes']) + \" jokes containing \" + q)\n",
      "q = \"dog\"\n",
      "find_joke(q)\n",
      "24/78: help(requests.models.Response)\n",
      "24/79:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"})\n",
      "joke_request = response.json()\n",
      "24/80:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"})\n",
      "joke_request = response.json()\n",
      "joke_request\n",
      "24/81:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"})\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "24/82:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"})\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = ''\n",
      "24/83:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"})\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = ''\n",
      "joke_request\n",
      "24/84:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"}) #return all jokes\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = ''\n",
      "joke_request.url\n",
      "24/85:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"}) #return all jokes\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = ''\n",
      "response.url\n",
      "24/86:\n",
      "search_url = \"https://icanhazdadjoke.com\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}) #return all jokes\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = ''\n",
      "response.url\n",
      "24/87:\n",
      "search_url = \"https://icanhazdadjoke.com\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}) #return all jokes\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = ''\n",
      "joke_request\n",
      "24/88:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "for limit in range (10, 31, 10)\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": limit}) #return all jokes with limits = 10, 20 or 30\n",
      "joke_request = response.json()\n",
      "print(f\"Limit {limit} gives {joke_request['total_pages']} pages\")\n",
      "24/89:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "for limit in range (10, 31, 10):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": limit}) #return all jokes with limits = 10, 20 or 30\n",
      "joke_request = response.json()\n",
      "print(f\"Limit {limit} gives {joke_request['total_pages']} pages\")\n",
      "24/90:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "for limit in range (10, 31, 10):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": limit}) #return all jokes with limits = 10, 20 or 30\n",
      "    joke_request = response.json()\n",
      "    print(f\"Limit {limit} gives {joke_request['total_pages']} pages\")\n",
      "24/91:\n",
      "response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\"}) #return all jokes with limits = 10, 20 or 30\n",
      "joke_request = response.json()\n",
      "24/92:\n",
      "response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\"}) #return all jokes with limits = 10, 20 or 30\n",
      "joke_request = response.json()\n",
      "joke_request\n",
      "24/93:\n",
      "response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": 5,\n",
      "                                   \"page\": 2})\n",
      "response.json()\n",
      "24/94:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "        response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "        total_pages = response.json()[\"total_pages\"]\n",
      "        for page in range (1, total_pages + 1):\n",
      "                response = requests.get(search_url, \n",
      "                                        headers={\"Accept\": \"application/json\"}, \n",
      "                                        params={\"term\": term,\n",
      "                                               \"page\": page})\n",
      "                joke_request = response.json()\n",
      "                for joke in joke_request[\"results\"]:\n",
      "                    joke_list.append = joke\n",
      "                \n",
      "    return joke_list\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "24/96:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "        response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "        total_pages = response.json()[\"total_pages\"]\n",
      "        for page in range (1, total_pages + 1):\n",
      "                response = requests.get(search_url, \n",
      "                                        headers={\"Accept\": \"application/json\"}, \n",
      "                                        params={\"term\": term,\n",
      "                                               \"page\": page})\n",
      "                joke_request = response.json()\n",
      "                for joke in joke_request[\"results\"]:\n",
      "                    joke_list.append = joke\n",
      "                \n",
      "    print(joke_list)\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "24/98:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "        response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "        total_pages = response.json()[\"total_pages\"]\n",
      "        for page in range (1, total_pages + 1):\n",
      "                response = requests.get(search_url, \n",
      "                                        headers={\"Accept\": \"application/json\"}, \n",
      "                                        params={\"term\": term,\n",
      "                                               \"page\": page})\n",
      "                joke_request = response.json()\n",
      "                for joke in joke_request[\"results\"]:\n",
      "                    joke_list.append(joke)\n",
      "                \n",
      "    print(joke_list)\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "24/100:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "        response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "        total_pages = response.json()[\"total_pages\"]\n",
      "        for page in range (1, total_pages + 1):\n",
      "                response = requests.get(search_url, \n",
      "                                        headers={\"Accept\": \"application/json\"}, \n",
      "                                        params={\"term\": term,\n",
      "                                               \"page\": page})\n",
      "                joke_request = response.json()\n",
      "                for joke in joke_request[\"results\"]:\n",
      "                    joke_list.append(joke)\n",
      "                \n",
      "        print(joke_list)\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "24/101:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "        response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "        total_pages = response.json()[\"total_pages\"]\n",
      "        for page in range (1, total_pages + 1):\n",
      "                response = requests.get(search_url, \n",
      "                                        headers={\"Accept\": \"application/json\"}, \n",
      "                                        params={\"term\": term,\n",
      "                                               \"page\": page})\n",
      "                joke_request = response.json()\n",
      "                for joke in joke_request[\"results\"]:\n",
      "                    joke_list.append(joke)\n",
      "                \n",
      "                print(joke_list)\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "24/102:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "        response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "        total_pages = response.json()[\"total_pages\"]\n",
      "        for page in range (1, total_pages + 1):\n",
      "                response = requests.get(search_url, \n",
      "                                        headers={\"Accept\": \"application/json\"}, \n",
      "                                        params={\"term\": term,\n",
      "                                               \"page\": page})\n",
      "                joke_request = response.json()\n",
      "                for joke in joke_request[\"results\"]:\n",
      "                    joke_list.append(joke)\n",
      "                print(joke_list)\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "24/103:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "        response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "        total_pages = response.json()[\"total_pages\"]\n",
      "        for page in range (1, total_pages + 1):\n",
      "                response = requests.get(search_url, \n",
      "                                        headers={\"Accept\": \"application/json\"}, \n",
      "                                        params={\"term\": term,\n",
      "                                               \"page\": page})\n",
      "                joke_request = response.json()\n",
      "                for joke in joke_request[\"results\"]:\n",
      "                    joke_list.append(joke)\n",
      "        print(joke_list)\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "24/104:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    total_pages = response.json()[\"total_pages\"]\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        for joke in joke_request[\"results\"]:\n",
      "            joke_list.append(joke)\n",
      "    print(joke_list)\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "26/1:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "joke_request = response.json() \n",
      "print(joke_request)\n",
      "26/2:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "joke_request = response.json()\n",
      "print(joke_request)\n",
      "26/3: # your answer goes here!\n",
      "26/4:\n",
      "# Question 1 \n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"dog\"})\n",
      "joke_request = response.json()\n",
      "print(f\"The number of dog jokes is: {joke_request['total_jokes']}\")\n",
      "26/5:\n",
      "# Question 2\n",
      "def find_jokes(term):\n",
      "    search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    joke_request = response.json()\n",
      "    num_results = joke_request['total_jokes']\n",
      "    return num_results\n",
      "\n",
      "find_jokes(\"some-searchterm-you-would-like-to-try-out\")\n",
      "26/6:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"})\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = '' # let's remove all jokes, and only look at the other attributes in the JSON response\n",
      "joke_request\n",
      "26/7:\n",
      "for limit in range(10, 31, 10):  # note that range(a, b) runs from a to b-1; so the last value is exclusive (so from 10 to 30 with steps of 10)\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": limit})\n",
      "    joke_request = response.json()\n",
      "    print(f\"Limit {limit} gives {joke_request['total_pages']} pages\")\n",
      "26/8:\n",
      "response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": 5,\n",
      "                                   \"page\": 2})\n",
      "response.json()\n",
      "26/9: # your answer goes here!\n",
      "26/10:\n",
      "def find_jokes(term):\n",
      "    search_url = \"https://icanhazdadjoke.com/search\"\n",
      "    page = 1\n",
      "    jokes = []\n",
      "\n",
      "    while True:  # alternatively you can also use a for-loop that goes from page 1 to total_results / 20 (rounded up)\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,  # optionally you can add \"limit\": 20 but that's already the default so it doesn't change anything\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        jokes.extend(joke_request['results'])\n",
      "        if joke_request['current_page'] <= joke_request['total_pages']:\n",
      "            page += 1\n",
      "        else: \n",
      "            return jokes\n",
      "\n",
      "output = find_jokes(\"cat\") # try running it with \"\", too!\n",
      "26/11: print(f\"You've collected {len(output)} jokes\")\n",
      "24/105:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    total_pages = response.json()[\"total_pages\"]\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        for joke in joke_request[\"results\"]:\n",
      "            joke_list.append(joke)\n",
      "    print(joke_list)\n",
      "term = \"cat\"\n",
      "find_joke(term)\n",
      "24/106:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    total_pages = response.json()[\"total_pages\"]\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        for joke in joke_request[\"results\"]:\n",
      "            joke_list.append(joke)\n",
      "    print(joke_list)\n",
      "term = \"cat\"\n",
      "len(find_joke(term))\n",
      "24/107:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    total_pages = response.json()[\"total_pages\"]\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        for joke in joke_request[\"results\"]:\n",
      "            joke_list.append(joke)\n",
      "    return joke_list\n",
      "term = \"cat\"\n",
      "len(find_joke(term))\n",
      "24/108:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    total_pages = response.json()[\"total_pages\"]\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        for joke in joke_request[\"results\"]:\n",
      "            joke_list.append(joke)\n",
      "    return joke_list\n",
      "term = \"dog\"\n",
      "len(find_joke(term))\n",
      "24/109:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    total_pages = response.json()[\"total_pages\"]\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        for joke in joke_request[\"results\"]:\n",
      "            joke_list.append(joke)\n",
      "    return joke_list\n",
      "term = \"rain\"\n",
      "len(find_joke(term))\n",
      "26/12:\n",
      "def find_jokes(term):\n",
      "    search_url = \"https://icanhazdadjoke.com/search\"\n",
      "    page = 1\n",
      "    jokes = []\n",
      "\n",
      "    while True:  # alternatively you can also use a for-loop that goes from page 1 to total_results / 20 (rounded up)\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,  # optionally you can add \"limit\": 20 but that's already the default so it doesn't change anything\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        jokes.extend(joke_request['results'])\n",
      "        if joke_request['current_page'] <= joke_request['total_pages']:\n",
      "            page += 1\n",
      "        else: \n",
      "            return jokes\n",
      "\n",
      "output = find_jokes(\"rain\") # try running it with \"\", too!\n",
      "26/13: print(f\"You've collected {len(output)} jokes\")\n",
      "28/1:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://github.com/search?q=open+education\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/2:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories?q=open+education\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "27/1:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "joke_request = response.json() \n",
      "print(joke_request)\n",
      "28/3:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"term\": \"open education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/4:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": \"open education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/5:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"term\": \"open education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/6:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": \"open education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/7:\n",
      "# request JSON output from github search API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": \"open education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/8:\n",
      "# request JSON output from github search API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": \"open+education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/9:\n",
      "# request JSON output from github search API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": \"open education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/10: dir(git_request)\n",
      "28/11: git_request['total_count']\n",
      "28/12: git_request['total_count']\n",
      "28/13: git_request['total_count']\n",
      "28/14:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {response_request['total_git']} respositories that contains the term {term})\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/15:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_git']} respositories that contains the term {term})\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/16:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_git']} respositories that contains the term {term}\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/17:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contains the term {term}\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/18:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term {term}\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/19:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/20:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term \"{term}\"\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/21:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*)\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/22:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\"\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/23:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*)\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/24:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/25:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/26: git_request\n",
      "28/27: git_request['items'] = ''\n",
      "28/28: git_request['items'] = ''\n",
      "28/29:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "28/30: git_request['items']\n",
      "28/31: git_request\n",
      "28/32:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "28/33: git_request\n",
      "28/34:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "28/35:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "print (f\"There are {git_request['total_count']} respositories\")\n",
      "28/36: git_request\n",
      "28/37: len(git_request['items'])\n",
      "28/38: git_request['items'] = \"\"\n",
      "28/39:\n",
      "git_request['items'] = \"\"\n",
      "git_request\n",
      "28/40:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "print (f\"There are {git_request['total_count']} respositories\")\n",
      "28/41:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"limit\": 50})\n",
      "git_request = response.json()\n",
      "print (f\"There are {git_request['total_count']} respositories\")\n",
      "28/42: len(git_request['items'])\n",
      "28/43:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"limit\": 10,\n",
      "                                 \"page\": 2})\n",
      "git_request = response.json()\n",
      "print (f\"There are {git_request['total_count']} respositories\")\n",
      "28/44: len(git_request['items'])\n",
      "28/45:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"page\": 2})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "28/46:\n",
      "# url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "28/47:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"python\")\n",
      "28/48:\n",
      "# url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "28/49: len(git_request['items'])\n",
      "28/50:\n",
      "total_page = git_request['total_count']/len(git_request['items']\n",
      "total_page\n",
      "28/51: total_page = git_request['total_count']/len(git_request['items']\n",
      "28/52: git_request['total_count']\n",
      "28/53: git_request['total_count']/(len(git_request['items']))\n",
      "28/54: total_page = git_request['total_count']/(len(git_request['items']))\n",
      "28/55:\n",
      "total_page = git_request['total_count']/(len(git_request['items']))\n",
      "total_page\n",
      "28/56:\n",
      "total_page = git_request['total_count']/(len(git_request['items']))\n",
      "math.ceil(total_page)\n",
      "28/57:\n",
      "import math\n",
      "total_page = git_request['total_count']/(len(git_request['items']))\n",
      "math.ceil(total_page)\n",
      "28/58:\n",
      "import math\n",
      "total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "28/59:\n",
      "import math\n",
      "total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "total_page\n",
      "28/60:\n",
      "import math\n",
      "\n",
      "for page in range(1, total_page + 1):\n",
      "\n",
      "repo_list =[]\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 100})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "28/61:\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 100})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "28/62:\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 100})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "28/63:\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "28/64:\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/1:\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/2:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/3:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    total_page = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/4:\n",
      "#using parameter per_page to set the number of results for each request\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"page\": 2})\n",
      "git_request = response.json()\n",
      "git_request['items']\n",
      "30/5:\n",
      "#using parameter per_page to set the number of results for each request\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"page\": 2})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "30/6:\n",
      "#using parameter per_page to set the number of results for each request\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"page\": 2})\n",
      "git_request = response.json()\n",
      "git_request['items']\n",
      "30/7:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    total_page = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/8:\n",
      "#using parameter per_page to set the number of results for each request\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"page\": 2})\n",
      "git_request = response.json()\n",
      "git_request['items']\n",
      "30/9:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"python\")\n",
      "30/10:\n",
      "#using parameter per_page to set the number of results for each request\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"page\": 1})\n",
      "git_request = response.json()\n",
      "git_request['items']\n",
      "30/11:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    total_page = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/12:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    search_url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "\n",
      "    while True:\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 50,\n",
      "                                        \"page\": page})\n",
      "        git_request = response.json()\n",
      "        total_page = math.ceil(git_request['total_count']/50)\n",
      "        repo_list.extend(git_request['items'])\n",
      "        if page <= total_page:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "30/13:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    search_url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "\n",
      "    while True:\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 50,\n",
      "                                        \"page\": page})\n",
      "        repo_request = response.json()\n",
      "        total_page = math.ceil(repo_request['total_count']/50)\n",
      "        repo_list.extend(repo_request['items'])\n",
      "        if page <= total_page:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "30/14:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "\n",
      "    while True:\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 50,\n",
      "                                        \"page\": page})\n",
      "        repo_request = response.json()\n",
      "        total_page = math.ceil(repo_request['total_count']/50)\n",
      "        repo_list.extend(repo_request['items'])\n",
      "        if page <= total_page:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "30/15:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    total_page = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    for page in range (1, 50):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/16:\n",
      "#using parameter per_page to set the number of results for each request\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"page\": 1})\n",
      "git_request = response.json()\n",
      "git_request['items']\n",
      "30/17:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    total_page = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "\n",
      "def find_repo(term):\n",
      "    total_page(term)\n",
      "    for page in range (1, total_page+1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/18:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "\n",
      "def find_repo(term):\n",
      "    total_page(term)\n",
      "    for page in range (1, pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/19:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "def find_repo(term):\n",
      "    total_page(term)\n",
      "    for page in range (1, pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/20:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "def find_repo(term):\n",
      "    total_page(term)\n",
      "    for page in range (1, pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "\n",
      "term = \"open education\"\n",
      "find_repo(term)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/21:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "term = \"open education\"\n",
      "total_page(term)\n",
      "30/22:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "term = \"open education\"\n",
      "total_page(term)\n",
      "30/23:\n",
      "repo_list =[]\n",
      "def find_repo(term):\n",
      "    total_page(term)\n",
      "    for page in range (1, pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "30/24:\n",
      "repo_list =[]\n",
      "def find_repo(term):\n",
      "    total_page(term)\n",
      "    for page in range (1, pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "30/25:\n",
      "repo_list =[]\n",
      "def find_repo(term):\n",
      "    pages = total_page(term)\n",
      "    for page in range (1, pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "30/26:\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    for page in range (1, pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "pages = total_page(term)\n",
      "find_repo(term)\n",
      "30/27:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"open education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "git_request = response.json() \n",
      "while 'next' in response.links.keys():\n",
      "  response = requests.get(response.links['next']['url'],headers={\"Authorization\": git_token})\n",
      "  git_request.extend(response.json())\n",
      "30/28:\n",
      "git_token = \"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\"\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "res=requests.get(url,\n",
      "                 headers={\"Authorization\": git_token},\n",
      "                 params = {\"q\": \"open education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "repos=res.json()\n",
      "while 'next' in res.links.keys():\n",
      "  res=requests.get(res.links['next']['url'],headers={\"Authorization\": git_token})\n",
      "  repos.extend(res.json())\n",
      "30/29:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"open education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "git_request = response.json() \n",
      "git_request\n",
      "30/30:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"open education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "git_request = response.json() \n",
      "response.headers\n",
      "git_request\n",
      "30/31:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"open education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "git_request = response.json() \n",
      "response.headers\n",
      "30/32:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"open education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "git_request = response.json() \n",
      "response.headers['Link']\n",
      "30/33: type(response.headers['Link'])\n",
      "30/34:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"python\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "git_request = response.json() \n",
      "response.headers['Link']\n",
      "30/35:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"python\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":15})\n",
      "git_request = response.json() \n",
      "response.headers['Link']\n",
      "30/36:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"python\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":15})\n",
      "git_request = response.json() \n",
      "git_request\n",
      "33/1: runfile('D:/OneDrive/00_Tilburg/00_RM2021/Unit 1/oDCM/github-api/scraping/scap.py', wdir='D:/OneDrive/00_Tilburg/00_RM2021/Unit 1/oDCM/github-api/scraping')\n",
      "33/2: runfile('D:/OneDrive/00_Tilburg/00_RM2021/Unit 1/oDCM/github-api/scraping/scap.py', wdir='D:/OneDrive/00_Tilburg/00_RM2021/Unit 1/oDCM/github-api/scraping')\n",
      "35/1:\n",
      "# Run this code now\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find_all(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "35/2:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "soup.find_all(class_=\"product_pod\")[0].find(\"a\").attrs[\"href\"]\n",
      "35/3: # your answer goes here!\n",
      "35/4:\n",
      "# Question 1\n",
      "url_book = soup.find_all(class_=\"product_pod\")[1].find(\"a\").attrs[\"href\"]\n",
      "print(url_book)\n",
      "35/5:\n",
      "# Question 2 \n",
      "base_url = \"https://books.toscrape.com/catalogue/\" # gives a 403 error if you run the URL separately but works as expected once combined with the book url\n",
      "book_url = base_url + url_book[6:] # so we skip characters with index 0, 1, 2, 3, 4, 5: \"../../\"\n",
      "print(book_url)\n",
      "35/6:\n",
      "# Question 3\n",
      "base_url = \"https://books.toscrape.com/catalogue/\"\n",
      "book_url = base_url + url_book\n",
      "book_url = book_url.replace('../', '')\n",
      "print(book_url)\n",
      "35/7:\n",
      "# list of all books on the overview page\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book_urls = []\n",
      "\n",
      "for book in books: \n",
      "    book_url = book.find(\"a\").attrs[\"href\"]\n",
      "    book_urls.append(book_url)\n",
      "    \n",
      "# print the first five urls\n",
      "print(book_urls[0:5])\n",
      "35/8:\n",
      "book_list = []\n",
      "\n",
      "for book in books: \n",
      "    book_title = book.find(\"img\").attrs[\"alt\"] \n",
      "    book_url = book.find(\"a\").attrs[\"href\"]\n",
      "    book_list.append({'title': book_title,\n",
      "                      'url': book_url})\n",
      "35/9: next((book for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "35/10: # your answer goes here!\n",
      "35/11:\n",
      "# Question 1\n",
      "for id, book in enumerate(book_list):\n",
      "    book[\"full_url\"] = (base_url + book[\"url\"]).replace('../','')\n",
      "\n",
      "# show the first five elements\n",
      "book_list[0:5]\n",
      "35/12:\n",
      "# Question 2 \n",
      "next((book for book in book_list if book[\"title\"] == \"Black Dust\"), None)\n",
      "\n",
      "# it does not return any result because the book does not exist (this book is on shown on the 2nd page and we only scraped the first one!)\n",
      "35/13:\n",
      "counter = 1\n",
      "full_url = base_url + \"page-\" + str(counter) + \".html\" \n",
      "print(full_url)\n",
      "35/14:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = []\n",
      "\n",
      "for counter in range(1, 51):\n",
      "    full_url = base_url + \"page-\" + str(counter) + \".html\" \n",
      "    page_urls.append(full_url)\n",
      "35/15:\n",
      "# print the last five page urls (btw, run print(page_urls) for yourself to see all page URLs!)\n",
      "print(\"The number of page urls in the list is: \" + str(len(page_urls)))\n",
      "35/16: # your answer goes here!\n",
      "35/17:\n",
      "# Question 2\n",
      "base_url = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "\n",
      "for counter in range(1, 11):\n",
      "    full_url = base_url + str(counter)\n",
      "    quote_page_urls.append(full_url)\n",
      "\n",
      "print(quote_page_urls)\n",
      "35/18:\n",
      "# run this cell again to see the timer in action yourself!\n",
      "from time import sleep\n",
      "sleep(5)\n",
      "print(\"I'll be printed to the console after 5 seconds!\")\n",
      "35/19: # your answer goes here!\n",
      "35/20:\n",
      "sleep(2*60)\n",
      "print(\"Done!\")\n",
      "35/21:\n",
      "def generate_page_urls(base_url, num_pages):\n",
      "    '''generate a list of full page urls from a base url and counter that has takes on the values between 1 and num_pages'''\n",
      "    page_urls = []\n",
      "    \n",
      "    for counter in range(1, num_pages + 1):\n",
      "        full_url = base_url + \"page-\" + str(counter) + \".html\"\n",
      "        page_urls.append(full_url)\n",
      "        \n",
      "    return page_urls\n",
      "35/22: generate_page_urls(\"https://books.toscrape.com/catalogue/category/books_1/\", 5)\n",
      "35/23:\n",
      "def extract_book_urls(page_urls):\n",
      "    '''collect the book title and url for every book on all page urls'''\n",
      "    book_list = []\n",
      "    \n",
      "    # collect all books on page_url\n",
      "    for page_url in page_urls: \n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "        \n",
      "        # for each book on that page look up the title and url and store it in a list\n",
      "        for book in books: \n",
      "            book_title = book.find(\"img\").attrs[\"alt\"] \n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "            \n",
      "        sleep(1)  # pause 1 second after each request\n",
      "            \n",
      "    return book_list\n",
      "35/24:\n",
      "# this cell references functions in other cells, therefore make sure you have loaded all cells above first! (Cell > Run All Above)\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = generate_page_urls(base_url, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extract_book_urls(page_urls)\n",
      "35/25:\n",
      "# Preview the results\n",
      "book_list[0:5]\n",
      "35/26: # Your answer goes here\n",
      "35/27:\n",
      "# Question 1\n",
      "\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = generate_page_urls(base_url, 5) \n",
      "book_list = extract_book_urls(page_urls)\n",
      "book_list\n",
      "35/28:\n",
      "# Question 2\n",
      "def extract_book_urls(page_urls):\n",
      "    '''collect the book title and url for every book on all page urls'''\n",
      "    book_list = []\n",
      "    \n",
      "    # this part is the same as above\n",
      "    for page_url in page_urls: \n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "\n",
      "        for book in books: \n",
      "            book_title = book.find(\"img\").attrs[\"alt\"] \n",
      "            book_url = (\"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"]).replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text # only this changed!\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url,\n",
      "                             \"instock\": book_instock}) # and this line!\n",
      "            \n",
      "        sleep(1)  \n",
      "            \n",
      "    return book_list\n",
      "35/29:\n",
      "# Question 3\n",
      "def extract_book_urls(page_urls):\n",
      "    '''collect the book title and url for every book on all page urls'''\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls: \n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "\n",
      "        for book in books: \n",
      "            book_title = book.find(\"img\").attrs[\"alt\"] \n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "            \n",
      "            # addition to clean up the text (the rest remains the same!)\n",
      "            book_instock = book_instock.replace('\\n','').replace(' ','') # first replace a line-break (`\\n`) by an empty space, then replace a space (' ') by an empty space\n",
      "            \n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url,\n",
      "                             \"instock\": book_instock})\n",
      "            \n",
      "        sleep(1) \n",
      "            \n",
      "    return book_list\n",
      "\n",
      "# test function!\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = generate_page_urls(base_url, 2) \n",
      "book_list = extract_book_urls(page_urls)\n",
      "book_list\n",
      "35/30:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_btn = soup.find(class_= \"next\") # observe the similarity with the code snippet used above\n",
      "    return next_btn.find(\"a\").attrs[\"href\"] if next_btn else None\n",
      "\n",
      "page_1 = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "print(\"The next page is: \" + check_next_page(page_1))\n",
      "35/31: # your answer goes here!\n",
      "35/32:\n",
      "# Question 1 \n",
      "output = check_next_page(\"https://books.toscrape.com/catalogue/page-50.html\")\n",
      "print(output) # the output is None because page 50 is the last one\n",
      "35/33:\n",
      "# Question 2 \n",
      "def next_page_url(url):\n",
      "    base_url = \"https://books.toscrape.com/catalogue/\"\n",
      "    if url != None: \n",
      "        page_url = base_url + url \n",
      "        return page_url \n",
      "    else: \n",
      "        print(\"This is already the last page!\")\n",
      "        \n",
      "next_page_url(check_next_page(\"https://books.toscrape.com/catalogue/page-50.html\"))\n",
      "35/34:\n",
      "def extract_all_books(page_url):\n",
      "    books = []\n",
      "    while page_url:\n",
      "        print(page_url)\n",
      "        for book in extract_book_urls([page_url]):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(page_url) != None: \n",
      "            page_url = \"https://books.toscrape.com/catalogue/category/books_1/\" + check_next_page(page_url)\n",
      "        else: \n",
      "            break\n",
      "        \n",
      "        # if \"page-4\" in page_url: break # (activate this if you don't want to run the entire loop)\n",
      "    return books\n",
      "35/35: book_list = extract_all_books(\"https://books.toscrape.com/catalogue/page-1.html\")\n",
      "35/36: book_list\n",
      "35/37: # your answer goes here!\n",
      "35/38:\n",
      "# Question 1\n",
      "# There are 1000 books \n",
      "\n",
      "len(books)\n",
      "\n",
      "#That's 50 pages into 20 products, which matches our expectations.\n",
      "35/39: books[0]\n",
      "35/40:\n",
      "# Question 2\n",
      "# we use one of the code snippets from above to search for the title\n",
      "\n",
      "next((book for book in book_list if book[\"title\"] == 'The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics'), None)\n",
      "\n",
      "# we can view the URL and open it in the browser.\n",
      "35/41:\n",
      "# Question 3\n",
      "books_instock = [book for book in book_list if book[\"instock\"] == \"Instock\"]\n",
      "len(books_instock)\n",
      "\n",
      "# All books are in stock!\n",
      "35/42:\n",
      "# Question 4\n",
      "len([book for book in book_list if \"boat\" in book[\"title\"].lower()])\n",
      "\n",
      "# here, we're checking for the appearence of the word \"boat\" in the title.\n",
      "35/43: [book for book in book_list if book[\"title\"] == \"Black Dust\"]\n",
      "35/44:\n",
      "res = requests.get('https://books.toscrape.com/catalogue/black-dust_976/index.html')\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "len(soup.find(id=\"content_inner\").find(\"p\", class_ = \"star-rating\").find_all(class_ = \"icon-star\"))\n",
      "35/45: soup\n",
      "35/46: soup.find(id=\"content_inner\")\n",
      "35/47: soup.find(id=\"content_inner\").find_all(\"p\")\n",
      "35/48: soup.find(id=\"content_inner\").find_all(\"p\", class_ = \"star-rating\")\n",
      "35/49: soup.find(id=\"content_inner\").find(\"p\", class_ = \"star-rating\")\n",
      "35/50: soup.find(id=\"content_inner\").find(\"p\", class_ = \"star-rating\").find_all(class_ = \"icon-star\")\n",
      "35/51: len(soup.find(id=\"content_inner\").find(\"p\", class_ = \"star-rating\").find_all(class_ = \"icon-star\"))\n",
      "35/52: # your answer goes here!\n",
      "35/53:\n",
      "# Question 1\n",
      "def get_book_description(books):\n",
      "    book_descriptions = []\n",
      "    \n",
      "    for book in books: \n",
      "        page_url = book[\"url\"]\n",
      "\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        # tip: look at the Google Inspector screenshot below \n",
      "        description = soup.find(id=\"content_inner\").find_all(\"p\")[3].get_text()\n",
      "        title = soup.find(id=\"content_inner\").find('img')['alt']\n",
      "        book_descriptions.append({'url': page_url,\n",
      "                                  'title': title,\n",
      "                                  'description': description})\n",
      "    return book_descriptions\n",
      "\n",
      "book_descriptions = get_book_description(book_list[0:5])\n",
      "book_descriptions\n",
      "\n",
      "# Question 2\n",
      "# tÃ©gÃ© (or similarly encoded strings) are characters from languages other than English, which use an extended character space.\n",
      "35/54:\n",
      "from datetime import datetime\n",
      "\n",
      "now = datetime.now()\n",
      "print(now)\n",
      "35/55:\n",
      "import csv \n",
      "\n",
      "with open(\"book_descriptions.csv\", \"w\") as csv_file: # <<- this is the line with the \"flag\"l see exercises below\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"description\", \"date_time\"])\n",
      "    now = datetime.now()\n",
      "    for book in book_descriptions: # here we reference the book_descriptions list - make sure it's loaded otherwise you get an error! (Cell > Run All Above)\n",
      "        writer.writerow([book['title'], book['description'], now])\n",
      "print('done!')\n",
      "36/1:\n",
      "# request JSON output from github search API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": \"open education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "36/2:\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\",                                     },\n",
      "                       params = {\"q\": \"python\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":15})\n",
      "git_request = response.json() \n",
      "git_request\n",
      "36/3:\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\",                                     },\n",
      "                       params = {\"q\": \"python\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "git_request = response.json() \n",
      "git_request\n",
      "36/4:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"python\")\n",
      "36/5:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "36/6:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} r\"espositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "36/7:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "36/8:\n",
      "# Function to calculate the total pages\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "term = \"open education\"\n",
      "total_page(term)\n",
      "36/9:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "36/10:\n",
      "# Function to calculate the total pages\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "term = \"open education\"\n",
      "total_page(term)\n",
      "36/11:\n",
      "# Building a function to request more pages but didn't succeed yet\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "\n",
      "    while True: #while page <= total page, keep requesting and adding repository data to repo_list\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 50,\n",
      "                                        \"page\": page}) #setting 50 items per page\n",
      "        repo_request = response.json()\n",
      "        total_page = math.ceil(repo_request['total_count']/50) #total pages = total items/items per page\n",
      "        repo_list.extend(repo_request['items'])\n",
      "        if page <= total_page:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "36/12:\n",
      "# Tried with personal authentication token but it didn't work (limited results to only 1000 first items)\n",
      "git_token = \"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\" \n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "res=requests.get(url,\n",
      "                 headers={\"Authorization\": git_token},\n",
      "                 params = {\"q\": \"open education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "repos=res.json()\n",
      "while 'next' in res.links.keys():\n",
      "  res=requests.get(res.links['next']['url'],headers={\"Authorization\": git_token})\n",
      "  repos.extend(res.json())\n",
      "36/13:\n",
      "# Tried with personal authentication token but it didn't work (limited results to only 1000 first items)\n",
      "git_token = \"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\" \n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "res=requests.get(url,\n",
      "                 headers={\"Authorization\": git_token},\n",
      "                 params = {\"q\": \"python\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":15})\n",
      "repos=res.json()\n",
      "36/14:\n",
      "# Tried with personal authentication token but it didn't work (limited results to only 1000 first items)\n",
      "git_token = \"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\" \n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "res=requests.get(url,\n",
      "                 headers={\"Authorization\": git_token},\n",
      "                 params = {\"q\": \"python\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":15})\n",
      "repos=res.json()\n",
      "38/1:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "38/2:\n",
      "# Function to calculate the total pages\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "term = \"open education\"\n",
      "total_page(term)\n",
      "38/3:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "\n",
      "git_request.links\n",
      "38/4: git_request['headers']\n",
      "38/5:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "38/6: git_request\n",
      "38/7:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    returns (git_request)\n",
      "   \n",
      "find_repo(\"open education\")\n",
      "38/8:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    return git_request\n",
      "   \n",
      "find_repo(\"open education\")\n",
      "38/9:\n",
      "# Building a function to request more pages but didn't succeed yet\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "\n",
      "    while True: #while page <= total page, keep requesting and adding repository data to repo_list\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 50,\n",
      "                                        \"page\": page}) #setting 50 items per page\n",
      "        repo_request = response.json()\n",
      "        total_page = math.ceil(repo_request['total_count']/50) #total pages = total items/items per page\n",
      "        repo_list.extend(repo_request['items'])\n",
      "        if page <= total_page:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "38/10: git_request['headers']\n",
      "38/11: git_request\n",
      "38/12:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    return git_request\n",
      "   \n",
      "find_repo(\"open education\")\n",
      "38/13: git_request[\"Headers\"]\n",
      "38/14: git_request\n",
      "38/15:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    return git_request\n",
      "   \n",
      "git_request = find_repo(\"open education\")\n",
      "38/16:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    return git_request\n",
      "   \n",
      "git_request = find_repo(\"open education\")\n",
      "38/17: git_request\n",
      "38/18:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    git_request = find_repo(\"open education\")\n",
      "   \n",
      "git_request\n",
      "38/19:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "       \n",
      "git_request\n",
      "38/20:\n",
      "# Building a function to request more pages but didn't succeed yet\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "\n",
      "    while True: #while page <= total page, keep requesting and adding repository data to repo_list\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 50,\n",
      "                                        \"page\": page}) #setting 50 items per page\n",
      "        repo_request = response.json()\n",
      "        total_page = math.ceil(repo_request['total_count']/50) #total pages = total items/items per page\n",
      "        repo_list.extend(repo_request['items'])\n",
      "        if page <= total_page:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "41/1:\n",
      "# Make selenium and chromedriver work for github.com\n",
      "# install also - pip install selenium , pip install webdriver_manager\n",
      "\n",
      "import selenium.webdriver\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.chrome.options import Options\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "#driver = webdriver.Chrome()\n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "base_url = \"https://github.com/search?\"\n",
      "driver.get(base_url)\n",
      "43/1: ?HTMLParser\n",
      "43/2: ??HTMLParser\n",
      "43/3:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, params = {\"q\": term, \"per_page\":100, \"page\": 1})\n",
      "soup = BeautifulSoup(response.text, \"html.parser\")\n",
      "response=soup.find_all(class_=\"repo-list-item hx_hit-repo d-flex flex-justify-start py-4 public source\")\n",
      "43/4:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, params = {\"q\": \"education\", \"per_page\":100, \"page\": 1})\n",
      "soup = BeautifulSoup(response.text, \"html.parser\")\n",
      "response=soup.find_all(class_=\"repo-list-item hx_hit-repo d-flex flex-justify-start py-4 public source\")\n",
      "43/5: response\n",
      "43/6: soup\n",
      "43/7:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://github.com/search/repositories\"\n",
      "response = requests.get(url, params = {\"q\": \"education\", \"per_page\":100, \"page\": 1})\n",
      "soup = BeautifulSoup(response.text, \"html.parser\")\n",
      "response=soup.find_all(class_=\"repo-list-item hx_hit-repo d-flex flex-justify-start py-4 public source\")\n",
      "43/8: soup\n",
      "43/9: response\n",
      "43/10: soup\n",
      "43/11:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://github.com/search\"\n",
      "response = requests.get(url, params = {\"q\": \"education\", \"per_page\":100, \"page\": 1, \"type\": Repositories})\n",
      "soup = BeautifulSoup(response.text, \"html.parser\")\n",
      "response=soup.find_all(class_=\"repo-list-item hx_hit-repo d-flex flex-justify-start py-4 public source\")\n",
      "43/12:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://github.com/search\"\n",
      "response = requests.get(url, params = {\"q\": \"education\", \n",
      "                                       \"per_page\":100, \n",
      "                                       \"page\": 1, \n",
      "                                       \"type\": \"Repositories\"})\n",
      "soup = BeautifulSoup(response.text, \"html.parser\")\n",
      "response=soup.find_all(class_=\"repo-list-item hx_hit-repo d-flex flex-justify-start py-4 public source\")\n",
      "43/13: response\n",
      "43/14: soup\n",
      "45/1:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    total_items = git_request['total_count']\n",
      "    return total_items\n",
      "\n",
      "find_repo(\"open education\")\n",
      "45/2:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    total_pages = math.ceil(git_request['total_count'])/len(git_request['items'])\n",
      "    return total_pages\n",
      "\n",
      "find_repo(\"open education\")\n",
      "45/3:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    total_pages = math.ceil((git_request['total_count'])/len(git_request['items']))\n",
      "    return total_pages\n",
      "\n",
      "find_repo(\"open education\")\n",
      "45/4:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    total_pages = math.ceil((git_request['total_count'])/len(git_request['items']))\n",
      "    return total_pages\n",
      "\n",
      "find_repo(\"python\")\n",
      "45/5: find_repo(\"python\")\n",
      "45/6:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def total_pages(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    total_pages = math.ceil((git_request['total_count'])/len(git_request['items']))\n",
      "    return total_pages\n",
      "45/7: total_pages(\"python\")\n",
      "45/8:\n",
      "def find_repo(term):\n",
      "    no_of_pages = total_pages(term)\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "    while True:\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 100,\n",
      "                                        \"page\": page})\n",
      "        repo_request = response.json()\n",
      "        repo_list.extend(repo_request['items'])\n",
      "        if page < no_of_pages:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "45/9:\n",
      "def find_repo(term):\n",
      "    no_of_pages = total_pages(term)\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "    while True:\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 100,\n",
      "                                        \"page\": page})\n",
      "        repo_request = response.json()\n",
      "        repo_list.extend(repo_request['items'])\n",
      "        if page < no_of_pages:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "\n",
      "find_repo(\"education\")\n",
      "45/10:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "page = 1\n",
      "repo_list =[]\n",
      "while True:\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": term,\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": page})\n",
      "    repo_request = response.json()\n",
      "    repo_list.extend(repo_request['items'])\n",
      "    if page < 50:\n",
      "        page += 1\n",
      "    else: \n",
      "        return repo_list\n",
      "45/11:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "page = 1\n",
      "repo_list =[]\n",
      "while True:\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": term,\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": page})\n",
      "    repo_request = response.json()\n",
      "    repo_list.extend(repo_request['items'])\n",
      "    if page < 50:\n",
      "        page += 1\n",
      "    else: \n",
      "        repo_list\n",
      "45/12:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "page = 1\n",
      "repo_list =[]\n",
      "while True:\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": education,\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": page})\n",
      "    repo_request = response.json()\n",
      "    repo_list.extend(repo_request['items'])\n",
      "    if page < 50:\n",
      "        page += 1\n",
      "    else: \n",
      "        repo_list\n",
      "45/13:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "page = 1\n",
      "repo_list =[]\n",
      "while True:\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": \"education\",\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": page})\n",
      "    repo_request = response.json()\n",
      "    repo_list.extend(repo_request['items'])\n",
      "    if page < 50:\n",
      "        page += 1\n",
      "    else: \n",
      "        repo_list\n",
      "45/14:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url,\n",
      "                        headers={\"Accept\": \"application/json\"},\n",
      "                        params={\"q\": \"education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\": 1})\n",
      "repo_request = response.json()\n",
      "repo_request['items']\n",
      "45/15:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "repo_list[]\n",
      "i = 1\n",
      "for i in range (1: (no_of_pages + 1)):\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": \"education\",\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": i})\n",
      "    repo_request = response.json()\n",
      "    repo_list.append(repo_request['items'])\n",
      "45/16:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "repo_list = []\n",
      "i = 1\n",
      "for i in range (1: (no_of_pages + 1)):\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": \"education\",\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": i})\n",
      "    repo_request = response.json()\n",
      "    repo_list.append(repo_request['items'])\n",
      "45/17:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "repo_list = []\n",
      "i = 1\n",
      "for i in range (1, (no_of_pages + 1)):\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": \"education\",\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": i})\n",
      "    repo_request = response.json()\n",
      "    repo_list.append(repo_request['items'])\n",
      "45/18:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "repo_list = []\n",
      "i = 1\n",
      "for i in range (1, 10):\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": \"education\",\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": i})\n",
      "    repo_request = response.json()\n",
      "    repo_list.append(repo_request['items'])\n",
      "45/19:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "repo_list = []\n",
      "i = 1\n",
      "for i in range (1, 10):\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": \"education\",\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": i})\n",
      "    repo_request = response.json()\n",
      "    repo_list.append(repo_request['items'])\n",
      "45/20:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "repo_list = []\n",
      "i = 1\n",
      "for i in range (1, 10):\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": \"education\",\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": i})\n",
      "    repo_request = response.json()\n",
      "    repo_list.append(repo_request['items'])\n",
      "    i+=1\n",
      "repo_list\n",
      "45/21:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url,\n",
      "                        headers={\"Accept\": \"application/json\"},\n",
      "                        params={\"q\": \"education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\": i})\n",
      "repo_request = response.json()\n",
      "repo_request['items']\n",
      "45/22:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url,\n",
      "                        headers={\"Accept\": \"application/json\"},\n",
      "                        params={\"q\": \"education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\": 1})\n",
      "repo_request = response.json()\n",
      "repo_request['items']\n",
      "45/23:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url,\n",
      "                        headers={\"Accept\": \"application/json\"},\n",
      "                        params={\"q\": \"education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\": 15})\n",
      "repo_request = response.json()\n",
      "repo_request['items']\n",
      "45/24:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url,\n",
      "                        headers={\"Accept\": \"application/json\"},\n",
      "                        params={\"q\": \"education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\": 10})\n",
      "repo_request = response.json()\n",
      "repo_request['items']\n",
      "45/25:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url,\n",
      "                        headers={\"Accept\": \"application/json\"},\n",
      "                        params={\"q\": \"education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\": 11})\n",
      "repo_request = response.json()\n",
      "repo_request['items']\n",
      "45/26:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url,\n",
      "                        headers={\"Accept\": \"application/json\"},\n",
      "                        params={\"q\": \"education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\": 11})\n",
      "repo_request = response.json()\n",
      "repo_request['items']\n",
      "\n",
      "total_pages(\"education\")\n",
      "45/27: total_pages(\"education\")\n",
      "45/28: total_pages(\"open education\")\n",
      "45/29: total_pages(\"python\")\n",
      "45/30:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": term})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/31:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/32:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "response.headers\n",
      "45/33:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(response.headers, \"html.parser\")\n",
      "45/34:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "\n",
      "response.headers\n",
      "45/35:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\";\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "\n",
      "response.headers\n",
      "45/36:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "\n",
      "response.headers\n",
      "45/37:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "\n",
      "response.headers\n",
      "45/38:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python\" + \"created: 2020-01-01T00:00:00..2021-01-01T00:00:00\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "45/39:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python\" + \"created: 2020-01-01T00:00:00..2021-01-01T00:00:00\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/40:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python\" + \"created: 2020-01-01..2021-01-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/41:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"cats created:2016-01-01T00:00:00Z..2017-01-01T00:00:00Z\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/42:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2016-01-01T00:00:00Z..2017-01-01T00:00:00Z\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/43:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2016-01-01T00:00:00Z..2016-01-08T00:00:00Z\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/44:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2016-01-01T00:00:00..2016-01-07T00:00:00\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/45:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2016-01-01T00:00:00..2016-01-05T00:00:00\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/46:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2020-01-01T00:00:00..2020-01-05T00:00:00\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/47:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2020-01-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/48:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2020-10-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/49:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2020-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/50:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2016-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/51:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2021-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/52:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2015-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/53:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2021-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/54:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2021-01-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/55:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education created:2021-01-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/56:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education created:2021-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/57:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education pushed:>2021-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/58:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python pushed:>2021-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/59:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"customer behavior pushed:>2021-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/60:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning created:>2021-01-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/61:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/62:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2021-01-01T00:00:00Z..2021-01-02T00:00:00Z\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/63:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2021-05-01T00:00:00Z..2021-05-02T00:00:00Z\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/64:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2021-05-01T00:00:00Z..2021-05-01T00:12:00Z\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/65:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2021-05-01T00:00:00Z..2021-05-01T12:00:00Z\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/66:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today()\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/67:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today()\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/68:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today()\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/69:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today()\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/70: ?datetime.today\n",
      "45/71: ?datetime\n",
      "45/72:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "45/73:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today()\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/74:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today()\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/75:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today()\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/76: datetime.today()\n",
      "45/77:\n",
      "datetime.today()\n",
      "until\n",
      "45/78:\n",
      "datetime.today()\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)\n",
      "45/79: since\n",
      "45/80: until\n",
      "45/81: until < datetime.today()\n",
      "45/82:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/83:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {git_request[\"total_count\"]}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/84:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {git_request(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/85:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {git_request['total_count']}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/86:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/87:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request['total_count']\n",
      "45/88:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "print(f'Repositories created: {git_request['total_count']}')\n",
      "45/89:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f\"Repositories created between {since} and {until}: {git_request['total_count']}\")\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/90:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    git_request['total_count']\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/91:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "count = ()\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    count.append(git_request['total_count']) \n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/92:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "count = []\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    count.append(git_request['total_count']) \n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/93:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "count = []\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    \n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/94: git_request\n",
      "45/95:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"tilburguniversity\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "count = []\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    \n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/96: git_request\n",
      "45/97:\n",
      "# Function to calculate the total pages\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "term = \"open education\"\n",
      "total_page(term)\n",
      "45/98:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"tilburguniversity\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "count = []\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    git_request['total_count']\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/99:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "URL = f'https://api.github.com/search/repositories?q=open education created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/100:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"Tilburg University\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=3)  # Since 3 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/101:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=3)  # Since 3 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/102:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=3)  # Since 3 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/103:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 3 days ago\n",
      "until = since + timedelta(hour=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/104:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 3 days ago\n",
      "until = since + timedelta(hours=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/105:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=2)  # Since 3 days ago\n",
      "until = since + timedelta(hours=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/106:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(hours=24)  # Since 3 days ago\n",
      "until = since + timedelta(hours=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/107:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=24)  # Since 3 days ago\n",
      "until = since + timedelta(hours=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/108:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 3 days ago\n",
      "until = since + timedelta(hours=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=1)\n",
      "45/109:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 3 days ago\n",
      "until = since + timedelta(hours=12)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/110:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "45/111:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/112:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request(\"items\")\n",
      "45/113:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request.get(\"items\")\n",
      "45/114:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    repo_list.append(repo_request.json().get(\"items\"))\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/115: len(repo_list)\n",
      "45/116: repo_list\n",
      "45/117:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "len(git_request.get(\"items\"))\n",
      "45/118: git_request.get(\"items\")\n",
      "45/119: git_request.get(\"items\")[1]\n",
      "45/120:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    repo_list.extend(repo_request.json().get(\"items\"))\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/121: repo_list\n",
      "45/122: len(repo_list)\n",
      "45/123:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    repo_list.extend(repo_request.json().get(\"items\"))\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/124: repo_request\n",
      "45/125: repo_request(\"items\")\n",
      "45/126: repo_request.json[\"items\"]\n",
      "45/127: repo_request.json().get(\"items\")\n",
      "45/128: len(repo_request.json().get(\"items\"))\n",
      "45/129:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    repo_list.extend(repo_request.json().get(\"items\"))\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/130:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    repo_list.extend(repo_request.json().get(\"items\"))\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/131:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    repo_list.extend(repo_request.json().get(\"items\"))\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/132:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'No. of pages between {since} and {until}: {repo_request.json().get(\"total_count\")/100}')\n",
      "    \n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/133:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'No. of pages between {since} and {until}: {math.ceil(repo_request.json().get(\"total_count\")/100)}')\n",
      "    \n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/134:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/135: len(repo_list)\n",
      "45/136: repo_list\n",
      "45/137: repo_list[1]\n",
      "45/138: class(repo_list[1])\n",
      "45/139: len(repo_list[1])\n",
      "45/140: len(repo_list[15])\n",
      "45/141: len(repo_list)\n",
      "45/142: len(repo_list[2])\n",
      "45/143: len(repo_list[10])\n",
      "45/144: len(repo_list[11])\n",
      "45/145: len(repo_list[14])\n",
      "45/146: len(repo_list[15])\n",
      "45/147: len(repo_list[0])\n",
      "45/148: len(repo_list[15])\n",
      "45/149: len(repo_list[14])\n",
      "45/150:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/151:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"consumer\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/152: len(repo_list[14])\n",
      "45/153: len(repo_list)\n",
      "45/154: len(repo_list(1))\n",
      "45/155: len(repo_list[1])\n",
      "45/156: len(repo_list[2])\n",
      "45/157: len(repo_list[0])\n",
      "45/158:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/159: len(repo_list)\n",
      "45/160: len(repo_list[0])\n",
      "45/161: len(repo_list[1])\n",
      "45/162: len(repo_list[2])\n",
      "45/163:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/164: len(repo_list[7])\n",
      "45/165: len(repo_list[6])\n",
      "45/166: len(repo_list[14])\n",
      "48/1:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(months=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/2:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(months=2)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/3:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=2)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/4:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=2)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/5: len(repo_list)\n",
      "48/6: repo_list\n",
      "48/7: repo_list[1]\n",
      "48/8:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=2)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/9: len(repo_list)\n",
      "48/10: len(repo_list[1])\n",
      "48/11: repo_list[1]\n",
      "48/12:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(years=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/13:\n",
      "datetime.today()\n",
      "since = datetime.today() - timedelta(years=1)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)\n",
      "48/14:\n",
      "datetime.today()\n",
      "since = datetime.today() - timedelta(month=1)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)\n",
      "48/15:\n",
      "from dateutil.relativedelta import relativedelta\n",
      "datetime.today()\n",
      "since = datetime.today() + relativedelta(months=-6)  # Since 6 months ago\n",
      "until = since + timedelta(days=1)\n",
      "48/16:\n",
      "from dateutil.relativedelta import relativedelta\n",
      "datetime.today()\n",
      "since = datetime.today() + relativedelta(months=-6)  # Since 6 months ago\n",
      "until = since + timedelta(days=1) \n",
      "since\n",
      "48/17:\n",
      "from dateutil.relativedelta import relativedelta\n",
      "datetime.today()\n",
      "since = datetime.today() + relativedelta(months=-6)  # Since 6 months ago\n",
      "until = since + timedelta(days=1) \n",
      "since\n",
      "until\n",
      "48/18:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(years=-1)  # Since 1 year ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/19:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Since 1 month ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/20:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Since 1 month ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/21:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Since 1 month ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/22:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Since 1 month ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/23:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Since 1 month ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/24: len(repo_list[])\n",
      "48/25: len(repo_list())\n",
      "48/26: len(repo_list)\n",
      "48/27: repo_list\n",
      "48/28: repo_list[101]\n",
      "48/29: len(repo_list)\n",
      "48/30: repo_list[100]\n",
      "48/31: len(repo_list[1])\n",
      "48/32: len(repo_list[0])\n",
      "48/33:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "48/34:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "48/35: len(repo_list)\n",
      "48/36:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "48/37:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "48/38:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/1:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/2:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/3:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/4:\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/5:\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/6:\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/7:\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/8:\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/9:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/10: math.ceil(122/100)\n",
      "50/11:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(days=-1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/12: len(repo_list)\n",
      "50/13:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(days=-30)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/14:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(days=-5)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/15: len(repo_list)\n",
      "50/16:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    return repo_list\n",
      "50/17: find_repo(\"machine learning\",2)\n",
      "50/18: len(find_repo(\"machine learning\",2))\n",
      "50/19:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=2)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/20: repo_list = find_repo(\"education\",1))\n",
      "50/21: repo_list = find_repo(\"python\",1))\n",
      "50/22: repo_list = find_repo(\"python\",1)\n",
      "50/23: len(repo_list)\n",
      "50/24: repo_list[1]\n",
      "50/25: repo_list[1].get(\"id\")\n",
      "50/26: repo_list[1].get(\"id\",\"name\")\n",
      "50/27: repo_list[1].get(\"id\",\"name\")\n",
      "50/28: repo_list[1].get(\"id\").(\"name\")\n",
      "50/29: repo_list[1].get(\"id\").get(\"name\")\n",
      "50/30: repo_list[1].get(\"name\")\n",
      "50/31: repo_list[1]\n",
      "50/32: repo_list[1].items['id']\n",
      "50/33: repo_list[1].items('id')\n",
      "50/34:\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/35:\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\", default=0)}')\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/36:\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\", 0)}')\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/37:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=30)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\",0)/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/38:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=30)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\",0)/100) #calculating the total No. of pages\n",
      "    while no_page > 0:\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\",0)) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/39:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=30)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\",0)/100) #calculating the total No. of pages\n",
      "    if no_page > 0:\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    else:\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/40:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=30)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\",0)/100) #calculating the total No. of pages\n",
      "    if no_page > 0:\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    else:\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/41:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=30)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\",0)/100) #calculating the total No. of pages\n",
      "    if no_page > 0:\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    else:\n",
      "    # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "50/42: repo_list[1](1)\n",
      "50/43: repo_list[1]\n",
      "50/44: repo_list[1][1]\n",
      "50/45: repo_list[1].items(\"id\")\n",
      "50/46: repo_list[1].items(1)\n",
      "50/47: repo_list[1].items\n",
      "50/48: repo_list[1].items()\n",
      "50/49:\n",
      "col = ('id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count')\n",
      "col(1)\n",
      "50/50:\n",
      "col = ('id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count')\n",
      "col[0]\n",
      "50/51:\n",
      "col = ('id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count')\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    for col in col:\n",
      "        col = item.get(col)\n",
      "        repo.append(col)\n",
      "    dt.append(repo)\n",
      "dt\n",
      "50/52: type(col)\n",
      "50/53:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    for col in col:\n",
      "        col = item.get(col)\n",
      "        repo.append(col)\n",
      "    dt.append(repo)\n",
      "dt\n",
      "50/54: type(col)\n",
      "50/55:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": fork})\n",
      "dt[1]\n",
      "50/56:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt[1]\n",
      "50/57:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt[2]\n",
      "50/58:\n",
      "repo_list = find_repo(\"python\",1)\n",
      "len(repo_list)\n",
      "50/59:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "len(dt)\n",
      "50/60:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt[14]\n",
      "50/61:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt[100]\n",
      "50/62:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt[1000]\n",
      "50/63:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt[50]\n",
      "50/64:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt\n",
      "50/65:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt[0]\n",
      "50/66:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "    return dt\n",
      "50/67: dt = find_repo(\"python\",1)\n",
      "50/68: len(dt)\n",
      "50/69:\n",
      "#function to search for a specific \"term\" and fletch all repositories created in the last {day} days\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    return repo_list\n",
      "50/70:\n",
      "repo_list = find_repo(\"python\",1)\n",
      "len(repo_list)\n",
      "50/71:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "    return dt\n",
      "50/72: dt = find_repo(\"python\",1)\n",
      "50/73: len(dt)\n",
      "50/74: dt[1]\n",
      "50/75: datetime.today()\n",
      "50/76: datetime.date()\n",
      "50/77: datetime.datetime.now()\n",
      "50/78: datetime.now()\n",
      "50/79: datetime.now().date\n",
      "50/80: datetime.now().date()\n",
      "50/81: print(f\"today's date is: {datetime.now().date()}\")\n",
      "50/82:\n",
      "import csv\n",
      "\n",
      "with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "    for repo in dt:\n",
      "        writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "print('done!')\n",
      "50/83: pwd()\n",
      "50/84:\n",
      "import csv\n",
      "import pandas as pd   \n",
      "\n",
      "with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "    for repo in dt:\n",
      "        writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\")\n",
      "\n",
      "rep\n",
      "50/85:\n",
      "import csv\n",
      "import pandas as pd   \n",
      "\n",
      "with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "    for repo in dt:\n",
      "        writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= ’;’)\n",
      "\n",
      "rep\n",
      "50/86:\n",
      "import csv\n",
      "import pandas as pd   \n",
      "\n",
      "with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "    for repo in dt:\n",
      "        writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "\n",
      "rep\n",
      "50/87:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "50/88: find_repo(\"python\",1)\n",
      "55/1:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today:\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables into a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "55/2: find_repo(\"python\",1)\n",
      "58/1:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables into a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "58/2:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables into a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "58/3:\n",
      "#Give it a try:\n",
      "find_repo(\"education\",30)\n",
      "58/4:\n",
      "#Give it a try:\n",
      "find_repo(\"education\",1)\n",
      "58/5:\n",
      "#Give it a try:\n",
      "find_repo(\"education\",10)\n",
      "58/6:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables into a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "58/7:\n",
      "#Give it a try:\n",
      "find_repo(\"education\",5)\n",
      "58/8:\n",
      "#Give it a try:\n",
      "find_repo(\"machine learning\",5)\n",
      "58/9:\n",
      "#Give it a try:\n",
      "find_repo(\"python\",1)\n",
      "58/10:\n",
      "#Give it a try:\n",
      "find_repo(\"education\",10)\n",
      "58/11:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables into a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "58/12:\n",
      "#Give it a try:\n",
      "find_repo(\"machine learning\",5)\n",
      "58/13:\n",
      "#Give it a try:\n",
      "find_repo(\"education\",30)\n",
      "58/14:\n",
      "def print_total_repo(term, day):\n",
      "    URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days=1)  # Start fetching repo created 1 month ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\", 0)}')\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "58/15: print_total_repo(\"education\",30)\n",
      "58/16:\n",
      "def print_total_repo(term, day):\n",
      "    URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created 1 month ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\", 0)}')\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "58/17: print_total_repo(\"education\",30)\n",
      "58/18:\n",
      "def print_total_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created 1 month ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\", 0)}')\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "58/19: print_total_repo(\"education\",30)\n",
      "58/20: print_total_repo(\"education\",5)\n",
      "58/21:\n",
      "#Give it a try:\n",
      "find_repo(\"python\",1)\n",
      "60/1:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created N days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the retrieved repositories to a list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables to a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "60/2: find_repo(\"education\",5)\n",
      "60/3: find_repo(\"education\",30)\n",
      "60/4:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created N days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            print(i)\n",
      "            print(requests.status_code)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the retrieved repositories to a list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables to a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "60/5:\n",
      "#Give it a try:\n",
      "find_repo(\"python\",1)\n",
      "60/6:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created N days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            print(i)\n",
      "            print(page_request.status_code)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the retrieved repositories to a list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables to a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "60/7:\n",
      "#Give it a try:\n",
      "find_repo(\"python\",1)\n",
      "60/8:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created N days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            print(i)\n",
      "            print(page_request.status_code)\n",
      "            if (page_request.status_code!=200): print(page_request.text)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the retrieved repositories to a list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables to a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "60/9:\n",
      "#Give it a try:\n",
      "find_repo(\"python\",1)\n",
      "63/1:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GithubToken']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    #HEADERS = {'Authorization': 'token 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'}\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 12 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        a=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{a}.json','a',encoding='utf-8')\n",
      "        \n",
      "        file_list.append(f'{term}_{a}.json')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "    # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=4)\n",
      "    return file_list\n",
      "63/2: find_repo(\"education\",2)\n",
      "63/3: os.environ\n",
      "63/4:\n",
      "import os\n",
      "os.environ\n",
      "64/1:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GithubToken']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    #HEADERS = {'Authorization': 'token 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'}\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 12 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        a=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{a}.json','a',encoding='utf-8')\n",
      "        \n",
      "        file_list.append(f'{term}_{a}.json')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "    # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=4)\n",
      "    return file_list\n",
      "64/2:\n",
      "import os\n",
      "os.environ\n",
      "64/3:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    #HEADERS = {'Authorization': 'token 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'}\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 12 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        a=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{a}.json','a',encoding='utf-8')\n",
      "        \n",
      "        file_list.append(f'{term}_{a}.json')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "    # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=4)\n",
      "    return file_list\n",
      "64/4: find_repo(\"education\",2)\n",
      "64/5:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{fetch_time}.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        file_list.append(f'{term}_{fetch_time}.json')\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "    return file_list\n",
      "64/6:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{fetch_time}.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        file_list.append(f'{term}_{fetch_time}.json')\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "    return file_list\n",
      "64/7: file_list = find_repo(\"education\",2)\n",
      "64/8: file_list\n",
      "66/1:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "66/2:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "66/3:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content['items']\n",
      "66/4:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "66/5:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "len(content)\n",
      "66/6:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[]('items')\n",
      "66/7:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "66/8:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[1]\n",
      "66/9:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content(0)\n",
      "66/10:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[0]\n",
      "66/11:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[0]['items']\n",
      "66/12:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[0]('items')\n",
      "66/13:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[0]['items']\n",
      "66/14:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[0].get('items')\n",
      "66/15:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[0]\n",
      "66/16:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "json.loads(content)\n",
      "66/17:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "json.loads(content)\n",
      "66/18:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "66/19:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "json.loads(content[1])\n",
      "66/20:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "json.loads(content[0])\n",
      "66/21:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "json.loads(content[0])('items')\n",
      "66/22:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "json.loads(content[0])['items']\n",
      "66/23:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "json.loads(f.readlines()[0])['items']\n",
      "66/24:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo_list.append(json.loads(f.readlines()[0])['items'])\n",
      "repo_list\n",
      "66/25: file_list = find_repo(\"education\",2)\n",
      "66/26:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{fetch_time}.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        file_list.append(f'{term}_{fetch_time}.json')\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "    return file_list\n",
      "66/27: file_list = find_repo(\"education\",2)\n",
      "66/28:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo_list.append(json.loads(f.readlines()[0])['items'])\n",
      "repo_list\n",
      "66/29:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "\n",
      "json.loads(f.readlines()[0])['items']\n",
      "66/30:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.append(repo)\n",
      "repo_list\n",
      "66/31: file_list\n",
      "66/32:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "66/33:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/34: file_list = find_repo(\"education\",1)\n",
      "66/35:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.append(repo)\n",
      "repo_list\n",
      "66/36: file_list = find_repo(\"education\",2)\n",
      "66/37:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.append(repo)\n",
      "repo_list\n",
      "66/38:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/39:\n",
      "import json\n",
      "f=open('education_2021-10-05 12_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/40:\n",
      "import json\n",
      "f=open('education_2021-10-06 20_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/41:\n",
      "import json\n",
      "f=open('education_2021-10-06 16_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/42:\n",
      "import json\n",
      "f=open('education_2021-10-06 12_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/43:\n",
      "import json\n",
      "f=open('education_2021-10-06 08_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/44:\n",
      "import json\n",
      "f=open('education_2021-10-06 04_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/45:\n",
      "import json\n",
      "f=open('education_2021-10-06 00_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/46:\n",
      "import json\n",
      "f=open('education_2021-10-05 20_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/47:\n",
      "import json\n",
      "f=open('education_2021-10-05 16_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/48:\n",
      "import json\n",
      "f=open('education_2021-10-05 12_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/49:\n",
      "import json\n",
      "f=open('education_2021-10-07 00_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/50:\n",
      "import json\n",
      "f=open('education_2021-10-07 04_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/51:\n",
      "import json\n",
      "f=open('education_2021-10-07 04_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])\n",
      "repo\n",
      "66/52:\n",
      "import json\n",
      "f=open('education_2021-10-07 08_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])\n",
      "repo\n",
      "66/53: file_list = find_repo(\"education\",1)\n",
      "66/54:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.append(repo)\n",
      "repo_list\n",
      "66/55:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.extend(repo)\n",
      "repo_list\n",
      "66/56:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.extend(repo)\n",
      "len(repo_list)\n",
      "66/57:\n",
      "dt=[]\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name = repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name,\n",
      "               \"url\": url,\n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt\n",
      "66/58:\n",
      "#Sometimes this doesn't work because of rate-limit.\n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.extend(repo)\n",
      "repo_list\n",
      "66/59:\n",
      "#try the function\n",
      "file_list = find_repo(\"education\",3)\n",
      "66/60:\n",
      "#Sometimes this doesn't work because of rate-limit.\n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.extend(repo)\n",
      "repo_list\n",
      "66/61:\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{fetch_time}.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        file_list.append(f'{term}_{fetch_time}.json')\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        \n",
      "    return file_list #return the list of file names\n",
      "66/62:\n",
      "#try the function\n",
      "file_list = find_repo(\"education\",3)\n",
      "66/63:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{fetch_time}.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        file_list.append(f'{term}_{fetch_time}.json')\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "        \n",
      "    return file_list #return the list of file names\n",
      "66/64:\n",
      "#try the function\n",
      "file_list = find_repo(\"education\",3)\n",
      "66/65: file_list[11]\n",
      "66/66: file_list\n",
      "66/67:\n",
      "f=open('education_2021-10-07 09_07_08.537482.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "66/68:\n",
      "f=open('education_2021-10-07 09_07_08.537482.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "con[1]\n",
      "66/69:\n",
      "f=open('education_2021-10-07 09_07_08.537482.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "con\n",
      "66/70:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    # clean file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        file_list.append(filename)\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "        \n",
      "    return file_list #return the list of file names\n",
      "66/71:\n",
      "#try the function\n",
      "file_list = find_repo(\"education\",3)\n",
      "66/72: file_list\n",
      "66/73:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json, 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "con\n",
      "66/74:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "con\n",
      "66/75:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "for item in con:\n",
      "    jsonobj = json.loads(item)\n",
      "    print(jsonobj['total_count'])\n",
      "66/76: con[11]\n",
      "66/77: con[11]['total_count']\n",
      "66/78:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "for item in con:\n",
      "    jsonobj = json.loads(item)\n",
      "    try:\n",
      "        jsonobj['total_count']\n",
      "    except:\n",
      "        next\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    print(jsonobj['total_count'])\n",
      "66/79:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "for item in con:\n",
      "    jsonobj = json.loads(item)\n",
      "    try:\n",
      "        print(jsonobj['total_count'])\n",
      "    except:\n",
      "        next\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    print(jsonobj['total_count'])\n",
      "66/80:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "for item in con:\n",
      "    try:\n",
      "        jsonobj = json.loads(item)\n",
      "        print(jsonobj['total_count'])\n",
      "    except:\n",
      "        next\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    print(jsonobj['total_count'])\n",
      "66/81:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "for item in con:\n",
      "    try:\n",
      "        jsonobj = json.loads(item)\n",
      "        print(jsonobj['total_count'])\n",
      "    except:\n",
      "        next\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    print(jsonobj['total_count'])\n",
      "66/82:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "cntr=0\n",
      "for item in con:\n",
      "    print(cntr)\n",
      "    try:\n",
      "        jsonobj = json.loads(item)\n",
      "        print(jsonobj['total_count'])\n",
      "    except:\n",
      "        next\n",
      "    cnt+=1\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    print(jsonobj['total_count'])\n",
      "66/83:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "cntr=0\n",
      "for item in con:\n",
      "    print(cntr)\n",
      "    try:\n",
      "        jsonobj = json.loads(item)\n",
      "        #print(jsonobj['total_count'])\n",
      "    except:\n",
      "        next\n",
      "    cnt+=1\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    #print(jsonobj['total_count'])\n",
      "66/84:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "cntr=0\n",
      "for item in con:\n",
      "    print(cntr)\n",
      "    try:\n",
      "        jsonobj = json.loads(item)\n",
      "        #print(jsonobj['total_count'])\n",
      "    except:\n",
      "        next\n",
      "    cntr+=1\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    #print(jsonobj['total_count'])\n",
      "66/85:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "cntr=0\n",
      "for item in con:\n",
      "    print(cntr)\n",
      "    try:\n",
      "        jsonobj = json.loads(item)\n",
      "        #print(jsonobj['total_count'])\n",
      "    except:\n",
      "        next\n",
      "    cntr+=1\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    print(jsonobj['total_count'])\n",
      "69/1:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    # clean file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return file_list #return the list of file names\n",
      "69/2:\n",
      "#try the function\n",
      "file_list = find_repo(\"education\",3)\n",
      "69/3: file_list\n",
      "69/4:\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "69/5:\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "\n",
      "con\n",
      "69/6:\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "\n",
      "len(con)\n",
      "69/7:\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "\n",
      "con\n",
      "69/8:\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "\n",
      "con[0]\n",
      "69/9:\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "\n",
      "con[1]\n",
      "69/10:\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt=f.readlines()\n",
      "repo = json.loads(dt[0])\n",
      "69/11:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt=f.readlines()\n",
      "repo = json.loads(dt[0])\n",
      "69/12:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt=f.readlines()\n",
      "repo = json.loads(dt[0])\n",
      "repo\n",
      "69/13:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "69/14:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "69/15:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "len(repo)\n",
      "69/16:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = json.loads(f.readlines()[0])\n",
      "69/17:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = json.loads(f.readlines()[0])\n",
      "69/18:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = json.loads(f.readlines()[0])\n",
      "dt\n",
      "69/19:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = json.loads(f.readlines()[1])\n",
      "dt\n",
      "69/20:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = json.loads(f.readlines())\n",
      "\n",
      "dt\n",
      "69/21:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = json.loads(f.readlines()[0])\n",
      "len(repo)\n",
      "69/22:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[0])\n",
      "69/23:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[0])\n",
      "len(dt)\n",
      "69/24:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[0])\n",
      "dt\n",
      "69/25:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[0])\n",
      "dt[1]\n",
      "69/26:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[0])\n",
      "dt[2]\n",
      "69/27:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[0])\n",
      "len(dt)\n",
      "69/28:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[1])\n",
      "len(dt)\n",
      "69/29:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[4])\n",
      "len(dt)\n",
      "69/30:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = json.loads(f.readlines())\n",
      "len(dt)\n",
      "69/31:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "dt\n",
      "69/32:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "dt[1]\n",
      "69/33:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "dt[0]\n",
      "69/34:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "len(dt)\n",
      "69/35:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "repo=json.loads(dt)\n",
      "repo\n",
      "69/36:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "repo=json.loads(dt)\n",
      "repo\n",
      "dt\n",
      "69/37:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "\n",
      "dt\n",
      "69/38:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "repo = []\n",
      "for dt in dt:\n",
      "    repo.append(json.loads(dt)[\"items\"])\n",
      "repo\n",
      "69/39:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "dt[0]\n",
      "69/40:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "json.loads(dt[0])\n",
      "69/41:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "json.loads(dt[0])['items']\n",
      "69/42:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "repo = []\n",
      "for counter in range(len(dt))\n",
      "    repo.extend(json.loads(dt[counter])['items'])\n",
      "69/43:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "repo = []\n",
      "for counter in range(0,len(dt)+1)\n",
      "    repo.extend(json.loads(dt[counter])['items'])\n",
      "69/44:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "repo = []\n",
      "for dt in dt\n",
      "    repo.extend(json.loads(dt)['items'])\n",
      "69/45:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "repo = []\n",
      "for dt in dt:\n",
      "    repo.extend(json.loads(dt)['items'])\n",
      "72/1:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    # clean file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return file_list #return the list of file names\n",
      "72/2:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    # clean file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return file_list #return the list of file names\n",
      "72/3:\n",
      "#try the function\n",
      "file_list = find_repo(\"education\",2)\n",
      "72/4: file_list\n",
      "72/5: filename\n",
      "72/6:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return filename\n",
      "72/7:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",2)\n",
      "72/8: filename\n",
      "72/9:\n",
      "#run the below \n",
      "#copy paste the json file below\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "73/1:\n",
      "#if there is an error-continue in the next line \n",
      "repo =[]\n",
      "import json \n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list=jsonobj[\"items\"]\n",
      "    \n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "        \n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "        repo.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "73/2: repo\n",
      "73/3:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return filename\n",
      "73/4:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "73/5: filename\n",
      "73/6:\n",
      "#run the below \n",
      "#copy paste the json file below\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "73/7:\n",
      "#if there is an error-continue in the next line \n",
      "repo =[]\n",
      "import json \n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list=jsonobj[\"items\"]\n",
      "    \n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "        \n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "        repo.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "73/8: repo\n",
      "73/9: len(repo)\n",
      "73/10:\n",
      "#Writing data into .csv file and returning the table:\n",
      "#error here\n",
      "import csv\n",
      "import pandas as pd \n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"repo.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\",\"readme\"])\n",
      "    for repo in repo:\n",
      "        writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks'],repo['readme']])\n",
      "data = pd.read_csv(\"repo.csv\", delimiter= \";\")\n",
      "73/11: type(repo)\n",
      "73/12: repo\n",
      "73/13: len(content)\n",
      "73/14:\n",
      "#if there is an error-continue in the next line \n",
      "repo =[]\n",
      "repo_list = []\n",
      "import json \n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "        \n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "        \n",
      "        repo.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "73/15: repo_list\n",
      "73/16: len(repo_list)\n",
      "73/17: repo_list[0]\n",
      "73/18: content\n",
      "73/19: content[1]\n",
      "73/20: print(len(content))\n",
      "73/21: content[0]\n",
      "73/22: content[1]\n",
      "73/23: content[2]\n",
      "73/24: content[3]\n",
      "73/25: content[4]\n",
      "73/26: content[5]\n",
      "73/27:\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "73/28:\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "\n",
      "repo_list\n",
      "73/29:\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "\n",
      "len(repo_list)\n",
      "73/30:\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "\n",
      "repo_list\n",
      "73/31:\n",
      "#if there is an error-continue in the next line \n",
      "repo =[]\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    repo.extend({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "73/32:\n",
      "#if there is an error-continue in the next line \n",
      "repo =[]\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    repo.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "73/33:\n",
      "#taking list of repo\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "73/34:\n",
      "#taking list of repo\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "repo_list\n",
      "73/35:\n",
      "#taking list of repo\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "repo_list[0]\n",
      "73/36:\n",
      "repo =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    repo.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "73/37:\n",
      "repo =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    repo.extend([{\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme}])\n",
      "73/38:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "73/39: dt\n",
      "73/40: len(dt)\n",
      "73/41:\n",
      "#Writing data into .csv file and returning the table:\n",
      "#error here\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\",\"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([dt['id'], dt['name'], dt['url'], dt['language'], dt['created'], dt['stars'], dt['watch'], dt['forks'],dt['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "73/42: dt[0]\n",
      "73/43: dt[0]['id']\n",
      "73/44:\n",
      "#error here\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\",\"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "73/45:\n",
      "#error here\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\",\"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "73/46:\n",
      "#error here\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "73/47: data\n",
      "73/48:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\",\"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "75/1:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return filename\n",
      "75/2:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "75/3: filename\n",
      "75/4:\n",
      "#run the below \n",
      "#copy paste the json file below\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "75/5: len(content)\n",
      "75/6:\n",
      "#taking list of repo\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "75/7:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "75/8: dt[0]\n",
      "75/9: dt[0]\n",
      "75/10: data\n",
      "75/11: data\n",
      "75/12:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "#if we remove the readme, then file can be saved and the following works well\n",
      "\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "75/13: data\n",
      "75/14:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "#if we remove the readme, then file can be saved and the following works well\n",
      "\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "75/15:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text() #.replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "75/16: dt[0]\n",
      "75/17:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "#if we remove the readme, then file can be saved and the following works well\n",
      "\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "75/18:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "#if we remove the readme, then file can be saved and the following works well\n",
      "\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "81/1:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    \n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return filename\n",
      "81/2:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    \n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return filename\n",
      "81/3:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "81/4: filename\n",
      "81/5: file_list\n",
      "81/6:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(fetch_time+day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    \n",
      "    return filename\n",
      "81/7:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "81/8: filename\n",
      "81/9:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(since+day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    \n",
      "    return filename\n",
      "81/10:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "81/11:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(str(since)+'\\t'+day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    \n",
      "    return filename\n",
      "81/12:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "81/13:\n",
      "#run the below \n",
      "#copy paste the json file below\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "81/14: len(content)\n",
      "81/15:\n",
      "#taking list of repo\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "81/16:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "81/17: dt[0]\n",
      "81/18: dt[1]\n",
      "81/19: dt[3]\n",
      "81/20:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "#if we remove the readme, then file can be saved and the following works well\n",
      "\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "81/21:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "#if we remove the readme, then file can be saved and the following works well\n",
      "\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\", encoding='utf-8') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "81/22: data\n",
      "81/23:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "#if we remove the readme, then file can be saved and the following works well\n",
      "\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "81/24: data\n",
      "82/1:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(str(since)+'\\t'+day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "82/2:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "82/3: filename\n",
      "82/4:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "82/5: len(content)\n",
      "82/6:\n",
      "#taking list of repo\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "82/7: content[1]\n",
      "82/8:\n",
      "total_count = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    total_count.extend(jsonobj[\"total_count\"])\n",
      "\n",
      "total_count\n",
      "82/9:\n",
      "no_of_repo = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    no_of_repo.extend(jsonobj[\"total_count\"])\n",
      "\n",
      "no_of_repo\n",
      "82/10:\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "\n",
      "no_of_repo\n",
      "82/11: URL\n",
      "83/1:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=python+created:2021-10-10T20:09:08Z..2021-10-11T00:09:08Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(day_url, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "repo_request\n",
      "83/2:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=python+created:2021-10-10T20:09:08Z..2021-10-11T00:09:08Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "repo_request\n",
      "83/3:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=python+created:2021-10-10T20:09:08Z..2021-10-11T00:09:08Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "len(repo_request['items'])\n",
      "83/4:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        \n",
      "        #pagination:\n",
      "        for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_response = requests.get(page_url, headers=HEADERS)\n",
      "            \n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            \n",
      "            #writing all request in the json file (filename):\n",
      "            repo_request=page_response.json()\n",
      "            converted_to_string=json.dumps(repo_request)\n",
      "            f=open(filename,'a',encoding='utf-8')\n",
      "            f.write(converted_to_string + '\\n')\n",
      "            f.close()\n",
      "        \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/5:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/6:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        if search_response.json().get(\"total_count\") == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/7:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/8:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        if search_response.json().get(\"total_count\") == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/9:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/10:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "len(repo_request['items'])\n",
      "83/11:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "repo_request\n",
      "83/12:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "repo_request['items']\n",
      "83/13:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "len(repo_request['items'])\n",
      "83/14: repo_request\n",
      "83/15:\n",
      "no_page = math.ceil(repo_request.get(\"total_count\")/100)\n",
      "no_page\n",
      "83/16:\n",
      "no_page = math.ceil(repo_request.get(\"total_count\")/100)\n",
      "for i in range(1, no_page + 1):\n",
      "    print(i)\n",
      "83/17:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100&page=1'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "len(repo_request['items'])\n",
      "83/18:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "len(repo_request['items'])\n",
      "83/19:\n",
      "no_page = math.ceil(repo_request.get(\"total_count\")/100)\n",
      "for i in range(1, no_page + 1):\n",
      "    page_url = f'{URL}&page={i}'\n",
      "    page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "page_response\n",
      "83/20: repo_request['total_count']\n",
      "83/21:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        if search_response.json().get(\"total_count\") == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/22:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/23: repo_request.get('total_count')\n",
      "83/24:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "\n",
      "response.json().get('total_count')\n",
      "83/25:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100&page=1'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "repo_request\n",
      "83/26: repo_request.get('total_count')\n",
      "83/27:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(search_response.json().get('total_count'))\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/28:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/29:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No. of repositories created between {since} and {until}:{search_response.json().get('total_count')})\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/30:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No. of repositories created between {since} and {until}:{search_response.json().get('total_count')}'')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/31:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No. of repositories created between {since} and {until}:{search_response.json().get('total_count')}')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/32:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/33:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/34:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "83/35:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            if (page_response.status_code!=200): print(page_response.text)\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/36:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/37:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "83/38:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",3)\n",
      "83/39:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",3)\n",
      "82/12:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(str(since)+'\\t'+day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "            \n",
      "        #writing all request in the json file (filename):\n",
      "        repo_request=response.json()\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "82/13:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "82/14: filename\n",
      "82/15:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "82/16:\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "\n",
      "no_of_repo\n",
      "82/17:\n",
      "#taking list of repo\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "83/40:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            if (page_response.status_code!=200): print(page_response.text)\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/41:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "83/42:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/43:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(page_response.text)\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/44:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/45:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(page_response.text)\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/46:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",3)\n",
      "83/47:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "83/48:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(page_response.text)\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/49:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "83/50:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "83/51:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/52:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "83/53:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",2)\n",
      "83/54:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/55:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "82/18:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "82/19:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "82/20: filename\n",
      "82/21:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "82/22:\n",
      "#getting the number of repositories in each search\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "\n",
      "no_of_repo\n",
      "82/23:\n",
      "#taking list of repo and the number of the repos\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "82/24:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "82/25: filename\n",
      "82/26:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "82/27:\n",
      "#getting the number of repositories in each search\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "\n",
      "no_of_repo\n",
      "82/28:\n",
      "#taking list of repo and the number of the repos\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "82/29:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "83/56:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/57:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/58:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    #import os\n",
      "    import time\n",
      "    \n",
      "    #token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':'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'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/59:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/60:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #token=os.environ['GITHUBTOKEN']\n",
      "    token='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'\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':token}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/61:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/62:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    #HEADERS = {'Authorization':f'{token}'}\n",
      "    HEADERS = {'Authorization':'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'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/63:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    #HEADERS = {'Authorization':f'{token}'}\n",
      "    HEADERS = {'Authorization':'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'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/64:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/65:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    #HEADERS = {'Authorization':f'{token}'}\n",
      "    HEADERS = {'Authorization':'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'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/66:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/67:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/68:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/69:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/70:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'% self.api_token}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/71:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/72:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/73:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/74:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100&access_token={token}'\n",
      "    #HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/75:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/76:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/77:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100&access_token={token}'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/78:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/79:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/80:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/81:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/82:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/83:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/84:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/85:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "87/1:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(str(since)+'\\t'+day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "            \n",
      "        #writing all request in the json file (filename):\n",
      "        repo_request=response.json()\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "87/2:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "87/3: filename\n",
      "87/4:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "87/5:\n",
      "#getting the number of repositories in each search\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "\n",
      "no_of_repo\n",
      "87/6:\n",
      "#getting the number of repositories in each search\n",
      "import json\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "\n",
      "no_of_repo\n",
      "87/7:\n",
      "#taking list of repo and the number of the repos\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "87/8:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "87/9:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "87/10:\n",
      "#taking list of repo and the number of the repositories\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "87/11:\n",
      "#getting No. of repositories in each search\n",
      "import json\n",
      "\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj = json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "    \n",
      "no_of_repo\n",
      "88/1:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "88/2:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "87/12: content[0]\n",
      "87/13: len(content)\n",
      "87/14:\n",
      "#taking list of repo and the number of the repositories\n",
      "import json \n",
      "\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    print(len(jsonobj['items']))\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "88/3:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'} #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\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(2)\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "88/4:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "88/5:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'} #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\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7)\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "88/6:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "88/7:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'} #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\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7)\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "88/8:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "88/9:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'} #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\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7)\n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7)\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "       \n",
      "    return filename\n",
      "88/10:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "88/11:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "88/12: filename\n",
      "88/13:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "88/14: content\n",
      "88/15:\n",
      "import json \n",
      "\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    print(len(jsonobj['items']))\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "88/16:\n",
      "#getting No. of repositories in each search\n",
      "import json\n",
      "\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj = json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "    \n",
      "no_of_repo\n",
      "88/17:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "89/1: filename\n",
      "89/2:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'} #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\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7)\n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7)\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "       \n",
      "    return filename\n",
      "89/3:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",3)\n",
      "89/4: filename\n",
      "89/5:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "89/6:\n",
      "import json \n",
      "\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    print(len(jsonobj['items']))\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "89/7:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "89/8: dt[0]\n",
      "89/9: len(dt)\n",
      "89/10:\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "89/11: data\n",
      "89/12:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "89/13: filename\n",
      "89/14:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "89/15:\n",
      "import json \n",
      "\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    print(len(jsonobj['items']))\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "89/16:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "89/17: len(dt)\n",
      "89/18:\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "89/19: data\n",
      "89/20:\n",
      "#try the function\n",
      "filename = find_repo(\"machine learning\",1)\n",
      "89/21: filename\n",
      "89/22:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "89/23:\n",
      "import json \n",
      "\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    print(len(jsonobj['items']))\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "89/24:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "89/25: len(dt)\n",
      "89/26:\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "89/27: data\n",
      "89/28:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",3)\n",
      "89/29: filename\n",
      "89/30:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "89/31:\n",
      "import json \n",
      "\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    print(len(jsonobj['items']))\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "89/32:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "89/33: len(dt)\n",
      "89/34:\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "89/35: data\n",
      "94/1:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "       \n",
      "    return filename\n",
      "94/2:\n",
      "#try the function\n",
      "filename = find_repo(\"open education\",3)\n",
      "97/1:\n",
      "#Function1 find_repo: Looking for respositories containing a specific \"term\"\n",
      "#created within a specific timeframe of [day] days\n",
      "\n",
      "def find_repo(term, day):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the fetching URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##building URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{datetime.today()}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "       \n",
      "    return filename\n",
      "97/2:\n",
      "#Function 2:  Extracting the list of repositories from the json file\n",
      "\n",
      "def extract_repo_list(filename):\n",
      "    import json \n",
      "    f=open(filename,'r',encoding='utf-8')\n",
      "    content=f.readlines()\n",
      "    repo_list = []\n",
      "\n",
      "    for item in content:\n",
      "        jsonobj=json.loads(item)\n",
      "        print(len(jsonobj['items']))\n",
      "        repo_list.extend(jsonobj[\"items\"])\n",
      "    return repo_list\n",
      "97/3:\n",
      "#Function 3: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    \n",
      "    dt =[]\n",
      "\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "        return dt\n",
      "97/4:\n",
      "#Function 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "97/5:\n",
      "# Search for repositories:\n",
      "filename = find_repo(\"education\",1)\n",
      "# Extract repository lists:\n",
      "repo_list = extract_repo_list(filename)\n",
      "# Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "# Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/6:\n",
      "#Function1 find_repo: Looking for respositories containing a specific \"term\"\n",
      "#created within a specific timeframe of [day] days\n",
      "\n",
      "def find_repo(term, day):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the fetching URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##building URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "       \n",
      "    return filename\n",
      "97/7:\n",
      "#Function 2:  Extracting the list of repositories from the json file\n",
      "\n",
      "def extract_repo_list(filename):\n",
      "    import json \n",
      "    f=open(filename,'r',encoding='utf-8')\n",
      "    content=f.readlines()\n",
      "    repo_list = []\n",
      "\n",
      "    for item in content:\n",
      "        jsonobj=json.loads(item)\n",
      "        print(len(jsonobj['items']))\n",
      "        repo_list.extend(jsonobj[\"items\"])\n",
      "    return repo_list\n",
      "97/8:\n",
      "#Function 3: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    \n",
      "    dt =[]\n",
      "\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "        return dt\n",
      "97/9:\n",
      "#Function 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "97/10:\n",
      "# Search for repositories:\n",
      "filename = find_repo(\"education\",1)\n",
      "# Extract repository lists:\n",
      "repo_list = extract_repo_list(filename)\n",
      "# Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "# Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/11: len(dt)\n",
      "97/12: len(repo_list)\n",
      "97/13:\n",
      "#Function 3: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    \n",
      "    dt =[]\n",
      "\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "    return dt\n",
      "97/14:\n",
      "# Search for repositories:\n",
      "filename = find_repo(\"education\",1)\n",
      "# Extract repository lists:\n",
      "repo_list = extract_repo_list(filename)\n",
      "# Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "# Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/15: len(repo_list)\n",
      "97/16: len(dt)\n",
      "97/17:\n",
      "#Function1 find_repo: Looking for respositories containing a specific \"term\"\n",
      "#created within a specific timeframe of [day] days\n",
      "\n",
      "def find_repo(term, day):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the fetching URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##building URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=12)\n",
      "       \n",
      "    return filename\n",
      "97/18:\n",
      "#Function1 find_repo: Looking for respositories containing a specific \"term\"\n",
      "#created within a specific timeframe of [day] days\n",
      "\n",
      "def find_repo(term, day, h):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the fetching URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##building URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours=h)\n",
      "       \n",
      "    return filename\n",
      "97/19:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"python\",1,10)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/20:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"python\",3,5)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/21:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"python\",3,8)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/22:\n",
      "#Function1 grf - GitHub Repository Finder: \n",
      "#Looking for respositories containing a specific \"term\"\n",
      "#created within a specific timeframe of [day] days\n",
      "#search results are crawled on every h-hour segment\n",
      "\n",
      "def grf(term, day, h):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the API URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##building URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file to monitor error\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file to combine all json files returned from each call:\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until <= datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every h-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours=h)\n",
      "       \n",
      "    return filename\n",
      "97/23:\n",
      "#Function 2:  Extracting the list of repositories from the json file\n",
      "\n",
      "def extract_repo_list(filename):\n",
      "    import json \n",
      "    f=open(filename,'r',encoding='utf-8')\n",
      "    content=f.readlines()\n",
      "    repo_list = []\n",
      "\n",
      "    for item in content:\n",
      "        jsonobj=json.loads(item)\n",
      "        repo_list.extend(jsonobj[\"items\"])\n",
      "    return repo_list\n",
      "97/24:\n",
      "#Function 3: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    \n",
      "    dt =[]\n",
      "\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "    return dt\n",
      "97/25:\n",
      "#Function 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "97/26:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"python\",3,8)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/27:\n",
      "#Function1 grf - GitHub Repository Finder: 3 parameters are defined:\n",
      "#term: specifies the keyword to search for respositories\n",
      "#day: specifies the timeframe to search for repositories. If t is the moment extraction is started then the function will return all repositories created between (t-d) and t.\n",
      "#search results are crawled on every h-hour segment\n",
      "\n",
      "def grf(term, day, h):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the API URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##building URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file to monitor error\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file to combine all json files returned from each call:\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until <= datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every h-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours=h)\n",
      "       \n",
      "    return filename\n",
      "97/28:\n",
      "#Function1 grf - GitHub Repository Finder: 3 parameters are defined:\n",
      "#>term: specifies the keyword to search for respositories. Type: str\n",
      "#>d: specifies the timeframe to search for repositories. Type: int \n",
      "##If [now] is the moment when extraction is started, function will return all repositories \n",
      "##created between (now-d) days and now.\n",
      "#>h: specifies the time duration (hours) of each query. Type: int\n",
      "\n",
      "def grf(term, d, h):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the API URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##building URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days = d)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours = h) # dividing the total No.of repo into segments of h hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file to monitor error\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file to combine all json files returned from each call:\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until <= datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every h-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours=h)\n",
      "       \n",
      "    return filename\n",
      "97/29:\n",
      "#Function 2:  Extracting the list of repositories from the json file\n",
      "\n",
      "def extract_repo_list(filename):\n",
      "    import json \n",
      "    f=open(filename,'r',encoding='utf-8')\n",
      "    content=f.readlines()\n",
      "    repo_list = []\n",
      "\n",
      "    for item in content:\n",
      "        jsonobj=json.loads(item)\n",
      "        repo_list.extend(jsonobj[\"items\"])\n",
      "    return repo_list\n",
      "97/30:\n",
      "#Function 3: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    \n",
      "    dt =[]\n",
      "\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "    return dt\n",
      "97/31:\n",
      "#Function 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "97/32:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"education\",1,8)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/33:\n",
      "#Function1 find_repo - Looking for repositories defined by 3 parameters:\n",
      "#>term: specifies the keyword to search for respositories. Type: str\n",
      "#>d: specifies the timeframe to search for repositories. Type: int \n",
      "##If [now] is the moment when extraction is started, function will return all repositories \n",
      "##created between (now-d) days and now.\n",
      "#>h: specifies the time duration (hours) of each query. Type: int\n",
      "\n",
      "def grf(term, d, h):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the API URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##constructing API URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days = d)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours = h) # dividing the total No.of repo into segments of h hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file to monitor error\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file to combine all json files returned from each call:\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until <= datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every h-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours=h)\n",
      "       \n",
      "    return filename\n",
      "97/34:\n",
      "#Function 2:  Extracting the list of repositories from the json file\n",
      "\n",
      "def extract_repo_list(filename):\n",
      "    import json \n",
      "    f=open(filename,'r',encoding='utf-8')\n",
      "    content=f.readlines()\n",
      "    repo_list = []\n",
      "\n",
      "    for item in content:\n",
      "        jsonobj=json.loads(item)\n",
      "        repo_list.extend(jsonobj[\"items\"])\n",
      "    return repo_list\n",
      "97/35:\n",
      "#Function 3: save_column: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    \n",
      "    dt =[]\n",
      "\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "        \n",
      "        #scraping readme by going through each repository url\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "    return dt\n",
      "97/36:\n",
      "#Function 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "97/37:\n",
      "#Function 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "97/38:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"education\",1,8)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "96/1:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=8) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=8)\n",
      "       \n",
      "    return filename\n",
      "96/2:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "96/3: filename\n",
      "97/39:\n",
      "#Function1 find_repo - Looking for repositories defined by 3 parameters:\n",
      "#>term: specifies the keyword to search for respositories. Type: str\n",
      "#>d: specifies the timeframe to search for repositories. Type: int \n",
      "##If [now] is the moment when extraction is started, function will return all repositories \n",
      "##created between (now-d) days and now.\n",
      "#>h: specifies the time duration (hours) of each query. Type: int\n",
      "\n",
      "def find_repo(term, d, h):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the API URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days = d)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours = h) # dividing the total No.of repo into segments of h hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file to monitor error\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file to combine all json files returned from each call:\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until <= datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every h-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours = h)\n",
      "       \n",
      "    return filename\n",
      "97/40: find_repo(\"education\", 1, 8)\n",
      "97/41:\n",
      "#Function1 find_repo - Looking for repositories defined by 3 parameters:\n",
      "#>term: specifies the keyword to search for respositories. Type: str\n",
      "#>d: specifies the timeframe to search for repositories. Type: int \n",
      "##If [now] is the moment when extraction is started, function will return all repositories \n",
      "##created between (now-d) days and now.\n",
      "#>h: specifies the time duration (hours) of each query. Type: int\n",
      "\n",
      "def find_repo(term, d, h):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the API URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days = d)  # Start fetching repo created {d} days ago\n",
      "    until = since + timedelta(hours = h) # dividing the total No.of repo into segments of h hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{d}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file to monitor error\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file to combine all json files returned from each call:\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until <= datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every h-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request = page_response.json()\n",
      "                converted_to_string = json.dumps(repo_request)\n",
      "                f = open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours = h)\n",
      "       \n",
      "    return filename\n",
      "97/42: find_repo(\"education\", 1, 8)\n",
      "97/43:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"education\",1,8)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/44:\n",
      "#Wrapping up into one single function:\n",
      "def grf(term,d,h):\n",
      "    filename = find_repo(\"education\",1,8)\n",
      "    repo_list = extract_repo_list(filename)\n",
      "    dt = save_column(repo_list)\n",
      "    data = save_dt(dt)\n",
      "    return data\n",
      "97/45: grf(\"education\", 1, 8)\n",
      "98/1:\n",
      "import requests\n",
      "url = 'https://www.reddit.com/r/politics.json'\n",
      "\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "politics_response = response.json()\n",
      "98/2: politics_response\n",
      "98/3: politics_response['data'][1]\n",
      "98/4: politics_response('data')\n",
      "98/5: politics_response['data']\n",
      "98/6: politics_response['data']['children']\n",
      "98/7: len(politics_response['data']['children'])\n",
      "98/8: politics_response['data']['children'][0]\n",
      "98/9: politics_response['data']['children'][0]['data']\n",
      "98/10: politics_response['data']['children'][0]['data']['selftext']\n",
      "98/11: len(politics_response['data']['children'])\n",
      "98/12: politics_response['data']\n",
      "98/13:\n",
      "mod = 'politics'\n",
      "after = None\n",
      "item_type = []\n",
      "\n",
      "for counter in range(5): \n",
      "    url = f'https://www.reddit.com/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['kind'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/14:\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": t3_q9dtmj})\n",
      "json_response = response.json()\n",
      "after = json_response['data']['after']\n",
      "98/15:\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": t3_q9dtmj})\n",
      "json_response = response.json()\n",
      "98/16:\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": 't3_q9dtmj'})\n",
      "json_response = response.json()\n",
      "98/17:\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": 't3_q9dtmj'})\n",
      "json_response = response.json()\n",
      "after = json_response['data']['after']\n",
      "98/18: json_response\n",
      "98/19:\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "after = t3_q9dtmj\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": after})\n",
      "json_response = response.json()\n",
      "after = json_response['data']['after']\n",
      "98/20:\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "after = 't3_q9dtmj'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": after})\n",
      "json_response = response.json()\n",
      "after = json_response['data']['after']\n",
      "98/21:\n",
      "url = f'https://www.reddit.com/{mod}.json?after=t3_1jg35w'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers)\n",
      "json_response = response.json()\n",
      "after = json_response['data']['after']\n",
      "98/22:\n",
      "url = f'https://www.reddit.com/{mod}.json?after=t3_1jg35w'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers)\n",
      "json_response = response.json()\n",
      "98/23: json_response\n",
      "99/1:\n",
      "after = None\n",
      "item_type = []\n",
      "mod = \"sixwaystop313\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/user/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['kind'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/24:\n",
      "import json\n",
      "import requests\n",
      "\n",
      "mod = 'politics'\n",
      "after = None\n",
      "item_type = []\n",
      "\n",
      "for counter in range(5): \n",
      "    url = f'https://www.reddit.com/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['kind'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/25:\n",
      "import json\n",
      "import requests\n",
      "\n",
      "mod = 'politics'\n",
      "after = None\n",
      "item_type = []\n",
      "\n",
      "for counter in range(5): \n",
      "    url = f'https://www.reddit.com/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after']\n",
      "98/26:\n",
      "import json\n",
      "import requests\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "after = 't3_q9dtmj'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": after})\n",
      "json_response = response.json()\n",
      "after = json_response['data']['after']\n",
      "99/2:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "joke_request = response.json() \n",
      "print(joke_request)\n",
      "99/3:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "joke_request = response.json()\n",
      "print(joke_request)\n",
      "99/4: # your answer goes here!\n",
      "99/5:\n",
      "# Question 1 \n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"dog\"})\n",
      "joke_request = response.json()\n",
      "print(f\"The number of dog jokes is: {joke_request['total_jokes']}\")\n",
      "99/6:\n",
      "# Question 2\n",
      "def find_jokes(term):\n",
      "    search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    joke_request = response.json()\n",
      "    num_results = joke_request['total_jokes']\n",
      "    return num_results\n",
      "\n",
      "find_jokes(\"some-searchterm-you-would-like-to-try-out\")\n",
      "99/7:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"})\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = '' # let's remove all jokes, and only look at the other attributes in the JSON response\n",
      "joke_request\n",
      "99/8:\n",
      "for limit in range(10, 31, 10):  # note that range(a, b) runs from a to b-1; so the last value is exclusive (so from 10 to 30 with steps of 10)\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": limit})\n",
      "    joke_request = response.json()\n",
      "    print(f\"Limit {limit} gives {joke_request['total_pages']} pages\")\n",
      "99/9:\n",
      "response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": 5,\n",
      "                                   \"page\": 2})\n",
      "response.json()\n",
      "99/10: # your answer goes here!\n",
      "99/11:\n",
      "def find_jokes(term):\n",
      "    search_url = \"https://icanhazdadjoke.com/search\"\n",
      "    page = 1\n",
      "    jokes = []\n",
      "\n",
      "    while True:  # alternatively you can also use a for-loop that goes from page 1 to total_results / 20 (rounded up)\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,  # optionally you can add \"limit\": 20 but that's already the default so it doesn't change anything\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        jokes.extend(joke_request['results'])\n",
      "        if joke_request['current_page'] <= joke_request['total_pages']:\n",
      "            page += 1\n",
      "        else: \n",
      "            return jokes\n",
      "\n",
      "output = find_jokes(\"rain\") # try running it with \"\", too!\n",
      "99/12: print(f\"You've collected {len(output)} jokes\")\n",
      "99/13:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    total_page = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "99/14:\n",
      "import requests\n",
      "url = 'https://www.reddit.com/r/marketing/about/.json'\n",
      "\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "json_response = response.json()\n",
      "99/15:\n",
      "# Question 2 \n",
      "import time\n",
      "\n",
      "i = 1\n",
      "while i <= 3:\n",
      "    url = 'https://www.reddit.com/r/marketing/about/.json'\n",
      "    headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "    response = requests.get(url, headers=headers)\n",
      "    json_response = response.json()\n",
      "    \n",
      "    print(json_response['data']['active_user_count'])\n",
      "    i += 1\n",
      "    time.sleep(5)\n",
      "99/16:\n",
      "# Question 3\n",
      "def get_usercount(subreddit):\n",
      "    response = response = requests.get(f'https://www.reddit.com/r/{subreddit}/about/.json', headers=headers)\n",
      "    json_response = response.json()\n",
      "    out = {}\n",
      "    out['subreddit'] = subreddit\n",
      "    out['total_users'] = json_response['data']['subscribers']\n",
      "    out['active_users'] = json_response['data']['active_user_count']\n",
      "    return out\n",
      "    \n",
      "get_usercount('science')\n",
      "99/17:\n",
      "mod = \"sixwaystop313\"\n",
      "response = requests.get(f'https://www.reddit.com/user/{mod}.json', headers=headers)\n",
      "json_response = response.json()\n",
      "json_response\n",
      "99/18:\n",
      "import json\n",
      "json_response = json.loads(response.text.replace('null', '\"None\"').replace('True','\"True\"').replace('False','\"False\"'))\n",
      "json_response\n",
      "99/19: json_response['data']['after']\n",
      "99/20:\n",
      "after = json_response['data']['after']\n",
      "url = f'https://www.reddit.com/user/{mod}.json'\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": after})\n",
      "json_response_after = response.json()\n",
      "json_response_after\n",
      "99/21:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "item_type = []\n",
      "mod = \"sixwaystop313\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/user/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['kind'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/27:\n",
      "import json\n",
      "import requests\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "after = 't3_q9dtmj'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": after})\n",
      "json_response = response.json()\n",
      "after = json_response['data']['after']\n",
      "98/28:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "item_type = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['kind'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/29:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "item_type = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['kind'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/30:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "self_text = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['selftext'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/31: politics_response['data']['children']\n",
      "98/32:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "self_text = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['selftext'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/33: politics_response['data']['children']['selftext']\n",
      "98/34: politics_response['data']['children']('selftext')\n",
      "98/35: politics_response['data']['children'][0]('selftext')\n",
      "98/36: politics_response['data']['children'][0]['selftext']\n",
      "98/37: politics_response['data']['children'][0]\n",
      "98/38: politics_response['data']['children'][0]['data']['selftext']\n",
      "98/39:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "self_text = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['data']['selftext'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/40:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "self_text = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        self_text.append(item['data']['selftext'])\n",
      "\n",
      "# Let's view the item types: \n",
      "self_text\n",
      "98/41: politics_response['data']['children'][1]['data']['selftext']\n",
      "98/42: politics_response['data']['children'][3]['data']['selftext']\n",
      "98/43: politics_response['data']['children'][3]['data']\n",
      "98/44: politics_response['data']['children'][3]\n",
      "98/45:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']['data']:\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append(\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups)\n",
      "\n",
      "post_list\n",
      "98/46:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']['data']:\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/47: politics_response['data']['children'][3]['data']\n",
      "98/48:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "    politics = json_response['data']['children']['data']\n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in politics:\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/49: politics_response['data']['children'][3]['data'].get(\"ups\")\n",
      "98/50: politics_response['data']['children'][3]['data'][\"ups\"]\n",
      "98/51:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "    json_response['data']['children']['data']\n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i['data']\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/52:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "    \n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i['data']\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/53:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "selftext = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "    \n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        selftext.append(item['data'][\"selftext\"])\n",
      "    \n",
      "\n",
      "\n",
      "selftext\n",
      "98/54:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "    json_response['data']['children']['data']\n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/55:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i['data']\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/56: len(post_list)\n",
      "98/57:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "selftext = []\n",
      "mod = \"politics\"\n",
      "for counter in range(100): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "    \n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        selftext.append(item['data'][\"selftext\"])\n",
      "    \n",
      "selftext\n",
      "98/58:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "for counter in range(100): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i['data']\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/59: len(post_list)\n",
      "99/22:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "item_type = []\n",
      "mod = \"sixwaystop313\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/user/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['kind'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "99/23: len(item_type)\n",
      "98/60:\n",
      "import json\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        id = item['data'][\"id\"]\n",
      "        selftext = item['data'][\"selftext\"]\n",
      "        downs = item['data']['downs']\n",
      "        ups = item['data']['ups']\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/61:\n",
      "import json\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        id = item['data'][\"id\"]\n",
      "        selftext = item['data'][\"selftext\"]\n",
      "        downs = item['data']['downs']\n",
      "        ups = item['data']['ups']\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/62: len(post_list)\n",
      "98/63:\n",
      "import json\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        id = item['data'][\"id\"]\n",
      "        selftext = item['data'][\"selftext\"]\n",
      "        downs = item['data']['downs']\n",
      "        ups = item['data']['ups']\n",
      "        post_list.append({\"id\":id,\n",
      "                        \"selftext\":selftext,\n",
      "                        \"downs\": downs,\n",
      "                        \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/64: len(post_list)\n",
      "98/65: post_list[2]\n",
      "98/66: post_list[6]\n",
      "98/67: post_list[10]\n",
      "98/68: post_list\n",
      "98/69:\n",
      "for i in post_list:\n",
      "    print(i['selftext'])\n",
      "98/70:\n",
      "import json\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i['data']\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "        subreddit = item.get('subreddit')\n",
      "        post_list.append({\"id\":id,\n",
      "                        \"selftext\":selftext,\n",
      "                        \"downs\": downs,\n",
      "                        \"ups\":ups,\n",
      "                         \"community\":subreddit})\n",
      "\n",
      "post_list\n",
      "98/71:\n",
      "def post_list(mod)\n",
      "    import json\n",
      "    for counter in range(10): \n",
      "        url = f'https://www.reddit.com/r/{mod}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            subreddit = item.get('subreddit')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups,\n",
      "                             \"community\":subreddit})\n",
      "\n",
      "    return post_list\n",
      "98/72:\n",
      "def post_list(mod):\n",
      "    import json\n",
      "    for counter in range(10): \n",
      "        url = f'https://www.reddit.com/r/{mod}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            subreddit = item.get('subreddit')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups,\n",
      "                             \"community\":subreddit})\n",
      "\n",
      "    return post_list\n",
      "98/73:\n",
      "after = None\n",
      "post_list = []\n",
      "post_list = post_list(\"science\")\n",
      "post_list(\"politics\")\n",
      "98/74:\n",
      "after = None\n",
      "post_list = []\n",
      "post_list = export_post(\"science\")\n",
      "export_post(\"politics\")\n",
      "98/75:\n",
      "def export_post(com):\n",
      "    import json\n",
      "    for counter in range(10): \n",
      "        url = f'https://www.reddit.com/r/{com}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            subreddit = item.get('subreddit')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups,\n",
      "                             \"community\":subreddit})\n",
      "\n",
      "    return post_list\n",
      "98/76:\n",
      "after = None\n",
      "post_list = []\n",
      "post_list = export_post(\"science\")\n",
      "export_post(\"politics\")\n",
      "98/77:\n",
      "def export_post(com):\n",
      "    import json\n",
      "    after = None\n",
      "    for counter in range(10): \n",
      "        url = f'https://www.reddit.com/r/{com}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            subreddit = item.get('subreddit')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups,\n",
      "                             \"community\":subreddit})\n",
      "\n",
      "    return post_list\n",
      "98/78:\n",
      "post_list = []\n",
      "post_list = export_post(\"science\")\n",
      "export_post(\"politics\")\n",
      "98/79:\n",
      "post_list = []\n",
      "post_list = export_post(\"science\")\n",
      "data = export_post(\"politics\")\n",
      "98/80: data\n",
      "98/81: len(data)\n",
      "98/82: post_list\n",
      "98/83:\n",
      "post_list = []\n",
      "post_list = export_post(\"science\")\n",
      "post_list = export_post(\"politics\")\n",
      "98/84: len(post_list)\n",
      "98/85: post_list\n",
      "98/86:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i['data']\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "        post_list.append({\"id\":id,\n",
      "                        \"selftext\":selftext,\n",
      "                        \"downs\": downs,\n",
      "                        \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/87:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i['data']\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "        post_list.append({\"id\":id,\n",
      "                        \"selftext\":selftext,\n",
      "                        \"downs\": downs,\n",
      "                        \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/88:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(filename,'w',encoding='utf-8')\n",
      "f.close()\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{mod}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups})\n",
      "98/89:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{mod}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups})\n",
      "97/46:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"python\",3,8)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/47:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"python\",3,8)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "97/48:\n",
      "#Wrapping up into one single function: Github Repository Finder (grf)\n",
      "def grf(term, d, h):\n",
      "    filename = find_repo(term, d, h)\n",
      "    repo_list = extract_repo_list(filename)\n",
      "    dt = save_column(repo_list)\n",
      "    data = save_dt(dt)\n",
      "    return data\n",
      "97/49: grf(\"python\", 3, 8)\n",
      "100/1:\n",
      "#Step 2:  Extracting the list of repositories from the json file\n",
      "\n",
      "def extract_repo_list(filename):\n",
      "    '''\n",
      "    Reads .json file of repository data, returns a list of repositories.\n",
      "\n",
      "            Parameters:\n",
      "                    filename (str): Name of the .json file obtained from find_repo()\n",
      "                    \n",
      "            Returns:\n",
      "                    repo_list (list): List of all repositories found in the search queries \n",
      "    ''' \n",
      "    import json \n",
      "    f=open(filename,'r',encoding='utf-8')\n",
      "    content=f.readlines()\n",
      "    repo_list = []\n",
      "\n",
      "    for item in content:\n",
      "        jsonobj=json.loads(item)\n",
      "        repo_list.extend(jsonobj[\"items\"])\n",
      "    return repo_list\n",
      "100/2:\n",
      "#Step1: Searching for repositories, crawling data, storing responses in a .json file\n",
      "\n",
      "def find_repo(term, d, h):\n",
      "    '''\n",
      "    Searches for repositories on GitHub, returns name of a .json file of original data.\n",
      "\n",
      "            Parameters:\n",
      "                    term (str): A string defining the search term\n",
      "                    d (int): An integer defining the timeframe to search (in days)\n",
      "                    h (int): An integer defining the duration (in hours) of each query\n",
      "\n",
      "            Returns:\n",
      "                    filename (str): Name of a single .json file which stores the original response data\n",
      "                    .json file: A file stored in working directory\n",
      "                    log.txt: Log file recording all API calls and their status\n",
      "    \n",
      "    Check GRF documentation for detailed function explanation. \n",
      "    ''' \n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #constructing URLs\n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    #setting timeframe and search segments\n",
      "    since = datetime.today() - relativedelta(days = d)  \n",
      "    until = since + timedelta(hours = h)\n",
      "    \n",
      "    #setting name for the final json file\n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{d}days_{fetch_time}.json' \n",
      "    \n",
      "    # creating the log file to monitor error\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # creating an empty json file to combine all json files returned from each call:\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7)\n",
      "        \n",
      "        #print out No of repos in every h-hour request\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #If no Total_count found, print out the failed link and additional information\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            \n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) \n",
      "        \n",
      "        #if extraction successful, do pagination:\n",
      "        else:\n",
      "            #calculating the total No. of pages\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) \n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7)\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request = page_response.json()\n",
      "                converted_to_string = json.dumps(repo_request)\n",
      "                f = open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        #update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours = h)\n",
      "       \n",
      "    return filename\n",
      "100/3:\n",
      "#Step 3: save_column: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    '''\n",
      "    Extracts relevant repository information, saves it in a list.\n",
      "\n",
      "            Parameters:\n",
      "                    repo_list (list): List obtained from export_repo_list()\n",
      "                    \n",
      "            Returns:\n",
      "                    dt (list): List of repositories with only desired information \n",
      "    ''' \n",
      "    dt =[]\n",
      "    \n",
      "    #extracting data directly from the list\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "        \n",
      "        #scraping readme by going through each repository url\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        #if readme is empty, recording as blank (NaN)\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "    return dt\n",
      "100/4:\n",
      "#Step 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    '''\n",
      "    Saves desired data in a .csv file, returns a preview of the final dataset.\n",
      "\n",
      "            Parameters:\n",
      "                    dt (list): List obtained from save_column()\n",
      "                    \n",
      "            Returns:\n",
      "                    data.csv: Final dataset stored in working directory\n",
      "    ''' \n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "100/5: ?find_repo\n",
      "100/6: ?export_repo_list\n",
      "100/7:\n",
      "#Step 2:  Extracting the list of repositories from the json file\n",
      "\n",
      "def extract_repo_list(filename):\n",
      "    '''\n",
      "    Reads .json file of repository data, returns a list of repositories.\n",
      "\n",
      "            Parameters:\n",
      "                    filename (str): Name of the .json file obtained from find_repo()\n",
      "                    \n",
      "            Returns:\n",
      "                    repo_list (list): List of all repositories found in the search queries \n",
      "    ''' \n",
      "    import json \n",
      "    f=open(filename,'r',encoding='utf-8')\n",
      "    content=f.readlines()\n",
      "    repo_list = []\n",
      "\n",
      "    for item in content:\n",
      "        jsonobj=json.loads(item)\n",
      "        repo_list.extend(jsonobj[\"items\"])\n",
      "    return repo_list\n",
      "100/8:\n",
      "#Step 3: save_column: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    '''\n",
      "    Extracts relevant repository information, saves it in a list.\n",
      "\n",
      "            Parameters:\n",
      "                    repo_list (list): List obtained from extract_repo_list()\n",
      "                    \n",
      "            Returns:\n",
      "                    dt (list): List of repositories with only desired information \n",
      "    ''' \n",
      "    dt =[]\n",
      "    \n",
      "    #extracting data directly from the list\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "        \n",
      "        #scraping readme by going through each repository url\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        #if readme is empty, recording as blank (NaN)\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "    return dt\n",
      "100/9:\n",
      "#Step 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    '''\n",
      "    Saves desired data in a .csv file, returns a preview of the final dataset.\n",
      "\n",
      "            Parameters:\n",
      "                    dt (list): List obtained from save_column()\n",
      "                    \n",
      "            Returns:\n",
      "                    data.csv: Final dataset stored in working directory\n",
      "    ''' \n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "100/10: ?extract_repo_list\n",
      "100/11: ?save_column\n",
      "100/12: ?save_dt\n",
      "100/13:\n",
      "#Wrapping up into one single function: Github Repository Finder (grf)\n",
      "def grf(term, d, h):\n",
      "    '''\n",
      "    Searches for repositories on GitHub, returns a .csv file of final dataset.\n",
      "\n",
      "            Parameters:\n",
      "                    term (str): A string defining the search term\n",
      "                    d (int): An integer defining the timeframe to search (in days)\n",
      "                    h (int): An integer defining the duration (in hours) of each query\n",
      "\n",
      "            Returns:\n",
      "                    data.csv: Final dataset stored in working directory\n",
      "                    .json file: A file of raw data stored in working directory\n",
      "                    log.txt: Log file recording all API calls and their status\n",
      "    \n",
      "    Check GRF documenation for more details on the function and its components.\n",
      "    ''' \n",
      "    filename = find_repo(term, d, h)\n",
      "    repo_list = extract_repo_list(filename)\n",
      "    dt = save_column(repo_list)\n",
      "    data = save_dt(dt)\n",
      "    return data\n",
      "100/14: ?grf\n",
      "100/15: grf(\"python\", 3, 8)\n",
      "100/16: grf(\"python\", 3, 8)\n",
      "100/17:\n",
      "import pandas as pd\n",
      "pd.read_csv(\"data/dt.csv\", delimiter= \";\",nrows=100)\n",
      "100/18:\n",
      "import pandas as pd\n",
      "pd.read_csv(\"data/dt.csv\", delimiter= \";\",nrows=10)\n",
      "102/1:\n",
      "import pandas as pd\n",
      "pd.read_csv(\"../data/dt.csv\", delimiter= \";\",nrows=10)\n",
      "106/1:\n",
      "# extracts all links (the a tag!), and prints the URL (href) to the screen;\n",
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find_all(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "106/2: soup\n",
      "106/3: res\n",
      "106/4: res.text\n",
      "106/5: soup\n",
      "106/6: res.text\n",
      "106/7: soup.find_all(\"a\")\n",
      "106/8: soup.find_all(\"a\")[0].attrs[\"href\"]\n",
      "106/9: soup.find_all(\"a\")[0]\n",
      "106/10: soup.find_all(\"a\")[0].attrs[\"href\"]\n",
      "106/11: soup.find_all(\"a\")\n",
      "106/12: soup.find_all(\"product_pod\")\n",
      "106/13: soup.find_all(class_ = \"product_pod\")\n",
      "106/14: len(soup.find_all(class_ = \"product_pod\"))\n",
      "106/15: soup.find_all(class_ = \"product_pod\")[0]\n",
      "106/16: soup.find_all(class_ = \"product_pod\")[0].find_all[\"h3\"]\n",
      "106/17: soup.find_all(class_ = \"product_pod\")[0].find_all(\"h3\")\n",
      "106/18: soup.find_all(class_ = \"product_pod\")[0].find_all(\"a\").attrs[\"href\"]\n",
      "106/19: soup.find_all(class_ = \"product_pod\")[0].find_all(\"a\")\n",
      "106/20: soup.find_all(class_ = \"product_pod\")[0].find(\"a\").attrs[\"href\"]\n",
      "106/21:\n",
      "# extracts all links (the a tag!), and prints the URL (href) to the screen;\n",
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "106/22:\n",
      "# extracts all links (the a tag!), and prints the URL (href) to the screen;\n",
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find_all(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "106/23: soup.find(\"a\")\n",
      "106/24: soup.find_all(\"a\")\n",
      "106/25: soup.findl(\"a\").link.attrs[\"href\"]\n",
      "106/26: soup.findl(\"a\").attrs[\"href\"]\n",
      "106/27: soup.findl(\"a\")\n",
      "106/28: soup.find(\"a\").attrs[\"href\"]\n",
      "106/29: soup.find_all(class_ = \"product_pod\")[1].find(\"a\").attrs[\"href\"]\n",
      "106/30:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "book_url = base_url + url_book[6:]\n",
      "book_url\n",
      "106/31: url_book = soup.find_all(class_ = \"product_pod\")[1].find(\"a\").attrs[\"href\"]\n",
      "106/32:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "book_url = base_url + url_book[6:]\n",
      "book_url\n",
      "106/33: url_book.replace(\"../../\",base_url)\n",
      "106/34: soup.find_all(class_ = \"product_pod\")[1]\n",
      "106/35: soup.find_all(class_ = \"product_pod\")\n",
      "106/36:\n",
      "books = soup.find_all(class_ = \"product_pod\")\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url_list = []\n",
      "for book in books:\n",
      "    url_book = book.find(\"a\").attrs[\"href\"]\n",
      "    url = url_book.replace(\"../../\", base_url)\n",
      "    url_list.append(url)\n",
      "url_list\n",
      "106/37:\n",
      "books = soup.find_all(class_ = \"product_pod\")\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url_list = []\n",
      "for book in books:\n",
      "    url_book = book.find(\"a\").attrs[\"href\"]\n",
      "    url = url_book.replace(\"../../\", base_url)\n",
      "    url_list.append(url)\n",
      "url_list\n",
      "106/38: soup.find_all(class_ = \"product_pod\")\n",
      "106/39: soup.find_all(class_ = \"product_pod\").find(\"img\")\n",
      "106/40: soup.find_all(class_ = \"product_pod\")[0].find(\"img\")\n",
      "106/41: soup.find_all(class_ = \"product_pod\")[0].find(\"img\").attrs['alt']\n",
      "106/42:\n",
      "books = soup.find_all(class_ = \"product_pod\")\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url_list = []\n",
      "\n",
      "for book in books:\n",
      "    url_book = book.find(\"a\").attrs[\"href\"]\n",
      "    url = url_book.replace(\"../../\", base_url)\n",
      "    book_title = book.find(\"img\").attrs['alt']\n",
      "    url_list.append({\"title\": book_title,\n",
      "                     \"url\": url})\n",
      "url_list\n",
      "106/43: next((book for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "106/44:\n",
      "books = soup.find_all(class_ = \"product_pod\")\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "book_list = []\n",
      "\n",
      "for book in books:\n",
      "    url_book = book.find(\"a\").attrs[\"href\"]\n",
      "    url = url_book.replace(\"../../\", base_url)\n",
      "    book_title = book.find(\"img\").attrs['alt']\n",
      "    book_list.append({\"title\": book_title,\n",
      "                     \"url\": url})\n",
      "book_list\n",
      "106/45: next((book for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "106/46: next((book[\"url\"] for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "106/47:\n",
      "next((book for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "\n",
      "#or:\n",
      "next((book[\"url\"] for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "106/48:\n",
      "next((book for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "\n",
      "#or:\n",
      "next((book[\"url\"] for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "106/49: ?enumerate\n",
      "106/50: enumerate(book_list)\n",
      "106/51: ?enumerate\n",
      "106/52:\n",
      "# iterate over the dictionaries and update URLs accordingly:\n",
      "\n",
      "for id, book in enumerate(book_list):\n",
      "    book[\"org_url\"] = (book[\"url\"]).replace('https://books.toscrape.com/catalogue/','../../')\n",
      "\n",
      "# show the first five elements\n",
      "book_list[0:5]\n",
      "106/53:\n",
      "# creating a list of page_url:\n",
      "\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = []\n",
      "\n",
      "for counter in range(1, 51):\n",
      "    full_url = f'{base_url}page-{counter}.html' \n",
      "    page_urls.append(full_url)\n",
      "106/54:\n",
      "# creating a list of page_url:\n",
      "\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = []\n",
      "\n",
      "for counter in range(1, 51):\n",
      "    full_url = f'{base_url}page-{counter}.html' \n",
      "    page_urls.append(full_url)\n",
      "\n",
      "page_urls[0:5]\n",
      "106/55:\n",
      "quote_base = \"https://quotes.toscrape.com/page/\"\n",
      "url_list =[]\n",
      "\n",
      "for counter in range(1:11):\n",
      "    quote_url = f'{quote_base}{counter}/'\n",
      "    url_list.append(quote_url)\n",
      "\n",
      "url_list\n",
      "106/56:\n",
      "quote_base = \"https://quotes.toscrape.com/page/\"\n",
      "url_list =[]\n",
      "\n",
      "for counter in range(11):\n",
      "    quote_url = f'{quote_base}{counter}/'\n",
      "    url_list.append(quote_url)\n",
      "\n",
      "url_list\n",
      "106/57:\n",
      "quote_base = \"https://quotes.toscrape.com/page/\"\n",
      "url_list =[]\n",
      "\n",
      "for counter in range(1,11):\n",
      "    quote_url = f'{quote_base}{counter}/'\n",
      "    url_list.append(quote_url)\n",
      "\n",
      "url_list\n",
      "106/58:\n",
      "quote_base = \"https://quotes.toscrape.com/page/\"\n",
      "url_list =[]\n",
      "\n",
      "for counter in range(1,11):\n",
      "    quote_url = f'{quote_base}{counter}/'\n",
      "    url_list.append(quote_url)\n",
      "\n",
      "url_list\n",
      "106/59:\n",
      "# creating a list of page_url:\n",
      "def generate_url_list(base_url, num_page):\n",
      "    \n",
      "    page_urls = []\n",
      "\n",
      "    for counter in range(1, num_page + 1):\n",
      "        full_url = f'{base_url}page-{counter}.html' \n",
      "        page_urls.append(full_url)\n",
      "\n",
      "    return page_urls\n",
      "106/60:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 15\n",
      "generate_url_list(base_url, num_page)\n",
      "106/61:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 5\n",
      "generate_url_list(base_url, num_page)\n",
      "106/62:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 5\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "106/63:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    for page in pages\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #extract book information from the list of books\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"]\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url})\n",
      "    return book_list\n",
      "106/64:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #extract book information from the list of books\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"]\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url})\n",
      "    return book_list\n",
      "106/65:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 5\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "extract_books(pages)\n",
      "106/66:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 5\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "books = extract_books(pages)\n",
      "106/67: len(books)\n",
      "106/68: books[1]\n",
      "106/69:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 2\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "books = extract_books(pages)\n",
      "106/70: books[0:5]\n",
      "106/71:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    # collect all books on page_url\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #for each book on that page look up the title and url and store it in a list\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"] #search for a particular tag and its attribute\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            availability = book.find(\"p\", class_= \"instock availability\") #search for a particular class\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url,\n",
      "                             \"availability\": availability})\n",
      "    return book_list\n",
      "106/72:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 2\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "books = extract_books(pages)\n",
      "106/73: books[0:5]\n",
      "106/74:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    # collect all books on page_url\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #for each book on that page look up the title and url and store it in a list\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"] #search for a particular tag and its attribute\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            availability = book.find(\"p\", class_= \"icon-ok\") #search for a particular class\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url,\n",
      "                             \"availability\": availability})\n",
      "    return book_list\n",
      "106/75:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 2\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "books = extract_books(pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/76: books[0:5]\n",
      "106/77:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    # collect all books on page_url\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #for each book on that page look up the title and url and store it in a list\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"] #search for a particular tag and its attribute\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            availability = book.find(\"p\", class_= \"instock availability\") #search for a particular class\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url,\n",
      "                             \"availability\": availability})\n",
      "    return book_list\n",
      "106/78:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "soup\n",
      "106/79:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "soup.find_all(class_ = \"product_pod\")\n",
      "106/80:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "soup.find_all(class_ = \"product_pod\").find(class_= \"instock availability\")\n",
      "106/81:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "book = soup.find_all(class_ = \"product_pod\")\n",
      "106/82: book.find(class_= \"instock availability\")\n",
      "106/83: book\n",
      "106/84: len(book)\n",
      "106/85: book[0]\n",
      "106/86: book[0].find(class_= \"instock availability\")\n",
      "106/87: book[0].find(\"p\", class_= \"instock availability\")\n",
      "106/88: book[0]\n",
      "106/89: book[0]find(\"p\", class_= \"instock availability\").text\n",
      "106/90: book[0]find(\"p\", class_= \"instock availability\")\n",
      "106/91: book[0].find(\"p\", class_= \"instock availability\").text\n",
      "106/92:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    # collect all books on page_url\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #for each book on that page look up the title and url and store it in a list\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"] #search for a particular tag and its attribute\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            #search for a particular class\n",
      "            availability = book.find(\"p\", class_= \"instock availability\").text.replace(\"\\n\", \"\")\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url,\n",
      "                             \"availability\": availability})\n",
      "    return book_list\n",
      "106/93:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 5\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "books = extract_books(pages)\n",
      "106/94: books[0:5]\n",
      "106/95:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    # collect all books on page_url\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #for each book on that page look up the title and url and store it in a list\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"] #search for a particular tag and its attribute\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            #search for a particular class\n",
      "            availability = book.find(\"p\", class_= \"instock availability\").text.replace(\"\\n\", \"\").replace(\" \",\"\")\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url,\n",
      "                             \"availability\": availability})\n",
      "    return book_list\n",
      "106/96:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 5\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "books = extract_books(pages)\n",
      "106/97: books[0:5]\n",
      "107/1:\n",
      "# extracts all links (the a tag!), and prints the URL (href) to the screen;\n",
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find_all(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "107/2: soup.find_all(class_ = \"product_pod\")[0].find(\"a\").attrs[\"href\"]\n",
      "107/3: url_book = soup.find_all(class_ = \"product_pod\")[1].find(\"a\").attrs[\"href\"]\n",
      "107/4:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "book_url = base_url + url_book[6:]\n",
      "book_url\n",
      "107/5: url_book.replace(\"../../\",base_url)\n",
      "107/6:\n",
      "books = soup.find_all(class_ = \"product_pod\")\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url_list = []\n",
      "for book in books:\n",
      "    url_book = book.find(\"a\").attrs[\"href\"]\n",
      "    url = url_book.replace(\"../../\", base_url)\n",
      "    url_list.append(url)\n",
      "url_list\n",
      "107/7:\n",
      "books = soup.find_all(class_ = \"product_pod\")\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "book_list = []\n",
      "\n",
      "for book in books:\n",
      "    url_book = book.find(\"a\").attrs[\"href\"]\n",
      "    url = url_book.replace(\"../../\", base_url)\n",
      "    book_title = book.find(\"img\").attrs['alt']\n",
      "    book_list.append({\"title\": book_title,\n",
      "                     \"url\": url})\n",
      "book_list\n",
      "107/8:\n",
      "next((book for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "#or:\n",
      "next((book[\"url\"] for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "107/9:\n",
      "# iterate over the dictionaries and update URLs accordingly:\n",
      "\n",
      "for id, book in enumerate(book_list):\n",
      "    book[\"org_url\"] = (book[\"url\"]).replace('https://books.toscrape.com/catalogue/','../../')\n",
      "\n",
      "# show the first five elements\n",
      "book_list[0:5]\n",
      "107/10:\n",
      "# creating a list of page_url:\n",
      "def generate_url_list(base_url, num_page):\n",
      "    \n",
      "    page_urls = []\n",
      "\n",
      "    for counter in range(1, num_page + 1):\n",
      "        full_url = f'{base_url}page-{counter}.html' \n",
      "        page_urls.append(full_url)\n",
      "\n",
      "    return page_urls\n",
      "107/11:\n",
      "from time import sleep\n",
      "sleep(5)\n",
      "#sleep (2*60) --> sleep for 2 minutes\n",
      "107/12:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    # collect all books on page_url\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #for each book on that page look up the title and url and store it in a list\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"] #search for a particular tag and its attribute\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            #search for a particular class\n",
      "            availability = book.find(\"p\", class_= \"instock availability\").text.replace(\"\\n\", \"\").replace(\" \",\"\")\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url,\n",
      "                             \"availability\": availability})\n",
      "    return book_list\n",
      "107/13:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 5\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "books = extract_books(pages)\n",
      "107/14: books[0:5]\n",
      "107/15:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "book = soup.find_all(class_ = \"product_pod\")\n",
      "107/16:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1'\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\").attrs[\"href\"].text == \"next\"\n",
      "107/17:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\").attrs[\"href\"].text == \"next\"\n",
      "107/18:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\").attrs[\"href\"]\n",
      "107/19:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup\n",
      "107/20:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup\n",
      "107/21:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\").attrs['href'].text\n",
      "107/22:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\").attrs['href']\n",
      "107/23:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\")\n",
      "107/24:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\").attrs['href']\n",
      "107/25:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\")[0]\n",
      "107/26:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\")[0].attrs['href']\n",
      "107/27:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\")[0].attrs['href'].text\n",
      "107/28:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\")[0].text\n",
      "107/29:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\")[19].text\n",
      "107/30:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_button = soup.find(\"p\", class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "\n",
      "check_next_page(url)\n",
      "107/31:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "\n",
      "check_next_page(url)\n",
      "107/32:\n",
      "url = 'https://books.toscrape.com/catalogue/page-50.html'\n",
      "\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "\n",
      "check_next_page(url)\n",
      "107/33:\n",
      "url = 'https://books.toscrape.com/catalogue/page-50.html'\n",
      "\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else print(\"This is the last page\")\n",
      "\n",
      "check_next_page(url)\n",
      "107/34:\n",
      "def next_page_url(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser)\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    if next_button.find('a').attrs['href']!= None:\n",
      "                         next_page = next_button.find('a').attrs['href']\n",
      "                         return next_url = next_page.replace(\"../../\",base_url)              \n",
      "    else print(\"This is the last page\")\n",
      "107/35:\n",
      "def next_page_url(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    if next_button.find('a').attrs['href']!= None:\n",
      "                         next_page = next_button.find('a').attrs['href']\n",
      "                         return next_url = next_page.replace(\"../../\",base_url)              \n",
      "    else print(\"This is the last page\")\n",
      "107/36:\n",
      "def next_page_url(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    if next_button.find('a').attrs['href']!= None:\n",
      "        next_page = next_button.find('a').attrs['href']\n",
      "        next_url = next_page.replace(\"../../\",base_url)\n",
      "        return next_url\n",
      "    else print(\"This is the last page\")\n",
      "107/37:\n",
      "def next_page_url(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    if next_button.find('a').attrs['href']!= None:\n",
      "        next_page = next_button.find('a').attrs['href']\n",
      "        next_url = next_page.replace(\"../../\",base_url)\n",
      "        return next_url\n",
      "    else:\n",
      "        print(\"This is the last page\")\n",
      "107/38:\n",
      "url = 'https://books.toscrape.com/catalogue/page-10.html'\n",
      "next_page_url(url)\n",
      "107/39:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url = 'https://books.toscrape.com/catalogue/page-10.html'\n",
      "next_page_url(url)\n",
      "107/40:\n",
      "def next_page_url(url):\n",
      "    if url!= None:\n",
      "        next_page = url.replace(\"../../\", base_url)\n",
      "        return next_url\n",
      "    else:\n",
      "        print(\"This is the last page\")\n",
      "107/41:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    \n",
      "    #check next page:\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "\n",
      "check_next_page('https://books.toscrape.com/catalogue/page-10.html')\n",
      "107/42:\n",
      "def next_page_url(url):\n",
      "    if url!= None:\n",
      "        next_page = f'{base_url}{url}'\n",
      "        return next_url\n",
      "    else:\n",
      "        print(\"This is the last page\")\n",
      "107/43:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url = 'https://books.toscrape.com/catalogue/page-10.html'\n",
      "next_page_url(url)\n",
      "107/44:\n",
      "def next_page_url(url):\n",
      "    if url!= None:\n",
      "        next_page = f'{base_url}{url}'\n",
      "        return next_page\n",
      "    else:\n",
      "        print(\"This is the last page\")\n",
      "107/45:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url = 'https://books.toscrape.com/catalogue/page-10.html'\n",
      "next_page_url(url)\n",
      "107/46:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url = check_next_page('https://books.toscrape.com/catalogue/page-10.html')\n",
      "next_page_url(url)\n",
      "107/47:\n",
      "def next_page_url(url):\n",
      "    if url!= None:\n",
      "        next_page = base_url + url\n",
      "        return next_page\n",
      "    else:\n",
      "        print(\"This is the last page\")\n",
      "107/48:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url = check_next_page('https://books.toscrape.com/catalogue/page-10.html')\n",
      "next_page_url(url)\n",
      "107/49:\n",
      "def extract_all_books(page_url):\n",
      "    books = []\n",
      "    while page_url:\n",
      "        print(page_url)\n",
      "        for book in extract_book_urls([page_url]):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(page_url) != None: \n",
      "            page_url = \"https://books.toscrape.com/catalogue/category/books_1/\" + check_next_page(page_url)\n",
      "        else: \n",
      "            break\n",
      "        \n",
      "        # if \"page-4\" in page_url: break # (activate this if you don't want to run the entire loop)\n",
      "    return books\n",
      "107/50: extract_all_books(\"https://books.toscrape.com/catalogue/page-1.html\")\n",
      "107/51:\n",
      "def extract_all_books(page_url):\n",
      "    books = []\n",
      "    while page_url:\n",
      "        print(page_url)\n",
      "        for book in extract_books([page_url]):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(page_url) != None: \n",
      "            page_url = \"https://books.toscrape.com/catalogue/category/books_1/\" + check_next_page(page_url)\n",
      "        else: \n",
      "            break\n",
      "        \n",
      "        # if \"page-4\" in page_url: break # (activate this if you don't want to run the entire loop)\n",
      "    return books\n",
      "107/52: extract_all_books(\"https://books.toscrape.com/catalogue/page-1.html\")\n",
      "107/53: len(book_list)\n",
      "107/54: len(books)\n",
      "107/55: books = extract_all_books(\"https://books.toscrape.com/catalogue/page-1.html\")\n",
      "107/56: len(books)\n",
      "107/57: next((book for book in book_list if book[\"title\"] == 'The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics'), None)\n",
      "107/58:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    # collect all books on page_url\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #for each book on that page look up the title and url and store it in a list\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"] #search for a particular tag and its attribute\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            #search for a particular class\n",
      "            availability = book.find(\"p\", class_= \"instock availability\").text.replace(\"\\n\", \"\").replace(\" \",\"\")\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url,\n",
      "                             \"availability\": availability})\n",
      "    return book_list\n",
      "107/59:\n",
      "def extract_all_books(page_url):\n",
      "    books = []\n",
      "    while page_url:\n",
      "        print(page_url)\n",
      "        for book in extract_books([page_url]):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(page_url) != None: \n",
      "            page_url = \"https://books.toscrape.com/catalogue/category/books_1/\" + check_next_page(page_url)\n",
      "        else: \n",
      "            break\n",
      "        \n",
      "        # if \"page-4\" in page_url: break # (activate this if you don't want to run the entire loop)\n",
      "    return books\n",
      "107/60: books = extract_all_books(\"https://books.toscrape.com/catalogue/page-49.html\")\n",
      "107/61: len(books)\n",
      "107/62: books[0]\n",
      "107/63:\n",
      "books_instock = [book for book in book_list if book[\"Availability\"] == \"Instock\"]\n",
      "len(books_instock)\n",
      "107/64:\n",
      "books_instock = [book for book in book_list if book[\"availability\"] == \"Instock\"]\n",
      "len(books_instock)\n",
      "107/65:\n",
      "books_instock = [book for book in books if book[\"availability\"] == \"Instock\"]\n",
      "len(books_instock)\n",
      "107/66: next((book for book in book_list if book[\"title\"] == 'On the Road (Duluoz Legend)'), None)\n",
      "107/67: next((book for book in books if book[\"title\"] == 'On the Road (Duluoz Legend)'), None)\n",
      "107/68: books[9]\n",
      "107/69:\n",
      "#### Checking if there is a specific word in title:\n",
      "len([book for book in books if \"boat\" in book[\"title\"].lower()])\n",
      "107/70:\n",
      "#### Checking if there is a specific word in title:\n",
      "len([book for book in books if \"road\" in book[\"title\"].lower()])\n",
      "107/71:\n",
      "#### Checking if there is a specific word in title:\n",
      "next[book for book in books if \"road\" in book[\"title\"].lower()])\n",
      "107/72:\n",
      "#### Checking if there is a specific word in title:\n",
      "next((book for book in books if \"road\" in book[\"title\"].lower()]), None)\n",
      "107/73:\n",
      "#### Checking if there is a specific word in title:\n",
      "next((book for book in books if \"road\" in book[\"title\"].lower()), None)\n",
      "107/74: len([book for book in books if \"boat\" in book[\"title\"].lower()])\n",
      "107/75: len([book for book in books if \"love\" in book[\"title\"].lower()])\n",
      "107/76:\n",
      "url = \"https://books.toscrape.com/catalogue/black-dust_976/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "107/77: soup\n",
      "107/78: soup.find_all(\"p\", class_=\"star-rating\")\n",
      "107/79: soup.find(\"p\", class_=\"star-rating\")\n",
      "107/80: soup.find(\"p\", class_=\"star-rating\").text\n",
      "107/81: soup.find(\"p\", class_=\"star-rating\")\n",
      "107/82: soup.find(\"p\", class_=\"star-rating\").find(\"i\")\n",
      "107/83: soup.find(\"p\", class_=\"star-rating\").find_all(\"i\")\n",
      "107/84: len(soup.find(\"p\", class_=\"star-rating\").find_all(\"i\"))\n",
      "107/85: soup.find(id=\"content_inner\")\n",
      "107/86: soup.find(id=\"content_inner\").find(class_=\"product_page\")\n",
      "107/87: soup.find(id=\"content_inner\").find(\"p\", class_=\"product_page\")\n",
      "107/88: soup.find(id=\"content_inner\").find(\"p\", class_=\"product_page\")\n",
      "107/89: soup.find(id=\"content_inner\").find(\"p\", class_=\"star-rating\")\n",
      "107/90: soup.find(id=\"content_inner\").find_all(\"p\", class_=\"star-rating\")\n",
      "107/91: soup.find(id=\"content_inner\").find_all(class_=\"star-rating\")\n",
      "107/92: soup.find(id=\"content_inner\").find(class_=\"star-rating\")\n",
      "107/93: soup.find(id=\"content_inner\").find(\"i\", class_=\"star-rating\")\n",
      "107/94: soup.find(id=\"content_inner\").find_all(\"i\", class_=\"star-rating\")\n",
      "107/95: soup.find(id=\"content_inner\").find_all(\"p\", class_=\"star-rating\")\n",
      "107/96: soup.find(id=\"content_inner\").find(\"p\", class_=\"star-rating\")\n",
      "107/97: soup.find(id=\"content_inner\").find_all(\"i\", class_=\"icon-star\")\n",
      "107/98: soup.find(id=\"content_inner\").find(\"i\", class_=\"icon-star\")\n",
      "107/99:\n",
      "url = \"https://books.toscrape.com/catalogue/black-dust_976/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "107/100: soup.find(\"article\")\n",
      "107/101: soup.find(\"article\").find_all(\"p\")\n",
      "107/102: soup.find(id=\"content_inner\")\n",
      "107/103: soup.find(id=\"content_inner\").find_all(\"p\")\n",
      "107/104: soup.find(\"article\").find_all(\"p\")\n",
      "107/105: soup.find(\"article\").find_all(\"p\")[3]\n",
      "107/106: soup.find(\"article\").find_all(\"p\")[3].get_text()\n",
      "107/107:\n",
      "def get_book_description(books):\n",
      "    book_descriptions = []\n",
      "    \n",
      "    for book in books: \n",
      "        page_url = book[\"url\"]\n",
      "\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        # tip: look at the Google Inspector screenshot below \n",
      "        description = soup.find(id=\"content_inner\").find_all(\"p\")[3].get_text()\n",
      "        title = soup.find(id=\"content_inner\").find('img')['alt']\n",
      "        book_descriptions.append({'url': page_url,\n",
      "                                  'title': title,\n",
      "                                  'description': description})\n",
      "    return book_descriptions\n",
      "\n",
      "book_descriptions = get_book_description(book_list[0:2])\n",
      "book_descriptions\n",
      "107/108:\n",
      "def get_book_description(books):\n",
      "    book_descriptions = []\n",
      "    \n",
      "    for book in books: \n",
      "        page_url = book[\"url\"]\n",
      "\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        # tip: look at the Google Inspector screenshot below \n",
      "        description = soup.find(id=\"content_inner\").find_all(\"p\")[3].get_text()\n",
      "        title = soup.find(id=\"content_inner\").find('img')['alt']\n",
      "        book_descriptions.append({'url': page_url,\n",
      "                                  'title': title,\n",
      "                                  'description': description})\n",
      "    return book_descriptions\n",
      "\n",
      "book_descriptions = get_book_description(book_list[0:1])\n",
      "book_descriptions\n",
      "107/109:\n",
      "def get_book_description(books):\n",
      "    book_descriptions = []\n",
      "    \n",
      "    for book in books: \n",
      "        page_url = book[\"url\"]\n",
      "\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        # tip: look at the Google Inspector screenshot below \n",
      "        description = soup.find(id=\"content_inner\").find_all(\"p\")[3].get_text()\n",
      "        title = soup.find(id=\"content_inner\").find('img')['alt']\n",
      "        book_descriptions.append({'url': page_url,\n",
      "                                  'title': title,\n",
      "                                  'description': description})\n",
      "    return book_descriptions\n",
      "\n",
      "book_descriptions = get_book_description(book_list[0:2])\n",
      "book_descriptions\n",
      "107/110:\n",
      "from datetime import datetime\n",
      "now = datetime.now()\n",
      "\n",
      "import csv \n",
      "\n",
      "with open(\"book_descriptions.csv\", \"a\") as csv_file: # <<- this is the line with the \"flag\"l see exercises below\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"description\", \"date_time\"])\n",
      "    now = datetime.now()\n",
      "    for book in book_descriptions: # here we reference the book_descriptions list - make sure it's loaded otherwise you get an error! (Cell > Run All Above)\n",
      "        writer.writerow([book['title'], book['description'], now])\n",
      "print('done!')\n",
      "110/1:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    \n",
      "    #check next page:\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "\n",
      "check_next_page(url)\n",
      "110/2:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "110/3:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    \n",
      "    #check next page:\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "110/4:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "check_next_page(url)\n",
      "110/5:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "check_next_page(url)\n",
      "110/6:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        for book in extract_books([url]):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(page_url) != None: \n",
      "            page_url = \"https://books.toscrape.com/catalogue/category/books/nonfiction_13/\" + check_next_page(page_url)\n",
      "        else: \n",
      "            break\n",
      "110/7:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "extract_all_books(url)\n",
      "110/8:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "books = soup.find_all(\"h3\")\n",
      "110/9:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "books = soup.find_all(\"h3\")\n",
      "110/10:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"h3\")\n",
      "110/11:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"h3\").text\n",
      "110/12:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"h3\")[0]\n",
      "110/13:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"h3\")[0].text\n",
      "110/14: soup.find(\"img\").attrs['alt']\n",
      "110/15: soup.find_all(\"img\").attrs['alt']\n",
      "110/16: soup.find_all(\"img\")\n",
      "110/17: soup.find_all(\"img\")[0].attrs['alt']\n",
      "110/18: soup.find_all(\"img\")\n",
      "110/19: soup.find_all(\"img\")['alt']\n",
      "110/20: soup.find_all(\"img\").text\n",
      "110/21: soup.find_all(\"img\")\n",
      "110/22: soup.find_all(class_=\"product_pod\")\n",
      "110/23: soup.find_all(class_=\"product_pod\")[0]\n",
      "110/24: soup.find_all(class_=\"product_pod\")[0].find(\"a\")['alt']\n",
      "110/25: soup.find_all(class_=\"product_pod\")[0].find(\"img\")['alt']\n",
      "110/26: soup.find_all(class_=\"product_pod\")[0]\n",
      "110/27: soup.find_all(class_=\"product_pod\")[0].find(class_=\"product_price\").text\n",
      "110/28: soup.find_all(class_=\"product_pod\")[0].find(class_=\"product_price\")\n",
      "110/29: soup.find_all(class_=\"product_pod\")[0].find(class_=\"price_color\")\n",
      "110/30: soup.find_all(class_=\"product_pod\")[0].find(class_=\"price_color\").text\n",
      "110/31:\n",
      "def extract_nonfiction_book(url):\n",
      "    nonfic_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for books in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text\n",
      "        nonfic_book.append({'title': title,\n",
      "                            'price': price})\n",
      "110/32:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "nonfic_book = extract_nonfiction_book(url)\n",
      "110/33:\n",
      "def extract_nonfiction_book(url):\n",
      "    nonfic_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text\n",
      "        nonfic_book.append({'title': title,\n",
      "                            'price': price})\n",
      "110/34:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "nonfic_book = extract_nonfiction_book(url)\n",
      "110/35: nonfic_book\n",
      "110/36:\n",
      "def extract_nonfiction_book(url):\n",
      "    nonfic_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text\n",
      "        nonfic_book.append({'title': title,\n",
      "                            'price': price})\n",
      "    return nonfic_book\n",
      "110/37:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "nonfic_book = extract_nonfiction_book(url)\n",
      "110/38: nonfic_book\n",
      "110/39:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        for book in extract_nonfiction_book([url]):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/nonfiction_13/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "110/40:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "extract_all_books(url)\n",
      "110/41:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        for book in extract_nonfiction_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/nonfiction_13/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "110/42:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "extract_all_books(url)\n",
      "110/43:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "nonfic_book_list = extract_all_books(url)\n",
      "110/44: nonfic_book_list\n",
      "110/45: len(nonfic_book_list)\n",
      "110/46:\n",
      "from datetime import datetime\n",
      "now = datetime.now()\n",
      "\n",
      "import csv \n",
      "\n",
      "with open(\"nonfic_book.csv\", \"a\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"date_time\"])\n",
      "    now = datetime.now()\n",
      "    for book in nonfic_book_list:\n",
      "        writer.writerow([book['title'], book['price'], now])\n",
      "        \n",
      "pd.read_csv(\"nonfic_book.csv\", delimiter= \";\")\n",
      "110/47:\n",
      "from datetime import datetime\n",
      "now = datetime.now()\n",
      "\n",
      "import csv \n",
      "\n",
      "with open(\"nonfic_book.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"date_time\"])\n",
      "    now = datetime.now()\n",
      "    for book in nonfic_book_list:\n",
      "        writer.writerow([book['title'], book['price'], now])\n",
      "        \n",
      "pd.read_csv(\"nonfic_book.csv\", delimiter= \";\")\n",
      "110/48:\n",
      "from datetime import datetime\n",
      "now = datetime.now()\n",
      "\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"nonfic_book.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"date_time\"])\n",
      "    now = datetime.now()\n",
      "    for book in nonfic_book_list:\n",
      "        writer.writerow([book['title'], book['price'], now])\n",
      "        \n",
      "pd.read_csv(\"nonfic_book.csv\", delimiter= \";\")\n",
      "110/49:\n",
      "def extract_nonfiction_book(url):\n",
      "    nonfic_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text.replace(\"Â\",\"\")\n",
      "        nonfic_book.append({'title': title,\n",
      "                            'price': price})\n",
      "    return nonfic_book\n",
      "110/50:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "nonfic_book_list = extract_all_books(url)\n",
      "110/51: nonfic_book_list[0]\n",
      "110/52:\n",
      "from datetime import datetime\n",
      "now = datetime.now()\n",
      "\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"nonfic_book.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"date_time\"])\n",
      "    now = datetime.now()\n",
      "    for book in nonfic_book_list:\n",
      "        writer.writerow([book['title'], book['price'], now])\n",
      "        \n",
      "pd.read_csv(\"nonfic_book.csv\", delimiter= \";\")\n",
      "110/53: pd.read_csv(\"nonfic_book.csv\", delimiter= \";\")[0:6]\n",
      "110/54: pd.read_csv(\"nonfic_book.csv\", delimiter= \";\")[0:5]\n",
      "110/55:\n",
      "from datetime import datetime\n",
      "now = datetime.now()\n",
      "\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"nonfic_book.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"date_time\"])\n",
      "    now = datetime.now()\n",
      "    for book in nonfic_book_list:\n",
      "        writer.writerow([book['title'], book['price'], now])\n",
      "\n",
      "pd.read_csv(\"nonfic_book.csv\", delimiter= \";\")[0:5]\n",
      "114/1:\n",
      "import requests\n",
      "import json\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{mod}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups})\n",
      "114/2:\n",
      "import requests\n",
      "import json\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups})\n",
      "114/3:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups})\n",
      "114/4: post_list\n",
      "114/5: post_list[10]\n",
      "114/6: post_list[0:5]\n",
      "114/7:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            url = item['url']\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups})\n",
      "114/8:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            url = item['url']\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups,\n",
      "                             \"url\":url})\n",
      "114/9: post_list[0:5]\n",
      "114/10:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            ext_url = item['url_overridden_by_dest']\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups,\n",
      "                             \"ext_url\":ext_url})\n",
      "114/11:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups)\n",
      "114/12:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups})\n",
      "114/13: post_list[0:5]\n",
      "114/14:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"reddit.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"selftext\", \"downs\", \"ups\"])\n",
      "    now = datetime.now()\n",
      "    for post in post_list:\n",
      "        writer.writerow([post['id'], post['selftext'], post['downs'], post['ups'])\n",
      "\n",
      "pd.read_csv(\"reddit.csv\", delimiter= \";\")[0:5]\n",
      "114/15:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"reddit.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"selftext\", \"downs\", \"ups\"])\n",
      "    now = datetime.now()\n",
      "    for post in post_list:\n",
      "        writer.writerow([post['id'], post['selftext'], post['downs'], post['ups']])\n",
      "\n",
      "pd.read_csv(\"reddit.csv\", delimiter= \";\")[0:5]\n",
      "114/16:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"reddit.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"selftext\", \"downs\", \"ups\"])\n",
      "    for post in post_list:\n",
      "        writer.writerow([post['id'], post['selftext'], post['downs'], post['ups']])\n",
      "\n",
      "pd.read_csv(\"reddit.csv\", delimiter= \";\")[0:5]\n",
      "114/17: len(post_list)\n",
      "114/18:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"reddit.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"selftext\", \"downs\", \"ups\"])\n",
      "    for post in post_list:\n",
      "        writer.writerow([post['id'], post['selftext'], post['downs'], post['ups']])\n",
      "\n",
      "pd.read_csv(\"reddit.csv\", delimiter= \";\")[250:300]\n",
      "114/19:\n",
      "url = 'https://www.reddit.com/r/politics.json'\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": after})\n",
      "json_response = response.json()\n",
      "114/20: json_response\n",
      "114/21: len(json_response)\n",
      "114/22:\n",
      "url = 'https://www.reddit.com/r/politics.json'\n",
      "response = requests.get(url, \n",
      "                        headers=headers)\n",
      "json_response = response.json()\n",
      "114/23: json_response\n",
      "114/24: json_response['children']\n",
      "114/25: json_response[0]['children']\n",
      "114/26: json_response[0]\n",
      "114/27: json_response\n",
      "114/28: json_response['data']\n",
      "114/29: json_response['data'][children]\n",
      "114/30: json_response['data']['children']\n",
      "114/31: len(json_response['data']['children'])\n",
      "114/32: json_response['data']['children']\n",
      "114/33: json_response['data']['children']['title']\n",
      "114/34: json_response['data']['children'][0]\n",
      "114/35: json_response['data']['children'][0]['title']\n",
      "114/36: json_response['data']['children'][0]\n",
      "114/37: json_response['data']['children'][0]['data']\n",
      "114/38: json_response['data']['children'][0]['data']['title']\n",
      "114/39:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            title = item['data']['title']\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            post_list.append({\"id\":id,\n",
      "                              \"title\": title,\n",
      "                              \"selftext\":selftext,\n",
      "                              \"downs\": downs,\n",
      "                              \"ups\":ups})\n",
      "114/40:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            item_data = item['data']\n",
      "            title = item_data['title']\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            post_list.append({\"id\":id,\n",
      "                              \"title\": title,\n",
      "                              \"selftext\":selftext,\n",
      "                              \"downs\": downs,\n",
      "                              \"ups\":ups})\n",
      "114/41: json_response['data']['children'][0]['data']\n",
      "114/42: json_response['data']['children']\n",
      "114/43: json_response['data']['children'][0]\n",
      "114/44: json_response['data']['children'][0]['data']\n",
      "114/45: json_response['data']['children'][0]['selftext']\n",
      "114/46:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            title = item['title']\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            post_list.append({\"id\":id,\n",
      "                              \"title\": title,\n",
      "                              \"selftext\":selftext,\n",
      "                              \"downs\": downs,\n",
      "                              \"ups\":ups})\n",
      "114/47:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"reddit.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"selftext\", \"downs\", \"ups\"])\n",
      "    for post in post_list:\n",
      "        writer.writerow([post['id'], post['selftext'], post['downs'], post['ups']])\n",
      "\n",
      "pd.read_csv(\"reddit.csv\", delimiter= \";\")[250:260]\n",
      "114/48: post_list[0]\n",
      "114/49:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"reddit.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\",\"title\", \"selftext\", \"downs\", \"ups\"])\n",
      "    for post in post_list:\n",
      "        writer.writerow([post['id'], post['title'], post['selftext'], post['downs'], post['ups']])\n",
      "\n",
      "pd.read_csv(\"reddit.csv\", delimiter= \";\")[250:260]\n",
      "118/1:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append(item['data']['author'])\n",
      "    return users\n",
      "\n",
      "users = get_users('marketing')\n",
      "users\n",
      "118/2:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append({\"name\": item['data']['author'],\n",
      "                     \"community\": subreddit})\n",
      "    return users\n",
      "118/3: get_users('marketing')\n",
      "118/4:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append({\"name\": item['data']['author'],\n",
      "                     \"subreddit\": subreddit})\n",
      "    return users\n",
      "118/5: get_users('marketing')\n",
      "118/6:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "\n",
      "user_list =[]\n",
      "for sub in subreddits:\n",
      "    user_list.append(get_users(subreddit))\n",
      "118/7:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "\n",
      "user_list =[]\n",
      "for sub in subreddits:\n",
      "    user_list.append(get_users(sub))\n",
      "118/8: user_list.append(get_users('marketing'))\n",
      "118/9: user_list\n",
      "118/10: user_list = get_users('marketing')\n",
      "118/11: user_list\n",
      "118/12:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append({\"name\": item['data']['author'])\n",
      "    return users\n",
      "118/13:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append(item['data']['author'])\n",
      "    return users\n",
      "118/14: get_users('marketing')\n",
      "118/15:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "\n",
      "user_list =[]\n",
      "for sub in subreddits:\n",
      "    user_list.append(get_users(sub))\n",
      "118/16:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "\n",
      "user_list =[]\n",
      "for sub in subreddits:\n",
      "    get_users(sub)\n",
      "118/17: user_list = get_users('surfing')\n",
      "118/18: user_list\n",
      "118/19:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "\n",
      "for subreddit in subreddits:\n",
      "    get_users(subreddit)\n",
      "121/1:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "121/2:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    \n",
      "    #check next page:\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "121/3:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "121/4: soup.find_all(class_=\"product_pod\")\n",
      "121/5: soup.find_all(class_=\"product_pod\").find(\"p\", class_=\"star-rating\")\n",
      "121/6: soup.find_all(class_=\"product_pod\")\n",
      "121/7: soup.find_all(class_=\"product_pod\").find(\"p\")\n",
      "121/8: soup.find_all(class_=\"product_pod\")\n",
      "121/9: soup.find_all(class_=\"product_pod\")[0]\n",
      "121/10: soup.find_all(class_=\"product_pod\")[0].find(\"p\")\n",
      "121/11: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"star-rating\")\n",
      "121/12: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"star-rating\").find_all(\"i\")\n",
      "121/13: len(soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"star-rating\").find_all(\"i\"))\n",
      "121/14: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"star-rating\").find_all(\"i\")\n",
      "121/15: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"star-rating\")\n",
      "121/16:\n",
      "def extract_mystery_book(url):\n",
      "    mystery_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text.replace(\"Â\",\"\")\n",
      "        rating = book.find(\"p\", class_=\"star-rating\")\n",
      "        mystery_book.append({'title': title,\n",
      "                            'price': price})\n",
      "    return mistery_book\n",
      "121/17:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "mystery_book_list = extract_all_books(url)\n",
      "121/18:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/19:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "mystery_book_list = extract_all_books(url)\n",
      "121/20:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    \n",
      "    #check next page:\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "121/21:\n",
      "def extract_mystery_book(url):\n",
      "    mystery_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text.replace(\"Â\",\"\")\n",
      "        rating = book.find(\"p\", class_=\"star-rating\")\n",
      "        mystery_book.append({'title': title,\n",
      "                            'price': price,\n",
      "                            'rating': rating})\n",
      "    return mistery_book\n",
      "121/22:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/23:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "mystery_book_list = extract_all_books(url)\n",
      "121/24:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "extract_mystery_book(url)\n",
      "121/25:\n",
      "def extract_mystery_book(url):\n",
      "    mystery_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text.replace(\"Â\",\"\")\n",
      "        rating = book.find(\"p\", class_=\"star-rating\")\n",
      "        mystery_book.append({'title': title,\n",
      "                            'price': price,\n",
      "                            'rating': rating})\n",
      "    return mystery_book\n",
      "121/26:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/27:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "extract_all_books(url)\n",
      "121/28:\n",
      "def extract_mystery_book(url):\n",
      "    mystery_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text.replace(\"Â\",\"\")\n",
      "        rating = len(book.find(\"p\", class_=\"star-rating\"))\n",
      "        mystery_book.append({'title': title,\n",
      "                            'price': price,\n",
      "                            'rating': rating})\n",
      "    return mystery_book\n",
      "121/29: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"star-rating\")\n",
      "121/30: soup.find_all(class_=\"product_pod\")[0].find(class_).\n",
      "121/31: soup.find_all(class_=\"product_pod\")[0].find(class_='').\n",
      "121/32: soup.find_all(class_=\"product_pod\")[0].find(class_='icon-star').\n",
      "121/33: soup.find_all(class_=\"product_pod\")[0].find(class_='icon-star')\n",
      "121/34: soup.find_all(class_=\"product_pod\")[0].find(class_='star-rating')\n",
      "121/35: soup.find_all(class_=\"product_pod\")[6].find(class_='star-rating')\n",
      "121/36: soup.find_all(class_=\"product_pod\")[6].find(class_='star-rating')[0]\n",
      "121/37: soup.find_all(class_=\"product_pod\")[6].find(class_='star-rating')\n",
      "121/38:\n",
      "def extract_mystery_book(url):\n",
      "    mystery_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text.replace(\"Â\",\"\")\n",
      "        rating = len(book.find(\"p\", class_=\"star-rating\").find_all(\"i\"))\n",
      "        mystery_book.append({'title': title,\n",
      "                            'price': price,\n",
      "                            'rating': rating})\n",
      "    return mystery_book\n",
      "121/39:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/40:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "mystery_book = extract_all_books(url)\n",
      "121/41: mystery_book[0:5]\n",
      "121/42:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "121/43: soup.find_all(class_=\"product_pod\")\n",
      "121/44: soup.find_all(class_=\"product_pod\")[0]\n",
      "121/45: soup.find_all(class_=\"product_pod\")[0]find(\"p\", class_=\"star-rating\")\n",
      "121/46: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"star-rating\")\n",
      "121/47: soup.find_all(class_=\"product_pod\")[0].find(\"p\")\n",
      "121/48: soup.find_all(class_=\"product_pod\")[0].find(\"p\").get_text()\n",
      "121/49: soup.find_all(class_=\"product_pod\")[0].find(\"p\")\n",
      "121/50: soup.find_all(class_=\"product_pod\")[0].find(\"Four\")\n",
      "121/51: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"Four\")\n",
      "121/52: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"One\")\n",
      "121/53:\n",
      "from datetime import datetime\n",
      "now = datetime.now()\n",
      "\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"books.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"rating\"])\n",
      "    now = datetime.now()\n",
      "    for book in mystery_book:\n",
      "        writer.writerow([book['title'], book['price'], book['rating']])\n",
      "\n",
      "pd.read_csv(\"books.csv\", delimiter= \";\")[0:5]\n",
      "121/54:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"books.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"rating\"])\n",
      "    for book in mystery_book:\n",
      "        writer.writerow([book['title'], book['price'], book['rating']])\n",
      "\n",
      "pd.read_csv(\"books.csv\", delimiter= \";\")[0:5]\n",
      "121/55:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        \n",
      "        with open(\"seeds.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/56:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "mystery_book = extract_all_books(url)\n",
      "121/57:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        \n",
      "        with open(\"seeds.csv\", \"a\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/58:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "mystery_book = extract_all_books(url)\n",
      "121/59:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        \n",
      "        with open(\"seeds.csv\", \"a\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/60:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        \n",
      "        with open(\"seeds.csv\", \"a\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/61:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "mystery_book = extract_all_books(url)\n",
      "121/62:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "121/63:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    \n",
      "    #check next page:\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "121/64:\n",
      "def extract_mystery_book(url):\n",
      "    mystery_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text.replace(\"Â\",\"\")\n",
      "        rating = len(book.find(\"p\", class_=\"star-rating\").find_all(\"i\"))\n",
      "        mystery_book.append({'title': title,\n",
      "                            'price': price,\n",
      "                            'rating': rating})\n",
      "    return mystery_book\n",
      "121/65:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        \n",
      "        with open(\"seeds.csv\", \"a\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/66:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "mystery_book = extract_all_books(url)\n",
      "121/67:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"books.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"rating\"])\n",
      "    for book in mystery_book:\n",
      "        writer.writerow([book['title'], book['price'], book['rating']])\n",
      "\n",
      "pd.read_csv(\"books.csv\", delimiter= \";\")[0:5]\n",
      "121/68:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        \n",
      "        with open(\"seeds.csv\", \"a\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/69:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "mystery_book = extract_all_books(url)\n",
      "121/70:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"books.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"rating\"])\n",
      "    for book in mystery_book:\n",
      "        writer.writerow([book['title'], book['price'], book['rating']])\n",
      "\n",
      "pd.read_csv(\"books.csv\", delimiter= \";\")[0:5]\n",
      "121/71: len(mystery_book)\n",
      "121/72:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    with open(\"seeds.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"seeds\"])\n",
      "    while url:\n",
      "        print(url)\n",
      "        with open(\"seeds.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/73:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "mystery_book = extract_all_books(url)\n",
      "121/74:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"books.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"rating\"])\n",
      "    for book in mystery_book:\n",
      "        writer.writerow([book['title'], book['price'], book['rating']])\n",
      "\n",
      "pd.read_csv(\"books.csv\", delimiter= \";\")[0:5]\n",
      "121/75:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        with open(\"seeds.csv\", \"a\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/76:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "mystery_book = extract_all_books(url)\n",
      "121/77:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "121/78:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        with open(\"seeds.csv\", \"a\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/79:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "mystery_book = extract_all_books(url)\n",
      "121/80:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"books.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"rating\"])\n",
      "    for book in mystery_book:\n",
      "        writer.writerow([book['title'], book['price'], book['rating']])\n",
      "\n",
      "pd.read_csv(\"books.csv\", delimiter= \";\")[0:5]\n",
      "118/20:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append(item['data']['author'])\n",
      "    return users\n",
      "\n",
      "users = get_users('marketing')\n",
      "users\n",
      "118/21:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "\n",
      "for subreddit in subreddits:\n",
      "    get_users(subreddit)\n",
      "118/22:\n",
      "users = get_users('skating')\n",
      "users\n",
      "118/23:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "users = []\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append(item['data']['author'])\n",
      "    return users\n",
      "118/24:\n",
      "users = get_users('skating')\n",
      "users\n",
      "118/25:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append(item['data']['author'])\n",
      "    return users\n",
      "118/26:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "users = []\n",
      "\n",
      "for subreddit in subreddits:\n",
      "    users.append = get_users('skating')\n",
      "118/27:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "users = []\n",
      "\n",
      "for subreddit in subreddits:\n",
      "    users.append (get_users('skating'))\n",
      "118/28: users[1]\n",
      "118/29: len(users)\n",
      "118/30:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "\n",
      "for subreddit in subreddits:\n",
      "    users.append(get_users(subreddit))\n",
      "118/31:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "users = []\n",
      "for subreddit in subreddits:\n",
      "    users.append(get_users(subreddit))\n",
      "118/32:\n",
      "url = 'https://www.reddit.com/r/surfing.json'\n",
      "response = requests.get(url,headers=headers)\n",
      "json_response = response.json()\n",
      "118/33: json_response\n",
      "118/34: json_response['data']\n",
      "118/35:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append(item['data']['author'])\n",
      "    return users\n",
      "118/36:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append(item['data']['author'])\n",
      "    return users\n",
      "118/37:\n",
      "users = get_users('skating')\n",
      "users\n",
      "118/38:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "users = []\n",
      "for item in subreddits:\n",
      "    print(get_users(item))\n",
      "118/39:\n",
      "url = 'https://www.reddit.com/r/horseriding.json'\n",
      "response = requests.get(url,headers=headers)\n",
      "json_response = response.json()\n",
      "118/40: json_response['data']\n",
      "118/41: response\n",
      "118/42:\n",
      "subreddits = ['surfing','skating', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "users = []\n",
      "for item in subreddits:\n",
      "    users.append(get_users(item))\n",
      "118/43: users\n",
      "118/44: len(users)\n",
      "118/45:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        user = item['data']['author']\n",
      "        subred = subreddit\n",
      "        users.append({\"user\":user,\n",
      "                      \"subred\":subreddit})\n",
      "    return users\n",
      "118/46:\n",
      "users = get_users('skating')\n",
      "users\n",
      "118/47:\n",
      "subreddits = ['surfing','skating', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "users = []\n",
      "for item in subreddits:\n",
      "    users.append(get_users(item))\n",
      "121/81:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "soup.find_all(class_=\"product_pod\")\n",
      "121/82:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "soup.find_all(class_=\"product_pod\")[0]\n",
      "121/83: soup.find_all(class_=\"product_pod\")[0].find(\"p\").attrs['class']\n",
      "121/84: soup.find_all(class_=\"product_pod\")[0].find(\"p\").attrs['class'].[1]\n",
      "121/85: soup.find_all(class_=\"product_pod\")[0].find(\"p\").attrs['class']\n",
      "121/86: soup.find_all(class_=\"product_pod\")[0].find(\"p\").attrs['class'][0]\n",
      "121/87: soup.find_all(class_=\"product_pod\")[0].find(\"p\").attrs['class'][1]\n",
      "123/1:\n",
      "import requests\n",
      "import lxml.html as lh\n",
      "import pandas as pd\n",
      "123/2:\n",
      "import requests\n",
      "import lxml.html as lh\n",
      "import pandas as pd\n",
      "123/3:\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "#Create a handle, page, to handle the contents of the website\n",
      "page = requests.get(url)\n",
      "#Store the contents of the website under doc\n",
      "doc = lh.fromstring(page.content)\n",
      "#Parse data that are stored between <tr>..</tr> of HTML\n",
      "tr_elements = doc.xpath('//tr')\n",
      "123/4:\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "#Create a handle, page, to handle the contents of the website\n",
      "page = requests.get(url)\n",
      "#Store the contents of the website under doc\n",
      "doc = lh.fromstring(page.content)\n",
      "#Parse data that are stored between <tr>..</tr> of HTML\n",
      "tr_elements = doc.xpath('//tr')\n",
      "123/5:\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "#Create a handle, page, to handle the contents of the website\n",
      "page = requests.get(url)\n",
      "#Store the contents of the website under doc\n",
      "doc = lh.fromstring(page.content)\n",
      "#Parse data that are stored between <tr>..</tr> of HTML\n",
      "tr_elements = doc.xpath('//tr')\n",
      "123/6: tr_elements\n",
      "123/7: page\n",
      "123/8: doc\n",
      "123/9: page.content\n",
      "123/10: page.header\n",
      "123/11:\n",
      "tr_elements = doc.xpath('//tr')\n",
      "#Create empty list\n",
      "col=[]\n",
      "i=0\n",
      "#For each row, store each first element (header) and an empty list\n",
      "for t in tr_elements[0]:\n",
      "    i+=1\n",
      "    name=t.text_content()\n",
      "    print '%d:\"%s\"'%(i,name)\n",
      "    col.append((name,[]))\n",
      "123/12:\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "#Create a handle, page, to handle the contents of the website\n",
      "page = requests.get(url)\n",
      "#Store the contents of the website under doc\n",
      "doc = lh.fromstring(page.content)\n",
      "#Parse data that are stored between <tr>..</tr> of HTML\n",
      "tr_elements = doc.xpath('//tr')\n",
      "123/13: [len(T) for T in tr_elements[:12]]\n",
      "123/14: tr_elements = doc.xpath('//tr')\n",
      "123/15: tr_elements = doc.xpath('//tr')\n",
      "123/16: tr_elements\n",
      "123/17: tr_elements[0]\n",
      "123/18:\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "#Create a handle, page, to handle the contents of the website\n",
      "page = requests.get(url)\n",
      "#Store the contents of the website under doc\n",
      "doc = lh.fromstring(page.content)\n",
      "#Parse data that are stored between <tr>..</tr> of HTML\n",
      "tr_elements = doc.xpath('//tr')\n",
      "123/19: tr_elements\n",
      "123/20: tr_elements[0]\n",
      "123/21: tr_elements[0].content\n",
      "123/22: tr_elements[0].content()\n",
      "123/23: tr_elements[0].text\n",
      "123/24: tr_elements[0].text_content()\n",
      "123/25: tr_elements[0]\n",
      "123/26: tr_elements[0].text_content()\n",
      "123/27: len(tr_elements[0])\n",
      "123/28: tr_elements[0][0]\n",
      "123/29: tr_elements[0][1]\n",
      "123/30:\n",
      "tr_elements = doc.xpath('//tr')\n",
      "#Create empty list\n",
      "col=[]\n",
      "i=0\n",
      "#For each row, store each first element (header) and an empty list\n",
      "for t in tr_elements[0]:\n",
      "    i+=1\n",
      "    name=t.text_content()\n",
      "    print '%d:\"%s\"'%(i,name)\n",
      "    col.append((name,[]))\n",
      "123/31:\n",
      "tr_elements = doc.xpath('//tr')\n",
      "#Create empty list\n",
      "col=[]\n",
      "i=0\n",
      "#For each row, store each first element (header) and an empty list\n",
      "for t in tr_elements[0]:\n",
      "    i+=1\n",
      "    name=t.text_content()\n",
      "    col.append((name,[]))\n",
      "123/32: col\n",
      "123/33: col[0]\n",
      "123/34: col[1]\n",
      "123/35: col\n",
      "123/36:\n",
      "#Since out first row is the header, data is stored on the second row onwards\n",
      "for j in range(1,len(tr_elements)):\n",
      "    #T is our j'th row\n",
      "    T=tr_elements[j]\n",
      "    \n",
      "    #If row is not of size 10, the //tr data is not from our table \n",
      "    if len(T)!=5:\n",
      "        break\n",
      "    \n",
      "    #i is the index of our column\n",
      "    i=0\n",
      "    \n",
      "    #Iterate through each element of the row\n",
      "    for t in T.iterchildren():\n",
      "        data=t.text_content() \n",
      "        #Check if row is empty\n",
      "        if i>0:\n",
      "        #Convert any numerical value to integers\n",
      "            try:\n",
      "                data=int(data)\n",
      "            except:\n",
      "                pass\n",
      "        #Append the data to the empty list of the i'th column\n",
      "        col[i][1].append(data)\n",
      "        #Increment i for the next column\n",
      "        i+=1\n",
      "123/37:\n",
      "Dict={title:column for (title,column) in col}\n",
      "df=pd.DataFrame(Dict)\n",
      "123/38: df.head()\n",
      "123/39: df.head(nrows = 100)\n",
      "123/40: df.head(100)\n",
      "123/41:\n",
      "tr_elements = doc.xpath('//tr')\n",
      "#Create empty list\n",
      "col=[]\n",
      "i=0\n",
      "#For each row, store each first element (header) and an empty list\n",
      "for t in tr_elements[0]:\n",
      "    i+=1\n",
      "    name=t.text_content().replace(\"\\n\",\"\")\n",
      "    col.append((name,[]))\n",
      "123/42: col\n",
      "123/43:\n",
      "#Since out first row is the header, data is stored on the second row onwards\n",
      "for j in range(1,len(tr_elements)):\n",
      "    #T is our j'th row\n",
      "    T=tr_elements[j]\n",
      "    \n",
      "    #If row is not of size 10, the //tr data is not from our table \n",
      "    if len(T)!=5:\n",
      "        break\n",
      "    \n",
      "    #i is the index of our column\n",
      "    i=0\n",
      "    \n",
      "    #Iterate through each element of the row\n",
      "    for t in T.iterchildren():\n",
      "        data=t.text_content() \n",
      "        #Check if row is empty\n",
      "        if i>0:\n",
      "        #Convert any numerical value to integers\n",
      "            try:\n",
      "                data=int(data)\n",
      "            except:\n",
      "                pass\n",
      "        #Append the data to the empty list of the i'th column\n",
      "        col[i][1].append(data)\n",
      "        #Increment i for the next column\n",
      "        i+=1\n",
      "123/44:\n",
      "Dict={title:column for (title,column) in col}\n",
      "df=pd.DataFrame(Dict)\n",
      "123/45: df.head(100)\n",
      "123/46: len(df)\n",
      "123/47:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find_all(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "123/48:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(class_=\"f3-widget-paginator\")[0].find(\"a\").attrs[\"href\"]\n",
      "123/49:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "123/50:\n",
      "url_list = []\n",
      "for i in range(104):\n",
      "    url_list.append(soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"])\n",
      "123/51:\n",
      "url_list = []\n",
      "for i in range(1:105):\n",
      "    url_list.append(soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"])\n",
      "123/52:\n",
      "url_list = []\n",
      "for i in range(1,105):\n",
      "    url_list.append(soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"])\n",
      "123/53:\n",
      "url_list = []\n",
      "for i in range(0,105):\n",
      "    url_list.append(soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"])\n",
      "123/54:\n",
      "url_list = []\n",
      "for i in range(0,104):\n",
      "    url_list.append(soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"])\n",
      "123/55:\n",
      "url_list = []\n",
      "for i in range(0,104):\n",
      "    url = soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"])\n",
      "    url_list.append(url)\n",
      "123/56:\n",
      "url_list = []\n",
      "for i in range(0,104):\n",
      "    url = soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"]\n",
      "    url_list.append(url)\n",
      "123/57:\n",
      "### url_list = []\n",
      "for i in range(0,103):\n",
      "    url = soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"]\n",
      "    url_list.append(url)\n",
      "123/58:\n",
      "### url_list = []\n",
      "for i in range(1,103):\n",
      "    url = soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"]\n",
      "    url_list.append(url)\n",
      "123/59:\n",
      "### url_list = []\n",
      "for i in range(10):\n",
      "    url = soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"]\n",
      "    url_list.append(url)\n",
      "123/60: soup.find_all(class_=\"f3-widget-paginator\")\n",
      "123/61: len(soup.find_all(class_=\"f3-widget-paginator\"))\n",
      "123/62: soup.find_all(class_=\"f3-widget-paginator\").find_all(\"a\").attrs[\"href\"]\n",
      "123/63: soup.find_all(class_=\"f3-widget-paginator\").find(\"a\").attrs[\"href\"]\n",
      "123/64: soup.find_all(class_=\"f3-widget-paginator\")\n",
      "123/65: soup.find_all(class_=\"f3-widget-paginator\")[0]\n",
      "123/66: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\").attrs[\"href\"]\n",
      "123/67: soup.find_all(class_=\"f3-widget-paginator\")[0].find(\"a\").attrs[\"href\"]\n",
      "123/68: soup.find_all(class_=\"f3-widget-paginator\")[1].find(\"a\").attrs[\"href\"]\n",
      "123/69: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\").attrs[\"href\"]\n",
      "123/70: soup.find_all(class_=\"f3-widget-paginator\")[0]\n",
      "123/71: soup.find_all(class_=\"pagerLink\")\n",
      "123/72: len(soup.find_all(class_=\"pagerLink\"))\n",
      "123/73: soup.find_all(class_=\"pagerLink\")\n",
      "125/1:\n",
      "import requests\n",
      "import lxml.html as lh\n",
      "import pandas as pd\n",
      "125/2:\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "#Create a handle, page, to handle the contents of the website\n",
      "page = requests.get(url)\n",
      "#Store the contents of the website under doc\n",
      "doc = lh.fromstring(page.content)\n",
      "#Parse data that are stored between <tr>..</tr> of HTML\n",
      "tr_elements = doc.xpath('//tr')\n",
      "125/3:\n",
      "tr_elements = doc.xpath('//tr')\n",
      "#Create empty list\n",
      "col=[]\n",
      "i=0\n",
      "#For each row, store each first element (header) and an empty list\n",
      "for t in tr_elements[0]:\n",
      "    i+=1\n",
      "    name=t.text_content().replace(\"\\n\",\"\")\n",
      "    col.append((name,[]))\n",
      "125/4: col\n",
      "125/5:\n",
      "#Since out first row is the header, data is stored on the second row onwards\n",
      "for j in range(1,len(tr_elements)):\n",
      "    #T is our j'th row\n",
      "    T=tr_elements[j]\n",
      "    \n",
      "    #If row is not of size 10, the //tr data is not from our table \n",
      "    if len(T)!=5:\n",
      "        break\n",
      "    \n",
      "    #i is the index of our column\n",
      "    i=0\n",
      "    \n",
      "    #Iterate through each element of the row\n",
      "    for t in T.iterchildren():\n",
      "        data=t.text_content() \n",
      "        #Check if row is empty\n",
      "        if i>0:\n",
      "        #Convert any numerical value to integers\n",
      "            try:\n",
      "                data=int(data)\n",
      "            except:\n",
      "                pass\n",
      "        #Append the data to the empty list of the i'th column\n",
      "        col[i][1].append(data)\n",
      "        #Increment i for the next column\n",
      "        i+=1\n",
      "125/6:\n",
      "Dict={title:column for (title,column) in col}\n",
      "df=pd.DataFrame(Dict)\n",
      "125/7: df\n",
      "125/8:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/9: len(soup.find_all(class_=\"pagerLink\"))\n",
      "125/10: soup.find_all(class_=\"pagerLink\")[19]\n",
      "125/11: soup.find_all(class_=\"pagerLink\")[18]\n",
      "125/12: soup.find_all(class_=\"pagerLink\")[19]\n",
      "125/13: soup.find_all(class_=\"pagerLink\")[19].find(\"a\").attrs(\"href\")\n",
      "125/14: soup.find_all(class_=\"pagerLink\")[19].find(\"a\").attrs[\"href\"]\n",
      "125/15: soup.find_all(class_=\"pagerLink\").find(\"a\").attrs[\"href\"]\n",
      "125/16: soup.find_all(class_=\"pagerLink\")\n",
      "125/17: soup.find_all(class_=\"pagerLink\").attrs[\"href\"]\n",
      "125/18: soup.find_all(class_=\"pagerLink\").text\n",
      "125/19: soup.find_all(class_=\"pagerLink\")[0].text\n",
      "125/20: soup.find_all(class_=\"pagerLink\")[0]\n",
      "125/21: soup.find_all(class_=\"pagerLink\")[0].find(\"href\")\n",
      "125/22: soup.find_all(class_=\"pagerLink\")[0].find(\"a\")\n",
      "125/23: soup.find_all(class_=\"a\")\n",
      "125/24: soup.find_all(\"a\")\n",
      "125/25: soup.find_all(class_=\"f3-widget-paginator\")\n",
      "125/26: soup.find_all(class_=\"f3-widget-paginator\").find_all(\"a\")\n",
      "125/27: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "125/28: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\").attrs['href']\n",
      "125/29: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[1].attrs['href']\n",
      "125/30: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[0].attrs['href']\n",
      "125/31: len(soup.find_all(class_=\"f3-widget-paginator\")[0])\n",
      "125/32: len(soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[0])\n",
      "125/33: len(soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\"))\n",
      "125/34:\n",
      "url_list = []\n",
      "for item in soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\"):\n",
      "    url = item.attrs[\"href\"]\n",
      "    url_list.append(url)\n",
      "125/35:\n",
      "url_list = []\n",
      "for item in soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\"):\n",
      "    url = f'https://www.ohnegentechnik.org{item.attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "125/36: url_list\n",
      "125/37:\n",
      "url_list = []\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "for i in range(20):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "125/38: url_list\n",
      "125/39:\n",
      "url_list = []\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "for i in range(19):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "125/40: url_list\n",
      "125/41: len(url_list)\n",
      "125/42: page = url[len(url_list)-1]\n",
      "125/43:\n",
      "page = url[len(url_list)-1]\n",
      "page\n",
      "125/44:\n",
      "page = url_list[len(url_list)-1]\n",
      "page\n",
      "125/45:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/46:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/47: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "125/48: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[20]\n",
      "125/49: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[0]\n",
      "125/50: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[1]\n",
      "125/51: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[2]\n",
      "125/52: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "125/53: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11]\n",
      "125/54: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:18]\n",
      "125/55: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:19]\n",
      "125/56: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "125/57: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:21]\n",
      "125/58: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "125/59:\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "for i in range(10):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "    page = url_list[len(url_list)-1]\n",
      "125/60: all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "125/61:\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "len(all_url)\n",
      "125/62:\n",
      "for i in range(9):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "    page = url_list[len(url_list)-1]\n",
      "125/63: url_list\n",
      "125/64: len(url_list)\n",
      "125/65: url_list\n",
      "125/66:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/67:\n",
      "url_list = []\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "for i in range(19):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "    page = url_list[len(url_list)-1]\n",
      "125/68: url_list\n",
      "125/69:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/70:\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "len(all_url)\n",
      "125/71:\n",
      "for i in range(9):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "    page = url_list[len(url_list)-1]\n",
      "125/72: url_list\n",
      "125/73: len(url_list)\n",
      "125/74: url_list\n",
      "125/75: page\n",
      "125/76:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "all_url\n",
      "125/77:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "all_url\n",
      "125/78:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "all_url\n",
      "125/79:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "for i in range(9):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "    page = url_list[len(url_list)-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/80: url_list\n",
      "125/81: len(url_list)\n",
      "125/82:\n",
      "while len(url_list) < 105:\n",
      "    res = requests.get(page)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "    for i in range(9):\n",
      "        url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "        url_list.append(url)\n",
      "        page = url_list[len(url_list)-1]\n",
      "125/83: len(url_list)\n",
      "125/84: url_list\n",
      "125/85:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/86:\n",
      "url_list = []\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "for i in range(19):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "    page = url_list[len(url_list)-1]\n",
      "125/87:\n",
      "while len(url_list) < 100:\n",
      "    res = requests.get(page)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "    for i in range(9):\n",
      "        url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "        url_list.append(url)\n",
      "        page = url_list[len(url_list)-1]\n",
      "125/88: url_list\n",
      "125/89: page\n",
      "125/90:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/91:\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "all_url\n",
      "125/92:\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "len(all_url)\n",
      "125/93: all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[17:19]\n",
      "125/94:\n",
      "\n",
      "all_url\n",
      "125/95:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[16:19]\n",
      "125/96: all_url\n",
      "125/97:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[17:19]\n",
      "125/98: all_url\n",
      "125/99:\n",
      "for i in range(2):\n",
      "        url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "        url_list.append(url)\n",
      "125/100: url_list\n",
      "125/101:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[17:20]\n",
      "for i in range(2):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "125/102: url_list\n",
      "125/103:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/104:\n",
      "url_list = []\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "for i in range(19):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "    page = url_list[len(url_list)-1]\n",
      "125/105:\n",
      "while len(url_list) < 100:\n",
      "    res = requests.get(page)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "    for i in range(9):\n",
      "        url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "        url_list.append(url)\n",
      "        page = url_list[len(url_list)-1]\n",
      "125/106:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "125/107: all_url[17:21]\n",
      "125/108: url_list\n",
      "125/109:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[17:21]\n",
      "125/110:\n",
      "for i in range(3):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "125/111: url_list\n",
      "125/112: len(url_list)\n",
      "125/113: url\n",
      "125/114: page1 = [\"https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche\"]\n",
      "125/115: page1.append(url_list)\n",
      "125/116: page1\n",
      "125/117: urls = [\"https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche\"]\n",
      "125/118: urls.extend(url_list)\n",
      "125/119: urls\n",
      "125/120: len(urls)\n",
      "125/121:\n",
      "#Create a handle, page, to handle the contents of the website\n",
      "url= url_list[0]\n",
      "page = requests.get(url)\n",
      "#Store the contents of the website under doc\n",
      "doc = lh.fromstring(page.content)\n",
      "#Parse data that are stored between <tr>..</tr> of HTML\n",
      "tr_elements = doc.xpath('//tr')\n",
      "\n",
      "#Create empty list\n",
      "col=[]\n",
      "i=0\n",
      "#For each row, store each first element (header) and an empty list\n",
      "for t in tr_elements[0]:\n",
      "    i+=1\n",
      "    name=t.text_content().replace(\"\\n\",\"\")\n",
      "    col.append((name,[]))\n",
      "\n",
      "#Since out first row is the header, data is stored on the second row onwards\n",
      "for url in url_list:\n",
      "    page = requests.get(url)\n",
      "    doc = lh.fromstring(page.content)\n",
      "    tr_elements = doc.xpath('//tr')\n",
      "    for j in range(1,len(tr_elements)):\n",
      "        #T is our j'th row\n",
      "        T=tr_elements[j]\n",
      "\n",
      "        #If row is not of size 10, the //tr data is not from our table \n",
      "        if len(T)!=5:\n",
      "            break\n",
      "\n",
      "        #i is the index of our column\n",
      "        i=0\n",
      "\n",
      "        #Iterate through each element of the row\n",
      "        for t in T.iterchildren():\n",
      "            data=t.text_content() \n",
      "            #Check if row is empty\n",
      "            if i>0:\n",
      "            #Convert any numerical value to integers\n",
      "                try:\n",
      "                    data=int(data)\n",
      "                except:\n",
      "                    pass\n",
      "            #Append the data to the empty list of the i'th column\n",
      "            col[i][1].append(data)\n",
      "            #Increment i for the next column\n",
      "            i+=1\n",
      "125/122:\n",
      "Dict={title:column for (title,column) in col}\n",
      "df=pd.DataFrame(Dict)\n",
      "125/123: df\n",
      "125/124: df.to_csv(\"nonGMO.csv\")\n",
      "129/1:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://info.lidl/en'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "129/2:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://info.lidl/en'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "129/3: soup.find_all(class_=\"lidl-m-svg-map-marker__text\").text\n",
      "129/4: soup.find_all(class_=\"lidl-m-svg-map-marker__text\")\n",
      "129/5: soup.find_all(class_=\"lidl-m-svg-map-marker__text\")[1]\n",
      "129/6: soup.find_all(class_=\"lidl-m-svg-map-marker__text\")[1].text\n",
      "129/7:\n",
      "for i in soup.find_all(class_=\"lidl-m-svg-map-marker__text\"):\n",
      "    print i.text\n",
      "129/8:\n",
      "for i in soup.find_all(class_=\"lidl-m-svg-map-marker__text\"):\n",
      "    print (i.text)\n",
      "130/1:\n",
      "url='https://www.international.tiffany.com/jewelry-stores/store-list/'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "130/2:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "url='https://www.international.tiffany.com/jewelry-stores/store-list/'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "130/3: soup.find_all(class_=\"stores-filter__regions-content-dropdown-list\")\n",
      "131/1:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://storelocator.yves-rocher.com/en/'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "131/2: soup.find_all(\"option\")\n",
      "131/3:\n",
      "for i in soup.find_all(\"option\"):\n",
      "    print(i.text)\n",
      "133/1:\n",
      "import pandas as pd\n",
      "import requests\n",
      "import json\n",
      "133/2:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup\n",
      "import pandas as pd\n",
      "import requests\n",
      "import json\n",
      "133/3:\n",
      "#DE - Woman sportwear\n",
      "url1 = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "de1 = requests.get(url)\n",
      "soup_de1 = BeautifulSoup(res.text, \"html.parser\")\n",
      "133/4:\n",
      "#DE - Woman sportwear\n",
      "url1 = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "de1 = requests.get(url1)\n",
      "soup_de1 = BeautifulSoup(res.text, \"html.parser\")\n",
      "133/5:\n",
      "#DE - Woman sportwear\n",
      "url1 = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "de1 = requests.get(url1)\n",
      "soup_de1 = BeautifulSoup(de1.text, \"html.parser\")\n",
      "133/6:\n",
      "#DE - Woman sportwear\n",
      "url1 = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "de1 = requests.get(url1)\n",
      "soup_de1 = BeautifulSoup(de1.text, \"html.parser\")\n",
      "\n",
      "soup_de1\n",
      "133/7:\n",
      "#DE - Woman sportwear\n",
      "url1 = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "de1 = requests.get(url1)\n",
      "soup_de1 = BeautifulSoup(de1.text, \"html.parser\")\n",
      "\n",
      "de1\n",
      "133/8:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup\n",
      "import pandas as pd\n",
      "import requests\n",
      "from urllib.request import urlopen\n",
      "133/9:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "from urllib.request import urlopen\n",
      "133/10:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "from urllib.request import urlopen\n",
      "133/11:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "def getAndParseURL(url): \n",
      "result = requests.get(url)\n",
      "soup = bts(result.text, 'html.parser')\n",
      "return soup\n",
      "133/12:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url)\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "133/13:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url)\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "\n",
      "getAndParseURL(url)\n",
      "133/14:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "\n",
      "getAndParseURL(url)\n",
      "133/15:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/98.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "\n",
      "getAndParseURL(url)\n",
      "133/16:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "\n",
      "getAndParseURL(url)\n",
      "133/17:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 <- getAndParseURL(url)\n",
      "\n",
      "de1\n",
      "133/18:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "133/19:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 <- getAndParseURL(url)\n",
      "\n",
      "de1\n",
      "133/20:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "from urllib.request import urlopen\n",
      "133/21:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "de1\n",
      "133/22: print(de1.find('product-list')\n",
      "133/23:\n",
      "for i in de1.findAll(\"h2\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "133/24:\n",
      "for i in de1.findAll(\"strong\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "133/25:\n",
      "# Getting product category\n",
      "de1.find(\"h1\")\n",
      "\n",
      "# Getting product name\n",
      "for i in de1.findAll(\"h2\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "    \n",
      "# Getting brand name\n",
      "\n",
      "for i in de1.findAll(\"strong\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "133/26: de1.find(\"h1\")\n",
      "133/27: de1.find(\"h1\").text\n",
      "133/28: print(de1.find(\"h1\").text)\n",
      "133/29: print(de1.find(\"div\", {\"class\": \"plp-bar-info svelte-1uqvrhu\"}).text)\n",
      "133/30: print(de1.find(\"span\", {\"class\": \"svelte-1uqvrhu\"}).text)\n",
      "133/31: 4096/40\n",
      "136/1:\n",
      "# Getting product category\n",
      "print(de1.find(\"h1\").text)\n",
      "\n",
      "# Getting product name\n",
      "for i in de1.findAll(\"h2\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "    \n",
      "# Getting brand name\n",
      "\n",
      "for i in de1.findAll(\"strong\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "    \n",
      "help(findAll)\n",
      "136/2: help(findAll)\n",
      "136/3: ?findAll\n",
      "136/4:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "from urllib.request import urlopen\n",
      "136/5:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "136/6:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "136/7:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "de1\n",
      "136/8:\n",
      "# Getting product category\n",
      "print(de1.find(\"h1\").text)\n",
      "136/9:\n",
      "# Getting product name\n",
      "for i in de1.findAll(\"h2\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "136/10:\n",
      "# Getting product name\n",
      "for i in de1.find_all(\"h2\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "136/11:\n",
      "# Getting product name\n",
      "for i in de1.find_all(\"h2\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "136/12:\n",
      "# Getting brand name\n",
      "\n",
      "for i in de1.find_All(\"strong\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "136/13:\n",
      "# Getting brand name\n",
      "for i in de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "136/14:\n",
      "# Getting brand name\n",
      "for i in de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    i.text\n",
      "136/15:\n",
      "# Getting brand name\n",
      "for i in de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "136/16:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "136/17:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "brand_name\n",
      "136/18:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "brand_name[1]\n",
      "136/19:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "brand_name[0]\n",
      "136/20:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "brand_name[0].text\n",
      "136/21:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "length(brand_name)\n",
      "136/22:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "len(brand_name)\n",
      "136/23:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "for name in brand_name:\n",
      "    print(brand_name.text)\n",
      "136/24:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "for name in brand_name:\n",
      "    print(name.text)\n",
      "136/25:\n",
      "# Getting product name\n",
      "prod_name = de1.find_all(\"h2\",{\"class\":\"svelte-wmr4s2\"})\n",
      "prod_name_list =[]\n",
      "\n",
      "for name in prod_name:\n",
      "    prod_name_list.append(name.text)\n",
      "136/26:\n",
      "# Getting product name\n",
      "prod_name = de1.find_all(\"h2\",{\"class\":\"svelte-wmr4s2\"})\n",
      "prod_name_list =[]\n",
      "\n",
      "for name in prod_name:\n",
      "    prod_name_list.append(name.text)\n",
      "    \n",
      "prod_name_list\n",
      "136/27:\n",
      "# Getting product name\n",
      "prod_name = de1.find_all(\"h2\",{\"class\":\"svelte-wmr4s2\"})\n",
      "prod_name_list =[]\n",
      "\n",
      "for name in prod_name:\n",
      "    prod_name_list.append(name.text)\n",
      "    \n",
      "len(prod_name_list)\n",
      "136/28:\n",
      "#Getting product name\n",
      "prod_name = de1.find_all(\"h2\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "#creating an empty array of product names\n",
      "prod_name_list =[]\n",
      "\n",
      "for name in prod_name:\n",
      "    prod_name_list.append(name.text)\n",
      "    \n",
      "len(prod_name_list)\n",
      "136/29: prod_name\n",
      "136/30: sport = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "136/31:\n",
      "sport = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "sport\n",
      "136/32:\n",
      "sport = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "len(sport)\n",
      "136/33:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "len(prod)\n",
      "\n",
      "type(prod)\n",
      "136/34:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "len(prod)\n",
      "136/35:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "len(prod)\n",
      "136/36:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[0]\n",
      "136/37:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[0].find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "136/38:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[0].find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "136/39:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[3].find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "136/40:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[3].find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "136/41:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[2].find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "136/42:\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = product.find(\"a\").attrs[\"href\"].text\n",
      "    prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'price': prod_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker})\n",
      "136/43:\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'price': prod_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker})\n",
      "136/44:\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'price': prod_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker})\n",
      "136/45:\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'price': prod_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker})\n",
      "136/46:\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'price': prod_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker})\n",
      "136/47: de1_list\n",
      "136/48: de1[3].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "136/49: prod[3].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "136/50: prod[3].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "136/51: prod[1].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "136/52: prod[1].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "136/53: prod[2].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "136/54: prod[3].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "136/55: prod[3].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text == None\n",
      "136/56:\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = product.find(\"a\").attrs[\"href\"]\n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'regular price': prod_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker})\n",
      "136/57: de1_list\n",
      "136/58:\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = product.find(\"a\").attrs[\"href\"]\n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "136/59: de1_list\n",
      "136/60:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}product.find(\"a\").attrs[\"href\"]'\n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "136/61: de1_list\n",
      "136/62:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{product.find(\"a\").attrs[\"href\"]}'\n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "136/63: de1_list\n",
      "140/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "140/2:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "140/3:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "de1\n",
      "140/4:\n",
      "# Getting product category\n",
      "print(de1.find(\"h1\").text)\n",
      "140/5:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{product.find(\"a\").attrs[\"href\"]}'\n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/6:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[2].find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "140/7:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{product.find(\"a\").attrs[\"href\"]}'\n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/8:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{product.find(\"a\").attrs[\"href\"]}'\n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/9: de1_list\n",
      "140/10: prod[1].find(\"div\").attrs[\"href\"]\n",
      "140/11: text = 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ'\n",
      "140/12:\n",
      "string = 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ'\n",
      "\n",
      "print(string.partition(\"?mc\")[2])\n",
      "140/13:\n",
      "string = 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ'\n",
      "\n",
      "print(string.partition(\"?mc=\")[2])\n",
      "140/14:\n",
      "string = 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ'\n",
      "\n",
      "print(string.partition(\"?mc=\")[2].rpartition('&')[0])\n",
      "140/15:\n",
      "string = 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ'\n",
      "\n",
      "print(string.partition(\"?mc=\")[2].rpartition('&')[0])\n",
      "140/16:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "print(string.partition(\"?mc=\")[2].rpartition('&')[0])\n",
      "140/17:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "string.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "140/18:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "string.partition(\"?mc=\")[2].rpartition('&')[0] == None\n",
      "140/19:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "print(string.partition(\"?mc=\")[2].rpartition('&')[0]) == None\n",
      "140/20:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "print(string.partition(\"?mc=\")[7])\n",
      "140/21:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "print(string.partition(\"?mc=\")[2].rpartition('&')[0]) == None\n",
      "140/22:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "print(string.partition(\"?mc=\")[2].rpartition('&')[0]) == None\n",
      "140/23:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2]\n",
      "140/24:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:6]\n",
      "140/25:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2]\n",
      "140/26:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:7]\n",
      "140/27:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][1:7]\n",
      "140/28:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:6]\n",
      "140/29:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0]\n",
      "140/30:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:1]\n",
      "140/31:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:2]\n",
      "140/32:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:7]\n",
      "140/33:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0]\n",
      "140/34:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][1]\n",
      "140/35:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][2]\n",
      "140/36:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][3]\n",
      "140/37:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][4]\n",
      "140/38:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:7]\n",
      "140/39:\n",
      "string = 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:7]\n",
      "140/40:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{product.find(\"a\").attrs[\"href\"]}'\n",
      "    prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/41: de1_list\n",
      "140/42:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    #prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0]) == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0])\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/43:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    #prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    \n",
      "    if print(link.partition(\"?mc=\")[2].rpartition('&')[0])) == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0])\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/44:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    #prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0]) == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0])\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/45:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    #prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0])\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/46:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    #prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/47: de1_list\n",
      "140/48:\n",
      "string = 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:7]\n",
      "140/49: string.replace('\\n',\" \")\n",
      "140/50: '8380104'.replace('\\n',\" \")\n",
      "140/51: '8380104\\n'.replace('\\n',\" \")\n",
      "140/52: '8380104\\n'.replace('\\n',\"\")\n",
      "140/53:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n',\"\")\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n',\"\")\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n',\"\")\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/54: de1_list\n",
      "140/55: '8380104\\n'.replace('\\n'&'*',\"\")\n",
      "140/56: '8380104\\n'.replace('\\n'|'*',\"\")\n",
      "140/57: '8380104\\n'.replace('\\n',\"\").replace('*',\"\")\n",
      "140/58: '8380104\\n**'.replace('\\n',\"\").replace('*',\"\")\n",
      "140/59:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n',\"\").replace('*',\"\")\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n',\"\").replace('*',\"\")\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n',\"\").replace('*',\"\")\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/60: de1_list\n",
      "140/61:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n',\"\").replace('*',\"\")\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n',\"\").replace('*',\"\")\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n',\"\").replace('*',\"\")\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n',\"\").replace('*',\"\")\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/62: de1_list\n",
      "140/63:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/64: de1_list\n",
      "143/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "143/2:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "143/3:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "de1\n",
      "143/4:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "143/5:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[2].find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "143/6:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "143/7: de1_list\n",
      "143/8: prod[0].find(\"a\").attrs[\"href\"]\n",
      "143/9:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        id1 = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        id1 = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "143/11:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        id1 = link.partition(\"?mc=\")[2]\n",
      "else:\n",
      "    id1 = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "143/12:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        id1 = link.partition(\"?mc=\")[2]\n",
      "else:\n",
      "    id1 = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "id1\n",
      "143/13:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        id1 = link.partition(\"?mc=\")[2]\n",
      "else:\n",
      "    id1 = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "143/14:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        id1 = link.partition(\"?mc=\")[2]\n",
      "else:\n",
      "    id1 = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "link\n",
      "143/15:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        id1 = link.partition(\"?mc=\")[2]\n",
      "else:\n",
      "    id1 = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "link.partition(\"?mc=\")[2].rpartition('&')[0] == None\n",
      "\n",
      "link\n",
      "143/16:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        id1 = link.partition(\"?mc=\")[2]\n",
      "else:\n",
      "    id1 = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "link.partition(\"?mc=\")[2].rpartition('&')[0] == None\n",
      "143/17:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "143/18:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "143/19:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "link.partition(\"?mc=\")[2]\n",
      "143/20:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if print(link.partition(\"?mc=\")[2].rpartition('&')[0]) == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "143/21:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if print(link.partition(\"?mc=\")[2].rpartition('&')[0]) == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "143/22: de1_list\n",
      "143/23:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "143/24: de1_list\n",
      "143/25:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"span\",{\"class\":\"svelte-1uqvrhu\"})\n",
      "143/26:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"span\",{\"class\":\"svelte-1uqvrhu\"})\n",
      "\n",
      "total_prod\n",
      "143/27:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"span\",{\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "total_prod\n",
      "143/28:\n",
      "#Getting total products in the category\n",
      "de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"})\n",
      "143/29:\n",
      "#Getting total products in the category\n",
      "de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "143/30:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "total_prod/40\n",
      "143/31:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "int(total_prod)/40\n",
      "143/32:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "# No of pages\n",
      "round(int(total_prod)/40)\n",
      "143/33:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "# No of pages\n",
      "math_ceil(int(total_prod)/40)\n",
      "143/34:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "143/35:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "# No of pages\n",
      "math_ceil(int(total_prod)/40)\n",
      "143/36:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "# No of pages\n",
      "math.ceil(int(total_prod)/40)\n",
      "143/37:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "# No of pages\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "math.ceil(1.2)\n",
      "143/38:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "# No of pages\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "math.ceil(1.1)\n",
      "143/39:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "# No of pages\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "math.ceil(1.0)\n",
      "143/40:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "[1:10]\n",
      "143/41:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "print([1:10])\n",
      "143/42:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "print(1:10)\n",
      "143/43:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in 1:10:\n",
      "        print(i)\n",
      "143/44:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in (1:10):\n",
      "        print(i)\n",
      "143/45:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1:10):\n",
      "        print(i)\n",
      "143/46:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1,10):\n",
      "        print(i)\n",
      "143/47:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1, math.ceil(int(total_prod)/40)):\n",
      "        print(i)\n",
      "143/48:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1, math.ceil(int(total_prod)/40)):\n",
      "        page = f'{page1}?from={40 * i}&size=40'\n",
      "        print(f'page{i + 1}: {page}')\n",
      "143/49:\n",
      "#Pagination\n",
      "page1 = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1, math.ceil(int(total_prod)/40)):\n",
      "        page = f'{page1}?from={40 * i}&size=40'\n",
      "        print(f'page{i + 1}: {page}')\n",
      "144/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "144/2:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "144/3:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "de1\n",
      "144/4:\n",
      "#Pagination\n",
      "page1 = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(0, math.ceil(int(total_prod)/40)):\n",
      "        page = f'{page1}?from={40 * i}&size=40'\n",
      "        print(f'page{i + 1}: {page}')\n",
      "144/5:\n",
      "#Pagination\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "url_list = [url]\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(0, math.ceil(int(total_prod)/40)):\n",
      "        url = f'{url}?from={40 * i}&size=40'\n",
      "        url_list.append(url)\n",
      "144/6:\n",
      "#Pagination\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "url_list = [url]\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(0, math.ceil(int(total_prod)/40)):\n",
      "        url = f'{url}?from={40 * i}&size=40'\n",
      "        url_list.append(url)\n",
      "        \n",
      "url_list\n",
      "144/7:\n",
      "#Pagination\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "url_list = [url]\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1, math.ceil(int(total_prod)/40)):\n",
      "        url = f'{url}?from={40 * i}&size=40'\n",
      "        url_list.append(url)\n",
      "        \n",
      "url_list\n",
      "144/8:\n",
      "#Pagination\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "url_list = [url]\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1, math.ceil(int(total_prod)/40)):\n",
      "        page = f'{url}?from={40 * i}&size=40'\n",
      "        url_list.append(page)\n",
      "        \n",
      "url_list\n",
      "144/9:\n",
      "#Pagination\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "url_list = [url]\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1, math.ceil(int(total_prod)/40)):\n",
      "        page = f'{url}?from={40 * i}&size=40'\n",
      "        url_list.append(page)\n",
      "144/10:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "   1: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
