{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "472e20ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up\n",
    "from bs4 import BeautifulSoup as bts\n",
    "import pandas as pd\n",
    "import requests\n",
    "import math #for rounding numbers\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa544997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# country input\n",
    "country = 'FR'\n",
    "base_index = 'https://www.decathlon.fr'\n",
    "base_cat = '/browse/c0-femme/_/N-ry4jwt'\n",
    "base_sport = 'Z1l5trmt'\n",
    "sold_by = 'Z1o76joc'\n",
    "prod_list_src = 'aim4wv'\n",
    "sticker_src = '1p0v3i7'\n",
    "total_page_src = '1uqvrhu'\n",
    "per_page = 40\n",
    "filter_position = 0\n",
    "fem_src = 'N-ry4jwt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df0d8d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(filter_link)\n",
    "\n",
    "#accept cookie popup\n",
    "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
    "\n",
    "\n",
    "#get all buttons and click on them all (show more) by \n",
    "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
    "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
    "\n",
    "#click all buttons\n",
    "for button in buttons:\n",
    "    driver.execute_script(\"arguments[0].click();\", button)\n",
    "    #getting the soup\n",
    "    soup = bts(driver.page_source)\n",
    "    \n",
    "# access all filters:\n",
    "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
    "\n",
    "# access sport filter at the second position:\n",
    "sports = filter[filter_position]\n",
    "\n",
    "sport_list = []\n",
    "sport_url = sports.find_all(\"a\")\n",
    "\n",
    "#first item\n",
    "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
    "sport_code = base_sport\n",
    "sport_list.append({'sport_type': sport_type,\n",
    "                   'sport_code': sport_code})\n",
    "\n",
    "for i in range(1, len(sport_url)):\n",
    "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
    "    sport_code = re.sub(\"\\/.*?\\_/\", \"\", sport_url[i].attrs[\"href\"]).replace(fem_src, \"\").replace(base_sport, \"\"). replace(sold_by,\"\")\n",
    "    sport_list.append({'sport_type': sport_type,\n",
    "                       'sport_code': sport_code})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "438babe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sport_type': ' 3x3 basketball  ', 'sport_code': 'Z1l5trmt'},\n",
       " {'sport_type': ' afrocafit   ', 'sport_code': 'Z1apkgwr'},\n",
       " {'sport_type': ' aïkido   ', 'sport_code': 'Z19xow4v'},\n",
       " {'sport_type': ' alpinisme rocheux   ', 'sport_code': 'Z1ibp5l5'},\n",
       " {'sport_type': ' apnée   ', 'sport_code': 'Zkyycuo'},\n",
       " {'sport_type': ' aqua cross training   ', 'sport_code': 'Z1oi8qqs'},\n",
       " {'sport_type': ' aquabike   ', 'sport_code': 'Z15mg0vy'},\n",
       " {'sport_type': ' aquafitness   ', 'sport_code': 'Z1tx3jkm'},\n",
       " {'sport_type': ' arnis   ', 'sport_code': 'Ztn9nnn'},\n",
       " {'sport_type': ' athlétisme   ', 'sport_code': 'Z10bae3z'},\n",
       " {'sport_type': ' aviron   ', 'sport_code': 'Zpzrlmd'},\n",
       " {'sport_type': ' badminton   ', 'sport_code': 'Z14kyc9g'},\n",
       " {'sport_type': ' balade à cheval   ', 'sport_code': 'Zlefn2g'},\n",
       " {'sport_type': ' baline gomme creuse   ', 'sport_code': 'Zaublyl'},\n",
       " {'sport_type': ' ball trap   ', 'sport_code': 'Z1o6d0rz'},\n",
       " {'sport_type': ' balle au tambourin   ', 'sport_code': 'Zsvyhx4'},\n",
       " {'sport_type': ' ballet contemporain   ', 'sport_code': 'Z1gi1m5g'},\n",
       " {'sport_type': ' barre au sol   ', 'sport_code': 'Zlkqwye'},\n",
       " {'sport_type': ' baseball   ', 'sport_code': 'Z1fmk2m8'},\n",
       " {'sport_type': ' basketball   ', 'sport_code': 'Z1tdshfw'},\n",
       " {'sport_type': ' bateau dragon   ', 'sport_code': 'Zqo363z'},\n",
       " {'sport_type': ' beach rugby   ', 'sport_code': 'Zdlvfon'},\n",
       " {'sport_type': ' beach soccer   ', 'sport_code': 'Z1yawglu'},\n",
       " {'sport_type': ' beach tennis   ', 'sport_code': 'Z1q3jr30'},\n",
       " {'sport_type': ' beach volley   ', 'sport_code': 'Z1bzhj00'},\n",
       " {'sport_type': ' billard américain   ', 'sport_code': 'Z1xgog1n'},\n",
       " {'sport_type': ' billard anglais   ', 'sport_code': 'Zb8sjkp'},\n",
       " {'sport_type': ' billard français   ', 'sport_code': 'Z1trxaap'},\n",
       " {'sport_type': ' bmx freestyle   ', 'sport_code': 'Z1yjpxk6'},\n",
       " {'sport_type': ' body attack   ', 'sport_code': 'Zohd7jd'},\n",
       " {'sport_type': ' bodyboard   ', 'sport_code': 'Zy6pw1n'},\n",
       " {'sport_type': ' bodysurf   ', 'sport_code': 'Zpssjvb'},\n",
       " {'sport_type': ' bokwa   ', 'sport_code': 'Z161gpiv'},\n",
       " {'sport_type': ' boomerang   ', 'sport_code': 'Z1v3a2tp'},\n",
       " {'sport_type': ' bouées tractées   ', 'sport_code': 'Z1j1ylpj'},\n",
       " {'sport_type': ' boxe anglaise   ', 'sport_code': 'Z1rmzuws'},\n",
       " {'sport_type': ' boxe française   ', 'sport_code': 'Zhzd147'},\n",
       " {'sport_type': ' break dance   ', 'sport_code': 'Z1tck2bl'},\n",
       " {'sport_type': ' bushcraft   ', 'sport_code': 'Z1fa92a3'},\n",
       " {'sport_type': ' canoë kayak course   ', 'sport_code': 'Z17ei8rn'},\n",
       " {'sport_type': ' canoë kayak randonnee   ', 'sport_code': 'Z1w2hkl3'},\n",
       " {'sport_type': ' canoë kayak vague   ', 'sport_code': 'Z1e45em'},\n",
       " {'sport_type': ' canyoning   ', 'sport_code': 'Zwt0s7o'},\n",
       " {'sport_type': ' capoeira   ', 'sport_code': 'Z16j4hh0'},\n",
       " {'sport_type': ' cardio boxing   ', 'sport_code': 'Z1eca7kr'},\n",
       " {'sport_type': ' cardio training   ', 'sport_code': 'Z1j0rald'},\n",
       " {'sport_type': ' carrom   ', 'sport_code': 'Znqr6x4'},\n",
       " {'sport_type': ' cascade de glace   ', 'sport_code': 'Zviv8q2'},\n",
       " {'sport_type': ' cerf-volant de traction   ', 'sport_code': 'Z1we98d4'},\n",
       " {'sport_type': ' cerf-volant pilotable   ', 'sport_code': 'Z1m6k2wf'},\n",
       " {'sport_type': ' cerf-volant statique   ', 'sport_code': 'Z1rwoocy'},\n",
       " {'sport_type': \" chasse à l'approche   \", 'sport_code': 'Zghcc6m'},\n",
       " {'sport_type': \" chasse à l'arc   \", 'sport_code': 'Z11tbh8i'},\n",
       " {'sport_type': \" chasse au gibier d'eau   \", 'sport_code': 'Z5nxi2w'},\n",
       " {'sport_type': ' chasse au grand gibier battue poste   ',\n",
       "  'sport_code': 'Zca107g'},\n",
       " {'sport_type': ' chasse au grand gibier battue traque   ',\n",
       "  'sport_code': 'Z17wb7m0'},\n",
       " {'sport_type': ' chasse au migrateur   ', 'sport_code': 'Z1g7hx7m'},\n",
       " {'sport_type': ' chasse au petit gibier   ', 'sport_code': 'Zgv72to'},\n",
       " {'sport_type': ' chasse sous marine   ', 'sport_code': 'Z1ay8qtn'},\n",
       " {'sport_type': ' chistera   ', 'sport_code': 'Zchhxap'},\n",
       " {'sport_type': ' commercial hip hop   ', 'sport_code': 'Z1flcfg3'},\n",
       " {'sport_type': ' commercial jazz   ', 'sport_code': 'Z2nsvnn'},\n",
       " {'sport_type': \" course d'endurance   \", 'sport_code': 'Z1k8srmz'},\n",
       " {'sport_type': \" course d'orientation   \", 'sport_code': 'Zriy26i'},\n",
       " {'sport_type': ' cricket   ', 'sport_code': 'Zrbcyv3'},\n",
       " {'sport_type': ' cross training   ', 'sport_code': 'Z16crwm7'},\n",
       " {'sport_type': ' crossminton   ', 'sport_code': 'Z1ntgak1'},\n",
       " {'sport_type': ' cruiser   ', 'sport_code': 'Z1hcmdwu'},\n",
       " {'sport_type': ' cyclosport\\u200e   ', 'sport_code': 'Z1hh0aii'},\n",
       " {'sport_type': ' cyclotourisme   ', 'sport_code': 'Z1ntk6aq'},\n",
       " {'sport_type': ' dancehall   ', 'sport_code': 'Z19i6twf'},\n",
       " {'sport_type': ' danse classique   ', 'sport_code': 'Z1chn37u'},\n",
       " {'sport_type': ' danse contemporaine   ', 'sport_code': 'Z18ukna6'},\n",
       " {'sport_type': ' danse modern jazz   ', 'sport_code': 'Zsua5p'},\n",
       " {'sport_type': ' danse néo classique   ', 'sport_code': 'Z1f5q0rd'},\n",
       " {'sport_type': ' dériveur / catamaran   ', 'sport_code': 'Z1p6jx5x'},\n",
       " {'sport_type': ' disc golf   ', 'sport_code': 'Zagn43y'},\n",
       " {'sport_type': ' disque volant   ', 'sport_code': 'Zockjlw'},\n",
       " {'sport_type': ' djembel   ', 'sport_code': 'Z9fptu1'},\n",
       " {'sport_type': ' endurance à cheval   ', 'sport_code': 'Z1224edg'},\n",
       " {'sport_type': ' entraînement fonctionnel   ', 'sport_code': 'Zq8v7b2'},\n",
       " {'sport_type': ' équitation   ', 'sport_code': 'Zz1nbvq'},\n",
       " {'sport_type': ' équitation concours complet   ', 'sport_code': 'Zxpecus'},\n",
       " {'sport_type': ' équitation dressage   ', 'sport_code': 'Z1n9wcuz'},\n",
       " {'sport_type': ' équitation éthologie   ', 'sport_code': 'Z1ybxqrq'},\n",
       " {'sport_type': ' équitation horse ball   ', 'sport_code': 'Zankxdd'},\n",
       " {'sport_type': \" équitation saut d'obstacles   \", 'sport_code': 'Zgcvumk'},\n",
       " {'sport_type': ' équitation western   ', 'sport_code': 'Z1ryz7vo'},\n",
       " {'sport_type': ' escalade en bloc   ', 'sport_code': 'Z1b3863u'},\n",
       " {'sport_type': ' escalade en falaise   ', 'sport_code': 'Z1xubfeg'},\n",
       " {'sport_type': ' escalade en grandes voies   ', 'sport_code': 'Z10v4ri7'},\n",
       " {'sport_type': ' escalade en salle   ', 'sport_code': 'Z1d75w9j'},\n",
       " {'sport_type': ' escrime   ', 'sport_code': 'Ztqsm9p'},\n",
       " {'sport_type': ' éveil aquatique   ', 'sport_code': 'Z3sj145'},\n",
       " {'sport_type': ' expédition haute altitude   ', 'sport_code': 'Zbmn9ui'},\n",
       " {'sport_type': ' fast touring   ', 'sport_code': 'Z7125yd'},\n",
       " {'sport_type': ' fitness ballet   ', 'sport_code': 'Zf1p1m6'},\n",
       " {'sport_type': ' flag football   ', 'sport_code': 'Z7kvlsd'},\n",
       " {'sport_type': ' flamenco   ', 'sport_code': 'Z1usc3l0'},\n",
       " {'sport_type': ' fléchettes pointe acier   ', 'sport_code': 'Z153ghh3'},\n",
       " {'sport_type': ' fléchettes pointe plastique   ', 'sport_code': 'Z1h2tjho'},\n",
       " {'sport_type': ' floorball   ', 'sport_code': 'Zs8hy1'},\n",
       " {'sport_type': ' foot5   ', 'sport_code': 'Zith9kx'},\n",
       " {'sport_type': ' football à 11   ', 'sport_code': 'Z1jxg75s'},\n",
       " {'sport_type': ' football américain   ', 'sport_code': 'Zf1vip'},\n",
       " {'sport_type': ' frescobol   ', 'sport_code': 'Z11se9ou'},\n",
       " {'sport_type': ' frontenis   ', 'sport_code': 'Za6mf6n'},\n",
       " {'sport_type': ' full contact   ', 'sport_code': 'Z1ksabgn'},\n",
       " {'sport_type': ' funk jazz   ', 'sport_code': 'Z18sygtn'},\n",
       " {'sport_type': ' futsal   ', 'sport_code': 'Z11cfqob'},\n",
       " {'sport_type': ' golf   ', 'sport_code': 'Z1o0t224'},\n",
       " {'sport_type': ' grappling   ', 'sport_code': 'Z1kv1ltm'},\n",
       " {'sport_type': ' gravel\\u200e   ', 'sport_code': 'Z4mdfnp'},\n",
       " {'sport_type': ' gym douce   ', 'sport_code': 'Z1dw8zwx'},\n",
       " {'sport_type': ' gymnastique artistique   ', 'sport_code': 'Z1mbcety'},\n",
       " {'sport_type': ' gymnastique éducative et sportive   ',\n",
       "  'sport_code': 'Z1mbq5a'},\n",
       " {'sport_type': ' gymnastique rythmique   ', 'sport_code': 'Z1b64cnk'},\n",
       " {'sport_type': ' gymnastique suédoise   ', 'sport_code': 'Z1a5u6vb'},\n",
       " {'sport_type': ' gymnastique volontaire   ', 'sport_code': 'Zdiimi2'},\n",
       " {'sport_type': ' handball   ', 'sport_code': 'Zp92fnv'},\n",
       " {'sport_type': ' hip-hop   ', 'sport_code': 'Zkn9r3g'},\n",
       " {'sport_type': ' hockey balle   ', 'sport_code': 'Zczyv2k'},\n",
       " {'sport_type': ' hockey russe   ', 'sport_code': 'Z1yldvec'},\n",
       " {'sport_type': ' hockey sur gazon   ', 'sport_code': 'Z1nd92mj'},\n",
       " {'sport_type': ' hockey sur glace   ', 'sport_code': 'Zfu1y6h'},\n",
       " {'sport_type': ' house dance   ', 'sport_code': 'Zncyh2'},\n",
       " {'sport_type': ' jiu-jitsu   ', 'sport_code': 'Zfbh0ur'},\n",
       " {'sport_type': ' jogging   ', 'sport_code': 'Zv2ki6b'},\n",
       " {'sport_type': ' jokari   ', 'sport_code': 'Z1k42cii'},\n",
       " {'sport_type': ' judo   ', 'sport_code': 'Z30obkk'},\n",
       " {'sport_type': ' karaté   ', 'sport_code': 'Z14bay1b'},\n",
       " {'sport_type': ' kempo   ', 'sport_code': 'Zk3afz0'},\n",
       " {'sport_type': ' kendo   ', 'sport_code': 'Z1bu7x1a'},\n",
       " {'sport_type': ' kick boxing   ', 'sport_code': 'Zl9itk9'},\n",
       " {'sport_type': ' kitesurf freeride   ', 'sport_code': 'Zbtmbwn'},\n",
       " {'sport_type': ' kitesurf freestyle   ', 'sport_code': 'Z1po6lfa'},\n",
       " {'sport_type': ' kitesurf race   ', 'sport_code': 'Zo1filq'},\n",
       " {'sport_type': ' kitesurf vagues   ', 'sport_code': 'Zqijg1d'},\n",
       " {'sport_type': ' kitewing   ', 'sport_code': 'Z4z96jz'},\n",
       " {'sport_type': ' kubb   ', 'sport_code': 'Z6uij0u'},\n",
       " {'sport_type': ' kung fu   ', 'sport_code': 'Z1l94zn5'},\n",
       " {'sport_type': ' lacrosse   ', 'sport_code': 'Z1ph0760'},\n",
       " {'sport_type': ' landkite   ', 'sport_code': 'Z3nr9yn'},\n",
       " {'sport_type': ' lia   ', 'sport_code': 'Zeoirff'},\n",
       " {'sport_type': ' locking   ', 'sport_code': 'Z1jm6lsx'},\n",
       " {'sport_type': ' longboard skate   ', 'sport_code': 'Z1n7iod'},\n",
       " {'sport_type': ' luge   ', 'sport_code': 'Z1024thx'},\n",
       " {'sport_type': ' lutte   ', 'sport_code': 'Z1fjdjak'},\n",
       " {'sport_type': ' marche athlétique de vitesse   ', 'sport_code': 'Zg9zdvv'},\n",
       " {'sport_type': ' marche athlétique longue distance   ',\n",
       "  'sport_code': 'Zc9ykmq'},\n",
       " {'sport_type': ' marche nordique   ', 'sport_code': 'Z1cilvx4'},\n",
       " {'sport_type': ' marche sportive   ', 'sport_code': 'Z1hb2txt'},\n",
       " {'sport_type': ' mountain touring   ', 'sport_code': 'Ziz4qs1'},\n",
       " {'sport_type': ' muay thaï   ', 'sport_code': 'Zpykkh2'},\n",
       " {'sport_type': ' musculation   ', 'sport_code': 'Z1mepo0o'},\n",
       " {'sport_type': ' nage eau libre   ', 'sport_code': 'Z1lqkn9u'},\n",
       " {'sport_type': ' natation sportive   ', 'sport_code': 'Z1hq2uvf'},\n",
       " {'sport_type': ' natation synchronisée artistique   ',\n",
       "  'sport_code': 'Z76popc'},\n",
       " {'sport_type': ' netball   ', 'sport_code': 'Z11ap84p'},\n",
       " {'sport_type': ' observation de la faune   ', 'sport_code': 'Zf7oenp'},\n",
       " {'sport_type': ' one wall   ', 'sport_code': 'Zgsldne'},\n",
       " {'sport_type': ' padel   ', 'sport_code': 'Zrrfoyu'},\n",
       " {'sport_type': ' pala gomme pleine   ', 'sport_code': 'Z17kl5wn'},\n",
       " {'sport_type': ' paleta cuir   ', 'sport_code': 'Z18h4gvn'},\n",
       " {'sport_type': ' palets breton   ', 'sport_code': 'Z9tb8cz'},\n",
       " {'sport_type': ' palets vendéens   ', 'sport_code': 'Z41swlc'},\n",
       " {'sport_type': ' parkour   ', 'sport_code': 'Zz3nfsx'},\n",
       " {'sport_type': ' pasaka   ', 'sport_code': 'Z1rqrgvp'},\n",
       " {'sport_type': ' patinage artistique   ', 'sport_code': 'Zez8lmj'},\n",
       " {'sport_type': ' patinage sur glace   ', 'sport_code': 'Z13hcfgo'},\n",
       " {'sport_type': \" pêche a l'anglaise   \", 'sport_code': 'Zgxfhev'},\n",
       " {'sport_type': ' pêche a la bolognaise   ', 'sport_code': 'Zjcs53u'},\n",
       " {'sport_type': ' pêche a la main   ', 'sport_code': 'Zgkjphv'},\n",
       " {'sport_type': ' pêche a la mouche   ', 'sport_code': 'Z4mrfv7'},\n",
       " {'sport_type': ' pêche a la traine   ', 'sport_code': 'Z15iprkb'},\n",
       " {'sport_type': ' pêche à pied   ', 'sport_code': 'Z1tsn4ot'},\n",
       " {'sport_type': ' pêche à soutenir bateau   ', 'sport_code': 'Z11hwcxg'},\n",
       " {'sport_type': ' pêche au coup chinoise   ', 'sport_code': 'Z1194oqf'},\n",
       " {'sport_type': ' pêche au coup emmanchement   ', 'sport_code': 'Z1401pju'},\n",
       " {'sport_type': ' pêche au coup télescopique   ', 'sport_code': 'Z18ofiam'},\n",
       " {'sport_type': ' pêche au flotteur en mer   ', 'sport_code': 'Zt979cd'},\n",
       " {'sport_type': ' pêche au jigging   ', 'sport_code': 'Z1e163jq'},\n",
       " {'sport_type': ' pêche au leurres en mer   ', 'sport_code': 'Z1yiaqua'},\n",
       " {'sport_type': ' pêche au poisson mort manie   ', 'sport_code': 'Z1gj5gk5'},\n",
       " {'sport_type': ' pêche au pose   ', 'sport_code': 'Z1ly9zks'},\n",
       " {'sport_type': ' pêche au posé en bord de mer   ', 'sport_code': 'Zj2jc91'},\n",
       " {'sport_type': ' pêche au quiver / feeder   ', 'sport_code': 'Zgfn7jg'},\n",
       " {'sport_type': ' pêche au vairon manié   ', 'sport_code': 'Z1w7uzjn'},\n",
       " {'sport_type': ' pêche aux leurres black bass   ', 'sport_code': 'Z14odic7'},\n",
       " {'sport_type': ' pêche aux leurres brochet   ', 'sport_code': 'Z1qvjmsz'},\n",
       " {'sport_type': ' pêche aux leurres carnassier   ', 'sport_code': 'Z16awwhy'},\n",
       " {'sport_type': ' pêche aux leurres perche   ', 'sport_code': 'Z9w1ph7'},\n",
       " {'sport_type': ' pêche aux leurres sandre   ', 'sport_code': 'Zlyp7dk'},\n",
       " {'sport_type': ' pêche aux leurres truite   ', 'sport_code': 'Z1vxzq1z'},\n",
       " {'sport_type': \" pêche de l'ecrevisse   \", 'sport_code': 'Z1i9c8l5'},\n",
       " {'sport_type': ' pêche de la carpe   ', 'sport_code': 'Z1nvndgq'},\n",
       " {'sport_type': ' pêche de la carpe au coup   ', 'sport_code': 'Zbuzovq'},\n",
       " {'sport_type': ' pêche de la truite a la bombette   ',\n",
       "  'sport_code': 'Zj342ds'},\n",
       " {'sport_type': ' pêche de la truite au toc   ', 'sport_code': 'Z1tfaxkp'},\n",
       " {'sport_type': ' pêche de la truite en étang   ', 'sport_code': 'Z1s9ndqi'},\n",
       " {'sport_type': ' pêche de seiches et calamars   ', 'sport_code': 'Z9xv7of'},\n",
       " {'sport_type': ' pêche du silure   ', 'sport_code': 'Z1cxdmhm'},\n",
       " {'sport_type': ' pêche en surf casting   ', 'sport_code': 'Z104wiy9'},\n",
       " {'sport_type': ' pêche sous la glace   ', 'sport_code': 'Zqe29s1'},\n",
       " {'sport_type': ' pelote   ', 'sport_code': 'Z18adb4a'},\n",
       " {'sport_type': ' pelote à main nue   ', 'sport_code': 'Z1255i3a'},\n",
       " {'sport_type': ' pétanque   ', 'sport_code': 'Zyusf7a'},\n",
       " {'sport_type': ' peteca   ', 'sport_code': 'Z1p6mfzv'},\n",
       " {'sport_type': ' pickleball   ', 'sport_code': 'Z17e3dnq'},\n",
       " {'sport_type': ' pilates   ', 'sport_code': 'Zi2j2zi'},\n",
       " {'sport_type': ' plongée bouteille   ', 'sport_code': 'Z1fp0w52'},\n",
       " {'sport_type': ' plongeon   ', 'sport_code': 'Z2536pn'},\n",
       " {'sport_type': ' pole dance   ', 'sport_code': 'Zhkn5nm'},\n",
       " {'sport_type': ' pony games   ', 'sport_code': 'Z19mbo52'},\n",
       " {'sport_type': ' popping   ', 'sport_code': 'Zz2pk0e'},\n",
       " {'sport_type': ' quad agressif   ', 'sport_code': 'Z1uic2es'},\n",
       " {'sport_type': ' quad artistique   ', 'sport_code': 'Zib0zdm'},\n",
       " {'sport_type': ' quad fitness   ', 'sport_code': 'Z180xpt2'},\n",
       " {'sport_type': ' quad freeride   ', 'sport_code': 'Z6ancao'},\n",
       " {'sport_type': ' quad vitesse   ', 'sport_code': 'Z11qeutp'},\n",
       " {'sport_type': ' quilles finlandaises   ', 'sport_code': 'Z189jh2c'},\n",
       " {'sport_type': ' racquetball   ', 'sport_code': 'Z98oqdm'},\n",
       " {'sport_type': ' randonnée à cheval   ', 'sport_code': 'Z2lmy84'},\n",
       " {'sport_type': ' randonnée glaciaire   ', 'sport_code': 'Z16e5u9g'},\n",
       " {'sport_type': ' randonnée montagne   ', 'sport_code': 'Z1hjiuhn'},\n",
       " {'sport_type': ' randonnée nature   ', 'sport_code': 'Zgefh29'},\n",
       " {'sport_type': ' randonnée neige   ', 'sport_code': 'Zt20l35'},\n",
       " {'sport_type': ' randonnée rapide   ', 'sport_code': 'Z17n1avj'},\n",
       " {'sport_type': ' rebot   ', 'sport_code': 'Z18tc8kp'},\n",
       " {'sport_type': ' régate   ', 'sport_code': 'Z8jgssn'},\n",
       " {'sport_type': ' resort touring   ', 'sport_code': 'Z18rwzt7'},\n",
       " {'sport_type': ' rink hockey   ', 'sport_code': 'Zwkom9y'},\n",
       " {'sport_type': ' roller agressif   ', 'sport_code': 'Z1etwncg'},\n",
       " {'sport_type': ' roller artistique   ', 'sport_code': 'Z1qm69mt'},\n",
       " {'sport_type': ' roller derby   ', 'sport_code': 'Z16k4bvo'},\n",
       " {'sport_type': ' roller fitness   ', 'sport_code': 'Z1y21l4y'},\n",
       " {'sport_type': ' roller freeride   ', 'sport_code': 'Zpnb5ve'},\n",
       " {'sport_type': ' roller freestyle   ', 'sport_code': 'Zwyy80f'},\n",
       " {'sport_type': ' roller hockey   ', 'sport_code': 'Z1066l8s'},\n",
       " {'sport_type': ' roller vitesse   ', 'sport_code': 'Znb81rg'},\n",
       " {'sport_type': ' rpm   ', 'sport_code': 'Z1nftr8z'},\n",
       " {'sport_type': ' rugby à 13   ', 'sport_code': 'Zov9s1l'},\n",
       " {'sport_type': ' rugby à 15   ', 'sport_code': 'Zlfmb30'},\n",
       " {'sport_type': ' rugby à 7   ', 'sport_code': 'Z17umjsz'},\n",
       " {'sport_type': ' running route   ', 'sport_code': 'Z10mtlnh'},\n",
       " {'sport_type': ' sambo   ', 'sport_code': 'Znypwnu'},\n",
       " {'sport_type': ' sauvetage sportif   ', 'sport_code': 'Ztav3dw'},\n",
       " {'sport_type': ' self defense   ', 'sport_code': 'Z1f50vu7'},\n",
       " {'sport_type': ' sepak takraw   ', 'sport_code': 'Zyxtphd'},\n",
       " {'sport_type': \" sh'bam   \", 'sport_code': 'Z1p4zei2'},\n",
       " {'sport_type': ' skateboard   ', 'sport_code': 'Z1lj1lqr'},\n",
       " {'sport_type': ' ski de fond a roue   ', 'sport_code': 'Z78em1l'},\n",
       " {'sport_type': ' ski de fond alternatif   ', 'sport_code': 'Zc43k03'},\n",
       " {'sport_type': ' ski de fond skating   ', 'sport_code': 'Z1y3hywi'},\n",
       " {'sport_type': ' ski de piste   ', 'sport_code': 'Z1dmrfuh'},\n",
       " {'sport_type': ' ski freeride   ', 'sport_code': 'Ztc34qh'},\n",
       " {'sport_type': ' ski freestyle   ', 'sport_code': 'Z1tftiv4'},\n",
       " {'sport_type': ' ski nautique   ', 'sport_code': 'Zr9s2ot'},\n",
       " {'sport_type': ' skimboard   ', 'sport_code': 'Zv768v5'},\n",
       " {'sport_type': ' slackline   ', 'sport_code': 'Zjngmn9'},\n",
       " {'sport_type': ' snooker   ', 'sport_code': 'Z1vcpxhx'},\n",
       " {'sport_type': ' snorkeling   ', 'sport_code': 'Zfb1vyu'},\n",
       " {'sport_type': ' snowboard all mountain   ', 'sport_code': 'Zgr4pom'},\n",
       " {'sport_type': ' snowboard carving   ', 'sport_code': 'Z4dodqu'},\n",
       " {'sport_type': ' snowboard freeride   ', 'sport_code': 'Zqmw656'},\n",
       " {'sport_type': ' snowboard freestyle   ', 'sport_code': 'Z157dx91'},\n",
       " {'sport_type': ' snowboard jib   ', 'sport_code': 'Zyjr9qj'},\n",
       " {'sport_type': ' snowboard split   ', 'sport_code': 'Z1qgsby9'},\n",
       " {'sport_type': ' snowkite   ', 'sport_code': 'Z14uqhms'},\n",
       " {'sport_type': ' softball   ', 'sport_code': 'Z12uowo8'},\n",
       " {'sport_type': ' speedball   ', 'sport_code': 'Z1lortsr'},\n",
       " {'sport_type': ' spéléologie   ', 'sport_code': 'Z1izxgz'},\n",
       " {'sport_type': ' squash   ', 'sport_code': 'Z1nddp65'},\n",
       " {'sport_type': ' stand up paddle course   ', 'sport_code': 'Z6n5g26'},\n",
       " {'sport_type': ' stand up paddle randonnée   ', 'sport_code': 'Z1g965rv'},\n",
       " {'sport_type': ' stand up paddle vague   ', 'sport_code': 'Zs4y1xl'},\n",
       " {'sport_type': ' step   ', 'sport_code': 'Zygawq6'},\n",
       " {'sport_type': ' street hockey   ', 'sport_code': 'Zb4l1c3'},\n",
       " {'sport_type': ' street jazz   ', 'sport_code': 'Z1k098sb'},\n",
       " {'sport_type': ' stretching\\u200e   ', 'sport_code': 'Z11qbfkl'},\n",
       " {'sport_type': ' surf   ', 'sport_code': 'Z13fnxz6'},\n",
       " {'sport_type': ' swimrun   ', 'sport_code': 'Z1h1ojpa'},\n",
       " {'sport_type': ' taekwondo   ', 'sport_code': 'Z9wjnn3'},\n",
       " {'sport_type': ' taï chi chuan   ', 'sport_code': 'Zl08mpv'},\n",
       " {'sport_type': ' tennis   ', 'sport_code': 'Znprbzh'},\n",
       " {'sport_type': ' tennis de table   ', 'sport_code': 'Z1yv2v44'},\n",
       " {'sport_type': \" tir à l'arc   \", 'sport_code': 'Z14vpsjf'},\n",
       " {'sport_type': ' tir sportif 22 long rifle   ', 'sport_code': 'Zmcokbr'},\n",
       " {'sport_type': ' tir sportif air comprimé   ', 'sport_code': 'Zas0gxc'},\n",
       " {'sport_type': ' tonification   ', 'sport_code': 'Z11wxic'},\n",
       " {'sport_type': ' touch rugby   ', 'sport_code': 'Z1x9ktma'},\n",
       " {'sport_type': ' trail   ', 'sport_code': 'Z1m3am8s'},\n",
       " {'sport_type': ' trampoline   ', 'sport_code': 'Zdzydve'},\n",
       " {'sport_type': ' trekking arctique   ', 'sport_code': 'Z7nc91w'},\n",
       " {'sport_type': ' trekking désert   ', 'sport_code': 'Zwun2p4'},\n",
       " {'sport_type': ' trekking montagne   ', 'sport_code': 'Z1wyp8i8'},\n",
       " {'sport_type': ' trekking tropical   ', 'sport_code': 'Z1yohc3s'},\n",
       " {'sport_type': ' trekking voyage   ', 'sport_code': 'Z1lnjjzm'},\n",
       " {'sport_type': ' triathlon   ', 'sport_code': 'Z1ipchpc'},\n",
       " {'sport_type': ' trottinette   ', 'sport_code': 'Z99d2z6'},\n",
       " {'sport_type': ' trottinette freestyle   ', 'sport_code': 'Zow6ej0'},\n",
       " {'sport_type': ' ultimate   ', 'sport_code': 'Zbna9cb'},\n",
       " {'sport_type': ' vélo tout chemin randonnée   ', 'sport_code': 'Zwblefr'},\n",
       " {'sport_type': ' vélo ville   ', 'sport_code': 'Zibvvg6'},\n",
       " {'sport_type': ' vélo voyage   ', 'sport_code': 'Zdapd5n'},\n",
       " {'sport_type': ' via ferrata   ', 'sport_code': 'Z109og2v'},\n",
       " {'sport_type': ' video clip   ', 'sport_code': 'Z17ybap0'},\n",
       " {'sport_type': ' voile habitable   ', 'sport_code': 'Z72lnrm'},\n",
       " {'sport_type': ' volleyball   ', 'sport_code': 'Z1y7tsls'},\n",
       " {'sport_type': ' vtt all mountain   ', 'sport_code': 'Z1969jq7'},\n",
       " {'sport_type': ' vtt cross country   ', 'sport_code': 'Z1lnnney'},\n",
       " {'sport_type': ' vtt randonnée   ', 'sport_code': 'Z1e9190j'},\n",
       " {'sport_type': ' waacking   ', 'sport_code': 'Z1nd1lhl'},\n",
       " {'sport_type': ' wakeboard   ', 'sport_code': 'Z178e06x'},\n",
       " {'sport_type': ' waterpolo   ', 'sport_code': 'Zyevn41'},\n",
       " {'sport_type': ' waveboard   ', 'sport_code': 'Z17ufcs1'},\n",
       " {'sport_type': ' windsurf free ride   ', 'sport_code': 'Z1yftmz0'},\n",
       " {'sport_type': ' windsurf freestyle   ', 'sport_code': 'Zwvpflf'},\n",
       " {'sport_type': ' windsurf slalom   ', 'sport_code': 'Z1gnn2p9'},\n",
       " {'sport_type': ' windsurf wave   ', 'sport_code': 'Z39b119'},\n",
       " {'sport_type': ' xare   ', 'sport_code': 'Z1pblw28'},\n",
       " {'sport_type': ' yoga   ', 'sport_code': 'Zcxu9cf'},\n",
       " {'sport_type': ' zumba   ', 'sport_code': 'Zj0ez0b'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sport_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e1ce51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get category links\n",
    "def getCatLink(base_index, base_cat, sold_by, cat_list):\n",
    "    url = f'{base_index}{base_cat}{sold_by}'\n",
    "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
    "    soup = bts(result.text, 'html.parser')\n",
    "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
    "    for cat in categories[0: len(categories) - 1]:\n",
    "        level_1 = soup.find(\"h1\").text\n",
    "        level_2 = cat.attrs['data-help']\n",
    "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
    "        cat_list.append({'level_1': level_1,\n",
    "                         'level_2': level_2,\n",
    "                         'cat_link': cat_link})\n",
    "    \n",
    "    #remove category \"bons plans\" from being scraped\n",
    "    cat_list = [cat for cat in cat_list if cat['level_2'] != 'Bons plans']\n",
    "    \n",
    "    return cat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "703df522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing category_url data\n",
    "cat_url = []  \n",
    "# opening the file using \"with\" statement\n",
    "with open('FR_cat_url.csv', 'r') as cat_data:\n",
    "    for cat in csv.DictReader(cat_data):\n",
    "        cat_url.append(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c177fd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cat': 'fem',\n",
       "  'base_cat': '/browse/c0-femme/_/N-ry4jwt',\n",
       "  'base_index': 'https://www.decathlon.fr',\n",
       "  'sold_by': 'Z1o76joc'},\n",
       " {'cat': 'male',\n",
       "  'base_cat': '/browse/c0-homme/_/N-1qu1ue2',\n",
       "  'base_index': 'https://www.decathlon.fr',\n",
       "  'sold_by': 'Z1o76joc'},\n",
       " {'cat': 'child',\n",
       "  'base_cat': '/browse/c0-enfant/_/N-14j5j37',\n",
       "  'base_index': 'https://www.decathlon.fr',\n",
       "  'sold_by': 'Z1o76joc'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fa05133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting category links for all genders:\n",
    "cat_list =[]\n",
    "for i in cat_url:\n",
    "    base_index = i[\"base_index\"]\n",
    "    base_cat = i[\"base_cat\"]\n",
    "    sold_by = i[\"sold_by\"]\n",
    "    cat_list = getCatLink(base_index, base_cat, sold_by, cat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3271c33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'level_1': 'Vêtements et chaussures femme',\n",
       "  'level_2': 'Vêtements',\n",
       "  'cat_link': 'https://www.decathlon.fr/browse/c0-femme/c1-vetements/_/N-kznd1yZ1o76joc'},\n",
       " {'level_1': 'Vêtements et chaussures femme',\n",
       "  'level_2': 'Chaussures',\n",
       "  'cat_link': 'https://www.decathlon.fr/browse/c0-femme/c1-chaussant/_/N-1ntm14cZ1o76joc'},\n",
       " {'level_1': 'Vêtements et chaussures femme',\n",
       "  'level_2': 'Accessoires',\n",
       "  'cat_link': 'https://www.decathlon.fr/browse/c0-femme/c1-accessoires/_/N-17i68zsZ1o76joc'},\n",
       " {'level_1': 'Vêtements et chaussures homme',\n",
       "  'level_2': 'Vêtements',\n",
       "  'cat_link': 'https://www.decathlon.fr/browse/c0-homme/c1-vetements/_/N-8fyx2oZ1o76joc'},\n",
       " {'level_1': 'Vêtements et chaussures homme',\n",
       "  'level_2': 'Chaussures',\n",
       "  'cat_link': 'https://www.decathlon.fr/browse/c0-homme/c1-chaussant/_/N-sukxpgZ1o76joc'},\n",
       " {'level_1': 'Vêtements et chaussures homme',\n",
       "  'level_2': 'Accessoires',\n",
       "  'cat_link': 'https://www.decathlon.fr/browse/c0-homme/c1-accessoires/_/N-n7l046Z1o76joc'},\n",
       " {'level_1': 'Vêtements, chaussures et matériel enfant',\n",
       "  'level_2': 'Vêtements',\n",
       "  'cat_link': 'https://www.decathlon.fr/browse/c0-enfant/c1-vetements/_/N-1u4s44iZ1o76joc'},\n",
       " {'level_1': 'Vêtements, chaussures et matériel enfant',\n",
       "  'level_2': 'Chaussures',\n",
       "  'cat_link': 'https://www.decathlon.fr/browse/c0-enfant/c1-chaussant/_/N-1p1qpc0Z1o76joc'},\n",
       " {'level_1': 'Vêtements, chaussures et matériel enfant',\n",
       "  'level_2': 'Matériel',\n",
       "  'cat_link': 'https://www.decathlon.fr/browse/c0-enfant/c1-materiel/_/N-1r2otkyZ1o76joc'},\n",
       " {'level_1': 'Vêtements, chaussures et matériel enfant',\n",
       "  'level_2': 'Accessoires',\n",
       "  'cat_link': 'https://www.decathlon.fr/browse/c0-enfant/c1-accessoires-enfant/_/N-19rt29kZ1o76joc'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee25aeb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74aa3d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine link for category level 2 and filtered by sport\n",
    "\n",
    "cat_level3=[]\n",
    "\n",
    "for cat in cat_list:\n",
    "    for sport in sport_list:\n",
    "        country = country\n",
    "        level_1 = cat[\"level_1\"]\n",
    "        level_2 = cat[\"level_2\"]\n",
    "        sport_type = sport[\"sport_type\"]\n",
    "        url = f'{cat[\"cat_link\"]}{sport[\"sport_code\"]}'\n",
    "        cat_level3.append({'country': country,\n",
    "                           'cat1': level_1,\n",
    "                           'cat2': level_2,\n",
    "                           'cat3': sport_type,\n",
    "                           'cat_url': url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de4ef203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': 'FR',\n",
       " 'cat1': 'Vêtements et chaussures femme',\n",
       " 'cat2': 'Vêtements',\n",
       " 'cat3': ' afrocafit   ',\n",
       " 'cat_url': 'https://www.decathlon.fr/browse/c0-femme/c1-vetements/_/N-kznd1yZ1o76jocZ1apkgwr'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_level3[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12aa1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving cat_level3\n",
    "\n",
    "with open(f\"{country}_cat_level3.csv\", \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter = \",\")\n",
    "    writer.writerow([\"country\",\"cat1\", \"cat2\", \"cat3\", \"cat_url\"])\n",
    "\n",
    "    for item in cat_level3:\n",
    "        writer.writerow([item['country'], item['cat1'], item['cat2'], item['cat3'], item['cat_url']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9771e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for parsing the URLs\n",
    "def cookSoup(url): \n",
    "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
    "    soup = bts(result.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3336a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for pagination - creating a list of urls from a category\n",
    "def pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page):\n",
    "    url_list = [cat_url]\n",
    "    total_prod = soup.find(\"div\",{\"class\":f\"plp-bar-info svelte-{total_page_src}\"}).find(\"span\", {\"class\":f\"svelte-{total_page_src}\"}).text\n",
    "    #Create list of urls within the cat\n",
    "    total_page = math.ceil(int(total_prod)/per_page)\n",
    "    for i in range(1, total_page + 1):\n",
    "        page = f'{cat_url}?from={per_page * i}&size={per_page}'\n",
    "        url_list.append(page)\n",
    "    print (f'{country}_{cat1}_{cat2}_{cat3}: There are {total_prod} products ({total_page} pages)')\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "722e1a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get main data\n",
    "def getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src):\n",
    "    for url in url_list:\n",
    "        page_soup = cookSoup(url)\n",
    "        prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
    "        \n",
    "        for product in prod:\n",
    "            cat1 = cat1\n",
    "            cat2 = cat2\n",
    "            sport = cat3\n",
    "            link = product.find(\"a\").attrs[\"href\"]\n",
    "            prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
    "            brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
    "            prod_url = f'{base_index}{link}'\n",
    "\n",
    "            #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
    "            #taking sku's even in case of more than 7 character id's:\n",
    "\n",
    "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
    "                prod_id = link.partition(\"?mc=\")[2]\n",
    "            else:\n",
    "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
    "\n",
    "            #Prices:\n",
    "            #for product without discount\n",
    "            if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
    "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
    "                act_price = None\n",
    "\n",
    "            #for product with discount\n",
    "            else:\n",
    "                if product.find(\"span\", {\"class\":\"prc__previous\"}) == None:\n",
    "                    reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
    "                    act_price = None\n",
    "                else:\n",
    "                    reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
    "                    act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
    "\n",
    "            #label:\n",
    "            if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
    "                prod_sticker = None\n",
    "            else:\n",
    "                prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
    "\n",
    "            prod_list.append({'title': prod_title,\n",
    "                             'sku': prod_id,\n",
    "                             'regular price': reg_price,\n",
    "                             'actual price' : act_price,\n",
    "                             'brand': brand_name,\n",
    "                             'url' : prod_url,\n",
    "                             'sticker' : prod_sticker,\n",
    "                             'cat_1' : cat1,\n",
    "                             'cat_2' : cat2,\n",
    "                             'cat_3' : cat3})\n",
    "\n",
    "    print(f'{country}: {len(prod_list)} products have been scraped!')\n",
    "        \n",
    "    return prod_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81558764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR_Vêtements et chaussures femme_Vêtements_ 3x3 basketball  : There are 0 products (0 pages)\n",
      "FR: 0 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ afrocafit   : There are 3 products (1 pages)\n",
      "FR: 3 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ aïkido   : There are 1 products (1 pages)\n",
      "FR: 4 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ alpinisme rocheux   : There are 21 products (1 pages)\n",
      "FR: 25 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ apnée   : There are 0 products (0 pages)\n",
      "FR: 25 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ aqua cross training   : There are 0 products (0 pages)\n",
      "FR: 25 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ aquabike   : There are 28 products (1 pages)\n",
      "FR: 53 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ aquafitness   : There are 40 products (1 pages)\n",
      "FR: 93 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ arnis   : There are 0 products (0 pages)\n",
      "FR: 93 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ athlétisme   : There are 38 products (1 pages)\n",
      "FR: 131 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ aviron   : There are 0 products (0 pages)\n",
      "FR: 131 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ badminton   : There are 32 products (1 pages)\n",
      "FR: 163 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ balade à cheval   : There are 7 products (1 pages)\n",
      "FR: 170 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ baline gomme creuse   : There are 1 products (1 pages)\n",
      "FR: 171 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ ball trap   : There are 0 products (0 pages)\n",
      "FR: 171 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ balle au tambourin   : There are 4 products (1 pages)\n",
      "FR: 175 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ ballet contemporain   : There are 11 products (1 pages)\n",
      "FR: 186 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ barre au sol   : There are 11 products (1 pages)\n",
      "FR: 197 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ baseball   : There are 3 products (1 pages)\n",
      "FR: 200 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ basketball   : There are 15 products (1 pages)\n",
      "FR: 215 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ bateau dragon   : There are 0 products (0 pages)\n",
      "FR: 215 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ beach rugby   : There are 16 products (1 pages)\n",
      "FR: 231 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ beach soccer   : There are 17 products (1 pages)\n",
      "FR: 248 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ beach tennis   : There are 3 products (1 pages)\n",
      "FR: 251 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ beach volley   : There are 1 products (1 pages)\n",
      "FR: 252 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ billard américain   : There are 0 products (0 pages)\n",
      "FR: 252 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ billard anglais   : There are 0 products (0 pages)\n",
      "FR: 252 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ billard français   : There are 0 products (0 pages)\n",
      "FR: 252 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ bmx freestyle   : There are 0 products (0 pages)\n",
      "FR: 252 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ body attack   : There are 49 products (2 pages)\n",
      "FR: 301 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ bodyboard   : There are 27 products (1 pages)\n",
      "FR: 328 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ bodysurf   : There are 1 products (1 pages)\n",
      "FR: 329 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ bokwa   : There are 3 products (1 pages)\n",
      "FR: 332 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ boomerang   : There are 0 products (0 pages)\n",
      "FR: 332 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ bouées tractées   : There are 0 products (0 pages)\n",
      "FR: 332 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ boxe anglaise   : There are 6 products (1 pages)\n",
      "FR: 338 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ boxe française   : There are 5 products (1 pages)\n",
      "FR: 343 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ break dance   : There are 11 products (1 pages)\n",
      "FR: 354 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ bushcraft   : There are 0 products (0 pages)\n",
      "FR: 354 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ canoë kayak course   : There are 3 products (1 pages)\n",
      "FR: 357 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ canoë kayak randonnee   : There are 3 products (1 pages)\n",
      "FR: 360 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ canoë kayak vague   : There are 3 products (1 pages)\n",
      "FR: 363 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ canyoning   : There are 0 products (0 pages)\n",
      "FR: 363 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ capoeira   : There are 0 products (0 pages)\n",
      "FR: 363 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ cardio boxing   : There are 6 products (1 pages)\n",
      "FR: 369 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ cardio training   : There are 118 products (3 pages)\n",
      "FR: 487 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ carrom   : There are 0 products (0 pages)\n",
      "FR: 487 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ cascade de glace   : There are 19 products (1 pages)\n",
      "FR: 506 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ cerf-volant de traction   : There are 1 products (1 pages)\n",
      "FR: 507 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ cerf-volant pilotable   : There are 0 products (0 pages)\n",
      "FR: 507 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ cerf-volant statique   : There are 0 products (0 pages)\n",
      "FR: 507 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ chasse à l'approche   : There are 6 products (1 pages)\n",
      "FR: 513 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ chasse à l'arc   : There are 1 products (1 pages)\n",
      "FR: 514 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ chasse au gibier d'eau   : There are 1 products (1 pages)\n",
      "FR: 515 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ chasse au grand gibier battue poste   : There are 12 products (1 pages)\n",
      "FR: 527 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ chasse au grand gibier battue traque   : There are 7 products (1 pages)\n",
      "FR: 534 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ chasse au migrateur   : There are 0 products (0 pages)\n",
      "FR: 534 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ chasse au petit gibier   : There are 21 products (1 pages)\n",
      "FR: 555 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ chasse sous marine   : There are 0 products (0 pages)\n",
      "FR: 555 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ chistera   : There are 16 products (1 pages)\n",
      "FR: 571 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ commercial hip hop   : There are 11 products (1 pages)\n",
      "FR: 582 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ commercial jazz   : There are 11 products (1 pages)\n",
      "FR: 593 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ course d'endurance   : There are 1 products (1 pages)\n",
      "FR: 594 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR_Vêtements et chaussures femme_Vêtements_ course d'orientation   : There are 20 products (1 pages)\n",
      "FR: 614 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ cricket   : There are 1 products (1 pages)\n",
      "FR: 615 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ cross training   : There are 5 products (1 pages)\n",
      "FR: 620 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ crossminton   : There are 3 products (1 pages)\n",
      "FR: 623 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ cruiser   : There are 4 products (1 pages)\n",
      "FR: 627 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ cyclosport‎   : There are 18 products (1 pages)\n",
      "FR: 645 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ cyclotourisme   : There are 21 products (1 pages)\n",
      "FR: 666 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ dancehall   : There are 11 products (1 pages)\n",
      "FR: 677 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ danse classique   : There are 10 products (1 pages)\n",
      "FR: 687 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ danse contemporaine   : There are 0 products (0 pages)\n",
      "FR: 687 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ danse modern jazz   : There are 39 products (1 pages)\n",
      "FR: 726 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ danse néo classique   : There are 11 products (1 pages)\n",
      "FR: 737 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ dériveur / catamaran   : There are 5 products (1 pages)\n",
      "FR: 742 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ disc golf   : There are 0 products (0 pages)\n",
      "FR: 742 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ disque volant   : There are 0 products (0 pages)\n",
      "FR: 742 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ djembel   : There are 3 products (1 pages)\n",
      "FR: 745 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ endurance à cheval   : There are 6 products (1 pages)\n",
      "FR: 751 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ entraînement fonctionnel   : There are 3 products (1 pages)\n",
      "FR: 754 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ équitation   : There are 51 products (2 pages)\n",
      "FR: 805 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ équitation concours complet   : There are 12 products (1 pages)\n",
      "FR: 817 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ équitation dressage   : There are 10 products (1 pages)\n",
      "FR: 827 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ équitation éthologie   : There are 9 products (1 pages)\n",
      "FR: 836 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ équitation horse ball   : There are 9 products (1 pages)\n",
      "FR: 845 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ équitation saut d'obstacles   : There are 17 products (1 pages)\n",
      "FR: 862 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ équitation western   : There are 7 products (1 pages)\n",
      "FR: 869 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ escalade en bloc   : There are 16 products (1 pages)\n",
      "FR: 885 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ escalade en falaise   : There are 22 products (1 pages)\n",
      "FR: 907 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ escalade en grandes voies   : There are 23 products (1 pages)\n",
      "FR: 930 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ escalade en salle   : There are 15 products (1 pages)\n",
      "FR: 945 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ escrime   : There are 2 products (1 pages)\n",
      "FR: 947 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ éveil aquatique   : There are 3 products (1 pages)\n",
      "FR: 950 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ expédition haute altitude   : There are 18 products (1 pages)\n",
      "FR: 968 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ fast touring   : There are 16 products (1 pages)\n",
      "FR: 984 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ fitness ballet   : There are 3 products (1 pages)\n",
      "FR: 987 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ flag football   : There are 0 products (0 pages)\n",
      "FR: 987 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ flamenco   : There are 0 products (0 pages)\n",
      "FR: 987 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ fléchettes pointe acier   : There are 0 products (0 pages)\n",
      "FR: 987 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ fléchettes pointe plastique   : There are 0 products (0 pages)\n",
      "FR: 987 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ floorball   : There are 1 products (1 pages)\n",
      "FR: 988 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ foot5   : There are 37 products (1 pages)\n",
      "FR: 1025 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ football à 11   : There are 46 products (2 pages)\n",
      "FR: 1071 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ football américain   : There are 1 products (1 pages)\n",
      "FR: 1072 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ frescobol   : There are 15 products (1 pages)\n",
      "FR: 1087 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ frontenis   : There are 20 products (1 pages)\n",
      "FR: 1107 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ full contact   : There are 5 products (1 pages)\n",
      "FR: 1112 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ funk jazz   : There are 11 products (1 pages)\n",
      "FR: 1123 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ futsal   : There are 34 products (1 pages)\n",
      "FR: 1157 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ golf   : There are 27 products (1 pages)\n",
      "FR: 1184 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ grappling   : There are 4 products (1 pages)\n",
      "FR: 1188 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ gravel‎   : There are 0 products (0 pages)\n",
      "FR: 1188 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ gym douce   : There are 98 products (3 pages)\n",
      "FR: 1286 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ gymnastique artistique   : There are 8 products (1 pages)\n",
      "FR: 1294 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ gymnastique éducative et sportive   : There are 2 products (1 pages)\n",
      "FR: 1296 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ gymnastique rythmique   : There are 7 products (1 pages)\n",
      "FR: 1303 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ gymnastique suédoise   : There are 94 products (3 pages)\n",
      "FR: 1397 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ gymnastique volontaire   : There are 3 products (1 pages)\n",
      "FR: 1400 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ handball   : There are 13 products (1 pages)\n",
      "FR: 1413 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ hip-hop   : There are 11 products (1 pages)\n",
      "FR: 1424 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ hockey balle   : There are 2 products (1 pages)\n",
      "FR: 1426 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ hockey russe   : There are 1 products (1 pages)\n",
      "FR: 1427 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ hockey sur gazon   : There are 5 products (1 pages)\n",
      "FR: 1432 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ hockey sur glace   : There are 1 products (1 pages)\n",
      "FR: 1433 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ house dance   : There are 11 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 1444 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ jiu-jitsu   : There are 3 products (1 pages)\n",
      "FR: 1447 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ jogging   : There are 77 products (2 pages)\n",
      "FR: 1524 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ jokari   : There are 16 products (1 pages)\n",
      "FR: 1540 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ judo   : There are 0 products (0 pages)\n",
      "FR: 1540 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ karaté   : There are 2 products (1 pages)\n",
      "FR: 1542 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ kempo   : There are 0 products (0 pages)\n",
      "FR: 1542 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ kendo   : There are 0 products (0 pages)\n",
      "FR: 1542 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ kick boxing   : There are 5 products (1 pages)\n",
      "FR: 1547 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ kitesurf freeride   : There are 2 products (1 pages)\n",
      "FR: 1549 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ kitesurf freestyle   : There are 2 products (1 pages)\n",
      "FR: 1551 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ kitesurf race   : There are 2 products (1 pages)\n",
      "FR: 1553 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ kitesurf vagues   : There are 2 products (1 pages)\n",
      "FR: 1555 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ kitewing   : There are 0 products (0 pages)\n",
      "FR: 1555 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ kubb   : There are 0 products (0 pages)\n",
      "FR: 1555 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ kung fu   : There are 0 products (0 pages)\n",
      "FR: 1555 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ lacrosse   : There are 1 products (1 pages)\n",
      "FR: 1556 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ landkite   : There are 1 products (1 pages)\n",
      "FR: 1557 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ lia   : There are 49 products (2 pages)\n",
      "FR: 1606 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ locking   : There are 11 products (1 pages)\n",
      "FR: 1617 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ longboard skate   : There are 8 products (1 pages)\n",
      "FR: 1625 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ luge   : There are 20 products (1 pages)\n",
      "FR: 1645 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ lutte   : There are 3 products (1 pages)\n",
      "FR: 1648 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ marche athlétique de vitesse   : There are 32 products (1 pages)\n",
      "FR: 1680 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ marche athlétique longue distance   : There are 29 products (1 pages)\n",
      "FR: 1709 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ marche nordique   : There are 35 products (1 pages)\n",
      "FR: 1744 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ marche sportive   : There are 41 products (2 pages)\n",
      "FR: 1785 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ mountain touring   : There are 20 products (1 pages)\n",
      "FR: 1805 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ muay thaï   : There are 5 products (1 pages)\n",
      "FR: 1810 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ musculation   : There are 1 products (1 pages)\n",
      "FR: 1811 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ nage eau libre   : There are 17 products (1 pages)\n",
      "FR: 1828 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ natation sportive   : There are 60 products (2 pages)\n",
      "FR: 1888 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ natation synchronisée artistique   : There are 20 products (1 pages)\n",
      "FR: 1908 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ netball   : There are 1 products (1 pages)\n",
      "FR: 1909 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ observation de la faune   : There are 0 products (0 pages)\n",
      "FR: 1909 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ one wall   : There are 1 products (1 pages)\n",
      "FR: 1910 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ padel   : There are 56 products (2 pages)\n",
      "FR: 1966 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pala gomme pleine   : There are 16 products (1 pages)\n",
      "FR: 1982 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ paleta cuir   : There are 0 products (0 pages)\n",
      "FR: 1982 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ palets breton   : There are 0 products (0 pages)\n",
      "FR: 1982 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ palets vendéens   : There are 0 products (0 pages)\n",
      "FR: 1982 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ parkour   : There are 0 products (0 pages)\n",
      "FR: 1982 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pasaka   : There are 16 products (1 pages)\n",
      "FR: 1998 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ patinage artistique   : There are 5 products (1 pages)\n",
      "FR: 2003 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ patinage sur glace   : There are 0 products (0 pages)\n",
      "FR: 2003 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche a l'anglaise   : There are 0 products (0 pages)\n",
      "FR: 2003 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche a la bolognaise   : There are 0 products (0 pages)\n",
      "FR: 2003 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche a la main   : There are 0 products (0 pages)\n",
      "FR: 2003 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche a la mouche   : There are 2 products (1 pages)\n",
      "FR: 2005 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche a la traine   : There are 0 products (0 pages)\n",
      "FR: 2005 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche à pied   : There are 0 products (0 pages)\n",
      "FR: 2005 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche à soutenir bateau   : There are 0 products (0 pages)\n",
      "FR: 2005 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche au coup chinoise   : There are 0 products (0 pages)\n",
      "FR: 2005 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche au coup emmanchement   : There are 0 products (0 pages)\n",
      "FR: 2005 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche au coup télescopique   : There are 0 products (0 pages)\n",
      "FR: 2005 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche au flotteur en mer   : There are 0 products (0 pages)\n",
      "FR: 2005 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche au jigging   : There are 0 products (0 pages)\n",
      "FR: 2005 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche au leurres en mer   : There are 4 products (1 pages)\n",
      "FR: 2009 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche au poisson mort manie   : There are 0 products (0 pages)\n",
      "FR: 2009 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche au pose   : There are 3 products (1 pages)\n",
      "FR: 2012 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche au posé en bord de mer   : There are 2 products (1 pages)\n",
      "FR: 2014 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche au quiver / feeder   : There are 0 products (0 pages)\n",
      "FR: 2014 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche au vairon manié   : There are 0 products (0 pages)\n",
      "FR: 2014 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR_Vêtements et chaussures femme_Vêtements_ pêche aux leurres black bass   : There are 0 products (0 pages)\n",
      "FR: 2014 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche aux leurres brochet   : There are 3 products (1 pages)\n",
      "FR: 2017 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche aux leurres carnassier   : There are 5 products (1 pages)\n",
      "FR: 2022 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche aux leurres perche   : There are 0 products (0 pages)\n",
      "FR: 2022 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche aux leurres sandre   : There are 0 products (0 pages)\n",
      "FR: 2022 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche aux leurres truite   : There are 4 products (1 pages)\n",
      "FR: 2026 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche de l'ecrevisse   : There are 1 products (1 pages)\n",
      "FR: 2027 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche de la carpe   : There are 2 products (1 pages)\n",
      "FR: 2029 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche de la carpe au coup   : There are 0 products (0 pages)\n",
      "FR: 2029 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche de la truite a la bombette   : There are 0 products (0 pages)\n",
      "FR: 2029 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche de la truite au toc   : There are 2 products (1 pages)\n",
      "FR: 2031 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche de la truite en étang   : There are 1 products (1 pages)\n",
      "FR: 2032 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche de seiches et calamars   : There are 0 products (0 pages)\n",
      "FR: 2032 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche du silure   : There are 0 products (0 pages)\n",
      "FR: 2032 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche en surf casting   : There are 4 products (1 pages)\n",
      "FR: 2036 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pêche sous la glace   : There are 0 products (0 pages)\n",
      "FR: 2036 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pelote   : There are 1 products (1 pages)\n",
      "FR: 2037 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pelote à main nue   : There are 16 products (1 pages)\n",
      "FR: 2053 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pétanque   : There are 0 products (0 pages)\n",
      "FR: 2053 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ peteca   : There are 0 products (0 pages)\n",
      "FR: 2053 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pickleball   : There are 18 products (1 pages)\n",
      "FR: 2071 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pilates   : There are 166 products (5 pages)\n",
      "FR: 2237 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ plongée bouteille   : There are 1 products (1 pages)\n",
      "FR: 2238 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ plongeon   : There are 16 products (1 pages)\n",
      "FR: 2254 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pole dance   : There are 2 products (1 pages)\n",
      "FR: 2256 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ pony games   : There are 8 products (1 pages)\n",
      "FR: 2264 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ popping   : There are 11 products (1 pages)\n",
      "FR: 2275 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ quad agressif   : There are 0 products (0 pages)\n",
      "FR: 2275 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ quad artistique   : There are 5 products (1 pages)\n",
      "FR: 2280 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ quad fitness   : There are 2 products (1 pages)\n",
      "FR: 2282 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ quad freeride   : There are 2 products (1 pages)\n",
      "FR: 2284 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ quad vitesse   : There are 0 products (0 pages)\n",
      "FR: 2284 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ quilles finlandaises   : There are 0 products (0 pages)\n",
      "FR: 2284 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ racquetball   : There are 17 products (1 pages)\n",
      "FR: 2301 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ randonnée à cheval   : There are 8 products (1 pages)\n",
      "FR: 2309 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ randonnée glaciaire   : There are 20 products (1 pages)\n",
      "FR: 2329 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ randonnée montagne   : There are 77 products (2 pages)\n",
      "FR: 2406 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ randonnée nature   : There are 68 products (2 pages)\n",
      "FR: 2474 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ randonnée neige   : There are 38 products (1 pages)\n",
      "FR: 2512 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ randonnée rapide   : There are 27 products (1 pages)\n",
      "FR: 2539 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ rebot   : There are 16 products (1 pages)\n",
      "FR: 2555 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ régate   : There are 8 products (1 pages)\n",
      "FR: 2563 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ resort touring   : There are 20 products (1 pages)\n",
      "FR: 2583 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ rink hockey   : There are 2 products (1 pages)\n",
      "FR: 2585 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ roller agressif   : There are 0 products (0 pages)\n",
      "FR: 2585 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ roller artistique   : There are 0 products (0 pages)\n",
      "FR: 2585 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ roller derby   : There are 0 products (0 pages)\n",
      "FR: 2585 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ roller fitness   : There are 2 products (1 pages)\n",
      "FR: 2587 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ roller freeride   : There are 0 products (0 pages)\n",
      "FR: 2587 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ roller freestyle   : There are 0 products (0 pages)\n",
      "FR: 2587 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ roller hockey   : There are 2 products (1 pages)\n",
      "FR: 2589 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ roller vitesse   : There are 0 products (0 pages)\n",
      "FR: 2589 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ rpm   : There are 51 products (2 pages)\n",
      "FR: 2640 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ rugby à 13   : There are 18 products (1 pages)\n",
      "FR: 2658 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ rugby à 15   : There are 18 products (1 pages)\n",
      "FR: 2676 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ rugby à 7   : There are 18 products (1 pages)\n",
      "FR: 2694 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ running route   : There are 67 products (2 pages)\n",
      "FR: 2761 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ sambo   : There are 1 products (1 pages)\n",
      "FR: 2762 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ sauvetage sportif   : There are 16 products (1 pages)\n",
      "FR: 2778 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ self defense   : There are 1 products (1 pages)\n",
      "FR: 2779 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ sepak takraw   : There are 1 products (1 pages)\n",
      "FR: 2780 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ sh'bam   : There are 3 products (1 pages)\n",
      "FR: 2783 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR_Vêtements et chaussures femme_Vêtements_ skateboard   : There are 9 products (1 pages)\n",
      "FR: 2792 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ ski de fond a roue   : There are 1 products (1 pages)\n",
      "FR: 2793 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ ski de fond alternatif   : There are 11 products (1 pages)\n",
      "FR: 2804 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ ski de fond skating   : There are 11 products (1 pages)\n",
      "FR: 2815 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ ski de piste   : There are 37 products (1 pages)\n",
      "FR: 2852 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ ski freeride   : There are 28 products (1 pages)\n",
      "FR: 2880 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ ski freestyle   : There are 20 products (1 pages)\n",
      "FR: 2900 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ ski nautique   : There are 0 products (0 pages)\n",
      "FR: 2900 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ skimboard   : There are 2 products (1 pages)\n",
      "FR: 2902 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ slackline   : There are 13 products (1 pages)\n",
      "FR: 2915 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ snooker   : There are 0 products (0 pages)\n",
      "FR: 2915 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ snorkeling   : There are 0 products (0 pages)\n",
      "FR: 2915 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ snowboard all mountain   : There are 22 products (1 pages)\n",
      "FR: 2937 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ snowboard carving   : There are 1 products (1 pages)\n",
      "FR: 2938 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ snowboard freeride   : There are 22 products (1 pages)\n",
      "FR: 2960 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ snowboard freestyle   : There are 22 products (1 pages)\n",
      "FR: 2982 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ snowboard jib   : There are 1 products (1 pages)\n",
      "FR: 2983 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ snowboard split   : There are 9 products (1 pages)\n",
      "FR: 2992 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ snowkite   : There are 1 products (1 pages)\n",
      "FR: 2993 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ softball   : There are 2 products (1 pages)\n",
      "FR: 2995 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ speedball   : There are 17 products (1 pages)\n",
      "FR: 3012 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ spéléologie   : There are 0 products (0 pages)\n",
      "FR: 3012 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ squash   : There are 35 products (1 pages)\n",
      "FR: 3047 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ stand up paddle course   : There are 5 products (1 pages)\n",
      "FR: 3052 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ stand up paddle randonnée   : There are 11 products (1 pages)\n",
      "FR: 3063 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ stand up paddle vague   : There are 3 products (1 pages)\n",
      "FR: 3066 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ step   : There are 51 products (2 pages)\n",
      "FR: 3117 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ street hockey   : There are 2 products (1 pages)\n",
      "FR: 3119 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ street jazz   : There are 11 products (1 pages)\n",
      "FR: 3130 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ stretching‎   : There are 159 products (4 pages)\n",
      "FR: 3289 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ surf   : There are 155 products (4 pages)\n",
      "FR: 3444 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ swimrun   : There are 5 products (1 pages)\n",
      "FR: 3449 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ taekwondo   : There are 0 products (0 pages)\n",
      "FR: 3449 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ taï chi chuan   : There are 0 products (0 pages)\n",
      "FR: 3449 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ tennis   : There are 52 products (2 pages)\n",
      "FR: 3501 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ tennis de table   : There are 18 products (1 pages)\n",
      "FR: 3519 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ tir à l'arc   : There are 0 products (0 pages)\n",
      "FR: 3519 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ tir sportif 22 long rifle   : There are 0 products (0 pages)\n",
      "FR: 3519 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ tir sportif air comprimé   : There are 0 products (0 pages)\n",
      "FR: 3519 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ tonification   : There are 136 products (4 pages)\n",
      "FR: 3655 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ touch rugby   : There are 17 products (1 pages)\n",
      "FR: 3672 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ trail   : There are 47 products (2 pages)\n",
      "FR: 3719 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ trampoline   : There are 2 products (1 pages)\n",
      "FR: 3721 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ trekking arctique   : There are 14 products (1 pages)\n",
      "FR: 3735 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ trekking désert   : There are 14 products (1 pages)\n",
      "FR: 3749 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ trekking montagne   : There are 59 products (2 pages)\n",
      "FR: 3808 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ trekking tropical   : There are 12 products (1 pages)\n",
      "FR: 3820 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ trekking voyage   : There are 49 products (2 pages)\n",
      "FR: 3869 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ triathlon   : There are 17 products (1 pages)\n",
      "FR: 3886 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ trottinette   : There are 9 products (1 pages)\n",
      "FR: 3895 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ trottinette freestyle   : There are 1 products (1 pages)\n",
      "FR: 3896 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ ultimate   : There are 0 products (0 pages)\n",
      "FR: 3896 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ vélo tout chemin randonnée   : There are 14 products (1 pages)\n",
      "FR: 3910 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ vélo ville   : There are 15 products (1 pages)\n",
      "FR: 3925 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ vélo voyage   : There are 12 products (1 pages)\n",
      "FR: 3937 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ via ferrata   : There are 22 products (1 pages)\n",
      "FR: 3959 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ video clip   : There are 11 products (1 pages)\n",
      "FR: 3970 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ voile habitable   : There are 28 products (1 pages)\n",
      "FR: 3998 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ volleyball   : There are 13 products (1 pages)\n",
      "FR: 4011 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ vtt all mountain   : There are 1 products (1 pages)\n",
      "FR: 4012 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ vtt cross country   : There are 2 products (1 pages)\n",
      "FR: 4014 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ vtt randonnée   : There are 11 products (1 pages)\n",
      "FR: 4025 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ waacking   : There are 11 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 4036 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ wakeboard   : There are 0 products (0 pages)\n",
      "FR: 4036 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ waterpolo   : There are 8 products (1 pages)\n",
      "FR: 4044 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ waveboard   : There are 0 products (0 pages)\n",
      "FR: 4044 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ windsurf free ride   : There are 2 products (1 pages)\n",
      "FR: 4046 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ windsurf freestyle   : There are 2 products (1 pages)\n",
      "FR: 4048 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ windsurf slalom   : There are 2 products (1 pages)\n",
      "FR: 4050 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ windsurf wave   : There are 2 products (1 pages)\n",
      "FR: 4052 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ xare   : There are 16 products (1 pages)\n",
      "FR: 4068 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ yoga   : There are 44 products (2 pages)\n",
      "FR: 4112 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Vêtements_ zumba   : There are 3 products (1 pages)\n",
      "FR: 4115 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ 3x3 basketball  : There are 0 products (0 pages)\n",
      "FR: 4115 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ afrocafit   : There are 2 products (1 pages)\n",
      "FR: 4117 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ aïkido   : There are 2 products (1 pages)\n",
      "FR: 4119 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ alpinisme rocheux   : There are 4 products (1 pages)\n",
      "FR: 4123 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ apnée   : There are 2 products (1 pages)\n",
      "FR: 4125 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ aqua cross training   : There are 12 products (1 pages)\n",
      "FR: 4137 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ aquabike   : There are 34 products (1 pages)\n",
      "FR: 4171 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ aquafitness   : There are 35 products (1 pages)\n",
      "FR: 4206 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ arnis   : There are 2 products (1 pages)\n",
      "FR: 4208 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ athlétisme   : There are 62 products (2 pages)\n",
      "FR: 4270 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ aviron   : There are 2 products (1 pages)\n",
      "FR: 4272 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ badminton   : There are 84 products (3 pages)\n",
      "FR: 4356 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ balade à cheval   : There are 7 products (1 pages)\n",
      "FR: 4363 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ baline gomme creuse   : There are 2 products (1 pages)\n",
      "FR: 4365 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ ball trap   : There are 2 products (1 pages)\n",
      "FR: 4367 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ balle au tambourin   : There are 7 products (1 pages)\n",
      "FR: 4374 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ ballet contemporain   : There are 6 products (1 pages)\n",
      "FR: 4380 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ barre au sol   : There are 6 products (1 pages)\n",
      "FR: 4386 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ baseball   : There are 2 products (1 pages)\n",
      "FR: 4388 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ basketball   : There are 31 products (1 pages)\n",
      "FR: 4419 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ bateau dragon   : There are 2 products (1 pages)\n",
      "FR: 4421 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ beach rugby   : There are 23 products (1 pages)\n",
      "FR: 4444 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ beach soccer   : There are 10 products (1 pages)\n",
      "FR: 4454 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ beach tennis   : There are 8 products (1 pages)\n",
      "FR: 4462 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ beach volley   : There are 3 products (1 pages)\n",
      "FR: 4465 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ billard américain   : There are 2 products (1 pages)\n",
      "FR: 4467 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ billard anglais   : There are 2 products (1 pages)\n",
      "FR: 4469 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ billard français   : There are 2 products (1 pages)\n",
      "FR: 4471 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ bmx freestyle   : There are 2 products (1 pages)\n",
      "FR: 4473 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ body attack   : There are 5 products (1 pages)\n",
      "FR: 4478 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ bodyboard   : There are 16 products (1 pages)\n",
      "FR: 4494 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ bodysurf   : There are 3 products (1 pages)\n",
      "FR: 4497 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ bokwa   : There are 2 products (1 pages)\n",
      "FR: 4499 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ boomerang   : There are 2 products (1 pages)\n",
      "FR: 4501 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ bouées tractées   : There are 2 products (1 pages)\n",
      "FR: 4503 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ boxe anglaise   : There are 3 products (1 pages)\n",
      "FR: 4506 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ boxe française   : There are 3 products (1 pages)\n",
      "FR: 4509 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ break dance   : There are 3 products (1 pages)\n",
      "FR: 4512 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ bushcraft   : There are 4 products (1 pages)\n",
      "FR: 4516 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ canoë kayak course   : There are 2 products (1 pages)\n",
      "FR: 4518 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ canoë kayak randonnee   : There are 2 products (1 pages)\n",
      "FR: 4520 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ canoë kayak vague   : There are 2 products (1 pages)\n",
      "FR: 4522 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ canyoning   : There are 4 products (1 pages)\n",
      "FR: 4526 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ capoeira   : There are 2 products (1 pages)\n",
      "FR: 4528 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ cardio boxing   : There are 2 products (1 pages)\n",
      "FR: 4530 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ cardio training   : There are 31 products (1 pages)\n",
      "FR: 4561 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ carrom   : There are 2 products (1 pages)\n",
      "FR: 4563 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ cascade de glace   : There are 4 products (1 pages)\n",
      "FR: 4567 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ cerf-volant de traction   : There are 2 products (1 pages)\n",
      "FR: 4569 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ cerf-volant pilotable   : There are 2 products (1 pages)\n",
      "FR: 4571 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ cerf-volant statique   : There are 2 products (1 pages)\n",
      "FR: 4573 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ chasse à l'approche   : There are 5 products (1 pages)\n",
      "FR: 4578 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ chasse à l'arc   : There are 3 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 4581 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ chasse au gibier d'eau   : There are 5 products (1 pages)\n",
      "FR: 4586 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ chasse au grand gibier battue poste   : There are 9 products (1 pages)\n",
      "FR: 4595 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ chasse au grand gibier battue traque   : There are 9 products (1 pages)\n",
      "FR: 4604 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ chasse au migrateur   : There are 4 products (1 pages)\n",
      "FR: 4608 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ chasse au petit gibier   : There are 20 products (1 pages)\n",
      "FR: 4628 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ chasse sous marine   : There are 2 products (1 pages)\n",
      "FR: 4630 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ chistera   : There are 57 products (2 pages)\n",
      "FR: 4687 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ commercial hip hop   : There are 3 products (1 pages)\n",
      "FR: 4690 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ commercial jazz   : There are 3 products (1 pages)\n",
      "FR: 4693 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ course d'endurance   : There are 0 products (0 pages)\n",
      "FR: 4693 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ course d'orientation   : There are 60 products (2 pages)\n",
      "FR: 4753 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ cricket   : There are 2 products (1 pages)\n",
      "FR: 4755 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ cross training   : There are 2 products (1 pages)\n",
      "FR: 4757 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ crossminton   : There are 9 products (1 pages)\n",
      "FR: 4766 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ cruiser   : There are 17 products (1 pages)\n",
      "FR: 4783 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ cyclosport‎   : There are 6 products (1 pages)\n",
      "FR: 4789 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ cyclotourisme   : There are 4 products (1 pages)\n",
      "FR: 4793 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ dancehall   : There are 3 products (1 pages)\n",
      "FR: 4796 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ danse classique   : There are 6 products (1 pages)\n",
      "FR: 4802 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ danse contemporaine   : There are 2 products (1 pages)\n",
      "FR: 4804 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ danse modern jazz   : There are 7 products (1 pages)\n",
      "FR: 4811 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ danse néo classique   : There are 6 products (1 pages)\n",
      "FR: 4817 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ dériveur / catamaran   : There are 2 products (1 pages)\n",
      "FR: 4819 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ disc golf   : There are 2 products (1 pages)\n",
      "FR: 4821 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ disque volant   : There are 2 products (1 pages)\n",
      "FR: 4823 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ djembel   : There are 2 products (1 pages)\n",
      "FR: 4825 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ endurance à cheval   : There are 4 products (1 pages)\n",
      "FR: 4829 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ entraînement fonctionnel   : There are 0 products (0 pages)\n",
      "FR: 4829 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ équitation   : There are 32 products (1 pages)\n",
      "FR: 4861 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ équitation concours complet   : There are 4 products (1 pages)\n",
      "FR: 4865 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ équitation dressage   : There are 4 products (1 pages)\n",
      "FR: 4869 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ équitation éthologie   : There are 2 products (1 pages)\n",
      "FR: 4871 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ équitation horse ball   : There are 2 products (1 pages)\n",
      "FR: 4873 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ équitation saut d'obstacles   : There are 5 products (1 pages)\n",
      "FR: 4878 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ équitation western   : There are 7 products (1 pages)\n",
      "FR: 4885 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ escalade en bloc   : There are 3 products (1 pages)\n",
      "FR: 4888 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ escalade en falaise   : There are 3 products (1 pages)\n",
      "FR: 4891 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ escalade en grandes voies   : There are 3 products (1 pages)\n",
      "FR: 4894 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ escalade en salle   : There are 3 products (1 pages)\n",
      "FR: 4897 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ escrime   : There are 2 products (1 pages)\n",
      "FR: 4899 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ éveil aquatique   : There are 13 products (1 pages)\n",
      "FR: 4912 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ expédition haute altitude   : There are 4 products (1 pages)\n",
      "FR: 4916 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ fast touring   : There are 4 products (1 pages)\n",
      "FR: 4920 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ fitness ballet   : There are 2 products (1 pages)\n",
      "FR: 4922 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ flag football   : There are 2 products (1 pages)\n",
      "FR: 4924 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ flamenco   : There are 2 products (1 pages)\n",
      "FR: 4926 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ fléchettes pointe acier   : There are 2 products (1 pages)\n",
      "FR: 4928 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ fléchettes pointe plastique   : There are 2 products (1 pages)\n",
      "FR: 4930 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ floorball   : There are 2 products (1 pages)\n",
      "FR: 4932 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ foot5   : There are 31 products (1 pages)\n",
      "FR: 4963 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ football à 11   : There are 33 products (1 pages)\n",
      "FR: 4996 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ football américain   : There are 2 products (1 pages)\n",
      "FR: 4998 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ frescobol   : There are 54 products (2 pages)\n",
      "FR: 5052 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ frontenis   : There are 66 products (2 pages)\n",
      "FR: 5118 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ full contact   : There are 2 products (1 pages)\n",
      "FR: 5120 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ funk jazz   : There are 3 products (1 pages)\n",
      "FR: 5123 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ futsal   : There are 29 products (1 pages)\n",
      "FR: 5152 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ golf   : There are 13 products (1 pages)\n",
      "FR: 5165 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ grappling   : There are 2 products (1 pages)\n",
      "FR: 5167 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ gravel‎   : There are 5 products (1 pages)\n",
      "FR: 5172 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ gym douce   : There are 2 products (1 pages)\n",
      "FR: 5174 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR_Vêtements et chaussures femme_Chaussures_ gymnastique artistique   : There are 2 products (1 pages)\n",
      "FR: 5176 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ gymnastique éducative et sportive   : There are 2 products (1 pages)\n",
      "FR: 5178 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ gymnastique rythmique   : There are 3 products (1 pages)\n",
      "FR: 5181 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ gymnastique suédoise   : There are 2 products (1 pages)\n",
      "FR: 5183 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ gymnastique volontaire   : There are 2 products (1 pages)\n",
      "FR: 5185 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ handball   : There are 16 products (1 pages)\n",
      "FR: 5201 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ hip-hop   : There are 3 products (1 pages)\n",
      "FR: 5204 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ hockey balle   : There are 3 products (1 pages)\n",
      "FR: 5207 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ hockey russe   : There are 2 products (1 pages)\n",
      "FR: 5209 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ hockey sur gazon   : There are 37 products (1 pages)\n",
      "FR: 5246 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ hockey sur glace   : There are 3 products (1 pages)\n",
      "FR: 5249 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ house dance   : There are 3 products (1 pages)\n",
      "FR: 5252 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ jiu-jitsu   : There are 2 products (1 pages)\n",
      "FR: 5254 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ jogging   : There are 102 products (3 pages)\n",
      "FR: 5356 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ jokari   : There are 57 products (2 pages)\n",
      "FR: 5413 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ judo   : There are 2 products (1 pages)\n",
      "FR: 5415 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ karaté   : There are 2 products (1 pages)\n",
      "FR: 5417 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ kempo   : There are 2 products (1 pages)\n",
      "FR: 5419 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ kendo   : There are 2 products (1 pages)\n",
      "FR: 5421 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ kick boxing   : There are 2 products (1 pages)\n",
      "FR: 5423 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ kitesurf freeride   : There are 2 products (1 pages)\n",
      "FR: 5425 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ kitesurf freestyle   : There are 2 products (1 pages)\n",
      "FR: 5427 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ kitesurf race   : There are 2 products (1 pages)\n",
      "FR: 5429 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ kitesurf vagues   : There are 2 products (1 pages)\n",
      "FR: 5431 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ kitewing   : There are 2 products (1 pages)\n",
      "FR: 5433 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ kubb   : There are 2 products (1 pages)\n",
      "FR: 5435 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ kung fu   : There are 2 products (1 pages)\n",
      "FR: 5437 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ lacrosse   : There are 2 products (1 pages)\n",
      "FR: 5439 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ landkite   : There are 2 products (1 pages)\n",
      "FR: 5441 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ lia   : There are 5 products (1 pages)\n",
      "FR: 5446 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ locking   : There are 3 products (1 pages)\n",
      "FR: 5449 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ longboard skate   : There are 19 products (1 pages)\n",
      "FR: 5468 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ luge   : There are 9 products (1 pages)\n",
      "FR: 5477 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ lutte   : There are 2 products (1 pages)\n",
      "FR: 5479 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ marche athlétique de vitesse   : There are 11 products (1 pages)\n",
      "FR: 5490 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ marche athlétique longue distance   : There are 8 products (1 pages)\n",
      "FR: 5498 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ marche nordique   : There are 26 products (1 pages)\n",
      "FR: 5524 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ marche sportive   : There are 91 products (3 pages)\n",
      "FR: 5615 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ mountain touring   : There are 5 products (1 pages)\n",
      "FR: 5620 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ muay thaï   : There are 2 products (1 pages)\n",
      "FR: 5622 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ musculation   : There are 2 products (1 pages)\n",
      "FR: 5624 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ nage eau libre   : There are 19 products (1 pages)\n",
      "FR: 5643 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ natation sportive   : There are 27 products (1 pages)\n",
      "FR: 5670 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ natation synchronisée artistique   : There are 21 products (1 pages)\n",
      "FR: 5691 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ netball   : There are 2 products (1 pages)\n",
      "FR: 5693 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ observation de la faune   : There are 8 products (1 pages)\n",
      "FR: 5701 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ one wall   : There are 2 products (1 pages)\n",
      "FR: 5703 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ padel   : There are 85 products (3 pages)\n",
      "FR: 5788 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pala gomme pleine   : There are 57 products (2 pages)\n",
      "FR: 5845 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ paleta cuir   : There are 2 products (1 pages)\n",
      "FR: 5847 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ palets breton   : There are 2 products (1 pages)\n",
      "FR: 5849 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ palets vendéens   : There are 2 products (1 pages)\n",
      "FR: 5851 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ parkour   : There are 2 products (1 pages)\n",
      "FR: 5853 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pasaka   : There are 57 products (2 pages)\n",
      "FR: 5910 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ patinage artistique   : There are 2 products (1 pages)\n",
      "FR: 5912 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ patinage sur glace   : There are 2 products (1 pages)\n",
      "FR: 5914 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche a l'anglaise   : There are 2 products (1 pages)\n",
      "FR: 5916 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche a la bolognaise   : There are 2 products (1 pages)\n",
      "FR: 5918 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche a la main   : There are 2 products (1 pages)\n",
      "FR: 5920 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche a la mouche   : There are 2 products (1 pages)\n",
      "FR: 5922 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche a la traine   : There are 2 products (1 pages)\n",
      "FR: 5924 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche à pied   : There are 2 products (1 pages)\n",
      "FR: 5926 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche à soutenir bateau   : There are 2 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 5928 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche au coup chinoise   : There are 2 products (1 pages)\n",
      "FR: 5930 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche au coup emmanchement   : There are 2 products (1 pages)\n",
      "FR: 5932 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche au coup télescopique   : There are 2 products (1 pages)\n",
      "FR: 5934 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche au flotteur en mer   : There are 2 products (1 pages)\n",
      "FR: 5936 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche au jigging   : There are 2 products (1 pages)\n",
      "FR: 5938 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche au leurres en mer   : There are 2 products (1 pages)\n",
      "FR: 5940 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche au poisson mort manie   : There are 2 products (1 pages)\n",
      "FR: 5942 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche au pose   : There are 2 products (1 pages)\n",
      "FR: 5944 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche au posé en bord de mer   : There are 2 products (1 pages)\n",
      "FR: 5946 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche au quiver / feeder   : There are 2 products (1 pages)\n",
      "FR: 5948 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche au vairon manié   : There are 2 products (1 pages)\n",
      "FR: 5950 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche aux leurres black bass   : There are 2 products (1 pages)\n",
      "FR: 5952 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche aux leurres brochet   : There are 2 products (1 pages)\n",
      "FR: 5954 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche aux leurres carnassier   : There are 2 products (1 pages)\n",
      "FR: 5956 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche aux leurres perche   : There are 2 products (1 pages)\n",
      "FR: 5958 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche aux leurres sandre   : There are 2 products (1 pages)\n",
      "FR: 5960 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche aux leurres truite   : There are 2 products (1 pages)\n",
      "FR: 5962 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche de l'ecrevisse   : There are 2 products (1 pages)\n",
      "FR: 5964 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche de la carpe   : There are 2 products (1 pages)\n",
      "FR: 5966 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche de la carpe au coup   : There are 2 products (1 pages)\n",
      "FR: 5968 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche de la truite a la bombette   : There are 2 products (1 pages)\n",
      "FR: 5970 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche de la truite au toc   : There are 2 products (1 pages)\n",
      "FR: 5972 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche de la truite en étang   : There are 2 products (1 pages)\n",
      "FR: 5974 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche de seiches et calamars   : There are 2 products (1 pages)\n",
      "FR: 5976 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche du silure   : There are 2 products (1 pages)\n",
      "FR: 5978 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche en surf casting   : There are 2 products (1 pages)\n",
      "FR: 5980 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pêche sous la glace   : There are 2 products (1 pages)\n",
      "FR: 5982 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pelote   : There are 2 products (1 pages)\n",
      "FR: 5984 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pelote à main nue   : There are 57 products (2 pages)\n",
      "FR: 6041 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pétanque   : There are 2 products (1 pages)\n",
      "FR: 6043 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ peteca   : There are 2 products (1 pages)\n",
      "FR: 6045 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pickleball   : There are 61 products (2 pages)\n",
      "FR: 6106 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pilates   : There are 8 products (1 pages)\n",
      "FR: 6114 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ plongée bouteille   : There are 2 products (1 pages)\n",
      "FR: 6116 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ plongeon   : There are 20 products (1 pages)\n",
      "FR: 6136 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pole dance   : There are 2 products (1 pages)\n",
      "FR: 6138 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ pony games   : There are 2 products (1 pages)\n",
      "FR: 6140 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ popping   : There are 3 products (1 pages)\n",
      "FR: 6143 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ quad agressif   : There are 2 products (1 pages)\n",
      "FR: 6145 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ quad artistique   : There are 2 products (1 pages)\n",
      "FR: 6147 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ quad fitness   : There are 4 products (1 pages)\n",
      "FR: 6151 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ quad freeride   : There are 4 products (1 pages)\n",
      "FR: 6155 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ quad vitesse   : There are 2 products (1 pages)\n",
      "FR: 6157 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ quilles finlandaises   : There are 2 products (1 pages)\n",
      "FR: 6159 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ racquetball   : There are 66 products (2 pages)\n",
      "FR: 6225 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ randonnée à cheval   : There are 9 products (1 pages)\n",
      "FR: 6234 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ randonnée glaciaire   : There are 4 products (1 pages)\n",
      "FR: 6238 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ randonnée montagne   : There are 90 products (3 pages)\n",
      "FR: 6328 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ randonnée nature   : There are 115 products (3 pages)\n",
      "FR: 6443 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ randonnée neige   : There are 54 products (2 pages)\n",
      "FR: 6497 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ randonnée rapide   : There are 27 products (1 pages)\n",
      "FR: 6524 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ rebot   : There are 57 products (2 pages)\n",
      "FR: 6581 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ régate   : There are 5 products (1 pages)\n",
      "FR: 6586 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ resort touring   : There are 5 products (1 pages)\n",
      "FR: 6591 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ rink hockey   : There are 3 products (1 pages)\n",
      "FR: 6594 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ roller agressif   : There are 2 products (1 pages)\n",
      "FR: 6596 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ roller artistique   : There are 2 products (1 pages)\n",
      "FR: 6598 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ roller derby   : There are 2 products (1 pages)\n",
      "FR: 6600 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ roller fitness   : There are 4 products (1 pages)\n",
      "FR: 6604 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ roller freeride   : There are 4 products (1 pages)\n",
      "FR: 6608 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ roller freestyle   : There are 2 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 6610 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ roller hockey   : There are 3 products (1 pages)\n",
      "FR: 6613 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ roller vitesse   : There are 2 products (1 pages)\n",
      "FR: 6615 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ rpm   : There are 16 products (1 pages)\n",
      "FR: 6631 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ rugby à 13   : There are 34 products (1 pages)\n",
      "FR: 6665 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ rugby à 15   : There are 34 products (1 pages)\n",
      "FR: 6699 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ rugby à 7   : There are 34 products (1 pages)\n",
      "FR: 6733 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ running route   : There are 126 products (4 pages)\n",
      "FR: 6859 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ sambo   : There are 2 products (1 pages)\n",
      "FR: 6861 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ sauvetage sportif   : There are 16 products (1 pages)\n",
      "FR: 6877 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ self defense   : There are 2 products (1 pages)\n",
      "FR: 6879 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ sepak takraw   : There are 2 products (1 pages)\n",
      "FR: 6881 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ sh'bam   : There are 2 products (1 pages)\n",
      "FR: 6883 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ skateboard   : There are 25 products (1 pages)\n",
      "FR: 6908 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ ski de fond a roue   : There are 8 products (1 pages)\n",
      "FR: 6916 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ ski de fond alternatif   : There are 7 products (1 pages)\n",
      "FR: 6923 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ ski de fond skating   : There are 6 products (1 pages)\n",
      "FR: 6929 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ ski de piste   : There are 15 products (1 pages)\n",
      "FR: 6944 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ ski freeride   : There are 13 products (1 pages)\n",
      "FR: 6957 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ ski freestyle   : There are 13 products (1 pages)\n",
      "FR: 6970 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ ski nautique   : There are 2 products (1 pages)\n",
      "FR: 6972 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ skimboard   : There are 3 products (1 pages)\n",
      "FR: 6975 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ slackline   : There are 2 products (1 pages)\n",
      "FR: 6977 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ snooker   : There are 2 products (1 pages)\n",
      "FR: 6979 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ snorkeling   : There are 13 products (1 pages)\n",
      "FR: 6992 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ snowboard all mountain   : There are 11 products (1 pages)\n",
      "FR: 7003 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ snowboard carving   : There are 3 products (1 pages)\n",
      "FR: 7006 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ snowboard freeride   : There are 10 products (1 pages)\n",
      "FR: 7016 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ snowboard freestyle   : There are 11 products (1 pages)\n",
      "FR: 7027 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ snowboard jib   : There are 3 products (1 pages)\n",
      "FR: 7030 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ snowboard split   : There are 6 products (1 pages)\n",
      "FR: 7036 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ snowkite   : There are 2 products (1 pages)\n",
      "FR: 7038 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ softball   : There are 2 products (1 pages)\n",
      "FR: 7040 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ speedball   : There are 62 products (2 pages)\n",
      "FR: 7102 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ spéléologie   : There are 2 products (1 pages)\n",
      "FR: 7104 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ squash   : There are 81 products (3 pages)\n",
      "FR: 7185 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ stand up paddle course   : There are 2 products (1 pages)\n",
      "FR: 7187 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ stand up paddle randonnée   : There are 2 products (1 pages)\n",
      "FR: 7189 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ stand up paddle vague   : There are 2 products (1 pages)\n",
      "FR: 7191 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ step   : There are 16 products (1 pages)\n",
      "FR: 7207 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ street hockey   : There are 3 products (1 pages)\n",
      "FR: 7210 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ street jazz   : There are 3 products (1 pages)\n",
      "FR: 7213 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ stretching‎   : There are 8 products (1 pages)\n",
      "FR: 7221 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ surf   : There are 51 products (2 pages)\n",
      "FR: 7272 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ swimrun   : There are 9 products (1 pages)\n",
      "FR: 7281 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ taekwondo   : There are 2 products (1 pages)\n",
      "FR: 7283 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ taï chi chuan   : There are 2 products (1 pages)\n",
      "FR: 7285 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ tennis   : There are 110 products (3 pages)\n",
      "FR: 7395 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ tennis de table   : There are 46 products (2 pages)\n",
      "FR: 7441 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ tir à l'arc   : There are 2 products (1 pages)\n",
      "FR: 7443 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ tir sportif 22 long rifle   : There are 2 products (1 pages)\n",
      "FR: 7445 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ tir sportif air comprimé   : There are 2 products (1 pages)\n",
      "FR: 7447 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ tonification   : There are 9 products (1 pages)\n",
      "FR: 7456 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ touch rugby   : There are 34 products (1 pages)\n",
      "FR: 7490 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ trail   : There are 76 products (2 pages)\n",
      "FR: 7566 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ trampoline   : There are 2 products (1 pages)\n",
      "FR: 7568 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ trekking arctique   : There are 12 products (1 pages)\n",
      "FR: 7580 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ trekking désert   : There are 29 products (1 pages)\n",
      "FR: 7609 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ trekking montagne   : There are 42 products (2 pages)\n",
      "FR: 7651 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ trekking tropical   : There are 29 products (1 pages)\n",
      "FR: 7680 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ trekking voyage   : There are 67 products (2 pages)\n",
      "FR: 7747 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ triathlon   : There are 58 products (2 pages)\n",
      "FR: 7805 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ trottinette   : There are 2 products (1 pages)\n",
      "FR: 7807 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ trottinette freestyle   : There are 11 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 7818 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ ultimate   : There are 2 products (1 pages)\n",
      "FR: 7820 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ vélo tout chemin randonnée   : There are 5 products (1 pages)\n",
      "FR: 7825 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ vélo ville   : There are 2 products (1 pages)\n",
      "FR: 7827 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ vélo voyage   : There are 5 products (1 pages)\n",
      "FR: 7832 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ via ferrata   : There are 3 products (1 pages)\n",
      "FR: 7835 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ video clip   : There are 3 products (1 pages)\n",
      "FR: 7838 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ voile habitable   : There are 12 products (1 pages)\n",
      "FR: 7850 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ volleyball   : There are 14 products (1 pages)\n",
      "FR: 7864 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ vtt all mountain   : There are 7 products (1 pages)\n",
      "FR: 7871 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ vtt cross country   : There are 5 products (1 pages)\n",
      "FR: 7876 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ vtt randonnée   : There are 9 products (1 pages)\n",
      "FR: 7885 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ waacking   : There are 3 products (1 pages)\n",
      "FR: 7888 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ wakeboard   : There are 2 products (1 pages)\n",
      "FR: 7890 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ waterpolo   : There are 22 products (1 pages)\n",
      "FR: 7912 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ waveboard   : There are 2 products (1 pages)\n",
      "FR: 7914 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ windsurf free ride   : There are 2 products (1 pages)\n",
      "FR: 7916 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ windsurf freestyle   : There are 2 products (1 pages)\n",
      "FR: 7918 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ windsurf slalom   : There are 2 products (1 pages)\n",
      "FR: 7920 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ windsurf wave   : There are 2 products (1 pages)\n",
      "FR: 7922 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ xare   : There are 57 products (2 pages)\n",
      "FR: 7979 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ yoga   : There are 4 products (1 pages)\n",
      "FR: 7983 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Chaussures_ zumba   : There are 2 products (1 pages)\n",
      "FR: 7985 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ 3x3 basketball  : There are 0 products (0 pages)\n",
      "FR: 7985 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ afrocafit   : There are 6 products (1 pages)\n",
      "FR: 7991 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ aïkido   : There are 6 products (1 pages)\n",
      "FR: 7997 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ alpinisme rocheux   : There are 8 products (1 pages)\n",
      "FR: 8005 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ apnée   : There are 6 products (1 pages)\n",
      "FR: 8011 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ aqua cross training   : There are 6 products (1 pages)\n",
      "FR: 8017 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ aquabike   : There are 7 products (1 pages)\n",
      "FR: 8024 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ aquafitness   : There are 7 products (1 pages)\n",
      "FR: 8031 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ arnis   : There are 6 products (1 pages)\n",
      "FR: 8037 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ athlétisme   : There are 23 products (1 pages)\n",
      "FR: 8060 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ aviron   : There are 6 products (1 pages)\n",
      "FR: 8066 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ badminton   : There are 14 products (1 pages)\n",
      "FR: 8080 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ balade à cheval   : There are 8 products (1 pages)\n",
      "FR: 8088 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ baline gomme creuse   : There are 11 products (1 pages)\n",
      "FR: 8099 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ ball trap   : There are 9 products (1 pages)\n",
      "FR: 8108 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ balle au tambourin   : There are 13 products (1 pages)\n",
      "FR: 8121 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ ballet contemporain   : There are 6 products (1 pages)\n",
      "FR: 8127 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ barre au sol   : There are 6 products (1 pages)\n",
      "FR: 8133 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ baseball   : There are 46 products (2 pages)\n",
      "FR: 8179 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ basketball   : There are 41 products (2 pages)\n",
      "FR: 8220 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ bateau dragon   : There are 6 products (1 pages)\n",
      "FR: 8226 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ beach rugby   : There are 16 products (1 pages)\n",
      "FR: 8242 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ beach soccer   : There are 41 products (2 pages)\n",
      "FR: 8283 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ beach tennis   : There are 15 products (1 pages)\n",
      "FR: 8298 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ beach volley   : There are 17 products (1 pages)\n",
      "FR: 8315 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ billard américain   : There are 6 products (1 pages)\n",
      "FR: 8321 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ billard anglais   : There are 6 products (1 pages)\n",
      "FR: 8327 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ billard français   : There are 6 products (1 pages)\n",
      "FR: 8333 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ bmx freestyle   : There are 6 products (1 pages)\n",
      "FR: 8339 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ body attack   : There are 17 products (1 pages)\n",
      "FR: 8356 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ bodyboard   : There are 6 products (1 pages)\n",
      "FR: 8362 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ bodysurf   : There are 6 products (1 pages)\n",
      "FR: 8368 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ bokwa   : There are 6 products (1 pages)\n",
      "FR: 8374 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ boomerang   : There are 6 products (1 pages)\n",
      "FR: 8380 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ bouées tractées   : There are 6 products (1 pages)\n",
      "FR: 8386 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ boxe anglaise   : There are 7 products (1 pages)\n",
      "FR: 8393 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ boxe française   : There are 6 products (1 pages)\n",
      "FR: 8399 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ break dance   : There are 6 products (1 pages)\n",
      "FR: 8405 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ bushcraft   : There are 8 products (1 pages)\n",
      "FR: 8413 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ canoë kayak course   : There are 14 products (1 pages)\n",
      "FR: 8427 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ canoë kayak randonnee   : There are 15 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 8442 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ canoë kayak vague   : There are 10 products (1 pages)\n",
      "FR: 8452 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ canyoning   : There are 7 products (1 pages)\n",
      "FR: 8459 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ capoeira   : There are 6 products (1 pages)\n",
      "FR: 8465 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ cardio boxing   : There are 7 products (1 pages)\n",
      "FR: 8472 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ cardio training   : There are 22 products (1 pages)\n",
      "FR: 8494 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ carrom   : There are 6 products (1 pages)\n",
      "FR: 8500 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ cascade de glace   : There are 7 products (1 pages)\n",
      "FR: 8507 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ cerf-volant de traction   : There are 9 products (1 pages)\n",
      "FR: 8516 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ cerf-volant pilotable   : There are 6 products (1 pages)\n",
      "FR: 8522 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ cerf-volant statique   : There are 6 products (1 pages)\n",
      "FR: 8528 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ chasse à l'approche   : There are 11 products (1 pages)\n",
      "FR: 8539 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ chasse à l'arc   : There are 7 products (1 pages)\n",
      "FR: 8546 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ chasse au gibier d'eau   : There are 13 products (1 pages)\n",
      "FR: 8559 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ chasse au grand gibier battue poste   : There are 24 products (1 pages)\n",
      "FR: 8583 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ chasse au grand gibier battue traque   : There are 21 products (1 pages)\n",
      "FR: 8604 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ chasse au migrateur   : There are 12 products (1 pages)\n",
      "FR: 8616 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ chasse au petit gibier   : There are 10 products (1 pages)\n",
      "FR: 8626 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ chasse sous marine   : There are 6 products (1 pages)\n",
      "FR: 8632 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ chistera   : There are 14 products (1 pages)\n",
      "FR: 8646 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ commercial hip hop   : There are 6 products (1 pages)\n",
      "FR: 8652 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ commercial jazz   : There are 6 products (1 pages)\n",
      "FR: 8658 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ course d'endurance   : There are 0 products (0 pages)\n",
      "FR: 8658 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ course d'orientation   : There are 16 products (1 pages)\n",
      "FR: 8674 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ cricket   : There are 8 products (1 pages)\n",
      "FR: 8682 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ cross training   : There are 7 products (1 pages)\n",
      "FR: 8689 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ crossminton   : There are 10 products (1 pages)\n",
      "FR: 8699 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ cruiser   : There are 10 products (1 pages)\n",
      "FR: 8709 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ cyclosport‎   : There are 17 products (1 pages)\n",
      "FR: 8726 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ cyclotourisme   : There are 16 products (1 pages)\n",
      "FR: 8742 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ dancehall   : There are 6 products (1 pages)\n",
      "FR: 8748 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ danse classique   : There are 6 products (1 pages)\n",
      "FR: 8754 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ danse contemporaine   : There are 6 products (1 pages)\n",
      "FR: 8760 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ danse modern jazz   : There are 6 products (1 pages)\n",
      "FR: 8766 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ danse néo classique   : There are 6 products (1 pages)\n",
      "FR: 8772 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ dériveur / catamaran   : There are 15 products (1 pages)\n",
      "FR: 8787 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ disc golf   : There are 6 products (1 pages)\n",
      "FR: 8793 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ disque volant   : There are 6 products (1 pages)\n",
      "FR: 8799 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ djembel   : There are 6 products (1 pages)\n",
      "FR: 8805 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ endurance à cheval   : There are 7 products (1 pages)\n",
      "FR: 8812 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ entraînement fonctionnel   : There are 0 products (0 pages)\n",
      "FR: 8812 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ équitation   : There are 20 products (1 pages)\n",
      "FR: 8832 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ équitation concours complet   : There are 6 products (1 pages)\n",
      "FR: 8838 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ équitation dressage   : There are 10 products (1 pages)\n",
      "FR: 8848 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ équitation éthologie   : There are 6 products (1 pages)\n",
      "FR: 8854 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ équitation horse ball   : There are 6 products (1 pages)\n",
      "FR: 8860 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ équitation saut d'obstacles   : There are 9 products (1 pages)\n",
      "FR: 8869 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ équitation western   : There are 8 products (1 pages)\n",
      "FR: 8877 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ escalade en bloc   : There are 8 products (1 pages)\n",
      "FR: 8885 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ escalade en falaise   : There are 8 products (1 pages)\n",
      "FR: 8893 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ escalade en grandes voies   : There are 8 products (1 pages)\n",
      "FR: 8901 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ escalade en salle   : There are 6 products (1 pages)\n",
      "FR: 8907 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ escrime   : There are 6 products (1 pages)\n",
      "FR: 8913 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ éveil aquatique   : There are 7 products (1 pages)\n",
      "FR: 8920 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ expédition haute altitude   : There are 7 products (1 pages)\n",
      "FR: 8927 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ fast touring   : There are 24 products (1 pages)\n",
      "FR: 8951 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ fitness ballet   : There are 6 products (1 pages)\n",
      "FR: 8957 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ flag football   : There are 9 products (1 pages)\n",
      "FR: 8966 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ flamenco   : There are 6 products (1 pages)\n",
      "FR: 8972 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ fléchettes pointe acier   : There are 6 products (1 pages)\n",
      "FR: 8978 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ fléchettes pointe plastique   : There are 6 products (1 pages)\n",
      "FR: 8984 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR_Vêtements et chaussures femme_Accessoires_ floorball   : There are 8 products (1 pages)\n",
      "FR: 8992 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ foot5   : There are 41 products (2 pages)\n",
      "FR: 9033 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ football à 11   : There are 42 products (2 pages)\n",
      "FR: 9075 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ football américain   : There are 39 products (1 pages)\n",
      "FR: 9114 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ frescobol   : There are 15 products (1 pages)\n",
      "FR: 9129 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ frontenis   : There are 17 products (1 pages)\n",
      "FR: 9146 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ full contact   : There are 6 products (1 pages)\n",
      "FR: 9152 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ funk jazz   : There are 6 products (1 pages)\n",
      "FR: 9158 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ futsal   : There are 41 products (2 pages)\n",
      "FR: 9199 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ golf   : There are 20 products (1 pages)\n",
      "FR: 9219 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ grappling   : There are 6 products (1 pages)\n",
      "FR: 9225 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ gravel‎   : There are 16 products (1 pages)\n",
      "FR: 9241 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ gym douce   : There are 6 products (1 pages)\n",
      "FR: 9247 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ gymnastique artistique   : There are 6 products (1 pages)\n",
      "FR: 9253 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ gymnastique éducative et sportive   : There are 6 products (1 pages)\n",
      "FR: 9259 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ gymnastique rythmique   : There are 6 products (1 pages)\n",
      "FR: 9265 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ gymnastique suédoise   : There are 6 products (1 pages)\n",
      "FR: 9271 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ gymnastique volontaire   : There are 6 products (1 pages)\n",
      "FR: 9277 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ handball   : There are 16 products (1 pages)\n",
      "FR: 9293 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ hip-hop   : There are 6 products (1 pages)\n",
      "FR: 9299 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ hockey balle   : There are 6 products (1 pages)\n",
      "FR: 9305 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ hockey russe   : There are 6 products (1 pages)\n",
      "FR: 9311 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ hockey sur gazon   : There are 8 products (1 pages)\n",
      "FR: 9319 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ hockey sur glace   : There are 8 products (1 pages)\n",
      "FR: 9327 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ house dance   : There are 6 products (1 pages)\n",
      "FR: 9333 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ jiu-jitsu   : There are 6 products (1 pages)\n",
      "FR: 9339 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ jogging   : There are 47 products (2 pages)\n",
      "FR: 9386 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ jokari   : There are 14 products (1 pages)\n",
      "FR: 9400 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ judo   : There are 6 products (1 pages)\n",
      "FR: 9406 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ karaté   : There are 6 products (1 pages)\n",
      "FR: 9412 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ kempo   : There are 6 products (1 pages)\n",
      "FR: 9418 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ kendo   : There are 6 products (1 pages)\n",
      "FR: 9424 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ kick boxing   : There are 6 products (1 pages)\n",
      "FR: 9430 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ kitesurf freeride   : There are 9 products (1 pages)\n",
      "FR: 9439 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ kitesurf freestyle   : There are 9 products (1 pages)\n",
      "FR: 9448 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ kitesurf race   : There are 9 products (1 pages)\n",
      "FR: 9457 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ kitesurf vagues   : There are 9 products (1 pages)\n",
      "FR: 9466 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ kitewing   : There are 6 products (1 pages)\n",
      "FR: 9472 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ kubb   : There are 6 products (1 pages)\n",
      "FR: 9478 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ kung fu   : There are 6 products (1 pages)\n",
      "FR: 9484 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ lacrosse   : There are 8 products (1 pages)\n",
      "FR: 9492 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ landkite   : There are 9 products (1 pages)\n",
      "FR: 9501 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ lia   : There are 16 products (1 pages)\n",
      "FR: 9517 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ locking   : There are 6 products (1 pages)\n",
      "FR: 9523 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ longboard skate   : There are 11 products (1 pages)\n",
      "FR: 9534 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ luge   : There are 28 products (1 pages)\n",
      "FR: 9562 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ lutte   : There are 6 products (1 pages)\n",
      "FR: 9568 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ marche athlétique de vitesse   : There are 18 products (1 pages)\n",
      "FR: 9586 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ marche athlétique longue distance   : There are 18 products (1 pages)\n",
      "FR: 9604 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ marche nordique   : There are 20 products (1 pages)\n",
      "FR: 9624 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ marche sportive   : There are 23 products (1 pages)\n",
      "FR: 9647 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ mountain touring   : There are 25 products (1 pages)\n",
      "FR: 9672 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ muay thaï   : There are 6 products (1 pages)\n",
      "FR: 9678 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ musculation   : There are 7 products (1 pages)\n",
      "FR: 9685 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ nage eau libre   : There are 8 products (1 pages)\n",
      "FR: 9693 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ natation sportive   : There are 8 products (1 pages)\n",
      "FR: 9701 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ natation synchronisée artistique   : There are 7 products (1 pages)\n",
      "FR: 9708 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ netball   : There are 8 products (1 pages)\n",
      "FR: 9716 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ observation de la faune   : There are 10 products (1 pages)\n",
      "FR: 9726 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ one wall   : There are 10 products (1 pages)\n",
      "FR: 9736 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ padel   : There are 22 products (1 pages)\n",
      "FR: 9758 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pala gomme pleine   : There are 14 products (1 pages)\n",
      "FR: 9772 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ paleta cuir   : There are 6 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 9778 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ palets breton   : There are 6 products (1 pages)\n",
      "FR: 9784 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ palets vendéens   : There are 6 products (1 pages)\n",
      "FR: 9790 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ parkour   : There are 6 products (1 pages)\n",
      "FR: 9796 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pasaka   : There are 14 products (1 pages)\n",
      "FR: 9810 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ patinage artistique   : There are 6 products (1 pages)\n",
      "FR: 9816 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ patinage sur glace   : There are 6 products (1 pages)\n",
      "FR: 9822 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche a l'anglaise   : There are 6 products (1 pages)\n",
      "FR: 9828 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche a la bolognaise   : There are 6 products (1 pages)\n",
      "FR: 9834 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche a la main   : There are 6 products (1 pages)\n",
      "FR: 9840 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche a la mouche   : There are 6 products (1 pages)\n",
      "FR: 9846 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche a la traine   : There are 6 products (1 pages)\n",
      "FR: 9852 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche à pied   : There are 6 products (1 pages)\n",
      "FR: 9858 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche à soutenir bateau   : There are 6 products (1 pages)\n",
      "FR: 9864 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche au coup chinoise   : There are 6 products (1 pages)\n",
      "FR: 9870 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche au coup emmanchement   : There are 6 products (1 pages)\n",
      "FR: 9876 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche au coup télescopique   : There are 6 products (1 pages)\n",
      "FR: 9882 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche au flotteur en mer   : There are 6 products (1 pages)\n",
      "FR: 9888 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche au jigging   : There are 6 products (1 pages)\n",
      "FR: 9894 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche au leurres en mer   : There are 6 products (1 pages)\n",
      "FR: 9900 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche au poisson mort manie   : There are 7 products (1 pages)\n",
      "FR: 9907 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche au pose   : There are 7 products (1 pages)\n",
      "FR: 9914 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche au posé en bord de mer   : There are 6 products (1 pages)\n",
      "FR: 9920 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche au quiver / feeder   : There are 6 products (1 pages)\n",
      "FR: 9926 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche au vairon manié   : There are 7 products (1 pages)\n",
      "FR: 9933 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche aux leurres black bass   : There are 6 products (1 pages)\n",
      "FR: 9939 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche aux leurres brochet   : There are 6 products (1 pages)\n",
      "FR: 9945 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche aux leurres carnassier   : There are 7 products (1 pages)\n",
      "FR: 9952 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche aux leurres perche   : There are 6 products (1 pages)\n",
      "FR: 9958 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche aux leurres sandre   : There are 6 products (1 pages)\n",
      "FR: 9964 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche aux leurres truite   : There are 6 products (1 pages)\n",
      "FR: 9970 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche de l'ecrevisse   : There are 6 products (1 pages)\n",
      "FR: 9976 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche de la carpe   : There are 6 products (1 pages)\n",
      "FR: 9982 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche de la carpe au coup   : There are 6 products (1 pages)\n",
      "FR: 9988 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche de la truite a la bombette   : There are 6 products (1 pages)\n",
      "FR: 9994 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche de la truite au toc   : There are 7 products (1 pages)\n",
      "FR: 10001 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche de la truite en étang   : There are 6 products (1 pages)\n",
      "FR: 10007 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche de seiches et calamars   : There are 6 products (1 pages)\n",
      "FR: 10013 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche du silure   : There are 6 products (1 pages)\n",
      "FR: 10019 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche en surf casting   : There are 6 products (1 pages)\n",
      "FR: 10025 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pêche sous la glace   : There are 6 products (1 pages)\n",
      "FR: 10031 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pelote   : There are 11 products (1 pages)\n",
      "FR: 10042 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pelote à main nue   : There are 14 products (1 pages)\n",
      "FR: 10056 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pétanque   : There are 6 products (1 pages)\n",
      "FR: 10062 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ peteca   : There are 6 products (1 pages)\n",
      "FR: 10068 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pickleball   : There are 16 products (1 pages)\n",
      "FR: 10084 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pilates   : There are 8 products (1 pages)\n",
      "FR: 10092 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ plongée bouteille   : There are 10 products (1 pages)\n",
      "FR: 10102 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ plongeon   : There are 7 products (1 pages)\n",
      "FR: 10109 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pole dance   : There are 6 products (1 pages)\n",
      "FR: 10115 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ pony games   : There are 6 products (1 pages)\n",
      "FR: 10121 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ popping   : There are 6 products (1 pages)\n",
      "FR: 10127 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ quad agressif   : There are 6 products (1 pages)\n",
      "FR: 10133 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ quad artistique   : There are 6 products (1 pages)\n",
      "FR: 10139 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ quad fitness   : There are 6 products (1 pages)\n",
      "FR: 10145 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ quad freeride   : There are 6 products (1 pages)\n",
      "FR: 10151 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ quad vitesse   : There are 6 products (1 pages)\n",
      "FR: 10157 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ quilles finlandaises   : There are 6 products (1 pages)\n",
      "FR: 10163 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ racquetball   : There are 11 products (1 pages)\n",
      "FR: 10174 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ randonnée à cheval   : There are 8 products (1 pages)\n",
      "FR: 10182 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ randonnée glaciaire   : There are 7 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 10189 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ randonnée montagne   : There are 68 products (2 pages)\n",
      "FR: 10257 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ randonnée nature   : There are 72 products (2 pages)\n",
      "FR: 10329 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ randonnée neige   : There are 34 products (1 pages)\n",
      "FR: 10363 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ randonnée rapide   : There are 28 products (1 pages)\n",
      "FR: 10391 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ rebot   : There are 14 products (1 pages)\n",
      "FR: 10405 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ régate   : There are 14 products (1 pages)\n",
      "FR: 10419 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ resort touring   : There are 25 products (1 pages)\n",
      "FR: 10444 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ rink hockey   : There are 6 products (1 pages)\n",
      "FR: 10450 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ roller agressif   : There are 6 products (1 pages)\n",
      "FR: 10456 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ roller artistique   : There are 6 products (1 pages)\n",
      "FR: 10462 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ roller derby   : There are 6 products (1 pages)\n",
      "FR: 10468 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ roller fitness   : There are 6 products (1 pages)\n",
      "FR: 10474 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ roller freeride   : There are 6 products (1 pages)\n",
      "FR: 10480 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ roller freestyle   : There are 6 products (1 pages)\n",
      "FR: 10486 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ roller hockey   : There are 7 products (1 pages)\n",
      "FR: 10493 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ roller vitesse   : There are 6 products (1 pages)\n",
      "FR: 10499 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ rpm   : There are 17 products (1 pages)\n",
      "FR: 10516 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ rugby à 13   : There are 17 products (1 pages)\n",
      "FR: 10533 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ rugby à 15   : There are 17 products (1 pages)\n",
      "FR: 10550 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ rugby à 7   : There are 16 products (1 pages)\n",
      "FR: 10566 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ running route   : There are 33 products (1 pages)\n",
      "FR: 10599 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ sambo   : There are 6 products (1 pages)\n",
      "FR: 10605 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ sauvetage sportif   : There are 7 products (1 pages)\n",
      "FR: 10612 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ self defense   : There are 6 products (1 pages)\n",
      "FR: 10618 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ sepak takraw   : There are 6 products (1 pages)\n",
      "FR: 10624 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ sh'bam   : There are 6 products (1 pages)\n",
      "FR: 10630 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ skateboard   : There are 11 products (1 pages)\n",
      "FR: 10641 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ ski de fond a roue   : There are 10 products (1 pages)\n",
      "FR: 10651 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ ski de fond alternatif   : There are 11 products (1 pages)\n",
      "FR: 10662 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ ski de fond skating   : There are 11 products (1 pages)\n",
      "FR: 10673 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ ski de piste   : There are 48 products (2 pages)\n",
      "FR: 10721 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ ski freeride   : There are 39 products (1 pages)\n",
      "FR: 10760 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ ski freestyle   : There are 33 products (1 pages)\n",
      "FR: 10793 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ ski nautique   : There are 6 products (1 pages)\n",
      "FR: 10799 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ skimboard   : There are 6 products (1 pages)\n",
      "FR: 10805 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ slackline   : There are 6 products (1 pages)\n",
      "FR: 10811 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ snooker   : There are 6 products (1 pages)\n",
      "FR: 10817 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ snorkeling   : There are 6 products (1 pages)\n",
      "FR: 10823 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ snowboard all mountain   : There are 34 products (1 pages)\n",
      "FR: 10857 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ snowboard carving   : There are 6 products (1 pages)\n",
      "FR: 10863 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ snowboard freeride   : There are 26 products (1 pages)\n",
      "FR: 10889 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ snowboard freestyle   : There are 29 products (1 pages)\n",
      "FR: 10918 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ snowboard jib   : There are 6 products (1 pages)\n",
      "FR: 10924 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ snowboard split   : There are 8 products (1 pages)\n",
      "FR: 10932 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ snowkite   : There are 9 products (1 pages)\n",
      "FR: 10941 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ softball   : There are 25 products (1 pages)\n",
      "FR: 10966 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ speedball   : There are 13 products (1 pages)\n",
      "FR: 10979 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ spéléologie   : There are 7 products (1 pages)\n",
      "FR: 10986 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ squash   : There are 17 products (1 pages)\n",
      "FR: 11003 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ stand up paddle course   : There are 14 products (1 pages)\n",
      "FR: 11017 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ stand up paddle randonnée   : There are 15 products (1 pages)\n",
      "FR: 11032 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ stand up paddle vague   : There are 10 products (1 pages)\n",
      "FR: 11042 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ step   : There are 17 products (1 pages)\n",
      "FR: 11059 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ street hockey   : There are 6 products (1 pages)\n",
      "FR: 11065 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ street jazz   : There are 6 products (1 pages)\n",
      "FR: 11071 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ stretching‎   : There are 6 products (1 pages)\n",
      "FR: 11077 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ surf   : There are 8 products (1 pages)\n",
      "FR: 11085 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ swimrun   : There are 7 products (1 pages)\n",
      "FR: 11092 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ taekwondo   : There are 6 products (1 pages)\n",
      "FR: 11098 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ taï chi chuan   : There are 6 products (1 pages)\n",
      "FR: 11104 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ tennis   : There are 25 products (1 pages)\n",
      "FR: 11129 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ tennis de table   : There are 7 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 11136 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ tir à l'arc   : There are 6 products (1 pages)\n",
      "FR: 11142 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ tir sportif 22 long rifle   : There are 7 products (1 pages)\n",
      "FR: 11149 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ tir sportif air comprimé   : There are 7 products (1 pages)\n",
      "FR: 11156 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ tonification   : There are 6 products (1 pages)\n",
      "FR: 11162 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ touch rugby   : There are 16 products (1 pages)\n",
      "FR: 11178 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ trail   : There are 27 products (1 pages)\n",
      "FR: 11205 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ trampoline   : There are 6 products (1 pages)\n",
      "FR: 11211 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ trekking arctique   : There are 21 products (1 pages)\n",
      "FR: 11232 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ trekking désert   : There are 21 products (1 pages)\n",
      "FR: 11253 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ trekking montagne   : There are 43 products (2 pages)\n",
      "FR: 11296 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ trekking tropical   : There are 17 products (1 pages)\n",
      "FR: 11313 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ trekking voyage   : There are 34 products (1 pages)\n",
      "FR: 11347 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ triathlon   : There are 11 products (1 pages)\n",
      "FR: 11358 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ trottinette   : There are 6 products (1 pages)\n",
      "FR: 11364 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ trottinette freestyle   : There are 6 products (1 pages)\n",
      "FR: 11370 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ ultimate   : There are 6 products (1 pages)\n",
      "FR: 11376 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ vélo tout chemin randonnée   : There are 7 products (1 pages)\n",
      "FR: 11383 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ vélo ville   : There are 9 products (1 pages)\n",
      "FR: 11392 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ vélo voyage   : There are 7 products (1 pages)\n",
      "FR: 11399 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ via ferrata   : There are 8 products (1 pages)\n",
      "FR: 11407 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ video clip   : There are 6 products (1 pages)\n",
      "FR: 11413 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ voile habitable   : There are 20 products (1 pages)\n",
      "FR: 11433 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ volleyball   : There are 16 products (1 pages)\n",
      "FR: 11449 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ vtt all mountain   : There are 19 products (1 pages)\n",
      "FR: 11468 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ vtt cross country   : There are 22 products (1 pages)\n",
      "FR: 11490 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ vtt randonnée   : There are 20 products (1 pages)\n",
      "FR: 11510 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ waacking   : There are 6 products (1 pages)\n",
      "FR: 11516 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ wakeboard   : There are 6 products (1 pages)\n",
      "FR: 11522 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ waterpolo   : There are 7 products (1 pages)\n",
      "FR: 11529 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ waveboard   : There are 7 products (1 pages)\n",
      "FR: 11536 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ windsurf free ride   : There are 13 products (1 pages)\n",
      "FR: 11549 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ windsurf freestyle   : There are 7 products (1 pages)\n",
      "FR: 11556 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ windsurf slalom   : There are 7 products (1 pages)\n",
      "FR: 11563 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ windsurf wave   : There are 9 products (1 pages)\n",
      "FR: 11572 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ xare   : There are 14 products (1 pages)\n",
      "FR: 11586 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ yoga   : There are 11 products (1 pages)\n",
      "FR: 11597 products have been scraped!\n",
      "FR_Vêtements et chaussures femme_Accessoires_ zumba   : There are 6 products (1 pages)\n",
      "FR: 11603 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ 3x3 basketball  : There are 0 products (0 pages)\n",
      "FR: 11603 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ afrocafit   : There are 0 products (0 pages)\n",
      "FR: 11603 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ aïkido   : There are 1 products (1 pages)\n",
      "FR: 11604 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ alpinisme rocheux   : There are 18 products (1 pages)\n",
      "FR: 11622 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ apnée   : There are 0 products (0 pages)\n",
      "FR: 11622 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ aqua cross training   : There are 0 products (0 pages)\n",
      "FR: 11622 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ aquabike   : There are 0 products (0 pages)\n",
      "FR: 11622 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ aquafitness   : There are 0 products (0 pages)\n",
      "FR: 11622 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ arnis   : There are 1 products (1 pages)\n",
      "FR: 11623 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ athlétisme   : There are 33 products (1 pages)\n",
      "FR: 11656 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ aviron   : There are 0 products (0 pages)\n",
      "FR: 11656 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ badminton   : There are 29 products (1 pages)\n",
      "FR: 11685 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ balade à cheval   : There are 1 products (1 pages)\n",
      "FR: 11686 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ baline gomme creuse   : There are 1 products (1 pages)\n",
      "FR: 11687 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ ball trap   : There are 4 products (1 pages)\n",
      "FR: 11691 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ balle au tambourin   : There are 5 products (1 pages)\n",
      "FR: 11696 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ ballet contemporain   : There are 2 products (1 pages)\n",
      "FR: 11698 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ barre au sol   : There are 2 products (1 pages)\n",
      "FR: 11700 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ baseball   : There are 17 products (1 pages)\n",
      "FR: 11717 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ basketball   : There are 31 products (1 pages)\n",
      "FR: 11748 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ bateau dragon   : There are 0 products (0 pages)\n",
      "FR: 11748 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ beach rugby   : There are 42 products (2 pages)\n",
      "FR: 11790 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ beach soccer   : There are 62 products (2 pages)\n",
      "FR: 11852 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ beach tennis   : There are 9 products (1 pages)\n",
      "FR: 11861 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ beach volley   : There are 14 products (1 pages)\n",
      "FR: 11875 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR_Vêtements et chaussures homme_Vêtements_ billard américain   : There are 0 products (0 pages)\n",
      "FR: 11875 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ billard anglais   : There are 0 products (0 pages)\n",
      "FR: 11875 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ billard français   : There are 0 products (0 pages)\n",
      "FR: 11875 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ bmx freestyle   : There are 3 products (1 pages)\n",
      "FR: 11878 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ body attack   : There are 13 products (1 pages)\n",
      "FR: 11891 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ bodyboard   : There are 25 products (1 pages)\n",
      "FR: 11916 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ bodysurf   : There are 12 products (1 pages)\n",
      "FR: 11928 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ bokwa   : There are 0 products (0 pages)\n",
      "FR: 11928 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ boomerang   : There are 0 products (0 pages)\n",
      "FR: 11928 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ bouées tractées   : There are 0 products (0 pages)\n",
      "FR: 11928 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ boxe anglaise   : There are 11 products (1 pages)\n",
      "FR: 11939 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ boxe française   : There are 9 products (1 pages)\n",
      "FR: 11948 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ break dance   : There are 0 products (0 pages)\n",
      "FR: 11948 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ bushcraft   : There are 43 products (2 pages)\n",
      "FR: 11991 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ canoë kayak course   : There are 3 products (1 pages)\n",
      "FR: 11994 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ canoë kayak randonnee   : There are 3 products (1 pages)\n",
      "FR: 11997 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ canoë kayak vague   : There are 3 products (1 pages)\n",
      "FR: 12000 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ canyoning   : There are 0 products (0 pages)\n",
      "FR: 12000 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ capoeira   : There are 1 products (1 pages)\n",
      "FR: 12001 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ cardio boxing   : There are 10 products (1 pages)\n",
      "FR: 12011 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ cardio training   : There are 94 products (3 pages)\n",
      "FR: 12105 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ carrom   : There are 0 products (0 pages)\n",
      "FR: 12105 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ cascade de glace   : There are 16 products (1 pages)\n",
      "FR: 12121 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ cerf-volant de traction   : There are 0 products (0 pages)\n",
      "FR: 12121 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ cerf-volant pilotable   : There are 0 products (0 pages)\n",
      "FR: 12121 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ cerf-volant statique   : There are 0 products (0 pages)\n",
      "FR: 12121 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ chasse à l'approche   : There are 44 products (2 pages)\n",
      "FR: 12165 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ chasse à l'arc   : There are 19 products (1 pages)\n",
      "FR: 12184 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ chasse au gibier d'eau   : There are 30 products (1 pages)\n",
      "FR: 12214 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ chasse au grand gibier battue poste   : There are 77 products (2 pages)\n",
      "FR: 12291 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ chasse au grand gibier battue traque   : There are 42 products (2 pages)\n",
      "FR: 12333 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ chasse au migrateur   : There are 18 products (1 pages)\n",
      "FR: 12351 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ chasse au petit gibier   : There are 117 products (3 pages)\n",
      "FR: 12468 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ chasse sous marine   : There are 0 products (0 pages)\n",
      "FR: 12468 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ chistera   : There are 26 products (1 pages)\n",
      "FR: 12494 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ commercial hip hop   : There are 0 products (0 pages)\n",
      "FR: 12494 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ commercial jazz   : There are 0 products (0 pages)\n",
      "FR: 12494 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ course d'endurance   : There are 0 products (0 pages)\n",
      "FR: 12494 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ course d'orientation   : There are 25 products (1 pages)\n",
      "FR: 12519 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ cricket   : There are 15 products (1 pages)\n",
      "FR: 12534 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ cross training   : There are 14 products (1 pages)\n",
      "FR: 12548 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ crossminton   : There are 3 products (1 pages)\n",
      "FR: 12551 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ cruiser   : There are 4 products (1 pages)\n",
      "FR: 12555 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ cyclosport‎   : There are 42 products (2 pages)\n",
      "FR: 12597 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ cyclotourisme   : There are 54 products (2 pages)\n",
      "FR: 12651 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ dancehall   : There are 0 products (0 pages)\n",
      "FR: 12651 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ danse classique   : There are 2 products (1 pages)\n",
      "FR: 12653 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ danse contemporaine   : There are 0 products (0 pages)\n",
      "FR: 12653 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ danse modern jazz   : There are 1 products (1 pages)\n",
      "FR: 12654 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ danse néo classique   : There are 2 products (1 pages)\n",
      "FR: 12656 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ dériveur / catamaran   : There are 7 products (1 pages)\n",
      "FR: 12663 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ disc golf   : There are 0 products (0 pages)\n",
      "FR: 12663 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ disque volant   : There are 0 products (0 pages)\n",
      "FR: 12663 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ djembel   : There are 0 products (0 pages)\n",
      "FR: 12663 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ endurance à cheval   : There are 0 products (0 pages)\n",
      "FR: 12663 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ entraînement fonctionnel   : There are 1 products (1 pages)\n",
      "FR: 12664 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ équitation   : There are 19 products (1 pages)\n",
      "FR: 12683 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ équitation concours complet   : There are 1 products (1 pages)\n",
      "FR: 12684 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ équitation dressage   : There are 1 products (1 pages)\n",
      "FR: 12685 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ équitation éthologie   : There are 0 products (0 pages)\n",
      "FR: 12685 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ équitation horse ball   : There are 1 products (1 pages)\n",
      "FR: 12686 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ équitation saut d'obstacles   : There are 4 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 12690 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ équitation western   : There are 1 products (1 pages)\n",
      "FR: 12691 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ escalade en bloc   : There are 14 products (1 pages)\n",
      "FR: 12705 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ escalade en falaise   : There are 19 products (1 pages)\n",
      "FR: 12724 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ escalade en grandes voies   : There are 19 products (1 pages)\n",
      "FR: 12743 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ escalade en salle   : There are 12 products (1 pages)\n",
      "FR: 12755 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ escrime   : There are 3 products (1 pages)\n",
      "FR: 12758 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ éveil aquatique   : There are 0 products (0 pages)\n",
      "FR: 12758 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ expédition haute altitude   : There are 15 products (1 pages)\n",
      "FR: 12773 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ fast touring   : There are 15 products (1 pages)\n",
      "FR: 12788 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ fitness ballet   : There are 0 products (0 pages)\n",
      "FR: 12788 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ flag football   : There are 1 products (1 pages)\n",
      "FR: 12789 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ flamenco   : There are 0 products (0 pages)\n",
      "FR: 12789 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ fléchettes pointe acier   : There are 0 products (0 pages)\n",
      "FR: 12789 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ fléchettes pointe plastique   : There are 0 products (0 pages)\n",
      "FR: 12789 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ floorball   : There are 15 products (1 pages)\n",
      "FR: 12804 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ foot5   : There are 105 products (3 pages)\n",
      "FR: 12909 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ football à 11   : There are 152 products (4 pages)\n",
      "FR: 13061 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ football américain   : There are 16 products (1 pages)\n",
      "FR: 13077 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ frescobol   : There are 25 products (1 pages)\n",
      "FR: 13102 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ frontenis   : There are 30 products (1 pages)\n",
      "FR: 13132 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ full contact   : There are 10 products (1 pages)\n",
      "FR: 13142 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ funk jazz   : There are 0 products (0 pages)\n",
      "FR: 13142 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ futsal   : There are 89 products (3 pages)\n",
      "FR: 13231 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ golf   : There are 42 products (2 pages)\n",
      "FR: 13273 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ grappling   : There are 4 products (1 pages)\n",
      "FR: 13277 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ gravel‎   : There are 40 products (1 pages)\n",
      "FR: 13317 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ gym douce   : There are 96 products (3 pages)\n",
      "FR: 13413 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ gymnastique artistique   : There are 2 products (1 pages)\n",
      "FR: 13415 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ gymnastique éducative et sportive   : There are 0 products (0 pages)\n",
      "FR: 13415 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ gymnastique rythmique   : There are 0 products (0 pages)\n",
      "FR: 13415 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ gymnastique suédoise   : There are 97 products (3 pages)\n",
      "FR: 13512 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ gymnastique volontaire   : There are 1 products (1 pages)\n",
      "FR: 13513 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ handball   : There are 35 products (1 pages)\n",
      "FR: 13548 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ hip-hop   : There are 0 products (0 pages)\n",
      "FR: 13548 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ hockey balle   : There are 2 products (1 pages)\n",
      "FR: 13550 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ hockey russe   : There are 1 products (1 pages)\n",
      "FR: 13551 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ hockey sur gazon   : There are 21 products (1 pages)\n",
      "FR: 13572 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ hockey sur glace   : There are 2 products (1 pages)\n",
      "FR: 13574 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ house dance   : There are 0 products (0 pages)\n",
      "FR: 13574 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ jiu-jitsu   : There are 4 products (1 pages)\n",
      "FR: 13578 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ jogging   : There are 68 products (2 pages)\n",
      "FR: 13646 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ jokari   : There are 26 products (1 pages)\n",
      "FR: 13672 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ judo   : There are 1 products (1 pages)\n",
      "FR: 13673 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ karaté   : There are 2 products (1 pages)\n",
      "FR: 13675 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ kempo   : There are 1 products (1 pages)\n",
      "FR: 13676 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ kendo   : There are 1 products (1 pages)\n",
      "FR: 13677 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ kick boxing   : There are 10 products (1 pages)\n",
      "FR: 13687 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ kitesurf freeride   : There are 2 products (1 pages)\n",
      "FR: 13689 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ kitesurf freestyle   : There are 2 products (1 pages)\n",
      "FR: 13691 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ kitesurf race   : There are 2 products (1 pages)\n",
      "FR: 13693 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ kitesurf vagues   : There are 2 products (1 pages)\n",
      "FR: 13695 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ kitewing   : There are 0 products (0 pages)\n",
      "FR: 13695 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ kubb   : There are 0 products (0 pages)\n",
      "FR: 13695 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ kung fu   : There are 1 products (1 pages)\n",
      "FR: 13696 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ lacrosse   : There are 14 products (1 pages)\n",
      "FR: 13710 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ landkite   : There are 0 products (0 pages)\n",
      "FR: 13710 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ lia   : There are 13 products (1 pages)\n",
      "FR: 13723 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ locking   : There are 0 products (0 pages)\n",
      "FR: 13723 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ longboard skate   : There are 9 products (1 pages)\n",
      "FR: 13732 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ luge   : There are 19 products (1 pages)\n",
      "FR: 13751 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ lutte   : There are 4 products (1 pages)\n",
      "FR: 13755 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ marche athlétique de vitesse   : There are 29 products (1 pages)\n",
      "FR: 13784 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ marche athlétique longue distance   : There are 27 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 13811 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ marche nordique   : There are 30 products (1 pages)\n",
      "FR: 13841 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ marche sportive   : There are 40 products (1 pages)\n",
      "FR: 13881 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ mountain touring   : There are 19 products (1 pages)\n",
      "FR: 13900 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ muay thaï   : There are 10 products (1 pages)\n",
      "FR: 13910 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ musculation   : There are 6 products (1 pages)\n",
      "FR: 13916 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ nage eau libre   : There are 18 products (1 pages)\n",
      "FR: 13934 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ natation sportive   : There are 64 products (2 pages)\n",
      "FR: 13998 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ natation synchronisée artistique   : There are 8 products (1 pages)\n",
      "FR: 14006 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ netball   : There are 13 products (1 pages)\n",
      "FR: 14019 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ observation de la faune   : There are 45 products (2 pages)\n",
      "FR: 14064 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ one wall   : There are 1 products (1 pages)\n",
      "FR: 14065 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ padel   : There are 45 products (2 pages)\n",
      "FR: 14110 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pala gomme pleine   : There are 26 products (1 pages)\n",
      "FR: 14136 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ paleta cuir   : There are 0 products (0 pages)\n",
      "FR: 14136 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ palets breton   : There are 0 products (0 pages)\n",
      "FR: 14136 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ palets vendéens   : There are 0 products (0 pages)\n",
      "FR: 14136 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ parkour   : There are 5 products (1 pages)\n",
      "FR: 14141 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pasaka   : There are 26 products (1 pages)\n",
      "FR: 14167 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ patinage artistique   : There are 0 products (0 pages)\n",
      "FR: 14167 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ patinage sur glace   : There are 0 products (0 pages)\n",
      "FR: 14167 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche a l'anglaise   : There are 0 products (0 pages)\n",
      "FR: 14167 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche a la bolognaise   : There are 0 products (0 pages)\n",
      "FR: 14167 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche a la main   : There are 0 products (0 pages)\n",
      "FR: 14167 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche a la mouche   : There are 3 products (1 pages)\n",
      "FR: 14170 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche a la traine   : There are 0 products (0 pages)\n",
      "FR: 14170 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche à pied   : There are 0 products (0 pages)\n",
      "FR: 14170 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche à soutenir bateau   : There are 0 products (0 pages)\n",
      "FR: 14170 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche au coup chinoise   : There are 1 products (1 pages)\n",
      "FR: 14171 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche au coup emmanchement   : There are 0 products (0 pages)\n",
      "FR: 14171 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche au coup télescopique   : There are 0 products (0 pages)\n",
      "FR: 14171 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche au flotteur en mer   : There are 0 products (0 pages)\n",
      "FR: 14171 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche au jigging   : There are 0 products (0 pages)\n",
      "FR: 14171 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche au leurres en mer   : There are 5 products (1 pages)\n",
      "FR: 14176 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche au poisson mort manie   : There are 0 products (0 pages)\n",
      "FR: 14176 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche au pose   : There are 4 products (1 pages)\n",
      "FR: 14180 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche au posé en bord de mer   : There are 3 products (1 pages)\n",
      "FR: 14183 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche au quiver / feeder   : There are 0 products (0 pages)\n",
      "FR: 14183 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche au vairon manié   : There are 0 products (0 pages)\n",
      "FR: 14183 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche aux leurres black bass   : There are 1 products (1 pages)\n",
      "FR: 14184 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche aux leurres brochet   : There are 4 products (1 pages)\n",
      "FR: 14188 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche aux leurres carnassier   : There are 6 products (1 pages)\n",
      "FR: 14194 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche aux leurres perche   : There are 1 products (1 pages)\n",
      "FR: 14195 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche aux leurres sandre   : There are 1 products (1 pages)\n",
      "FR: 14196 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche aux leurres truite   : There are 5 products (1 pages)\n",
      "FR: 14201 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche de l'ecrevisse   : There are 1 products (1 pages)\n",
      "FR: 14202 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche de la carpe   : There are 4 products (1 pages)\n",
      "FR: 14206 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche de la carpe au coup   : There are 0 products (0 pages)\n",
      "FR: 14206 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche de la truite a la bombette   : There are 0 products (0 pages)\n",
      "FR: 14206 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche de la truite au toc   : There are 2 products (1 pages)\n",
      "FR: 14208 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche de la truite en étang   : There are 1 products (1 pages)\n",
      "FR: 14209 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche de seiches et calamars   : There are 0 products (0 pages)\n",
      "FR: 14209 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche du silure   : There are 0 products (0 pages)\n",
      "FR: 14209 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche en surf casting   : There are 5 products (1 pages)\n",
      "FR: 14214 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pêche sous la glace   : There are 0 products (0 pages)\n",
      "FR: 14214 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pelote   : There are 1 products (1 pages)\n",
      "FR: 14215 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pelote à main nue   : There are 26 products (1 pages)\n",
      "FR: 14241 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pétanque   : There are 0 products (0 pages)\n",
      "FR: 14241 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ peteca   : There are 0 products (0 pages)\n",
      "FR: 14241 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pickleball   : There are 29 products (1 pages)\n",
      "FR: 14270 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pilates   : There are 151 products (4 pages)\n",
      "FR: 14421 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR_Vêtements et chaussures homme_Vêtements_ plongée bouteille   : There are 1 products (1 pages)\n",
      "FR: 14422 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ plongeon   : There are 13 products (1 pages)\n",
      "FR: 14435 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pole dance   : There are 0 products (0 pages)\n",
      "FR: 14435 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ pony games   : There are 1 products (1 pages)\n",
      "FR: 14436 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ popping   : There are 0 products (0 pages)\n",
      "FR: 14436 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ quad agressif   : There are 0 products (0 pages)\n",
      "FR: 14436 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ quad artistique   : There are 0 products (0 pages)\n",
      "FR: 14436 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ quad fitness   : There are 3 products (1 pages)\n",
      "FR: 14439 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ quad freeride   : There are 3 products (1 pages)\n",
      "FR: 14442 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ quad vitesse   : There are 0 products (0 pages)\n",
      "FR: 14442 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ quilles finlandaises   : There are 0 products (0 pages)\n",
      "FR: 14442 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ racquetball   : There are 28 products (1 pages)\n",
      "FR: 14470 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ randonnée à cheval   : There are 3 products (1 pages)\n",
      "FR: 14473 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ randonnée glaciaire   : There are 17 products (1 pages)\n",
      "FR: 14490 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ randonnée montagne   : There are 71 products (2 pages)\n",
      "FR: 14561 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ randonnée nature   : There are 60 products (2 pages)\n",
      "FR: 14621 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ randonnée neige   : There are 28 products (1 pages)\n",
      "FR: 14649 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ randonnée rapide   : There are 18 products (1 pages)\n",
      "FR: 14667 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ rebot   : There are 26 products (1 pages)\n",
      "FR: 14693 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ régate   : There are 11 products (1 pages)\n",
      "FR: 14704 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ resort touring   : There are 19 products (1 pages)\n",
      "FR: 14723 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ rink hockey   : There are 3 products (1 pages)\n",
      "FR: 14726 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ roller agressif   : There are 0 products (0 pages)\n",
      "FR: 14726 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ roller artistique   : There are 0 products (0 pages)\n",
      "FR: 14726 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ roller derby   : There are 1 products (1 pages)\n",
      "FR: 14727 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ roller fitness   : There are 3 products (1 pages)\n",
      "FR: 14730 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ roller freeride   : There are 0 products (0 pages)\n",
      "FR: 14730 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ roller freestyle   : There are 0 products (0 pages)\n",
      "FR: 14730 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ roller hockey   : There are 2 products (1 pages)\n",
      "FR: 14732 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ roller vitesse   : There are 0 products (0 pages)\n",
      "FR: 14732 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ rpm   : There are 15 products (1 pages)\n",
      "FR: 14747 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ rugby à 13   : There are 47 products (2 pages)\n",
      "FR: 14794 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ rugby à 15   : There are 55 products (2 pages)\n",
      "FR: 14849 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ rugby à 7   : There are 44 products (2 pages)\n",
      "FR: 14893 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ running route   : There are 58 products (2 pages)\n",
      "FR: 14951 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ sambo   : There are 1 products (1 pages)\n",
      "FR: 14952 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ sauvetage sportif   : There are 2 products (1 pages)\n",
      "FR: 14954 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ self defense   : There are 1 products (1 pages)\n",
      "FR: 14955 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ sepak takraw   : There are 9 products (1 pages)\n",
      "FR: 14964 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ sh'bam   : There are 0 products (0 pages)\n",
      "FR: 14964 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ skateboard   : There are 11 products (1 pages)\n",
      "FR: 14975 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ ski de fond a roue   : There are 0 products (0 pages)\n",
      "FR: 14975 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ ski de fond alternatif   : There are 11 products (1 pages)\n",
      "FR: 14986 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ ski de fond skating   : There are 11 products (1 pages)\n",
      "FR: 14997 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ ski de piste   : There are 39 products (1 pages)\n",
      "FR: 15036 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ ski freeride   : There are 24 products (1 pages)\n",
      "FR: 15060 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ ski freestyle   : There are 19 products (1 pages)\n",
      "FR: 15079 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ ski nautique   : There are 0 products (0 pages)\n",
      "FR: 15079 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ skimboard   : There are 11 products (1 pages)\n",
      "FR: 15090 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ slackline   : There are 10 products (1 pages)\n",
      "FR: 15100 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ snooker   : There are 0 products (0 pages)\n",
      "FR: 15100 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ snorkeling   : There are 0 products (0 pages)\n",
      "FR: 15100 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ snowboard all mountain   : There are 24 products (1 pages)\n",
      "FR: 15124 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ snowboard carving   : There are 2 products (1 pages)\n",
      "FR: 15126 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ snowboard freeride   : There are 23 products (1 pages)\n",
      "FR: 15149 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ snowboard freestyle   : There are 24 products (1 pages)\n",
      "FR: 15173 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ snowboard jib   : There are 2 products (1 pages)\n",
      "FR: 15175 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ snowboard split   : There are 10 products (1 pages)\n",
      "FR: 15185 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ snowkite   : There are 0 products (0 pages)\n",
      "FR: 15185 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ softball   : There are 16 products (1 pages)\n",
      "FR: 15201 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ speedball   : There are 28 products (1 pages)\n",
      "FR: 15229 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ spéléologie   : There are 0 products (0 pages)\n",
      "FR: 15229 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ squash   : There are 27 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 15256 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ stand up paddle course   : There are 1 products (1 pages)\n",
      "FR: 15257 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ stand up paddle randonnée   : There are 1 products (1 pages)\n",
      "FR: 15258 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ stand up paddle vague   : There are 1 products (1 pages)\n",
      "FR: 15259 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ step   : There are 15 products (1 pages)\n",
      "FR: 15274 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ street hockey   : There are 2 products (1 pages)\n",
      "FR: 15276 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ street jazz   : There are 0 products (0 pages)\n",
      "FR: 15276 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ stretching‎   : There are 129 products (4 pages)\n",
      "FR: 15405 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ surf   : There are 62 products (2 pages)\n",
      "FR: 15467 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ swimrun   : There are 0 products (0 pages)\n",
      "FR: 15467 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ taekwondo   : There are 1 products (1 pages)\n",
      "FR: 15468 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ taï chi chuan   : There are 1 products (1 pages)\n",
      "FR: 15469 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ tennis   : There are 57 products (2 pages)\n",
      "FR: 15526 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ tennis de table   : There are 12 products (1 pages)\n",
      "FR: 15538 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ tir à l'arc   : There are 0 products (0 pages)\n",
      "FR: 15538 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ tir sportif 22 long rifle   : There are 0 products (0 pages)\n",
      "FR: 15538 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ tir sportif air comprimé   : There are 0 products (0 pages)\n",
      "FR: 15538 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ tonification   : There are 115 products (3 pages)\n",
      "FR: 15653 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ touch rugby   : There are 46 products (2 pages)\n",
      "FR: 15699 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ trail   : There are 39 products (1 pages)\n",
      "FR: 15738 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ trampoline   : There are 0 products (0 pages)\n",
      "FR: 15738 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ trekking arctique   : There are 5 products (1 pages)\n",
      "FR: 15743 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ trekking désert   : There are 9 products (1 pages)\n",
      "FR: 15752 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ trekking montagne   : There are 59 products (2 pages)\n",
      "FR: 15811 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ trekking tropical   : There are 11 products (1 pages)\n",
      "FR: 15822 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ trekking voyage   : There are 43 products (2 pages)\n",
      "FR: 15865 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ triathlon   : There are 15 products (1 pages)\n",
      "FR: 15880 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ trottinette   : There are 9 products (1 pages)\n",
      "FR: 15889 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ trottinette freestyle   : There are 2 products (1 pages)\n",
      "FR: 15891 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ ultimate   : There are 0 products (0 pages)\n",
      "FR: 15891 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ vélo tout chemin randonnée   : There are 40 products (1 pages)\n",
      "FR: 15931 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ vélo ville   : There are 24 products (1 pages)\n",
      "FR: 15955 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ vélo voyage   : There are 34 products (1 pages)\n",
      "FR: 15989 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ via ferrata   : There are 19 products (1 pages)\n",
      "FR: 16008 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ video clip   : There are 0 products (0 pages)\n",
      "FR: 16008 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ voile habitable   : There are 28 products (1 pages)\n",
      "FR: 16036 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ volleyball   : There are 25 products (1 pages)\n",
      "FR: 16061 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ vtt all mountain   : There are 27 products (1 pages)\n",
      "FR: 16088 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ vtt cross country   : There are 28 products (1 pages)\n",
      "FR: 16116 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ vtt randonnée   : There are 43 products (2 pages)\n",
      "FR: 16159 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ waacking   : There are 0 products (0 pages)\n",
      "FR: 16159 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ wakeboard   : There are 0 products (0 pages)\n",
      "FR: 16159 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ waterpolo   : There are 7 products (1 pages)\n",
      "FR: 16166 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ waveboard   : There are 1 products (1 pages)\n",
      "FR: 16167 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ windsurf free ride   : There are 2 products (1 pages)\n",
      "FR: 16169 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ windsurf freestyle   : There are 1 products (1 pages)\n",
      "FR: 16170 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ windsurf slalom   : There are 1 products (1 pages)\n",
      "FR: 16171 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ windsurf wave   : There are 2 products (1 pages)\n",
      "FR: 16173 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ xare   : There are 26 products (1 pages)\n",
      "FR: 16199 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ yoga   : There are 18 products (1 pages)\n",
      "FR: 16217 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Vêtements_ zumba   : There are 0 products (0 pages)\n",
      "FR: 16217 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ 3x3 basketball  : There are 0 products (0 pages)\n",
      "FR: 16217 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ afrocafit   : There are 1 products (1 pages)\n",
      "FR: 16218 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ aïkido   : There are 1 products (1 pages)\n",
      "FR: 16219 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ alpinisme rocheux   : There are 3 products (1 pages)\n",
      "FR: 16222 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ apnée   : There are 1 products (1 pages)\n",
      "FR: 16223 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ aqua cross training   : There are 7 products (1 pages)\n",
      "FR: 16230 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ aquabike   : There are 11 products (1 pages)\n",
      "FR: 16241 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ aquafitness   : There are 12 products (1 pages)\n",
      "FR: 16253 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ arnis   : There are 1 products (1 pages)\n",
      "FR: 16254 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ athlétisme   : There are 62 products (2 pages)\n",
      "FR: 16316 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ aviron   : There are 1 products (1 pages)\n",
      "FR: 16317 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ badminton   : There are 92 products (3 pages)\n",
      "FR: 16409 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ balade à cheval   : There are 6 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 16415 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ baline gomme creuse   : There are 1 products (1 pages)\n",
      "FR: 16416 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ ball trap   : There are 1 products (1 pages)\n",
      "FR: 16417 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ balle au tambourin   : There are 6 products (1 pages)\n",
      "FR: 16423 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ ballet contemporain   : There are 5 products (1 pages)\n",
      "FR: 16428 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ barre au sol   : There are 5 products (1 pages)\n",
      "FR: 16433 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ baseball   : There are 1 products (1 pages)\n",
      "FR: 16434 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ basketball   : There are 29 products (1 pages)\n",
      "FR: 16463 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ bateau dragon   : There are 1 products (1 pages)\n",
      "FR: 16464 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ beach rugby   : There are 23 products (1 pages)\n",
      "FR: 16487 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ beach soccer   : There are 9 products (1 pages)\n",
      "FR: 16496 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ beach tennis   : There are 7 products (1 pages)\n",
      "FR: 16503 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ beach volley   : There are 1 products (1 pages)\n",
      "FR: 16504 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ billard américain   : There are 1 products (1 pages)\n",
      "FR: 16505 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ billard anglais   : There are 1 products (1 pages)\n",
      "FR: 16506 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ billard français   : There are 1 products (1 pages)\n",
      "FR: 16507 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ bmx freestyle   : There are 1 products (1 pages)\n",
      "FR: 16508 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ body attack   : There are 6 products (1 pages)\n",
      "FR: 16514 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ bodyboard   : There are 18 products (1 pages)\n",
      "FR: 16532 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ bodysurf   : There are 3 products (1 pages)\n",
      "FR: 16535 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ bokwa   : There are 1 products (1 pages)\n",
      "FR: 16536 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ boomerang   : There are 1 products (1 pages)\n",
      "FR: 16537 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ bouées tractées   : There are 1 products (1 pages)\n",
      "FR: 16538 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ boxe anglaise   : There are 2 products (1 pages)\n",
      "FR: 16540 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ boxe française   : There are 2 products (1 pages)\n",
      "FR: 16542 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ break dance   : There are 2 products (1 pages)\n",
      "FR: 16544 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ bushcraft   : There are 5 products (1 pages)\n",
      "FR: 16549 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ canoë kayak course   : There are 1 products (1 pages)\n",
      "FR: 16550 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ canoë kayak randonnee   : There are 1 products (1 pages)\n",
      "FR: 16551 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ canoë kayak vague   : There are 1 products (1 pages)\n",
      "FR: 16552 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ canyoning   : There are 3 products (1 pages)\n",
      "FR: 16555 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ capoeira   : There are 1 products (1 pages)\n",
      "FR: 16556 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ cardio boxing   : There are 1 products (1 pages)\n",
      "FR: 16557 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ cardio training   : There are 20 products (1 pages)\n",
      "FR: 16577 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ carrom   : There are 1 products (1 pages)\n",
      "FR: 16578 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ cascade de glace   : There are 3 products (1 pages)\n",
      "FR: 16581 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ cerf-volant de traction   : There are 1 products (1 pages)\n",
      "FR: 16582 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ cerf-volant pilotable   : There are 1 products (1 pages)\n",
      "FR: 16583 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ cerf-volant statique   : There are 1 products (1 pages)\n",
      "FR: 16584 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ chasse à l'approche   : There are 10 products (1 pages)\n",
      "FR: 16594 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ chasse à l'arc   : There are 2 products (1 pages)\n",
      "FR: 16596 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ chasse au gibier d'eau   : There are 13 products (1 pages)\n",
      "FR: 16609 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ chasse au grand gibier battue poste   : There are 16 products (1 pages)\n",
      "FR: 16625 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ chasse au grand gibier battue traque   : There are 11 products (1 pages)\n",
      "FR: 16636 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ chasse au migrateur   : There are 6 products (1 pages)\n",
      "FR: 16642 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ chasse au petit gibier   : There are 54 products (2 pages)\n",
      "FR: 16696 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ chasse sous marine   : There are 1 products (1 pages)\n",
      "FR: 16697 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ chistera   : There are 56 products (2 pages)\n",
      "FR: 16753 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ commercial hip hop   : There are 2 products (1 pages)\n",
      "FR: 16755 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ commercial jazz   : There are 2 products (1 pages)\n",
      "FR: 16757 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ course d'endurance   : There are 0 products (0 pages)\n",
      "FR: 16757 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ course d'orientation   : There are 59 products (2 pages)\n",
      "FR: 16816 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ cricket   : There are 1 products (1 pages)\n",
      "FR: 16817 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ cross training   : There are 2 products (1 pages)\n",
      "FR: 16819 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ crossminton   : There are 14 products (1 pages)\n",
      "FR: 16833 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ cruiser   : There are 16 products (1 pages)\n",
      "FR: 16849 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ cyclosport‎   : There are 7 products (1 pages)\n",
      "FR: 16856 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ cyclotourisme   : There are 3 products (1 pages)\n",
      "FR: 16859 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ dancehall   : There are 2 products (1 pages)\n",
      "FR: 16861 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ danse classique   : There are 5 products (1 pages)\n",
      "FR: 16866 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ danse contemporaine   : There are 1 products (1 pages)\n",
      "FR: 16867 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ danse modern jazz   : There are 6 products (1 pages)\n",
      "FR: 16873 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR_Vêtements et chaussures homme_Chaussures_ danse néo classique   : There are 5 products (1 pages)\n",
      "FR: 16878 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ dériveur / catamaran   : There are 1 products (1 pages)\n",
      "FR: 16879 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ disc golf   : There are 1 products (1 pages)\n",
      "FR: 16880 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ disque volant   : There are 1 products (1 pages)\n",
      "FR: 16881 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ djembel   : There are 1 products (1 pages)\n",
      "FR: 16882 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ endurance à cheval   : There are 3 products (1 pages)\n",
      "FR: 16885 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ entraînement fonctionnel   : There are 0 products (0 pages)\n",
      "FR: 16885 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ équitation   : There are 36 products (1 pages)\n",
      "FR: 16921 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ équitation concours complet   : There are 4 products (1 pages)\n",
      "FR: 16925 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ équitation dressage   : There are 4 products (1 pages)\n",
      "FR: 16929 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ équitation éthologie   : There are 2 products (1 pages)\n",
      "FR: 16931 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ équitation horse ball   : There are 2 products (1 pages)\n",
      "FR: 16933 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ équitation saut d'obstacles   : There are 5 products (1 pages)\n",
      "FR: 16938 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ équitation western   : There are 6 products (1 pages)\n",
      "FR: 16944 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ escalade en bloc   : There are 2 products (1 pages)\n",
      "FR: 16946 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ escalade en falaise   : There are 2 products (1 pages)\n",
      "FR: 16948 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ escalade en grandes voies   : There are 2 products (1 pages)\n",
      "FR: 16950 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ escalade en salle   : There are 2 products (1 pages)\n",
      "FR: 16952 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ escrime   : There are 1 products (1 pages)\n",
      "FR: 16953 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ éveil aquatique   : There are 8 products (1 pages)\n",
      "FR: 16961 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ expédition haute altitude   : There are 3 products (1 pages)\n",
      "FR: 16964 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ fast touring   : There are 3 products (1 pages)\n",
      "FR: 16967 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ fitness ballet   : There are 1 products (1 pages)\n",
      "FR: 16968 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ flag football   : There are 1 products (1 pages)\n",
      "FR: 16969 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ flamenco   : There are 1 products (1 pages)\n",
      "FR: 16970 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ fléchettes pointe acier   : There are 1 products (1 pages)\n",
      "FR: 16971 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ fléchettes pointe plastique   : There are 2 products (1 pages)\n",
      "FR: 16973 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ floorball   : There are 1 products (1 pages)\n",
      "FR: 16974 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ foot5   : There are 57 products (2 pages)\n",
      "FR: 17031 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ football à 11   : There are 96 products (3 pages)\n",
      "FR: 17127 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ football américain   : There are 1 products (1 pages)\n",
      "FR: 17128 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ frescobol   : There are 53 products (2 pages)\n",
      "FR: 17181 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ frontenis   : There are 66 products (2 pages)\n",
      "FR: 17247 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ full contact   : There are 1 products (1 pages)\n",
      "FR: 17248 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ funk jazz   : There are 2 products (1 pages)\n",
      "FR: 17250 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ futsal   : There are 29 products (1 pages)\n",
      "FR: 17279 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ golf   : There are 22 products (1 pages)\n",
      "FR: 17301 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ grappling   : There are 1 products (1 pages)\n",
      "FR: 17302 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ gravel‎   : There are 3 products (1 pages)\n",
      "FR: 17305 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ gym douce   : There are 2 products (1 pages)\n",
      "FR: 17307 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ gymnastique artistique   : There are 1 products (1 pages)\n",
      "FR: 17308 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ gymnastique éducative et sportive   : There are 1 products (1 pages)\n",
      "FR: 17309 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ gymnastique rythmique   : There are 2 products (1 pages)\n",
      "FR: 17311 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ gymnastique suédoise   : There are 2 products (1 pages)\n",
      "FR: 17313 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ gymnastique volontaire   : There are 1 products (1 pages)\n",
      "FR: 17314 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ handball   : There are 18 products (1 pages)\n",
      "FR: 17332 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ hip-hop   : There are 2 products (1 pages)\n",
      "FR: 17334 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ hockey balle   : There are 2 products (1 pages)\n",
      "FR: 17336 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ hockey russe   : There are 1 products (1 pages)\n",
      "FR: 17337 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ hockey sur gazon   : There are 34 products (1 pages)\n",
      "FR: 17371 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ hockey sur glace   : There are 2 products (1 pages)\n",
      "FR: 17373 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ house dance   : There are 2 products (1 pages)\n",
      "FR: 17375 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ jiu-jitsu   : There are 1 products (1 pages)\n",
      "FR: 17376 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ jogging   : There are 104 products (3 pages)\n",
      "FR: 17480 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ jokari   : There are 56 products (2 pages)\n",
      "FR: 17536 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ judo   : There are 1 products (1 pages)\n",
      "FR: 17537 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ karaté   : There are 1 products (1 pages)\n",
      "FR: 17538 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ kempo   : There are 1 products (1 pages)\n",
      "FR: 17539 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ kendo   : There are 1 products (1 pages)\n",
      "FR: 17540 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ kick boxing   : There are 1 products (1 pages)\n",
      "FR: 17541 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ kitesurf freeride   : There are 1 products (1 pages)\n",
      "FR: 17542 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ kitesurf freestyle   : There are 1 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 17543 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ kitesurf race   : There are 1 products (1 pages)\n",
      "FR: 17544 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ kitesurf vagues   : There are 1 products (1 pages)\n",
      "FR: 17545 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ kitewing   : There are 1 products (1 pages)\n",
      "FR: 17546 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ kubb   : There are 1 products (1 pages)\n",
      "FR: 17547 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ kung fu   : There are 1 products (1 pages)\n",
      "FR: 17548 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ lacrosse   : There are 1 products (1 pages)\n",
      "FR: 17549 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ landkite   : There are 1 products (1 pages)\n",
      "FR: 17550 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ lia   : There are 6 products (1 pages)\n",
      "FR: 17556 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ locking   : There are 2 products (1 pages)\n",
      "FR: 17558 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ longboard skate   : There are 18 products (1 pages)\n",
      "FR: 17576 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ luge   : There are 8 products (1 pages)\n",
      "FR: 17584 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ lutte   : There are 1 products (1 pages)\n",
      "FR: 17585 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ marche athlétique de vitesse   : There are 11 products (1 pages)\n",
      "FR: 17596 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ marche athlétique longue distance   : There are 10 products (1 pages)\n",
      "FR: 17606 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ marche nordique   : There are 28 products (1 pages)\n",
      "FR: 17634 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ marche sportive   : There are 107 products (3 pages)\n",
      "FR: 17741 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ mountain touring   : There are 4 products (1 pages)\n",
      "FR: 17745 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ muay thaï   : There are 1 products (1 pages)\n",
      "FR: 17746 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ musculation   : There are 1 products (1 pages)\n",
      "FR: 17747 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ nage eau libre   : There are 12 products (1 pages)\n",
      "FR: 17759 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ natation sportive   : There are 22 products (1 pages)\n",
      "FR: 17781 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ natation synchronisée artistique   : There are 14 products (1 pages)\n",
      "FR: 17795 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ netball   : There are 1 products (1 pages)\n",
      "FR: 17796 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ observation de la faune   : There are 8 products (1 pages)\n",
      "FR: 17804 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ one wall   : There are 1 products (1 pages)\n",
      "FR: 17805 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ padel   : There are 99 products (3 pages)\n",
      "FR: 17904 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pala gomme pleine   : There are 56 products (2 pages)\n",
      "FR: 17960 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ paleta cuir   : There are 1 products (1 pages)\n",
      "FR: 17961 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ palets breton   : There are 1 products (1 pages)\n",
      "FR: 17962 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ palets vendéens   : There are 1 products (1 pages)\n",
      "FR: 17963 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ parkour   : There are 1 products (1 pages)\n",
      "FR: 17964 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pasaka   : There are 56 products (2 pages)\n",
      "FR: 18020 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ patinage artistique   : There are 1 products (1 pages)\n",
      "FR: 18021 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ patinage sur glace   : There are 1 products (1 pages)\n",
      "FR: 18022 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche a l'anglaise   : There are 1 products (1 pages)\n",
      "FR: 18023 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche a la bolognaise   : There are 1 products (1 pages)\n",
      "FR: 18024 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche a la main   : There are 1 products (1 pages)\n",
      "FR: 18025 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche a la mouche   : There are 1 products (1 pages)\n",
      "FR: 18026 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche a la traine   : There are 1 products (1 pages)\n",
      "FR: 18027 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche à pied   : There are 1 products (1 pages)\n",
      "FR: 18028 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche à soutenir bateau   : There are 1 products (1 pages)\n",
      "FR: 18029 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche au coup chinoise   : There are 1 products (1 pages)\n",
      "FR: 18030 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche au coup emmanchement   : There are 1 products (1 pages)\n",
      "FR: 18031 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche au coup télescopique   : There are 1 products (1 pages)\n",
      "FR: 18032 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche au flotteur en mer   : There are 1 products (1 pages)\n",
      "FR: 18033 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche au jigging   : There are 1 products (1 pages)\n",
      "FR: 18034 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche au leurres en mer   : There are 1 products (1 pages)\n",
      "FR: 18035 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche au poisson mort manie   : There are 1 products (1 pages)\n",
      "FR: 18036 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche au pose   : There are 1 products (1 pages)\n",
      "FR: 18037 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche au posé en bord de mer   : There are 1 products (1 pages)\n",
      "FR: 18038 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche au quiver / feeder   : There are 1 products (1 pages)\n",
      "FR: 18039 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche au vairon manié   : There are 1 products (1 pages)\n",
      "FR: 18040 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche aux leurres black bass   : There are 1 products (1 pages)\n",
      "FR: 18041 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche aux leurres brochet   : There are 1 products (1 pages)\n",
      "FR: 18042 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche aux leurres carnassier   : There are 1 products (1 pages)\n",
      "FR: 18043 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche aux leurres perche   : There are 1 products (1 pages)\n",
      "FR: 18044 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche aux leurres sandre   : There are 1 products (1 pages)\n",
      "FR: 18045 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche aux leurres truite   : There are 1 products (1 pages)\n",
      "FR: 18046 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche de l'ecrevisse   : There are 1 products (1 pages)\n",
      "FR: 18047 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche de la carpe   : There are 1 products (1 pages)\n",
      "FR: 18048 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche de la carpe au coup   : There are 1 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 18049 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche de la truite a la bombette   : There are 1 products (1 pages)\n",
      "FR: 18050 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche de la truite au toc   : There are 1 products (1 pages)\n",
      "FR: 18051 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche de la truite en étang   : There are 1 products (1 pages)\n",
      "FR: 18052 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche de seiches et calamars   : There are 1 products (1 pages)\n",
      "FR: 18053 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche du silure   : There are 1 products (1 pages)\n",
      "FR: 18054 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche en surf casting   : There are 1 products (1 pages)\n",
      "FR: 18055 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pêche sous la glace   : There are 1 products (1 pages)\n",
      "FR: 18056 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pelote   : There are 1 products (1 pages)\n",
      "FR: 18057 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pelote à main nue   : There are 56 products (2 pages)\n",
      "FR: 18113 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pétanque   : There are 1 products (1 pages)\n",
      "FR: 18114 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ peteca   : There are 1 products (1 pages)\n",
      "FR: 18115 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pickleball   : There are 59 products (2 pages)\n",
      "FR: 18174 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pilates   : There are 8 products (1 pages)\n",
      "FR: 18182 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ plongée bouteille   : There are 1 products (1 pages)\n",
      "FR: 18183 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ plongeon   : There are 13 products (1 pages)\n",
      "FR: 18196 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pole dance   : There are 1 products (1 pages)\n",
      "FR: 18197 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ pony games   : There are 2 products (1 pages)\n",
      "FR: 18199 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ popping   : There are 2 products (1 pages)\n",
      "FR: 18201 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ quad agressif   : There are 1 products (1 pages)\n",
      "FR: 18202 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ quad artistique   : There are 1 products (1 pages)\n",
      "FR: 18203 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ quad fitness   : There are 3 products (1 pages)\n",
      "FR: 18206 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ quad freeride   : There are 3 products (1 pages)\n",
      "FR: 18209 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ quad vitesse   : There are 1 products (1 pages)\n",
      "FR: 18210 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ quilles finlandaises   : There are 1 products (1 pages)\n",
      "FR: 18211 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ racquetball   : There are 69 products (2 pages)\n",
      "FR: 18280 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ randonnée à cheval   : There are 8 products (1 pages)\n",
      "FR: 18288 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ randonnée glaciaire   : There are 3 products (1 pages)\n",
      "FR: 18291 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ randonnée montagne   : There are 86 products (3 pages)\n",
      "FR: 18377 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ randonnée nature   : There are 120 products (3 pages)\n",
      "FR: 18497 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ randonnée neige   : There are 51 products (2 pages)\n",
      "FR: 18548 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ randonnée rapide   : There are 26 products (1 pages)\n",
      "FR: 18574 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ rebot   : There are 56 products (2 pages)\n",
      "FR: 18630 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ régate   : There are 1 products (1 pages)\n",
      "FR: 18631 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ resort touring   : There are 4 products (1 pages)\n",
      "FR: 18635 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ rink hockey   : There are 2 products (1 pages)\n",
      "FR: 18637 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ roller agressif   : There are 1 products (1 pages)\n",
      "FR: 18638 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ roller artistique   : There are 1 products (1 pages)\n",
      "FR: 18639 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ roller derby   : There are 1 products (1 pages)\n",
      "FR: 18640 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ roller fitness   : There are 3 products (1 pages)\n",
      "FR: 18643 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ roller freeride   : There are 3 products (1 pages)\n",
      "FR: 18646 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ roller freestyle   : There are 1 products (1 pages)\n",
      "FR: 18647 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ roller hockey   : There are 2 products (1 pages)\n",
      "FR: 18649 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ roller vitesse   : There are 1 products (1 pages)\n",
      "FR: 18650 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ rpm   : There are 17 products (1 pages)\n",
      "FR: 18667 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ rugby à 13   : There are 36 products (1 pages)\n",
      "FR: 18703 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ rugby à 15   : There are 36 products (1 pages)\n",
      "FR: 18739 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ rugby à 7   : There are 36 products (1 pages)\n",
      "FR: 18775 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ running route   : There are 123 products (4 pages)\n",
      "FR: 18898 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ sambo   : There are 1 products (1 pages)\n",
      "FR: 18899 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ sauvetage sportif   : There are 10 products (1 pages)\n",
      "FR: 18909 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ self defense   : There are 1 products (1 pages)\n",
      "FR: 18910 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ sepak takraw   : There are 1 products (1 pages)\n",
      "FR: 18911 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ sh'bam   : There are 1 products (1 pages)\n",
      "FR: 18912 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ skateboard   : There are 24 products (1 pages)\n",
      "FR: 18936 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ ski de fond a roue   : There are 6 products (1 pages)\n",
      "FR: 18942 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ ski de fond alternatif   : There are 5 products (1 pages)\n",
      "FR: 18947 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ ski de fond skating   : There are 4 products (1 pages)\n",
      "FR: 18951 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ ski de piste   : There are 13 products (1 pages)\n",
      "FR: 18964 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ ski freeride   : There are 12 products (1 pages)\n",
      "FR: 18976 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ ski freestyle   : There are 12 products (1 pages)\n",
      "FR: 18988 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ ski nautique   : There are 1 products (1 pages)\n",
      "FR: 18989 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ skimboard   : There are 2 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 18991 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ slackline   : There are 1 products (1 pages)\n",
      "FR: 18992 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ snooker   : There are 1 products (1 pages)\n",
      "FR: 18993 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ snorkeling   : There are 12 products (1 pages)\n",
      "FR: 19005 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ snowboard all mountain   : There are 10 products (1 pages)\n",
      "FR: 19015 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ snowboard carving   : There are 2 products (1 pages)\n",
      "FR: 19017 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ snowboard freeride   : There are 9 products (1 pages)\n",
      "FR: 19026 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ snowboard freestyle   : There are 10 products (1 pages)\n",
      "FR: 19036 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ snowboard jib   : There are 2 products (1 pages)\n",
      "FR: 19038 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ snowboard split   : There are 5 products (1 pages)\n",
      "FR: 19043 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ snowkite   : There are 1 products (1 pages)\n",
      "FR: 19044 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ softball   : There are 1 products (1 pages)\n",
      "FR: 19045 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ speedball   : There are 62 products (2 pages)\n",
      "FR: 19107 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ spéléologie   : There are 1 products (1 pages)\n",
      "FR: 19108 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ squash   : There are 91 products (3 pages)\n",
      "FR: 19199 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ stand up paddle course   : There are 1 products (1 pages)\n",
      "FR: 19200 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ stand up paddle randonnée   : There are 1 products (1 pages)\n",
      "FR: 19201 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ stand up paddle vague   : There are 1 products (1 pages)\n",
      "FR: 19202 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ step   : There are 17 products (1 pages)\n",
      "FR: 19219 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ street hockey   : There are 2 products (1 pages)\n",
      "FR: 19221 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ street jazz   : There are 2 products (1 pages)\n",
      "FR: 19223 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ stretching‎   : There are 8 products (1 pages)\n",
      "FR: 19231 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ surf   : There are 59 products (2 pages)\n",
      "FR: 19290 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ swimrun   : There are 4 products (1 pages)\n",
      "FR: 19294 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ taekwondo   : There are 1 products (1 pages)\n",
      "FR: 19295 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ taï chi chuan   : There are 1 products (1 pages)\n",
      "FR: 19296 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ tennis   : There are 135 products (4 pages)\n",
      "FR: 19431 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ tennis de table   : There are 43 products (2 pages)\n",
      "FR: 19474 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ tir à l'arc   : There are 1 products (1 pages)\n",
      "FR: 19475 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ tir sportif 22 long rifle   : There are 1 products (1 pages)\n",
      "FR: 19476 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ tir sportif air comprimé   : There are 1 products (1 pages)\n",
      "FR: 19477 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ tonification   : There are 9 products (1 pages)\n",
      "FR: 19486 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ touch rugby   : There are 35 products (1 pages)\n",
      "FR: 19521 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ trail   : There are 82 products (3 pages)\n",
      "FR: 19603 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ trampoline   : There are 1 products (1 pages)\n",
      "FR: 19604 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ trekking arctique   : There are 12 products (1 pages)\n",
      "FR: 19616 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ trekking désert   : There are 30 products (1 pages)\n",
      "FR: 19646 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ trekking montagne   : There are 46 products (2 pages)\n",
      "FR: 19692 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ trekking tropical   : There are 30 products (1 pages)\n",
      "FR: 19722 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ trekking voyage   : There are 79 products (2 pages)\n",
      "FR: 19801 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ triathlon   : There are 58 products (2 pages)\n",
      "FR: 19859 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ trottinette   : There are 1 products (1 pages)\n",
      "FR: 19860 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ trottinette freestyle   : There are 10 products (1 pages)\n",
      "FR: 19870 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ ultimate   : There are 1 products (1 pages)\n",
      "FR: 19871 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ vélo tout chemin randonnée   : There are 4 products (1 pages)\n",
      "FR: 19875 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ vélo ville   : There are 1 products (1 pages)\n",
      "FR: 19876 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ vélo voyage   : There are 4 products (1 pages)\n",
      "FR: 19880 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ via ferrata   : There are 2 products (1 pages)\n",
      "FR: 19882 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ video clip   : There are 2 products (1 pages)\n",
      "FR: 19884 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ voile habitable   : There are 11 products (1 pages)\n",
      "FR: 19895 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ volleyball   : There are 10 products (1 pages)\n",
      "FR: 19905 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ vtt all mountain   : There are 7 products (1 pages)\n",
      "FR: 19912 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ vtt cross country   : There are 11 products (1 pages)\n",
      "FR: 19923 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ vtt randonnée   : There are 15 products (1 pages)\n",
      "FR: 19938 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ waacking   : There are 2 products (1 pages)\n",
      "FR: 19940 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ wakeboard   : There are 1 products (1 pages)\n",
      "FR: 19941 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ waterpolo   : There are 15 products (1 pages)\n",
      "FR: 19956 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ waveboard   : There are 1 products (1 pages)\n",
      "FR: 19957 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ windsurf free ride   : There are 1 products (1 pages)\n",
      "FR: 19958 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ windsurf freestyle   : There are 1 products (1 pages)\n",
      "FR: 19959 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ windsurf slalom   : There are 1 products (1 pages)\n",
      "FR: 19960 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ windsurf wave   : There are 1 products (1 pages)\n",
      "FR: 19961 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ xare   : There are 56 products (2 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 20017 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ yoga   : There are 3 products (1 pages)\n",
      "FR: 20020 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Chaussures_ zumba   : There are 1 products (1 pages)\n",
      "FR: 20021 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ 3x3 basketball  : There are 0 products (0 pages)\n",
      "FR: 20021 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ afrocafit   : There are 6 products (1 pages)\n",
      "FR: 20027 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ aïkido   : There are 6 products (1 pages)\n",
      "FR: 20033 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ alpinisme rocheux   : There are 8 products (1 pages)\n",
      "FR: 20041 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ apnée   : There are 7 products (1 pages)\n",
      "FR: 20048 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ aqua cross training   : There are 6 products (1 pages)\n",
      "FR: 20054 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ aquabike   : There are 7 products (1 pages)\n",
      "FR: 20061 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ aquafitness   : There are 7 products (1 pages)\n",
      "FR: 20068 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ arnis   : There are 6 products (1 pages)\n",
      "FR: 20074 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ athlétisme   : There are 22 products (1 pages)\n",
      "FR: 20096 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ aviron   : There are 6 products (1 pages)\n",
      "FR: 20102 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ badminton   : There are 13 products (1 pages)\n",
      "FR: 20115 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ balade à cheval   : There are 8 products (1 pages)\n",
      "FR: 20123 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ baline gomme creuse   : There are 10 products (1 pages)\n",
      "FR: 20133 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ ball trap   : There are 9 products (1 pages)\n",
      "FR: 20142 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ balle au tambourin   : There are 12 products (1 pages)\n",
      "FR: 20154 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ ballet contemporain   : There are 6 products (1 pages)\n",
      "FR: 20160 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ barre au sol   : There are 6 products (1 pages)\n",
      "FR: 20166 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ baseball   : There are 45 products (2 pages)\n",
      "FR: 20211 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ basketball   : There are 41 products (2 pages)\n",
      "FR: 20252 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ bateau dragon   : There are 6 products (1 pages)\n",
      "FR: 20258 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ beach rugby   : There are 16 products (1 pages)\n",
      "FR: 20274 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ beach soccer   : There are 41 products (2 pages)\n",
      "FR: 20315 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ beach tennis   : There are 13 products (1 pages)\n",
      "FR: 20328 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ beach volley   : There are 17 products (1 pages)\n",
      "FR: 20345 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ billard américain   : There are 6 products (1 pages)\n",
      "FR: 20351 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ billard anglais   : There are 6 products (1 pages)\n",
      "FR: 20357 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ billard français   : There are 6 products (1 pages)\n",
      "FR: 20363 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ bmx freestyle   : There are 6 products (1 pages)\n",
      "FR: 20369 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ body attack   : There are 17 products (1 pages)\n",
      "FR: 20386 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ bodyboard   : There are 6 products (1 pages)\n",
      "FR: 20392 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ bodysurf   : There are 6 products (1 pages)\n",
      "FR: 20398 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ bokwa   : There are 6 products (1 pages)\n",
      "FR: 20404 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ boomerang   : There are 6 products (1 pages)\n",
      "FR: 20410 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ bouées tractées   : There are 6 products (1 pages)\n",
      "FR: 20416 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ boxe anglaise   : There are 7 products (1 pages)\n",
      "FR: 20423 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ boxe française   : There are 6 products (1 pages)\n",
      "FR: 20429 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ break dance   : There are 6 products (1 pages)\n",
      "FR: 20435 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ bushcraft   : There are 18 products (1 pages)\n",
      "FR: 20453 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ canoë kayak course   : There are 14 products (1 pages)\n",
      "FR: 20467 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ canoë kayak randonnee   : There are 15 products (1 pages)\n",
      "FR: 20482 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ canoë kayak vague   : There are 10 products (1 pages)\n",
      "FR: 20492 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ canyoning   : There are 7 products (1 pages)\n",
      "FR: 20499 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ capoeira   : There are 6 products (1 pages)\n",
      "FR: 20505 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ cardio boxing   : There are 7 products (1 pages)\n",
      "FR: 20512 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ cardio training   : There are 22 products (1 pages)\n",
      "FR: 20534 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ carrom   : There are 6 products (1 pages)\n",
      "FR: 20540 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ cascade de glace   : There are 7 products (1 pages)\n",
      "FR: 20547 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ cerf-volant de traction   : There are 9 products (1 pages)\n",
      "FR: 20556 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ cerf-volant pilotable   : There are 6 products (1 pages)\n",
      "FR: 20562 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ cerf-volant statique   : There are 6 products (1 pages)\n",
      "FR: 20568 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ chasse à l'approche   : There are 17 products (1 pages)\n",
      "FR: 20585 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ chasse à l'arc   : There are 12 products (1 pages)\n",
      "FR: 20597 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ chasse au gibier d'eau   : There are 21 products (1 pages)\n",
      "FR: 20618 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ chasse au grand gibier battue poste   : There are 34 products (1 pages)\n",
      "FR: 20652 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ chasse au grand gibier battue traque   : There are 30 products (1 pages)\n",
      "FR: 20682 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ chasse au migrateur   : There are 17 products (1 pages)\n",
      "FR: 20699 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ chasse au petit gibier   : There are 38 products (1 pages)\n",
      "FR: 20737 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ chasse sous marine   : There are 6 products (1 pages)\n",
      "FR: 20743 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR_Vêtements et chaussures homme_Accessoires_ chistera   : There are 13 products (1 pages)\n",
      "FR: 20756 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ commercial hip hop   : There are 6 products (1 pages)\n",
      "FR: 20762 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ commercial jazz   : There are 6 products (1 pages)\n",
      "FR: 20768 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ course d'endurance   : There are 0 products (0 pages)\n",
      "FR: 20768 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ course d'orientation   : There are 15 products (1 pages)\n",
      "FR: 20783 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ cricket   : There are 8 products (1 pages)\n",
      "FR: 20791 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ cross training   : There are 7 products (1 pages)\n",
      "FR: 20798 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ crossminton   : There are 9 products (1 pages)\n",
      "FR: 20807 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ cruiser   : There are 10 products (1 pages)\n",
      "FR: 20817 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ cyclosport‎   : There are 18 products (1 pages)\n",
      "FR: 20835 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ cyclotourisme   : There are 17 products (1 pages)\n",
      "FR: 20852 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ dancehall   : There are 6 products (1 pages)\n",
      "FR: 20858 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ danse classique   : There are 6 products (1 pages)\n",
      "FR: 20864 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ danse contemporaine   : There are 6 products (1 pages)\n",
      "FR: 20870 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ danse modern jazz   : There are 6 products (1 pages)\n",
      "FR: 20876 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ danse néo classique   : There are 6 products (1 pages)\n",
      "FR: 20882 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ dériveur / catamaran   : There are 15 products (1 pages)\n",
      "FR: 20897 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ disc golf   : There are 6 products (1 pages)\n",
      "FR: 20903 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ disque volant   : There are 6 products (1 pages)\n",
      "FR: 20909 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ djembel   : There are 6 products (1 pages)\n",
      "FR: 20915 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ endurance à cheval   : There are 7 products (1 pages)\n",
      "FR: 20922 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ entraînement fonctionnel   : There are 0 products (0 pages)\n",
      "FR: 20922 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ équitation   : There are 15 products (1 pages)\n",
      "FR: 20937 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ équitation concours complet   : There are 6 products (1 pages)\n",
      "FR: 20943 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ équitation dressage   : There are 8 products (1 pages)\n",
      "FR: 20951 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ équitation éthologie   : There are 6 products (1 pages)\n",
      "FR: 20957 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ équitation horse ball   : There are 6 products (1 pages)\n",
      "FR: 20963 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ équitation saut d'obstacles   : There are 7 products (1 pages)\n",
      "FR: 20970 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ équitation western   : There are 8 products (1 pages)\n",
      "FR: 20978 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ escalade en bloc   : There are 8 products (1 pages)\n",
      "FR: 20986 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ escalade en falaise   : There are 8 products (1 pages)\n",
      "FR: 20994 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ escalade en grandes voies   : There are 8 products (1 pages)\n",
      "FR: 21002 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ escalade en salle   : There are 6 products (1 pages)\n",
      "FR: 21008 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ escrime   : There are 6 products (1 pages)\n",
      "FR: 21014 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ éveil aquatique   : There are 7 products (1 pages)\n",
      "FR: 21021 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ expédition haute altitude   : There are 7 products (1 pages)\n",
      "FR: 21028 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ fast touring   : There are 24 products (1 pages)\n",
      "FR: 21052 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ fitness ballet   : There are 6 products (1 pages)\n",
      "FR: 21058 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ flag football   : There are 9 products (1 pages)\n",
      "FR: 21067 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ flamenco   : There are 6 products (1 pages)\n",
      "FR: 21073 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ fléchettes pointe acier   : There are 6 products (1 pages)\n",
      "FR: 21079 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ fléchettes pointe plastique   : There are 6 products (1 pages)\n",
      "FR: 21085 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ floorball   : There are 8 products (1 pages)\n",
      "FR: 21093 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ foot5   : There are 41 products (2 pages)\n",
      "FR: 21134 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ football à 11   : There are 43 products (2 pages)\n",
      "FR: 21177 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ football américain   : There are 39 products (1 pages)\n",
      "FR: 21216 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ frescobol   : There are 13 products (1 pages)\n",
      "FR: 21229 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ frontenis   : There are 16 products (1 pages)\n",
      "FR: 21245 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ full contact   : There are 6 products (1 pages)\n",
      "FR: 21251 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ funk jazz   : There are 6 products (1 pages)\n",
      "FR: 21257 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ futsal   : There are 41 products (2 pages)\n",
      "FR: 21298 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ golf   : There are 21 products (1 pages)\n",
      "FR: 21319 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ grappling   : There are 6 products (1 pages)\n",
      "FR: 21325 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ gravel‎   : There are 16 products (1 pages)\n",
      "FR: 21341 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ gym douce   : There are 9 products (1 pages)\n",
      "FR: 21350 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ gymnastique artistique   : There are 6 products (1 pages)\n",
      "FR: 21356 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ gymnastique éducative et sportive   : There are 6 products (1 pages)\n",
      "FR: 21362 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ gymnastique rythmique   : There are 6 products (1 pages)\n",
      "FR: 21368 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ gymnastique suédoise   : There are 9 products (1 pages)\n",
      "FR: 21377 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ gymnastique volontaire   : There are 6 products (1 pages)\n",
      "FR: 21383 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ handball   : There are 16 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 21399 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ hip-hop   : There are 6 products (1 pages)\n",
      "FR: 21405 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ hockey balle   : There are 6 products (1 pages)\n",
      "FR: 21411 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ hockey russe   : There are 6 products (1 pages)\n",
      "FR: 21417 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ hockey sur gazon   : There are 8 products (1 pages)\n",
      "FR: 21425 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ hockey sur glace   : There are 8 products (1 pages)\n",
      "FR: 21433 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ house dance   : There are 6 products (1 pages)\n",
      "FR: 21439 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ jiu-jitsu   : There are 6 products (1 pages)\n",
      "FR: 21445 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ jogging   : There are 46 products (2 pages)\n",
      "FR: 21491 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ jokari   : There are 13 products (1 pages)\n",
      "FR: 21504 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ judo   : There are 6 products (1 pages)\n",
      "FR: 21510 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ karaté   : There are 6 products (1 pages)\n",
      "FR: 21516 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ kempo   : There are 6 products (1 pages)\n",
      "FR: 21522 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ kendo   : There are 6 products (1 pages)\n",
      "FR: 21528 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ kick boxing   : There are 6 products (1 pages)\n",
      "FR: 21534 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ kitesurf freeride   : There are 9 products (1 pages)\n",
      "FR: 21543 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ kitesurf freestyle   : There are 9 products (1 pages)\n",
      "FR: 21552 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ kitesurf race   : There are 9 products (1 pages)\n",
      "FR: 21561 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ kitesurf vagues   : There are 9 products (1 pages)\n",
      "FR: 21570 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ kitewing   : There are 6 products (1 pages)\n",
      "FR: 21576 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ kubb   : There are 6 products (1 pages)\n",
      "FR: 21582 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ kung fu   : There are 6 products (1 pages)\n",
      "FR: 21588 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ lacrosse   : There are 8 products (1 pages)\n",
      "FR: 21596 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ landkite   : There are 9 products (1 pages)\n",
      "FR: 21605 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ lia   : There are 16 products (1 pages)\n",
      "FR: 21621 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ locking   : There are 6 products (1 pages)\n",
      "FR: 21627 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ longboard skate   : There are 11 products (1 pages)\n",
      "FR: 21638 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ luge   : There are 28 products (1 pages)\n",
      "FR: 21666 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ lutte   : There are 6 products (1 pages)\n",
      "FR: 21672 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ marche athlétique de vitesse   : There are 17 products (1 pages)\n",
      "FR: 21689 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ marche athlétique longue distance   : There are 17 products (1 pages)\n",
      "FR: 21706 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ marche nordique   : There are 19 products (1 pages)\n",
      "FR: 21725 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ marche sportive   : There are 22 products (1 pages)\n",
      "FR: 21747 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ mountain touring   : There are 25 products (1 pages)\n",
      "FR: 21772 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ muay thaï   : There are 6 products (1 pages)\n",
      "FR: 21778 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ musculation   : There are 7 products (1 pages)\n",
      "FR: 21785 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ nage eau libre   : There are 8 products (1 pages)\n",
      "FR: 21793 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ natation sportive   : There are 8 products (1 pages)\n",
      "FR: 21801 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ natation synchronisée artistique   : There are 7 products (1 pages)\n",
      "FR: 21808 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ netball   : There are 8 products (1 pages)\n",
      "FR: 21816 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ observation de la faune   : There are 19 products (1 pages)\n",
      "FR: 21835 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ one wall   : There are 10 products (1 pages)\n",
      "FR: 21845 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ padel   : There are 20 products (1 pages)\n",
      "FR: 21865 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pala gomme pleine   : There are 13 products (1 pages)\n",
      "FR: 21878 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ paleta cuir   : There are 6 products (1 pages)\n",
      "FR: 21884 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ palets breton   : There are 6 products (1 pages)\n",
      "FR: 21890 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ palets vendéens   : There are 6 products (1 pages)\n",
      "FR: 21896 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ parkour   : There are 6 products (1 pages)\n",
      "FR: 21902 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pasaka   : There are 13 products (1 pages)\n",
      "FR: 21915 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ patinage artistique   : There are 6 products (1 pages)\n",
      "FR: 21921 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ patinage sur glace   : There are 6 products (1 pages)\n",
      "FR: 21927 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche a l'anglaise   : There are 6 products (1 pages)\n",
      "FR: 21933 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche a la bolognaise   : There are 6 products (1 pages)\n",
      "FR: 21939 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche a la main   : There are 6 products (1 pages)\n",
      "FR: 21945 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche a la mouche   : There are 6 products (1 pages)\n",
      "FR: 21951 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche a la traine   : There are 6 products (1 pages)\n",
      "FR: 21957 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche à pied   : There are 6 products (1 pages)\n",
      "FR: 21963 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche à soutenir bateau   : There are 6 products (1 pages)\n",
      "FR: 21969 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche au coup chinoise   : There are 6 products (1 pages)\n",
      "FR: 21975 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche au coup emmanchement   : There are 6 products (1 pages)\n",
      "FR: 21981 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche au coup télescopique   : There are 6 products (1 pages)\n",
      "FR: 21987 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche au flotteur en mer   : There are 6 products (1 pages)\n",
      "FR: 21993 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche au jigging   : There are 6 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 21999 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche au leurres en mer   : There are 6 products (1 pages)\n",
      "FR: 22005 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche au poisson mort manie   : There are 7 products (1 pages)\n",
      "FR: 22012 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche au pose   : There are 7 products (1 pages)\n",
      "FR: 22019 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche au posé en bord de mer   : There are 6 products (1 pages)\n",
      "FR: 22025 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche au quiver / feeder   : There are 6 products (1 pages)\n",
      "FR: 22031 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche au vairon manié   : There are 7 products (1 pages)\n",
      "FR: 22038 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche aux leurres black bass   : There are 6 products (1 pages)\n",
      "FR: 22044 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche aux leurres brochet   : There are 6 products (1 pages)\n",
      "FR: 22050 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche aux leurres carnassier   : There are 7 products (1 pages)\n",
      "FR: 22057 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche aux leurres perche   : There are 6 products (1 pages)\n",
      "FR: 22063 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche aux leurres sandre   : There are 6 products (1 pages)\n",
      "FR: 22069 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche aux leurres truite   : There are 6 products (1 pages)\n",
      "FR: 22075 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche de l'ecrevisse   : There are 6 products (1 pages)\n",
      "FR: 22081 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche de la carpe   : There are 6 products (1 pages)\n",
      "FR: 22087 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche de la carpe au coup   : There are 6 products (1 pages)\n",
      "FR: 22093 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche de la truite a la bombette   : There are 6 products (1 pages)\n",
      "FR: 22099 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche de la truite au toc   : There are 7 products (1 pages)\n",
      "FR: 22106 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche de la truite en étang   : There are 6 products (1 pages)\n",
      "FR: 22112 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche de seiches et calamars   : There are 6 products (1 pages)\n",
      "FR: 22118 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche du silure   : There are 6 products (1 pages)\n",
      "FR: 22124 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche en surf casting   : There are 6 products (1 pages)\n",
      "FR: 22130 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pêche sous la glace   : There are 6 products (1 pages)\n",
      "FR: 22136 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pelote   : There are 10 products (1 pages)\n",
      "FR: 22146 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pelote à main nue   : There are 13 products (1 pages)\n",
      "FR: 22159 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pétanque   : There are 6 products (1 pages)\n",
      "FR: 22165 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ peteca   : There are 6 products (1 pages)\n",
      "FR: 22171 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pickleball   : There are 15 products (1 pages)\n",
      "FR: 22186 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pilates   : There are 11 products (1 pages)\n",
      "FR: 22197 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ plongée bouteille   : There are 10 products (1 pages)\n",
      "FR: 22207 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ plongeon   : There are 7 products (1 pages)\n",
      "FR: 22214 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pole dance   : There are 6 products (1 pages)\n",
      "FR: 22220 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ pony games   : There are 6 products (1 pages)\n",
      "FR: 22226 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ popping   : There are 6 products (1 pages)\n",
      "FR: 22232 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ quad agressif   : There are 6 products (1 pages)\n",
      "FR: 22238 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ quad artistique   : There are 6 products (1 pages)\n",
      "FR: 22244 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ quad fitness   : There are 6 products (1 pages)\n",
      "FR: 22250 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ quad freeride   : There are 6 products (1 pages)\n",
      "FR: 22256 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ quad vitesse   : There are 6 products (1 pages)\n",
      "FR: 22262 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ quilles finlandaises   : There are 6 products (1 pages)\n",
      "FR: 22268 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ racquetball   : There are 10 products (1 pages)\n",
      "FR: 22278 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ randonnée à cheval   : There are 8 products (1 pages)\n",
      "FR: 22286 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ randonnée glaciaire   : There are 7 products (1 pages)\n",
      "FR: 22293 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ randonnée montagne   : There are 69 products (2 pages)\n",
      "FR: 22362 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ randonnée nature   : There are 73 products (2 pages)\n",
      "FR: 22435 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ randonnée neige   : There are 35 products (1 pages)\n",
      "FR: 22470 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ randonnée rapide   : There are 28 products (1 pages)\n",
      "FR: 22498 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ rebot   : There are 13 products (1 pages)\n",
      "FR: 22511 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ régate   : There are 14 products (1 pages)\n",
      "FR: 22525 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ resort touring   : There are 25 products (1 pages)\n",
      "FR: 22550 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ rink hockey   : There are 6 products (1 pages)\n",
      "FR: 22556 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ roller agressif   : There are 6 products (1 pages)\n",
      "FR: 22562 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ roller artistique   : There are 6 products (1 pages)\n",
      "FR: 22568 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ roller derby   : There are 6 products (1 pages)\n",
      "FR: 22574 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ roller fitness   : There are 6 products (1 pages)\n",
      "FR: 22580 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ roller freeride   : There are 6 products (1 pages)\n",
      "FR: 22586 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ roller freestyle   : There are 6 products (1 pages)\n",
      "FR: 22592 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ roller hockey   : There are 7 products (1 pages)\n",
      "FR: 22599 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ roller vitesse   : There are 6 products (1 pages)\n",
      "FR: 22605 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ rpm   : There are 17 products (1 pages)\n",
      "FR: 22622 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ rugby à 13   : There are 17 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 22639 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ rugby à 15   : There are 17 products (1 pages)\n",
      "FR: 22656 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ rugby à 7   : There are 16 products (1 pages)\n",
      "FR: 22672 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ running route   : There are 32 products (1 pages)\n",
      "FR: 22704 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ sambo   : There are 6 products (1 pages)\n",
      "FR: 22710 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ sauvetage sportif   : There are 7 products (1 pages)\n",
      "FR: 22717 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ self defense   : There are 6 products (1 pages)\n",
      "FR: 22723 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ sepak takraw   : There are 6 products (1 pages)\n",
      "FR: 22729 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ sh'bam   : There are 6 products (1 pages)\n",
      "FR: 22735 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ skateboard   : There are 11 products (1 pages)\n",
      "FR: 22746 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ ski de fond a roue   : There are 9 products (1 pages)\n",
      "FR: 22755 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ ski de fond alternatif   : There are 12 products (1 pages)\n",
      "FR: 22767 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ ski de fond skating   : There are 12 products (1 pages)\n",
      "FR: 22779 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ ski de piste   : There are 50 products (2 pages)\n",
      "FR: 22829 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ ski freeride   : There are 39 products (1 pages)\n",
      "FR: 22868 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ ski freestyle   : There are 33 products (1 pages)\n",
      "FR: 22901 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ ski nautique   : There are 6 products (1 pages)\n",
      "FR: 22907 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ skimboard   : There are 6 products (1 pages)\n",
      "FR: 22913 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ slackline   : There are 6 products (1 pages)\n",
      "FR: 22919 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ snooker   : There are 6 products (1 pages)\n",
      "FR: 22925 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ snorkeling   : There are 6 products (1 pages)\n",
      "FR: 22931 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ snowboard all mountain   : There are 34 products (1 pages)\n",
      "FR: 22965 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ snowboard carving   : There are 6 products (1 pages)\n",
      "FR: 22971 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ snowboard freeride   : There are 26 products (1 pages)\n",
      "FR: 22997 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ snowboard freestyle   : There are 29 products (1 pages)\n",
      "FR: 23026 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ snowboard jib   : There are 6 products (1 pages)\n",
      "FR: 23032 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ snowboard split   : There are 8 products (1 pages)\n",
      "FR: 23040 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ snowkite   : There are 9 products (1 pages)\n",
      "FR: 23049 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ softball   : There are 24 products (1 pages)\n",
      "FR: 23073 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ speedball   : There are 12 products (1 pages)\n",
      "FR: 23085 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ spéléologie   : There are 7 products (1 pages)\n",
      "FR: 23092 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ squash   : There are 16 products (1 pages)\n",
      "FR: 23108 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ stand up paddle course   : There are 14 products (1 pages)\n",
      "FR: 23122 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ stand up paddle randonnée   : There are 15 products (1 pages)\n",
      "FR: 23137 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ stand up paddle vague   : There are 10 products (1 pages)\n",
      "FR: 23147 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ step   : There are 17 products (1 pages)\n",
      "FR: 23164 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ street hockey   : There are 6 products (1 pages)\n",
      "FR: 23170 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ street jazz   : There are 6 products (1 pages)\n",
      "FR: 23176 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ stretching‎   : There are 9 products (1 pages)\n",
      "FR: 23185 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ surf   : There are 8 products (1 pages)\n",
      "FR: 23193 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ swimrun   : There are 7 products (1 pages)\n",
      "FR: 23200 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ taekwondo   : There are 6 products (1 pages)\n",
      "FR: 23206 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ taï chi chuan   : There are 6 products (1 pages)\n",
      "FR: 23212 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ tennis   : There are 24 products (1 pages)\n",
      "FR: 23236 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ tennis de table   : There are 7 products (1 pages)\n",
      "FR: 23243 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ tir à l'arc   : There are 6 products (1 pages)\n",
      "FR: 23249 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ tir sportif 22 long rifle   : There are 7 products (1 pages)\n",
      "FR: 23256 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ tir sportif air comprimé   : There are 7 products (1 pages)\n",
      "FR: 23263 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ tonification   : There are 9 products (1 pages)\n",
      "FR: 23272 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ touch rugby   : There are 16 products (1 pages)\n",
      "FR: 23288 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ trail   : There are 26 products (1 pages)\n",
      "FR: 23314 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ trampoline   : There are 6 products (1 pages)\n",
      "FR: 23320 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ trekking arctique   : There are 21 products (1 pages)\n",
      "FR: 23341 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ trekking désert   : There are 21 products (1 pages)\n",
      "FR: 23362 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ trekking montagne   : There are 47 products (2 pages)\n",
      "FR: 23409 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ trekking tropical   : There are 17 products (1 pages)\n",
      "FR: 23426 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ trekking voyage   : There are 34 products (1 pages)\n",
      "FR: 23460 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ triathlon   : There are 11 products (1 pages)\n",
      "FR: 23471 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ trottinette   : There are 6 products (1 pages)\n",
      "FR: 23477 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ trottinette freestyle   : There are 6 products (1 pages)\n",
      "FR: 23483 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ ultimate   : There are 6 products (1 pages)\n",
      "FR: 23489 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ vélo tout chemin randonnée   : There are 7 products (1 pages)\n",
      "FR: 23496 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ vélo ville   : There are 9 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 23505 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ vélo voyage   : There are 7 products (1 pages)\n",
      "FR: 23512 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ via ferrata   : There are 8 products (1 pages)\n",
      "FR: 23520 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ video clip   : There are 6 products (1 pages)\n",
      "FR: 23526 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ voile habitable   : There are 20 products (1 pages)\n",
      "FR: 23546 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ volleyball   : There are 16 products (1 pages)\n",
      "FR: 23562 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ vtt all mountain   : There are 19 products (1 pages)\n",
      "FR: 23581 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ vtt cross country   : There are 22 products (1 pages)\n",
      "FR: 23603 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ vtt randonnée   : There are 20 products (1 pages)\n",
      "FR: 23623 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ waacking   : There are 6 products (1 pages)\n",
      "FR: 23629 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ wakeboard   : There are 6 products (1 pages)\n",
      "FR: 23635 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ waterpolo   : There are 7 products (1 pages)\n",
      "FR: 23642 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ waveboard   : There are 7 products (1 pages)\n",
      "FR: 23649 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ windsurf free ride   : There are 13 products (1 pages)\n",
      "FR: 23662 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ windsurf freestyle   : There are 7 products (1 pages)\n",
      "FR: 23669 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ windsurf slalom   : There are 7 products (1 pages)\n",
      "FR: 23676 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ windsurf wave   : There are 9 products (1 pages)\n",
      "FR: 23685 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ xare   : There are 13 products (1 pages)\n",
      "FR: 23698 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ yoga   : There are 11 products (1 pages)\n",
      "FR: 23709 products have been scraped!\n",
      "FR_Vêtements et chaussures homme_Accessoires_ zumba   : There are 6 products (1 pages)\n",
      "FR: 23715 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ 3x3 basketball  : There are 0 products (0 pages)\n",
      "FR: 23715 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ afrocafit   : There are 0 products (0 pages)\n",
      "FR: 23715 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ aïkido   : There are 0 products (0 pages)\n",
      "FR: 23715 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ alpinisme rocheux   : There are 0 products (0 pages)\n",
      "FR: 23715 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ apnée   : There are 0 products (0 pages)\n",
      "FR: 23715 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ aqua cross training   : There are 0 products (0 pages)\n",
      "FR: 23715 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ aquabike   : There are 1 products (1 pages)\n",
      "FR: 23716 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ aquafitness   : There are 1 products (1 pages)\n",
      "FR: 23717 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ arnis   : There are 0 products (0 pages)\n",
      "FR: 23717 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ athlétisme   : There are 27 products (1 pages)\n",
      "FR: 23744 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ aviron   : There are 0 products (0 pages)\n",
      "FR: 23744 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ badminton   : There are 17 products (1 pages)\n",
      "FR: 23761 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ balade à cheval   : There are 0 products (0 pages)\n",
      "FR: 23761 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ baline gomme creuse   : There are 1 products (1 pages)\n",
      "FR: 23762 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ ball trap   : There are 0 products (0 pages)\n",
      "FR: 23762 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ balle au tambourin   : There are 3 products (1 pages)\n",
      "FR: 23765 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ ballet contemporain   : There are 11 products (1 pages)\n",
      "FR: 23776 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ barre au sol   : There are 11 products (1 pages)\n",
      "FR: 23787 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ baseball   : There are 7 products (1 pages)\n",
      "FR: 23794 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ basketball   : There are 27 products (1 pages)\n",
      "FR: 23821 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ bateau dragon   : There are 0 products (0 pages)\n",
      "FR: 23821 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ beach rugby   : There are 23 products (1 pages)\n",
      "FR: 23844 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ beach soccer   : There are 41 products (2 pages)\n",
      "FR: 23885 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ beach tennis   : There are 3 products (1 pages)\n",
      "FR: 23888 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ beach volley   : There are 7 products (1 pages)\n",
      "FR: 23895 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ billard américain   : There are 0 products (0 pages)\n",
      "FR: 23895 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ billard anglais   : There are 0 products (0 pages)\n",
      "FR: 23895 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ billard français   : There are 0 products (0 pages)\n",
      "FR: 23895 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ bmx freestyle   : There are 1 products (1 pages)\n",
      "FR: 23896 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ body attack   : There are 0 products (0 pages)\n",
      "FR: 23896 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ bodyboard   : There are 8 products (1 pages)\n",
      "FR: 23904 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ bodysurf   : There are 5 products (1 pages)\n",
      "FR: 23909 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ bokwa   : There are 0 products (0 pages)\n",
      "FR: 23909 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ boomerang   : There are 0 products (0 pages)\n",
      "FR: 23909 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ bouées tractées   : There are 0 products (0 pages)\n",
      "FR: 23909 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ boxe anglaise   : There are 3 products (1 pages)\n",
      "FR: 23912 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ boxe française   : There are 3 products (1 pages)\n",
      "FR: 23915 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ break dance   : There are 1 products (1 pages)\n",
      "FR: 23916 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ bushcraft   : There are 0 products (0 pages)\n",
      "FR: 23916 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ canoë kayak course   : There are 2 products (1 pages)\n",
      "FR: 23918 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ canoë kayak randonnee   : There are 2 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 23920 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ canoë kayak vague   : There are 2 products (1 pages)\n",
      "FR: 23922 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ canyoning   : There are 0 products (0 pages)\n",
      "FR: 23922 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ capoeira   : There are 0 products (0 pages)\n",
      "FR: 23922 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ cardio boxing   : There are 3 products (1 pages)\n",
      "FR: 23925 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ cardio training   : There are 5 products (1 pages)\n",
      "FR: 23930 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ carrom   : There are 0 products (0 pages)\n",
      "FR: 23930 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ cascade de glace   : There are 0 products (0 pages)\n",
      "FR: 23930 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ cerf-volant de traction   : There are 0 products (0 pages)\n",
      "FR: 23930 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ cerf-volant pilotable   : There are 0 products (0 pages)\n",
      "FR: 23930 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ cerf-volant statique   : There are 0 products (0 pages)\n",
      "FR: 23930 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ chasse à l'approche   : There are 0 products (0 pages)\n",
      "FR: 23930 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ chasse à l'arc   : There are 0 products (0 pages)\n",
      "FR: 23930 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ chasse au gibier d'eau   : There are 0 products (0 pages)\n",
      "FR: 23930 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ chasse au grand gibier battue poste   : There are 0 products (0 pages)\n",
      "FR: 23930 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ chasse au grand gibier battue traque   : There are 0 products (0 pages)\n",
      "FR: 23930 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ chasse au migrateur   : There are 0 products (0 pages)\n",
      "FR: 23930 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ chasse au petit gibier   : There are 13 products (1 pages)\n",
      "FR: 23943 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ chasse sous marine   : There are 0 products (0 pages)\n",
      "FR: 23943 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ chistera   : There are 9 products (1 pages)\n",
      "FR: 23952 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ commercial hip hop   : There are 1 products (1 pages)\n",
      "FR: 23953 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ commercial jazz   : There are 1 products (1 pages)\n",
      "FR: 23954 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ course d'endurance   : There are 0 products (0 pages)\n",
      "FR: 23954 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ course d'orientation   : There are 1 products (1 pages)\n",
      "FR: 23955 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ cricket   : There are 7 products (1 pages)\n",
      "FR: 23962 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ cross training   : There are 0 products (0 pages)\n",
      "FR: 23962 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ crossminton   : There are 2 products (1 pages)\n",
      "FR: 23964 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ cruiser   : There are 0 products (0 pages)\n",
      "FR: 23964 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ cyclosport‎   : There are 7 products (1 pages)\n",
      "FR: 23971 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ cyclotourisme   : There are 2 products (1 pages)\n",
      "FR: 23973 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ dancehall   : There are 1 products (1 pages)\n",
      "FR: 23974 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ danse classique   : There are 11 products (1 pages)\n",
      "FR: 23985 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ danse contemporaine   : There are 0 products (0 pages)\n",
      "FR: 23985 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ danse modern jazz   : There are 16 products (1 pages)\n",
      "FR: 24001 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ danse néo classique   : There are 11 products (1 pages)\n",
      "FR: 24012 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ dériveur / catamaran   : There are 2 products (1 pages)\n",
      "FR: 24014 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ disc golf   : There are 0 products (0 pages)\n",
      "FR: 24014 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ disque volant   : There are 0 products (0 pages)\n",
      "FR: 24014 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ djembel   : There are 0 products (0 pages)\n",
      "FR: 24014 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ endurance à cheval   : There are 0 products (0 pages)\n",
      "FR: 24014 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ entraînement fonctionnel   : There are 0 products (0 pages)\n",
      "FR: 24014 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ équitation   : There are 33 products (1 pages)\n",
      "FR: 24047 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ équitation concours complet   : There are 2 products (1 pages)\n",
      "FR: 24049 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ équitation dressage   : There are 2 products (1 pages)\n",
      "FR: 24051 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ équitation éthologie   : There are 2 products (1 pages)\n",
      "FR: 24053 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ équitation horse ball   : There are 2 products (1 pages)\n",
      "FR: 24055 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ équitation saut d'obstacles   : There are 3 products (1 pages)\n",
      "FR: 24058 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ équitation western   : There are 0 products (0 pages)\n",
      "FR: 24058 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ escalade en bloc   : There are 4 products (1 pages)\n",
      "FR: 24062 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ escalade en falaise   : There are 4 products (1 pages)\n",
      "FR: 24066 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ escalade en grandes voies   : There are 1 products (1 pages)\n",
      "FR: 24067 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ escalade en salle   : There are 4 products (1 pages)\n",
      "FR: 24071 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ escrime   : There are 7 products (1 pages)\n",
      "FR: 24078 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ éveil aquatique   : There are 41 products (2 pages)\n",
      "FR: 24119 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ expédition haute altitude   : There are 0 products (0 pages)\n",
      "FR: 24119 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ fast touring   : There are 3 products (1 pages)\n",
      "FR: 24122 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ fitness ballet   : There are 0 products (0 pages)\n",
      "FR: 24122 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ flag football   : There are 0 products (0 pages)\n",
      "FR: 24122 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ flamenco   : There are 0 products (0 pages)\n",
      "FR: 24122 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ fléchettes pointe acier   : There are 0 products (0 pages)\n",
      "FR: 24122 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ fléchettes pointe plastique   : There are 0 products (0 pages)\n",
      "FR: 24122 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ floorball   : There are 9 products (1 pages)\n",
      "FR: 24131 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ foot5   : There are 77 products (2 pages)\n",
      "FR: 24208 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ football à 11   : There are 94 products (3 pages)\n",
      "FR: 24302 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ football américain   : There are 8 products (1 pages)\n",
      "FR: 24310 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ frescobol   : There are 5 products (1 pages)\n",
      "FR: 24315 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ frontenis   : There are 14 products (1 pages)\n",
      "FR: 24329 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ full contact   : There are 3 products (1 pages)\n",
      "FR: 24332 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ funk jazz   : There are 1 products (1 pages)\n",
      "FR: 24333 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ futsal   : There are 71 products (2 pages)\n",
      "FR: 24404 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ golf   : There are 12 products (1 pages)\n",
      "FR: 24416 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ grappling   : There are 0 products (0 pages)\n",
      "FR: 24416 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ gravel‎   : There are 1 products (1 pages)\n",
      "FR: 24417 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ gym douce   : There are 0 products (0 pages)\n",
      "FR: 24417 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ gymnastique artistique   : There are 49 products (2 pages)\n",
      "FR: 24466 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ gymnastique éducative et sportive   : There are 90 products (3 pages)\n",
      "FR: 24556 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ gymnastique rythmique   : There are 16 products (1 pages)\n",
      "FR: 24572 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ gymnastique suédoise   : There are 0 products (0 pages)\n",
      "FR: 24572 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ gymnastique volontaire   : There are 1 products (1 pages)\n",
      "FR: 24573 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ handball   : There are 14 products (1 pages)\n",
      "FR: 24587 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ hip-hop   : There are 1 products (1 pages)\n",
      "FR: 24588 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ hockey balle   : There are 1 products (1 pages)\n",
      "FR: 24589 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ hockey russe   : There are 1 products (1 pages)\n",
      "FR: 24590 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ hockey sur gazon   : There are 14 products (1 pages)\n",
      "FR: 24604 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ hockey sur glace   : There are 1 products (1 pages)\n",
      "FR: 24605 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ house dance   : There are 1 products (1 pages)\n",
      "FR: 24606 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ jiu-jitsu   : There are 0 products (0 pages)\n",
      "FR: 24606 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ jogging   : There are 24 products (1 pages)\n",
      "FR: 24630 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ jokari   : There are 9 products (1 pages)\n",
      "FR: 24639 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ judo   : There are 0 products (0 pages)\n",
      "FR: 24639 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ karaté   : There are 0 products (0 pages)\n",
      "FR: 24639 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ kempo   : There are 0 products (0 pages)\n",
      "FR: 24639 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ kendo   : There are 0 products (0 pages)\n",
      "FR: 24639 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ kick boxing   : There are 3 products (1 pages)\n",
      "FR: 24642 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ kitesurf freeride   : There are 3 products (1 pages)\n",
      "FR: 24645 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ kitesurf freestyle   : There are 1 products (1 pages)\n",
      "FR: 24646 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ kitesurf race   : There are 1 products (1 pages)\n",
      "FR: 24647 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ kitesurf vagues   : There are 1 products (1 pages)\n",
      "FR: 24648 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ kitewing   : There are 0 products (0 pages)\n",
      "FR: 24648 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ kubb   : There are 0 products (0 pages)\n",
      "FR: 24648 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ kung fu   : There are 0 products (0 pages)\n",
      "FR: 24648 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ lacrosse   : There are 7 products (1 pages)\n",
      "FR: 24655 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ landkite   : There are 0 products (0 pages)\n",
      "FR: 24655 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ lia   : There are 0 products (0 pages)\n",
      "FR: 24655 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ locking   : There are 1 products (1 pages)\n",
      "FR: 24656 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ longboard skate   : There are 0 products (0 pages)\n",
      "FR: 24656 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ luge   : There are 33 products (1 pages)\n",
      "FR: 24689 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ lutte   : There are 0 products (0 pages)\n",
      "FR: 24689 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ marche athlétique de vitesse   : There are 17 products (1 pages)\n",
      "FR: 24706 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ marche athlétique longue distance   : There are 3 products (1 pages)\n",
      "FR: 24709 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ marche nordique   : There are 24 products (1 pages)\n",
      "FR: 24733 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ marche sportive   : There are 25 products (1 pages)\n",
      "FR: 24758 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ mountain touring   : There are 3 products (1 pages)\n",
      "FR: 24761 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ muay thaï   : There are 3 products (1 pages)\n",
      "FR: 24764 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ musculation   : There are 2 products (1 pages)\n",
      "FR: 24766 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ nage eau libre   : There are 23 products (1 pages)\n",
      "FR: 24789 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ natation sportive   : There are 106 products (3 pages)\n",
      "FR: 24895 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ natation synchronisée artistique   : There are 21 products (1 pages)\n",
      "FR: 24916 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ netball   : There are 7 products (1 pages)\n",
      "FR: 24923 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ observation de la faune   : There are 1 products (1 pages)\n",
      "FR: 24924 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ one wall   : There are 1 products (1 pages)\n",
      "FR: 24925 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ padel   : There are 33 products (1 pages)\n",
      "FR: 24958 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pala gomme pleine   : There are 9 products (1 pages)\n",
      "FR: 24967 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ paleta cuir   : There are 0 products (0 pages)\n",
      "FR: 24967 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ palets breton   : There are 0 products (0 pages)\n",
      "FR: 24967 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ palets vendéens   : There are 0 products (0 pages)\n",
      "FR: 24967 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ parkour   : There are 24 products (1 pages)\n",
      "FR: 24991 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pasaka   : There are 9 products (1 pages)\n",
      "FR: 25000 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ patinage artistique   : There are 7 products (1 pages)\n",
      "FR: 25007 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ patinage sur glace   : There are 1 products (1 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche a l'anglaise   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche a la bolognaise   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche a la main   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche a la mouche   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche a la traine   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche à pied   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche à soutenir bateau   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche au coup chinoise   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche au coup emmanchement   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche au coup télescopique   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche au flotteur en mer   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche au jigging   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche au leurres en mer   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche au poisson mort manie   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche au pose   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche au posé en bord de mer   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche au quiver / feeder   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche au vairon manié   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche aux leurres black bass   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche aux leurres brochet   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche aux leurres carnassier   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche aux leurres perche   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche aux leurres sandre   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche aux leurres truite   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche de l'ecrevisse   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche de la carpe   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche de la carpe au coup   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche de la truite a la bombette   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche de la truite au toc   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche de la truite en étang   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche de seiches et calamars   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche du silure   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche en surf casting   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pêche sous la glace   : There are 0 products (0 pages)\n",
      "FR: 25008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pelote   : There are 1 products (1 pages)\n",
      "FR: 25009 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pelote à main nue   : There are 9 products (1 pages)\n",
      "FR: 25018 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pétanque   : There are 0 products (0 pages)\n",
      "FR: 25018 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ peteca   : There are 0 products (0 pages)\n",
      "FR: 25018 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pickleball   : There are 9 products (1 pages)\n",
      "FR: 25027 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pilates   : There are 7 products (1 pages)\n",
      "FR: 25034 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ plongée bouteille   : There are 0 products (0 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 25034 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ plongeon   : There are 12 products (1 pages)\n",
      "FR: 25046 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pole dance   : There are 0 products (0 pages)\n",
      "FR: 25046 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ pony games   : There are 2 products (1 pages)\n",
      "FR: 25048 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ popping   : There are 1 products (1 pages)\n",
      "FR: 25049 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ quad agressif   : There are 0 products (0 pages)\n",
      "FR: 25049 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ quad artistique   : There are 6 products (1 pages)\n",
      "FR: 25055 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ quad fitness   : There are 1 products (1 pages)\n",
      "FR: 25056 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ quad freeride   : There are 0 products (0 pages)\n",
      "FR: 25056 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ quad vitesse   : There are 0 products (0 pages)\n",
      "FR: 25056 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ quilles finlandaises   : There are 0 products (0 pages)\n",
      "FR: 25056 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ racquetball   : There are 10 products (1 pages)\n",
      "FR: 25066 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ randonnée à cheval   : There are 0 products (0 pages)\n",
      "FR: 25066 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ randonnée glaciaire   : There are 0 products (0 pages)\n",
      "FR: 25066 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ randonnée montagne   : There are 90 products (3 pages)\n",
      "FR: 25156 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ randonnée nature   : There are 82 products (3 pages)\n",
      "FR: 25238 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ randonnée neige   : There are 43 products (2 pages)\n",
      "FR: 25281 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ randonnée rapide   : There are 0 products (0 pages)\n",
      "FR: 25281 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ rebot   : There are 9 products (1 pages)\n",
      "FR: 25290 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ régate   : There are 2 products (1 pages)\n",
      "FR: 25292 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ resort touring   : There are 3 products (1 pages)\n",
      "FR: 25295 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ rink hockey   : There are 1 products (1 pages)\n",
      "FR: 25296 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ roller agressif   : There are 0 products (0 pages)\n",
      "FR: 25296 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ roller artistique   : There are 0 products (0 pages)\n",
      "FR: 25296 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ roller derby   : There are 0 products (0 pages)\n",
      "FR: 25296 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ roller fitness   : There are 1 products (1 pages)\n",
      "FR: 25297 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ roller freeride   : There are 0 products (0 pages)\n",
      "FR: 25297 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ roller freestyle   : There are 0 products (0 pages)\n",
      "FR: 25297 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ roller hockey   : There are 2 products (1 pages)\n",
      "FR: 25299 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ roller vitesse   : There are 0 products (0 pages)\n",
      "FR: 25299 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ rpm   : There are 0 products (0 pages)\n",
      "FR: 25299 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ rugby à 13   : There are 25 products (1 pages)\n",
      "FR: 25324 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ rugby à 15   : There are 25 products (1 pages)\n",
      "FR: 25349 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ rugby à 7   : There are 25 products (1 pages)\n",
      "FR: 25374 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ running route   : There are 24 products (1 pages)\n",
      "FR: 25398 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ sambo   : There are 0 products (0 pages)\n",
      "FR: 25398 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ sauvetage sportif   : There are 11 products (1 pages)\n",
      "FR: 25409 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ self defense   : There are 0 products (0 pages)\n",
      "FR: 25409 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ sepak takraw   : There are 7 products (1 pages)\n",
      "FR: 25416 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ sh'bam   : There are 0 products (0 pages)\n",
      "FR: 25416 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ skateboard   : There are 0 products (0 pages)\n",
      "FR: 25416 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ ski de fond a roue   : There are 0 products (0 pages)\n",
      "FR: 25416 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ ski de fond alternatif   : There are 8 products (1 pages)\n",
      "FR: 25424 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ ski de fond skating   : There are 8 products (1 pages)\n",
      "FR: 25432 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ ski de piste   : There are 62 products (2 pages)\n",
      "FR: 25494 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ ski freeride   : There are 11 products (1 pages)\n",
      "FR: 25505 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ ski freestyle   : There are 9 products (1 pages)\n",
      "FR: 25514 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ ski nautique   : There are 0 products (0 pages)\n",
      "FR: 25514 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ skimboard   : There are 3 products (1 pages)\n",
      "FR: 25517 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ slackline   : There are 4 products (1 pages)\n",
      "FR: 25521 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ snooker   : There are 0 products (0 pages)\n",
      "FR: 25521 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ snorkeling   : There are 4 products (1 pages)\n",
      "FR: 25525 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ snowboard all mountain   : There are 15 products (1 pages)\n",
      "FR: 25540 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ snowboard carving   : There are 3 products (1 pages)\n",
      "FR: 25543 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ snowboard freeride   : There are 8 products (1 pages)\n",
      "FR: 25551 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ snowboard freestyle   : There are 11 products (1 pages)\n",
      "FR: 25562 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ snowboard jib   : There are 3 products (1 pages)\n",
      "FR: 25565 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ snowboard split   : There are 6 products (1 pages)\n",
      "FR: 25571 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ snowkite   : There are 0 products (0 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 25571 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ softball   : There are 7 products (1 pages)\n",
      "FR: 25578 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ speedball   : There are 10 products (1 pages)\n",
      "FR: 25588 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ spéléologie   : There are 0 products (0 pages)\n",
      "FR: 25588 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ squash   : There are 27 products (1 pages)\n",
      "FR: 25615 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ stand up paddle course   : There are 0 products (0 pages)\n",
      "FR: 25615 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ stand up paddle randonnée   : There are 1 products (1 pages)\n",
      "FR: 25616 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ stand up paddle vague   : There are 0 products (0 pages)\n",
      "FR: 25616 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ step   : There are 0 products (0 pages)\n",
      "FR: 25616 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ street hockey   : There are 1 products (1 pages)\n",
      "FR: 25617 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ street jazz   : There are 1 products (1 pages)\n",
      "FR: 25618 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ stretching‎   : There are 0 products (0 pages)\n",
      "FR: 25618 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ surf   : There are 68 products (2 pages)\n",
      "FR: 25686 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ swimrun   : There are 2 products (1 pages)\n",
      "FR: 25688 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ taekwondo   : There are 0 products (0 pages)\n",
      "FR: 25688 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ taï chi chuan   : There are 0 products (0 pages)\n",
      "FR: 25688 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ tennis   : There are 44 products (2 pages)\n",
      "FR: 25732 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ tennis de table   : There are 19 products (1 pages)\n",
      "FR: 25751 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ tir à l'arc   : There are 0 products (0 pages)\n",
      "FR: 25751 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ tir sportif 22 long rifle   : There are 0 products (0 pages)\n",
      "FR: 25751 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ tir sportif air comprimé   : There are 0 products (0 pages)\n",
      "FR: 25751 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ tonification   : There are 0 products (0 pages)\n",
      "FR: 25751 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ touch rugby   : There are 25 products (1 pages)\n",
      "FR: 25776 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ trail   : There are 22 products (1 pages)\n",
      "FR: 25798 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ trampoline   : There are 1 products (1 pages)\n",
      "FR: 25799 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ trekking arctique   : There are 0 products (0 pages)\n",
      "FR: 25799 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ trekking désert   : There are 0 products (0 pages)\n",
      "FR: 25799 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ trekking montagne   : There are 2 products (1 pages)\n",
      "FR: 25801 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ trekking tropical   : There are 0 products (0 pages)\n",
      "FR: 25801 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ trekking voyage   : There are 4 products (1 pages)\n",
      "FR: 25805 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ triathlon   : There are 2 products (1 pages)\n",
      "FR: 25807 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ trottinette   : There are 1 products (1 pages)\n",
      "FR: 25808 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ trottinette freestyle   : There are 0 products (0 pages)\n",
      "FR: 25808 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ ultimate   : There are 0 products (0 pages)\n",
      "FR: 25808 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ vélo tout chemin randonnée   : There are 3 products (1 pages)\n",
      "FR: 25811 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ vélo ville   : There are 6 products (1 pages)\n",
      "FR: 25817 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ vélo voyage   : There are 0 products (0 pages)\n",
      "FR: 25817 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ via ferrata   : There are 3 products (1 pages)\n",
      "FR: 25820 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ video clip   : There are 1 products (1 pages)\n",
      "FR: 25821 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ voile habitable   : There are 13 products (1 pages)\n",
      "FR: 25834 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ volleyball   : There are 13 products (1 pages)\n",
      "FR: 25847 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ vtt all mountain   : There are 3 products (1 pages)\n",
      "FR: 25850 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ vtt cross country   : There are 4 products (1 pages)\n",
      "FR: 25854 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ vtt randonnée   : There are 8 products (1 pages)\n",
      "FR: 25862 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ waacking   : There are 1 products (1 pages)\n",
      "FR: 25863 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ wakeboard   : There are 0 products (0 pages)\n",
      "FR: 25863 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ waterpolo   : There are 6 products (1 pages)\n",
      "FR: 25869 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ waveboard   : There are 0 products (0 pages)\n",
      "FR: 25869 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ windsurf free ride   : There are 2 products (1 pages)\n",
      "FR: 25871 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ windsurf freestyle   : There are 0 products (0 pages)\n",
      "FR: 25871 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ windsurf slalom   : There are 0 products (0 pages)\n",
      "FR: 25871 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ windsurf wave   : There are 2 products (1 pages)\n",
      "FR: 25873 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ xare   : There are 9 products (1 pages)\n",
      "FR: 25882 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ yoga   : There are 4 products (1 pages)\n",
      "FR: 25886 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Vêtements_ zumba   : There are 0 products (0 pages)\n",
      "FR: 25886 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ 3x3 basketball  : There are 0 products (0 pages)\n",
      "FR: 25886 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ afrocafit   : There are 0 products (0 pages)\n",
      "FR: 25886 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ aïkido   : There are 0 products (0 pages)\n",
      "FR: 25886 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ alpinisme rocheux   : There are 0 products (0 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 25886 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ apnée   : There are 0 products (0 pages)\n",
      "FR: 25886 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ aqua cross training   : There are 11 products (1 pages)\n",
      "FR: 25897 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ aquabike   : There are 14 products (1 pages)\n",
      "FR: 25911 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ aquafitness   : There are 14 products (1 pages)\n",
      "FR: 25925 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ arnis   : There are 0 products (0 pages)\n",
      "FR: 25925 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ athlétisme   : There are 37 products (1 pages)\n",
      "FR: 25962 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ aviron   : There are 0 products (0 pages)\n",
      "FR: 25962 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ badminton   : There are 60 products (2 pages)\n",
      "FR: 26022 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ balade à cheval   : There are 0 products (0 pages)\n",
      "FR: 26022 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ baline gomme creuse   : There are 0 products (0 pages)\n",
      "FR: 26022 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ ball trap   : There are 0 products (0 pages)\n",
      "FR: 26022 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ balle au tambourin   : There are 7 products (1 pages)\n",
      "FR: 26029 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ ballet contemporain   : There are 0 products (0 pages)\n",
      "FR: 26029 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ barre au sol   : There are 0 products (0 pages)\n",
      "FR: 26029 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ baseball   : There are 0 products (0 pages)\n",
      "FR: 26029 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ basketball   : There are 20 products (1 pages)\n",
      "FR: 26049 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ bateau dragon   : There are 0 products (0 pages)\n",
      "FR: 26049 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ beach rugby   : There are 8 products (1 pages)\n",
      "FR: 26057 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ beach soccer   : There are 0 products (0 pages)\n",
      "FR: 26057 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ beach tennis   : There are 5 products (1 pages)\n",
      "FR: 26062 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ beach volley   : There are 0 products (0 pages)\n",
      "FR: 26062 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ billard américain   : There are 0 products (0 pages)\n",
      "FR: 26062 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ billard anglais   : There are 0 products (0 pages)\n",
      "FR: 26062 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ billard français   : There are 0 products (0 pages)\n",
      "FR: 26062 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ bmx freestyle   : There are 0 products (0 pages)\n",
      "FR: 26062 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ body attack   : There are 0 products (0 pages)\n",
      "FR: 26062 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ bodyboard   : There are 5 products (1 pages)\n",
      "FR: 26067 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ bodysurf   : There are 1 products (1 pages)\n",
      "FR: 26068 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ bokwa   : There are 0 products (0 pages)\n",
      "FR: 26068 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ boomerang   : There are 0 products (0 pages)\n",
      "FR: 26068 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ bouées tractées   : There are 0 products (0 pages)\n",
      "FR: 26068 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ boxe anglaise   : There are 0 products (0 pages)\n",
      "FR: 26068 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ boxe française   : There are 0 products (0 pages)\n",
      "FR: 26068 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ break dance   : There are 1 products (1 pages)\n",
      "FR: 26069 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ bushcraft   : There are 0 products (0 pages)\n",
      "FR: 26069 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ canoë kayak course   : There are 0 products (0 pages)\n",
      "FR: 26069 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ canoë kayak randonnee   : There are 0 products (0 pages)\n",
      "FR: 26069 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ canoë kayak vague   : There are 0 products (0 pages)\n",
      "FR: 26069 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ canyoning   : There are 0 products (0 pages)\n",
      "FR: 26069 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ capoeira   : There are 0 products (0 pages)\n",
      "FR: 26069 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ cardio boxing   : There are 0 products (0 pages)\n",
      "FR: 26069 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ cardio training   : There are 2 products (1 pages)\n",
      "FR: 26071 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ carrom   : There are 0 products (0 pages)\n",
      "FR: 26071 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ cascade de glace   : There are 0 products (0 pages)\n",
      "FR: 26071 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ cerf-volant de traction   : There are 0 products (0 pages)\n",
      "FR: 26071 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ cerf-volant pilotable   : There are 0 products (0 pages)\n",
      "FR: 26071 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ cerf-volant statique   : There are 0 products (0 pages)\n",
      "FR: 26071 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ chasse à l'approche   : There are 0 products (0 pages)\n",
      "FR: 26071 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ chasse à l'arc   : There are 0 products (0 pages)\n",
      "FR: 26071 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ chasse au gibier d'eau   : There are 0 products (0 pages)\n",
      "FR: 26071 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ chasse au grand gibier battue poste   : There are 0 products (0 pages)\n",
      "FR: 26071 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ chasse au grand gibier battue traque   : There are 0 products (0 pages)\n",
      "FR: 26071 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ chasse au migrateur   : There are 0 products (0 pages)\n",
      "FR: 26071 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ chasse au petit gibier   : There are 7 products (1 pages)\n",
      "FR: 26078 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ chasse sous marine   : There are 0 products (0 pages)\n",
      "FR: 26078 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ chistera   : There are 38 products (1 pages)\n",
      "FR: 26116 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ commercial hip hop   : There are 1 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 26117 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ commercial jazz   : There are 1 products (1 pages)\n",
      "FR: 26118 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ course d'endurance   : There are 0 products (0 pages)\n",
      "FR: 26118 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ course d'orientation   : There are 0 products (0 pages)\n",
      "FR: 26118 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ cricket   : There are 0 products (0 pages)\n",
      "FR: 26118 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ cross training   : There are 0 products (0 pages)\n",
      "FR: 26118 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ crossminton   : There are 3 products (1 pages)\n",
      "FR: 26121 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ cruiser   : There are 5 products (1 pages)\n",
      "FR: 26126 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ cyclosport‎   : There are 1 products (1 pages)\n",
      "FR: 26127 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ cyclotourisme   : There are 1 products (1 pages)\n",
      "FR: 26128 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ dancehall   : There are 1 products (1 pages)\n",
      "FR: 26129 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ danse classique   : There are 0 products (0 pages)\n",
      "FR: 26129 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ danse contemporaine   : There are 0 products (0 pages)\n",
      "FR: 26129 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ danse modern jazz   : There are 5 products (1 pages)\n",
      "FR: 26134 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ danse néo classique   : There are 0 products (0 pages)\n",
      "FR: 26134 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ dériveur / catamaran   : There are 0 products (0 pages)\n",
      "FR: 26134 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ disc golf   : There are 0 products (0 pages)\n",
      "FR: 26134 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ disque volant   : There are 0 products (0 pages)\n",
      "FR: 26134 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ djembel   : There are 0 products (0 pages)\n",
      "FR: 26134 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ endurance à cheval   : There are 0 products (0 pages)\n",
      "FR: 26134 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ entraînement fonctionnel   : There are 0 products (0 pages)\n",
      "FR: 26134 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ équitation   : There are 18 products (1 pages)\n",
      "FR: 26152 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ équitation concours complet   : There are 0 products (0 pages)\n",
      "FR: 26152 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ équitation dressage   : There are 0 products (0 pages)\n",
      "FR: 26152 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ équitation éthologie   : There are 0 products (0 pages)\n",
      "FR: 26152 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ équitation horse ball   : There are 0 products (0 pages)\n",
      "FR: 26152 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ équitation saut d'obstacles   : There are 1 products (1 pages)\n",
      "FR: 26153 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ équitation western   : There are 0 products (0 pages)\n",
      "FR: 26153 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ escalade en bloc   : There are 0 products (0 pages)\n",
      "FR: 26153 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ escalade en falaise   : There are 0 products (0 pages)\n",
      "FR: 26153 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ escalade en grandes voies   : There are 0 products (0 pages)\n",
      "FR: 26153 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ escalade en salle   : There are 0 products (0 pages)\n",
      "FR: 26153 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ escrime   : There are 0 products (0 pages)\n",
      "FR: 26153 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ éveil aquatique   : There are 25 products (1 pages)\n",
      "FR: 26178 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ expédition haute altitude   : There are 0 products (0 pages)\n",
      "FR: 26178 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ fast touring   : There are 0 products (0 pages)\n",
      "FR: 26178 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ fitness ballet   : There are 0 products (0 pages)\n",
      "FR: 26178 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ flag football   : There are 0 products (0 pages)\n",
      "FR: 26178 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ flamenco   : There are 0 products (0 pages)\n",
      "FR: 26178 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ fléchettes pointe acier   : There are 0 products (0 pages)\n",
      "FR: 26178 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ fléchettes pointe plastique   : There are 0 products (0 pages)\n",
      "FR: 26178 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ floorball   : There are 4 products (1 pages)\n",
      "FR: 26182 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ foot5   : There are 35 products (1 pages)\n",
      "FR: 26217 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ football à 11   : There are 50 products (2 pages)\n",
      "FR: 26267 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ football américain   : There are 0 products (0 pages)\n",
      "FR: 26267 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ frescobol   : There are 26 products (1 pages)\n",
      "FR: 26293 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ frontenis   : There are 40 products (1 pages)\n",
      "FR: 26333 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ full contact   : There are 0 products (0 pages)\n",
      "FR: 26333 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ funk jazz   : There are 1 products (1 pages)\n",
      "FR: 26334 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ futsal   : There are 19 products (1 pages)\n",
      "FR: 26353 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ golf   : There are 20 products (1 pages)\n",
      "FR: 26373 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ grappling   : There are 0 products (0 pages)\n",
      "FR: 26373 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ gravel‎   : There are 0 products (0 pages)\n",
      "FR: 26373 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ gym douce   : There are 0 products (0 pages)\n",
      "FR: 26373 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ gymnastique artistique   : There are 9 products (1 pages)\n",
      "FR: 26382 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ gymnastique éducative et sportive   : There are 8 products (1 pages)\n",
      "FR: 26390 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ gymnastique rythmique   : There are 0 products (0 pages)\n",
      "FR: 26390 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ gymnastique suédoise   : There are 0 products (0 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 26390 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ gymnastique volontaire   : There are 0 products (0 pages)\n",
      "FR: 26390 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ handball   : There are 8 products (1 pages)\n",
      "FR: 26398 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ hip-hop   : There are 1 products (1 pages)\n",
      "FR: 26399 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ hockey balle   : There are 0 products (0 pages)\n",
      "FR: 26399 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ hockey russe   : There are 0 products (0 pages)\n",
      "FR: 26399 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ hockey sur gazon   : There are 18 products (1 pages)\n",
      "FR: 26417 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ hockey sur glace   : There are 0 products (0 pages)\n",
      "FR: 26417 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ house dance   : There are 1 products (1 pages)\n",
      "FR: 26418 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ jiu-jitsu   : There are 0 products (0 pages)\n",
      "FR: 26418 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ jogging   : There are 35 products (1 pages)\n",
      "FR: 26453 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ jokari   : There are 38 products (1 pages)\n",
      "FR: 26491 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ judo   : There are 0 products (0 pages)\n",
      "FR: 26491 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ karaté   : There are 0 products (0 pages)\n",
      "FR: 26491 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ kempo   : There are 0 products (0 pages)\n",
      "FR: 26491 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ kendo   : There are 0 products (0 pages)\n",
      "FR: 26491 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ kick boxing   : There are 0 products (0 pages)\n",
      "FR: 26491 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ kitesurf freeride   : There are 0 products (0 pages)\n",
      "FR: 26491 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ kitesurf freestyle   : There are 0 products (0 pages)\n",
      "FR: 26491 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ kitesurf race   : There are 0 products (0 pages)\n",
      "FR: 26491 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ kitesurf vagues   : There are 0 products (0 pages)\n",
      "FR: 26491 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ kitewing   : There are 0 products (0 pages)\n",
      "FR: 26491 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ kubb   : There are 0 products (0 pages)\n",
      "FR: 26491 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ kung fu   : There are 0 products (0 pages)\n",
      "FR: 26491 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ lacrosse   : There are 0 products (0 pages)\n",
      "FR: 26491 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ landkite   : There are 0 products (0 pages)\n",
      "FR: 26491 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ lia   : There are 0 products (0 pages)\n",
      "FR: 26491 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ locking   : There are 1 products (1 pages)\n",
      "FR: 26492 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ longboard skate   : There are 6 products (1 pages)\n",
      "FR: 26498 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ luge   : There are 13 products (1 pages)\n",
      "FR: 26511 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ lutte   : There are 0 products (0 pages)\n",
      "FR: 26511 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ marche athlétique de vitesse   : There are 3 products (1 pages)\n",
      "FR: 26514 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ marche athlétique longue distance   : There are 3 products (1 pages)\n",
      "FR: 26517 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ marche nordique   : There are 13 products (1 pages)\n",
      "FR: 26530 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ marche sportive   : There are 79 products (2 pages)\n",
      "FR: 26609 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ mountain touring   : There are 0 products (0 pages)\n",
      "FR: 26609 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ muay thaï   : There are 0 products (0 pages)\n",
      "FR: 26609 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ musculation   : There are 0 products (0 pages)\n",
      "FR: 26609 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ nage eau libre   : There are 14 products (1 pages)\n",
      "FR: 26623 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ natation sportive   : There are 26 products (1 pages)\n",
      "FR: 26649 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ natation synchronisée artistique   : There are 14 products (1 pages)\n",
      "FR: 26663 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ netball   : There are 0 products (0 pages)\n",
      "FR: 26663 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ observation de la faune   : There are 6 products (1 pages)\n",
      "FR: 26669 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ one wall   : There are 0 products (0 pages)\n",
      "FR: 26669 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ padel   : There are 53 products (2 pages)\n",
      "FR: 26722 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pala gomme pleine   : There are 38 products (1 pages)\n",
      "FR: 26760 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ paleta cuir   : There are 0 products (0 pages)\n",
      "FR: 26760 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ palets breton   : There are 0 products (0 pages)\n",
      "FR: 26760 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ palets vendéens   : There are 0 products (0 pages)\n",
      "FR: 26760 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ parkour   : There are 0 products (0 pages)\n",
      "FR: 26760 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pasaka   : There are 38 products (1 pages)\n",
      "FR: 26798 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ patinage artistique   : There are 2 products (1 pages)\n",
      "FR: 26800 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ patinage sur glace   : There are 2 products (1 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche a l'anglaise   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche a la bolognaise   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche a la main   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche a la mouche   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche a la traine   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche à pied   : There are 0 products (0 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche à soutenir bateau   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche au coup chinoise   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche au coup emmanchement   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche au coup télescopique   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche au flotteur en mer   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche au jigging   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche au leurres en mer   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche au poisson mort manie   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche au pose   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche au posé en bord de mer   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche au quiver / feeder   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche au vairon manié   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche aux leurres black bass   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche aux leurres brochet   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche aux leurres carnassier   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche aux leurres perche   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche aux leurres sandre   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche aux leurres truite   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche de l'ecrevisse   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche de la carpe   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche de la carpe au coup   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche de la truite a la bombette   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche de la truite au toc   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche de la truite en étang   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche de seiches et calamars   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche du silure   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche en surf casting   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pêche sous la glace   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pelote   : There are 0 products (0 pages)\n",
      "FR: 26802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pelote à main nue   : There are 38 products (1 pages)\n",
      "FR: 26840 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pétanque   : There are 0 products (0 pages)\n",
      "FR: 26840 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ peteca   : There are 0 products (0 pages)\n",
      "FR: 26840 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pickleball   : There are 32 products (1 pages)\n",
      "FR: 26872 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pilates   : There are 0 products (0 pages)\n",
      "FR: 26872 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ plongée bouteille   : There are 0 products (0 pages)\n",
      "FR: 26872 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ plongeon   : There are 14 products (1 pages)\n",
      "FR: 26886 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pole dance   : There are 0 products (0 pages)\n",
      "FR: 26886 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ pony games   : There are 0 products (0 pages)\n",
      "FR: 26886 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ popping   : There are 1 products (1 pages)\n",
      "FR: 26887 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ quad agressif   : There are 0 products (0 pages)\n",
      "FR: 26887 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ quad artistique   : There are 2 products (1 pages)\n",
      "FR: 26889 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ quad fitness   : There are 2 products (1 pages)\n",
      "FR: 26891 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ quad freeride   : There are 0 products (0 pages)\n",
      "FR: 26891 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ quad vitesse   : There are 0 products (0 pages)\n",
      "FR: 26891 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ quilles finlandaises   : There are 0 products (0 pages)\n",
      "FR: 26891 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ racquetball   : There are 44 products (2 pages)\n",
      "FR: 26935 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ randonnée à cheval   : There are 0 products (0 pages)\n",
      "FR: 26935 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ randonnée glaciaire   : There are 0 products (0 pages)\n",
      "FR: 26935 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ randonnée montagne   : There are 84 products (3 pages)\n",
      "FR: 27019 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ randonnée nature   : There are 82 products (3 pages)\n",
      "FR: 27101 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ randonnée neige   : There are 38 products (1 pages)\n",
      "FR: 27139 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ randonnée rapide   : There are 4 products (1 pages)\n",
      "FR: 27143 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ rebot   : There are 38 products (1 pages)\n",
      "FR: 27181 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ régate   : There are 0 products (0 pages)\n",
      "FR: 27181 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ resort touring   : There are 0 products (0 pages)\n",
      "FR: 27181 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ rink hockey   : There are 0 products (0 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 27181 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ roller agressif   : There are 0 products (0 pages)\n",
      "FR: 27181 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ roller artistique   : There are 0 products (0 pages)\n",
      "FR: 27181 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ roller derby   : There are 0 products (0 pages)\n",
      "FR: 27181 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ roller fitness   : There are 2 products (1 pages)\n",
      "FR: 27183 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ roller freeride   : There are 1 products (1 pages)\n",
      "FR: 27184 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ roller freestyle   : There are 0 products (0 pages)\n",
      "FR: 27184 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ roller hockey   : There are 0 products (0 pages)\n",
      "FR: 27184 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ roller vitesse   : There are 0 products (0 pages)\n",
      "FR: 27184 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ rpm   : There are 0 products (0 pages)\n",
      "FR: 27184 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ rugby à 13   : There are 8 products (1 pages)\n",
      "FR: 27192 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ rugby à 15   : There are 8 products (1 pages)\n",
      "FR: 27200 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ rugby à 7   : There are 8 products (1 pages)\n",
      "FR: 27208 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ running route   : There are 61 products (2 pages)\n",
      "FR: 27269 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ sambo   : There are 0 products (0 pages)\n",
      "FR: 27269 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ sauvetage sportif   : There are 14 products (1 pages)\n",
      "FR: 27283 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ self defense   : There are 0 products (0 pages)\n",
      "FR: 27283 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ sepak takraw   : There are 0 products (0 pages)\n",
      "FR: 27283 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ sh'bam   : There are 0 products (0 pages)\n",
      "FR: 27283 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ skateboard   : There are 7 products (1 pages)\n",
      "FR: 27290 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ ski de fond a roue   : There are 0 products (0 pages)\n",
      "FR: 27290 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ ski de fond alternatif   : There are 1 products (1 pages)\n",
      "FR: 27291 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ ski de fond skating   : There are 0 products (0 pages)\n",
      "FR: 27291 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ ski de piste   : There are 13 products (1 pages)\n",
      "FR: 27304 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ ski freeride   : There are 10 products (1 pages)\n",
      "FR: 27314 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ ski freestyle   : There are 7 products (1 pages)\n",
      "FR: 27321 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ ski nautique   : There are 0 products (0 pages)\n",
      "FR: 27321 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ skimboard   : There are 1 products (1 pages)\n",
      "FR: 27322 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ slackline   : There are 0 products (0 pages)\n",
      "FR: 27322 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ snooker   : There are 0 products (0 pages)\n",
      "FR: 27322 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ snorkeling   : There are 10 products (1 pages)\n",
      "FR: 27332 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ snowboard all mountain   : There are 7 products (1 pages)\n",
      "FR: 27339 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ snowboard carving   : There are 0 products (0 pages)\n",
      "FR: 27339 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ snowboard freeride   : There are 6 products (1 pages)\n",
      "FR: 27345 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ snowboard freestyle   : There are 7 products (1 pages)\n",
      "FR: 27352 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ snowboard jib   : There are 0 products (0 pages)\n",
      "FR: 27352 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ snowboard split   : There are 5 products (1 pages)\n",
      "FR: 27357 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ snowkite   : There are 0 products (0 pages)\n",
      "FR: 27357 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ softball   : There are 0 products (0 pages)\n",
      "FR: 27357 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ speedball   : There are 40 products (1 pages)\n",
      "FR: 27397 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ spéléologie   : There are 0 products (0 pages)\n",
      "FR: 27397 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ squash   : There are 53 products (2 pages)\n",
      "FR: 27450 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ stand up paddle course   : There are 0 products (0 pages)\n",
      "FR: 27450 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ stand up paddle randonnée   : There are 0 products (0 pages)\n",
      "FR: 27450 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ stand up paddle vague   : There are 0 products (0 pages)\n",
      "FR: 27450 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ step   : There are 0 products (0 pages)\n",
      "FR: 27450 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ street hockey   : There are 0 products (0 pages)\n",
      "FR: 27450 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ street jazz   : There are 1 products (1 pages)\n",
      "FR: 27451 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ stretching‎   : There are 2 products (1 pages)\n",
      "FR: 27453 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ surf   : There are 46 products (2 pages)\n",
      "FR: 27499 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ swimrun   : There are 11 products (1 pages)\n",
      "FR: 27510 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ taekwondo   : There are 0 products (0 pages)\n",
      "FR: 27510 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ taï chi chuan   : There are 0 products (0 pages)\n",
      "FR: 27510 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ tennis   : There are 111 products (3 pages)\n",
      "FR: 27621 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ tennis de table   : There are 44 products (2 pages)\n",
      "FR: 27665 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ tir à l'arc   : There are 0 products (0 pages)\n",
      "FR: 27665 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ tir sportif 22 long rifle   : There are 0 products (0 pages)\n",
      "FR: 27665 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ tir sportif air comprimé   : There are 0 products (0 pages)\n",
      "FR: 27665 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ tonification   : There are 0 products (0 pages)\n",
      "FR: 27665 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ touch rugby   : There are 8 products (1 pages)\n",
      "FR: 27673 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ trail   : There are 10 products (1 pages)\n",
      "FR: 27683 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ trampoline   : There are 0 products (0 pages)\n",
      "FR: 27683 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ trekking arctique   : There are 0 products (0 pages)\n",
      "FR: 27683 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ trekking désert   : There are 0 products (0 pages)\n",
      "FR: 27683 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ trekking montagne   : There are 3 products (1 pages)\n",
      "FR: 27686 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ trekking tropical   : There are 0 products (0 pages)\n",
      "FR: 27686 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ trekking voyage   : There are 10 products (1 pages)\n",
      "FR: 27696 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ triathlon   : There are 4 products (1 pages)\n",
      "FR: 27700 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ trottinette   : There are 0 products (0 pages)\n",
      "FR: 27700 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ trottinette freestyle   : There are 3 products (1 pages)\n",
      "FR: 27703 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ ultimate   : There are 0 products (0 pages)\n",
      "FR: 27703 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ vélo tout chemin randonnée   : There are 0 products (0 pages)\n",
      "FR: 27703 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ vélo ville   : There are 0 products (0 pages)\n",
      "FR: 27703 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ vélo voyage   : There are 0 products (0 pages)\n",
      "FR: 27703 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ via ferrata   : There are 0 products (0 pages)\n",
      "FR: 27703 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ video clip   : There are 1 products (1 pages)\n",
      "FR: 27704 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ voile habitable   : There are 5 products (1 pages)\n",
      "FR: 27709 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ volleyball   : There are 4 products (1 pages)\n",
      "FR: 27713 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ vtt all mountain   : There are 0 products (0 pages)\n",
      "FR: 27713 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ vtt cross country   : There are 0 products (0 pages)\n",
      "FR: 27713 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ vtt randonnée   : There are 0 products (0 pages)\n",
      "FR: 27713 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ waacking   : There are 1 products (1 pages)\n",
      "FR: 27714 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ wakeboard   : There are 0 products (0 pages)\n",
      "FR: 27714 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ waterpolo   : There are 14 products (1 pages)\n",
      "FR: 27728 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ waveboard   : There are 2 products (1 pages)\n",
      "FR: 27730 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ windsurf free ride   : There are 0 products (0 pages)\n",
      "FR: 27730 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ windsurf freestyle   : There are 0 products (0 pages)\n",
      "FR: 27730 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ windsurf slalom   : There are 0 products (0 pages)\n",
      "FR: 27730 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ windsurf wave   : There are 0 products (0 pages)\n",
      "FR: 27730 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ xare   : There are 38 products (1 pages)\n",
      "FR: 27768 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ yoga   : There are 0 products (0 pages)\n",
      "FR: 27768 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Chaussures_ zumba   : There are 0 products (0 pages)\n",
      "FR: 27768 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ 3x3 basketball  : There are 0 products (0 pages)\n",
      "FR: 27768 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ afrocafit   : There are 0 products (0 pages)\n",
      "FR: 27768 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ aïkido   : There are 0 products (0 pages)\n",
      "FR: 27768 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ alpinisme rocheux   : There are 2 products (1 pages)\n",
      "FR: 27770 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ apnée   : There are 0 products (0 pages)\n",
      "FR: 27770 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ aqua cross training   : There are 0 products (0 pages)\n",
      "FR: 27770 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ aquabike   : There are 0 products (0 pages)\n",
      "FR: 27770 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ aquafitness   : There are 0 products (0 pages)\n",
      "FR: 27770 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ arnis   : There are 0 products (0 pages)\n",
      "FR: 27770 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ athlétisme   : There are 1 products (1 pages)\n",
      "FR: 27771 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ aviron   : There are 0 products (0 pages)\n",
      "FR: 27771 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ badminton   : There are 14 products (1 pages)\n",
      "FR: 27785 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ balade à cheval   : There are 0 products (0 pages)\n",
      "FR: 27785 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ baline gomme creuse   : There are 0 products (0 pages)\n",
      "FR: 27785 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ ball trap   : There are 0 products (0 pages)\n",
      "FR: 27785 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ balle au tambourin   : There are 0 products (0 pages)\n",
      "FR: 27785 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ ballet contemporain   : There are 0 products (0 pages)\n",
      "FR: 27785 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ barre au sol   : There are 0 products (0 pages)\n",
      "FR: 27785 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ baseball   : There are 0 products (0 pages)\n",
      "FR: 27785 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ basketball   : There are 22 products (1 pages)\n",
      "FR: 27807 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ bateau dragon   : There are 0 products (0 pages)\n",
      "FR: 27807 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ beach rugby   : There are 2 products (1 pages)\n",
      "FR: 27809 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ beach soccer   : There are 2 products (1 pages)\n",
      "FR: 27811 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ beach tennis   : There are 0 products (0 pages)\n",
      "FR: 27811 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ beach volley   : There are 0 products (0 pages)\n",
      "FR: 27811 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ billard américain   : There are 2 products (1 pages)\n",
      "FR: 27813 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ billard anglais   : There are 0 products (0 pages)\n",
      "FR: 27813 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ billard français   : There are 0 products (0 pages)\n",
      "FR: 27813 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ bmx freestyle   : There are 0 products (0 pages)\n",
      "FR: 27813 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ body attack   : There are 0 products (0 pages)\n",
      "FR: 27813 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ bodyboard   : There are 0 products (0 pages)\n",
      "FR: 27813 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ bodysurf   : There are 0 products (0 pages)\n",
      "FR: 27813 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ bokwa   : There are 0 products (0 pages)\n",
      "FR: 27813 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ boomerang   : There are 1 products (1 pages)\n",
      "FR: 27814 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ bouées tractées   : There are 0 products (0 pages)\n",
      "FR: 27814 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ boxe anglaise   : There are 3 products (1 pages)\n",
      "FR: 27817 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ boxe française   : There are 3 products (1 pages)\n",
      "FR: 27820 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ break dance   : There are 0 products (0 pages)\n",
      "FR: 27820 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ bushcraft   : There are 0 products (0 pages)\n",
      "FR: 27820 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ canoë kayak course   : There are 0 products (0 pages)\n",
      "FR: 27820 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ canoë kayak randonnee   : There are 0 products (0 pages)\n",
      "FR: 27820 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ canoë kayak vague   : There are 0 products (0 pages)\n",
      "FR: 27820 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ canyoning   : There are 0 products (0 pages)\n",
      "FR: 27820 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ capoeira   : There are 0 products (0 pages)\n",
      "FR: 27820 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ cardio boxing   : There are 2 products (1 pages)\n",
      "FR: 27822 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ cardio training   : There are 0 products (0 pages)\n",
      "FR: 27822 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ carrom   : There are 0 products (0 pages)\n",
      "FR: 27822 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ cascade de glace   : There are 2 products (1 pages)\n",
      "FR: 27824 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ cerf-volant de traction   : There are 0 products (0 pages)\n",
      "FR: 27824 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ cerf-volant pilotable   : There are 13 products (1 pages)\n",
      "FR: 27837 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ cerf-volant statique   : There are 9 products (1 pages)\n",
      "FR: 27846 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ chasse à l'approche   : There are 0 products (0 pages)\n",
      "FR: 27846 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ chasse à l'arc   : There are 0 products (0 pages)\n",
      "FR: 27846 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ chasse au gibier d'eau   : There are 0 products (0 pages)\n",
      "FR: 27846 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ chasse au grand gibier battue poste   : There are 0 products (0 pages)\n",
      "FR: 27846 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ chasse au grand gibier battue traque   : There are 0 products (0 pages)\n",
      "FR: 27846 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ chasse au migrateur   : There are 0 products (0 pages)\n",
      "FR: 27846 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ chasse au petit gibier   : There are 0 products (0 pages)\n",
      "FR: 27846 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ chasse sous marine   : There are 0 products (0 pages)\n",
      "FR: 27846 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ chistera   : There are 0 products (0 pages)\n",
      "FR: 27846 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ commercial hip hop   : There are 0 products (0 pages)\n",
      "FR: 27846 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ commercial jazz   : There are 0 products (0 pages)\n",
      "FR: 27846 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ course d'endurance   : There are 0 products (0 pages)\n",
      "FR: 27846 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ course d'orientation   : There are 0 products (0 pages)\n",
      "FR: 27846 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ cricket   : There are 0 products (0 pages)\n",
      "FR: 27846 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ cross training   : There are 0 products (0 pages)\n",
      "FR: 27846 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ crossminton   : There are 0 products (0 pages)\n",
      "FR: 27846 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ cruiser   : There are 1 products (1 pages)\n",
      "FR: 27847 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ cyclosport‎   : There are 3 products (1 pages)\n",
      "FR: 27850 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ cyclotourisme   : There are 2 products (1 pages)\n",
      "FR: 27852 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ dancehall   : There are 0 products (0 pages)\n",
      "FR: 27852 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ danse classique   : There are 0 products (0 pages)\n",
      "FR: 27852 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ danse contemporaine   : There are 0 products (0 pages)\n",
      "FR: 27852 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ danse modern jazz   : There are 0 products (0 pages)\n",
      "FR: 27852 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ danse néo classique   : There are 0 products (0 pages)\n",
      "FR: 27852 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ dériveur / catamaran   : There are 0 products (0 pages)\n",
      "FR: 27852 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ disc golf   : There are 0 products (0 pages)\n",
      "FR: 27852 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ disque volant   : There are 1 products (1 pages)\n",
      "FR: 27853 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ djembel   : There are 0 products (0 pages)\n",
      "FR: 27853 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ endurance à cheval   : There are 0 products (0 pages)\n",
      "FR: 27853 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ entraînement fonctionnel   : There are 0 products (0 pages)\n",
      "FR: 27853 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ équitation   : There are 0 products (0 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 27853 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ équitation concours complet   : There are 0 products (0 pages)\n",
      "FR: 27853 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ équitation dressage   : There are 0 products (0 pages)\n",
      "FR: 27853 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ équitation éthologie   : There are 0 products (0 pages)\n",
      "FR: 27853 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ équitation horse ball   : There are 0 products (0 pages)\n",
      "FR: 27853 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ équitation saut d'obstacles   : There are 0 products (0 pages)\n",
      "FR: 27853 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ équitation western   : There are 0 products (0 pages)\n",
      "FR: 27853 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ escalade en bloc   : There are 2 products (1 pages)\n",
      "FR: 27855 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ escalade en falaise   : There are 2 products (1 pages)\n",
      "FR: 27857 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ escalade en grandes voies   : There are 2 products (1 pages)\n",
      "FR: 27859 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ escalade en salle   : There are 2 products (1 pages)\n",
      "FR: 27861 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ escrime   : There are 0 products (0 pages)\n",
      "FR: 27861 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ éveil aquatique   : There are 28 products (1 pages)\n",
      "FR: 27889 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ expédition haute altitude   : There are 2 products (1 pages)\n",
      "FR: 27891 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ fast touring   : There are 0 products (0 pages)\n",
      "FR: 27891 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ fitness ballet   : There are 0 products (0 pages)\n",
      "FR: 27891 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ flag football   : There are 0 products (0 pages)\n",
      "FR: 27891 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ flamenco   : There are 0 products (0 pages)\n",
      "FR: 27891 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ fléchettes pointe acier   : There are 0 products (0 pages)\n",
      "FR: 27891 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ fléchettes pointe plastique   : There are 4 products (1 pages)\n",
      "FR: 27895 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ floorball   : There are 0 products (0 pages)\n",
      "FR: 27895 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ foot5   : There are 2 products (1 pages)\n",
      "FR: 27897 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ football à 11   : There are 9 products (1 pages)\n",
      "FR: 27906 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ football américain   : There are 0 products (0 pages)\n",
      "FR: 27906 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ frescobol   : There are 0 products (0 pages)\n",
      "FR: 27906 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ frontenis   : There are 0 products (0 pages)\n",
      "FR: 27906 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ full contact   : There are 3 products (1 pages)\n",
      "FR: 27909 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ funk jazz   : There are 0 products (0 pages)\n",
      "FR: 27909 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ futsal   : There are 3 products (1 pages)\n",
      "FR: 27912 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ golf   : There are 0 products (0 pages)\n",
      "FR: 27912 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ grappling   : There are 0 products (0 pages)\n",
      "FR: 27912 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ gravel‎   : There are 0 products (0 pages)\n",
      "FR: 27912 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ gym douce   : There are 0 products (0 pages)\n",
      "FR: 27912 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ gymnastique artistique   : There are 4 products (1 pages)\n",
      "FR: 27916 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ gymnastique éducative et sportive   : There are 6 products (1 pages)\n",
      "FR: 27922 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ gymnastique rythmique   : There are 3 products (1 pages)\n",
      "FR: 27925 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ gymnastique suédoise   : There are 0 products (0 pages)\n",
      "FR: 27925 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ gymnastique volontaire   : There are 0 products (0 pages)\n",
      "FR: 27925 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ handball   : There are 0 products (0 pages)\n",
      "FR: 27925 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ hip-hop   : There are 0 products (0 pages)\n",
      "FR: 27925 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ hockey balle   : There are 1 products (1 pages)\n",
      "FR: 27926 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ hockey russe   : There are 0 products (0 pages)\n",
      "FR: 27926 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ hockey sur gazon   : There are 0 products (0 pages)\n",
      "FR: 27926 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ hockey sur glace   : There are 1 products (1 pages)\n",
      "FR: 27927 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ house dance   : There are 0 products (0 pages)\n",
      "FR: 27927 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ jiu-jitsu   : There are 0 products (0 pages)\n",
      "FR: 27927 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ jogging   : There are 0 products (0 pages)\n",
      "FR: 27927 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ jokari   : There are 0 products (0 pages)\n",
      "FR: 27927 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ judo   : There are 0 products (0 pages)\n",
      "FR: 27927 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ karaté   : There are 2 products (1 pages)\n",
      "FR: 27929 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ kempo   : There are 0 products (0 pages)\n",
      "FR: 27929 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ kendo   : There are 0 products (0 pages)\n",
      "FR: 27929 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ kick boxing   : There are 3 products (1 pages)\n",
      "FR: 27932 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ kitesurf freeride   : There are 0 products (0 pages)\n",
      "FR: 27932 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ kitesurf freestyle   : There are 0 products (0 pages)\n",
      "FR: 27932 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ kitesurf race   : There are 0 products (0 pages)\n",
      "FR: 27932 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ kitesurf vagues   : There are 0 products (0 pages)\n",
      "FR: 27932 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ kitewing   : There are 0 products (0 pages)\n",
      "FR: 27932 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ kubb   : There are 1 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 27933 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ kung fu   : There are 0 products (0 pages)\n",
      "FR: 27933 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ lacrosse   : There are 0 products (0 pages)\n",
      "FR: 27933 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ landkite   : There are 0 products (0 pages)\n",
      "FR: 27933 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ lia   : There are 0 products (0 pages)\n",
      "FR: 27933 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ locking   : There are 0 products (0 pages)\n",
      "FR: 27933 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ longboard skate   : There are 1 products (1 pages)\n",
      "FR: 27934 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ luge   : There are 16 products (1 pages)\n",
      "FR: 27950 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ lutte   : There are 0 products (0 pages)\n",
      "FR: 27950 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ marche athlétique de vitesse   : There are 0 products (0 pages)\n",
      "FR: 27950 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ marche athlétique longue distance   : There are 0 products (0 pages)\n",
      "FR: 27950 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ marche nordique   : There are 0 products (0 pages)\n",
      "FR: 27950 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ marche sportive   : There are 0 products (0 pages)\n",
      "FR: 27950 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ mountain touring   : There are 0 products (0 pages)\n",
      "FR: 27950 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ muay thaï   : There are 3 products (1 pages)\n",
      "FR: 27953 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ musculation   : There are 0 products (0 pages)\n",
      "FR: 27953 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ nage eau libre   : There are 1 products (1 pages)\n",
      "FR: 27954 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ natation sportive   : There are 25 products (1 pages)\n",
      "FR: 27979 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ natation synchronisée artistique   : There are 1 products (1 pages)\n",
      "FR: 27980 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ netball   : There are 0 products (0 pages)\n",
      "FR: 27980 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ observation de la faune   : There are 0 products (0 pages)\n",
      "FR: 27980 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ one wall   : There are 0 products (0 pages)\n",
      "FR: 27980 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ padel   : There are 0 products (0 pages)\n",
      "FR: 27980 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pala gomme pleine   : There are 0 products (0 pages)\n",
      "FR: 27980 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ paleta cuir   : There are 0 products (0 pages)\n",
      "FR: 27980 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ palets breton   : There are 1 products (1 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ palets vendéens   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ parkour   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pasaka   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ patinage artistique   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ patinage sur glace   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche a l'anglaise   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche a la bolognaise   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche a la main   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche a la mouche   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche a la traine   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche à pied   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche à soutenir bateau   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche au coup chinoise   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche au coup emmanchement   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche au coup télescopique   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche au flotteur en mer   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche au jigging   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche au leurres en mer   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche au poisson mort manie   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche au pose   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche au posé en bord de mer   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche au quiver / feeder   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche au vairon manié   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche aux leurres black bass   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche aux leurres brochet   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche aux leurres carnassier   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche aux leurres perche   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche aux leurres sandre   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche aux leurres truite   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche de l'ecrevisse   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche de la carpe   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche de la carpe au coup   : There are 0 products (0 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche de la truite a la bombette   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche de la truite au toc   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche de la truite en étang   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche de seiches et calamars   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche du silure   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche en surf casting   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pêche sous la glace   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pelote   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pelote à main nue   : There are 0 products (0 pages)\n",
      "FR: 27981 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pétanque   : There are 2 products (1 pages)\n",
      "FR: 27983 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ peteca   : There are 0 products (0 pages)\n",
      "FR: 27983 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pickleball   : There are 0 products (0 pages)\n",
      "FR: 27983 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pilates   : There are 0 products (0 pages)\n",
      "FR: 27983 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ plongée bouteille   : There are 0 products (0 pages)\n",
      "FR: 27983 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ plongeon   : There are 0 products (0 pages)\n",
      "FR: 27983 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pole dance   : There are 0 products (0 pages)\n",
      "FR: 27983 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ pony games   : There are 0 products (0 pages)\n",
      "FR: 27983 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ popping   : There are 0 products (0 pages)\n",
      "FR: 27983 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ quad agressif   : There are 0 products (0 pages)\n",
      "FR: 27983 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ quad artistique   : There are 7 products (1 pages)\n",
      "FR: 27990 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ quad fitness   : There are 7 products (1 pages)\n",
      "FR: 27997 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ quad freeride   : There are 1 products (1 pages)\n",
      "FR: 27998 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ quad vitesse   : There are 0 products (0 pages)\n",
      "FR: 27998 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ quilles finlandaises   : There are 2 products (1 pages)\n",
      "FR: 28000 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ racquetball   : There are 0 products (0 pages)\n",
      "FR: 28000 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ randonnée à cheval   : There are 0 products (0 pages)\n",
      "FR: 28000 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ randonnée glaciaire   : There are 2 products (1 pages)\n",
      "FR: 28002 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ randonnée montagne   : There are 3 products (1 pages)\n",
      "FR: 28005 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ randonnée nature   : There are 3 products (1 pages)\n",
      "FR: 28008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ randonnée neige   : There are 2 products (1 pages)\n",
      "FR: 28010 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ randonnée rapide   : There are 2 products (1 pages)\n",
      "FR: 28012 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ rebot   : There are 0 products (0 pages)\n",
      "FR: 28012 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ régate   : There are 0 products (0 pages)\n",
      "FR: 28012 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ resort touring   : There are 0 products (0 pages)\n",
      "FR: 28012 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ rink hockey   : There are 0 products (0 pages)\n",
      "FR: 28012 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ roller agressif   : There are 1 products (1 pages)\n",
      "FR: 28013 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ roller artistique   : There are 1 products (1 pages)\n",
      "FR: 28014 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ roller derby   : There are 0 products (0 pages)\n",
      "FR: 28014 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ roller fitness   : There are 7 products (1 pages)\n",
      "FR: 28021 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ roller freeride   : There are 2 products (1 pages)\n",
      "FR: 28023 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ roller freestyle   : There are 2 products (1 pages)\n",
      "FR: 28025 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ roller hockey   : There are 2 products (1 pages)\n",
      "FR: 28027 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ roller vitesse   : There are 1 products (1 pages)\n",
      "FR: 28028 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ rpm   : There are 0 products (0 pages)\n",
      "FR: 28028 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ rugby à 13   : There are 1 products (1 pages)\n",
      "FR: 28029 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ rugby à 15   : There are 2 products (1 pages)\n",
      "FR: 28031 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ rugby à 7   : There are 1 products (1 pages)\n",
      "FR: 28032 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ running route   : There are 1 products (1 pages)\n",
      "FR: 28033 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ sambo   : There are 1 products (1 pages)\n",
      "FR: 28034 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ sauvetage sportif   : There are 0 products (0 pages)\n",
      "FR: 28034 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ self defense   : There are 0 products (0 pages)\n",
      "FR: 28034 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ sepak takraw   : There are 0 products (0 pages)\n",
      "FR: 28034 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ sh'bam   : There are 0 products (0 pages)\n",
      "FR: 28034 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ skateboard   : There are 6 products (1 pages)\n",
      "FR: 28040 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ ski de fond a roue   : There are 0 products (0 pages)\n",
      "FR: 28040 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ ski de fond alternatif   : There are 2 products (1 pages)\n",
      "FR: 28042 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ ski de fond skating   : There are 0 products (0 pages)\n",
      "FR: 28042 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ ski de piste   : There are 10 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 28052 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ ski freeride   : There are 1 products (1 pages)\n",
      "FR: 28053 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ ski freestyle   : There are 1 products (1 pages)\n",
      "FR: 28054 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ ski nautique   : There are 0 products (0 pages)\n",
      "FR: 28054 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ skimboard   : There are 0 products (0 pages)\n",
      "FR: 28054 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ slackline   : There are 0 products (0 pages)\n",
      "FR: 28054 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ snooker   : There are 0 products (0 pages)\n",
      "FR: 28054 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ snorkeling   : There are 0 products (0 pages)\n",
      "FR: 28054 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ snowboard all mountain   : There are 0 products (0 pages)\n",
      "FR: 28054 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ snowboard carving   : There are 0 products (0 pages)\n",
      "FR: 28054 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ snowboard freeride   : There are 0 products (0 pages)\n",
      "FR: 28054 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ snowboard freestyle   : There are 0 products (0 pages)\n",
      "FR: 28054 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ snowboard jib   : There are 0 products (0 pages)\n",
      "FR: 28054 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ snowboard split   : There are 0 products (0 pages)\n",
      "FR: 28054 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ snowkite   : There are 0 products (0 pages)\n",
      "FR: 28054 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ softball   : There are 0 products (0 pages)\n",
      "FR: 28054 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ speedball   : There are 1 products (1 pages)\n",
      "FR: 28055 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ spéléologie   : There are 0 products (0 pages)\n",
      "FR: 28055 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ squash   : There are 0 products (0 pages)\n",
      "FR: 28055 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ stand up paddle course   : There are 0 products (0 pages)\n",
      "FR: 28055 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ stand up paddle randonnée   : There are 0 products (0 pages)\n",
      "FR: 28055 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ stand up paddle vague   : There are 0 products (0 pages)\n",
      "FR: 28055 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ step   : There are 0 products (0 pages)\n",
      "FR: 28055 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ street hockey   : There are 1 products (1 pages)\n",
      "FR: 28056 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ street jazz   : There are 0 products (0 pages)\n",
      "FR: 28056 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ stretching‎   : There are 0 products (0 pages)\n",
      "FR: 28056 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ surf   : There are 0 products (0 pages)\n",
      "FR: 28056 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ swimrun   : There are 0 products (0 pages)\n",
      "FR: 28056 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ taekwondo   : There are 1 products (1 pages)\n",
      "FR: 28057 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ taï chi chuan   : There are 0 products (0 pages)\n",
      "FR: 28057 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ tennis   : There are 0 products (0 pages)\n",
      "FR: 28057 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ tennis de table   : There are 8 products (1 pages)\n",
      "FR: 28065 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ tir à l'arc   : There are 6 products (1 pages)\n",
      "FR: 28071 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ tir sportif 22 long rifle   : There are 0 products (0 pages)\n",
      "FR: 28071 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ tir sportif air comprimé   : There are 0 products (0 pages)\n",
      "FR: 28071 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ tonification   : There are 0 products (0 pages)\n",
      "FR: 28071 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ touch rugby   : There are 2 products (1 pages)\n",
      "FR: 28073 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ trail   : There are 0 products (0 pages)\n",
      "FR: 28073 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ trampoline   : There are 11 products (1 pages)\n",
      "FR: 28084 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ trekking arctique   : There are 0 products (0 pages)\n",
      "FR: 28084 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ trekking désert   : There are 0 products (0 pages)\n",
      "FR: 28084 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ trekking montagne   : There are 0 products (0 pages)\n",
      "FR: 28084 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ trekking tropical   : There are 0 products (0 pages)\n",
      "FR: 28084 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ trekking voyage   : There are 0 products (0 pages)\n",
      "FR: 28084 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ triathlon   : There are 0 products (0 pages)\n",
      "FR: 28084 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ trottinette   : There are 18 products (1 pages)\n",
      "FR: 28102 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ trottinette freestyle   : There are 2 products (1 pages)\n",
      "FR: 28104 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ ultimate   : There are 0 products (0 pages)\n",
      "FR: 28104 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ vélo tout chemin randonnée   : There are 21 products (1 pages)\n",
      "FR: 28125 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ vélo ville   : There are 4 products (1 pages)\n",
      "FR: 28129 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ vélo voyage   : There are 0 products (0 pages)\n",
      "FR: 28129 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ via ferrata   : There are 2 products (1 pages)\n",
      "FR: 28131 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ video clip   : There are 0 products (0 pages)\n",
      "FR: 28131 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ voile habitable   : There are 0 products (0 pages)\n",
      "FR: 28131 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ volleyball   : There are 1 products (1 pages)\n",
      "FR: 28132 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ vtt all mountain   : There are 0 products (0 pages)\n",
      "FR: 28132 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ vtt cross country   : There are 0 products (0 pages)\n",
      "FR: 28132 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ vtt randonnée   : There are 7 products (1 pages)\n",
      "FR: 28139 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ waacking   : There are 0 products (0 pages)\n",
      "FR: 28139 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ wakeboard   : There are 0 products (0 pages)\n",
      "FR: 28139 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ waterpolo   : There are 6 products (1 pages)\n",
      "FR: 28145 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ waveboard   : There are 0 products (0 pages)\n",
      "FR: 28145 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ windsurf free ride   : There are 0 products (0 pages)\n",
      "FR: 28145 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ windsurf freestyle   : There are 0 products (0 pages)\n",
      "FR: 28145 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ windsurf slalom   : There are 0 products (0 pages)\n",
      "FR: 28145 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ windsurf wave   : There are 0 products (0 pages)\n",
      "FR: 28145 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ xare   : There are 0 products (0 pages)\n",
      "FR: 28145 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ yoga   : There are 1 products (1 pages)\n",
      "FR: 28146 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Matériel_ zumba   : There are 0 products (0 pages)\n",
      "FR: 28146 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ 3x3 basketball  : There are 0 products (0 pages)\n",
      "FR: 28146 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ afrocafit   : There are 6 products (1 pages)\n",
      "FR: 28152 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ aïkido   : There are 6 products (1 pages)\n",
      "FR: 28158 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ alpinisme rocheux   : There are 6 products (1 pages)\n",
      "FR: 28164 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ apnée   : There are 6 products (1 pages)\n",
      "FR: 28170 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ aqua cross training   : There are 7 products (1 pages)\n",
      "FR: 28177 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ aquabike   : There are 7 products (1 pages)\n",
      "FR: 28184 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ aquafitness   : There are 7 products (1 pages)\n",
      "FR: 28191 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ arnis   : There are 6 products (1 pages)\n",
      "FR: 28197 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ athlétisme   : There are 9 products (1 pages)\n",
      "FR: 28206 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ aviron   : There are 6 products (1 pages)\n",
      "FR: 28212 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ badminton   : There are 10 products (1 pages)\n",
      "FR: 28222 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ balade à cheval   : There are 6 products (1 pages)\n",
      "FR: 28228 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ baline gomme creuse   : There are 8 products (1 pages)\n",
      "FR: 28236 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ ball trap   : There are 6 products (1 pages)\n",
      "FR: 28242 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ balle au tambourin   : There are 10 products (1 pages)\n",
      "FR: 28252 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ ballet contemporain   : There are 6 products (1 pages)\n",
      "FR: 28258 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ barre au sol   : There are 6 products (1 pages)\n",
      "FR: 28264 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ baseball   : There are 32 products (1 pages)\n",
      "FR: 28296 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ basketball   : There are 38 products (1 pages)\n",
      "FR: 28334 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ bateau dragon   : There are 6 products (1 pages)\n",
      "FR: 28340 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ beach rugby   : There are 13 products (1 pages)\n",
      "FR: 28353 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ beach soccer   : There are 38 products (1 pages)\n",
      "FR: 28391 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ beach tennis   : There are 11 products (1 pages)\n",
      "FR: 28402 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ beach volley   : There are 12 products (1 pages)\n",
      "FR: 28414 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ billard américain   : There are 6 products (1 pages)\n",
      "FR: 28420 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ billard anglais   : There are 6 products (1 pages)\n",
      "FR: 28426 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ billard français   : There are 6 products (1 pages)\n",
      "FR: 28432 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ bmx freestyle   : There are 6 products (1 pages)\n",
      "FR: 28438 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ body attack   : There are 8 products (1 pages)\n",
      "FR: 28446 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ bodyboard   : There are 8 products (1 pages)\n",
      "FR: 28454 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ bodysurf   : There are 7 products (1 pages)\n",
      "FR: 28461 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ bokwa   : There are 6 products (1 pages)\n",
      "FR: 28467 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ boomerang   : There are 6 products (1 pages)\n",
      "FR: 28473 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ bouées tractées   : There are 6 products (1 pages)\n",
      "FR: 28479 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ boxe anglaise   : There are 6 products (1 pages)\n",
      "FR: 28485 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ boxe française   : There are 6 products (1 pages)\n",
      "FR: 28491 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ break dance   : There are 6 products (1 pages)\n",
      "FR: 28497 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ bushcraft   : There are 6 products (1 pages)\n",
      "FR: 28503 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ canoë kayak course   : There are 9 products (1 pages)\n",
      "FR: 28512 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ canoë kayak randonnee   : There are 10 products (1 pages)\n",
      "FR: 28522 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ canoë kayak vague   : There are 8 products (1 pages)\n",
      "FR: 28530 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ canyoning   : There are 6 products (1 pages)\n",
      "FR: 28536 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ capoeira   : There are 6 products (1 pages)\n",
      "FR: 28542 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ cardio boxing   : There are 6 products (1 pages)\n",
      "FR: 28548 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ cardio training   : There are 8 products (1 pages)\n",
      "FR: 28556 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ carrom   : There are 6 products (1 pages)\n",
      "FR: 28562 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ cascade de glace   : There are 6 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 28568 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ cerf-volant de traction   : There are 7 products (1 pages)\n",
      "FR: 28575 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ cerf-volant pilotable   : There are 6 products (1 pages)\n",
      "FR: 28581 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ cerf-volant statique   : There are 6 products (1 pages)\n",
      "FR: 28587 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ chasse à l'approche   : There are 6 products (1 pages)\n",
      "FR: 28593 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ chasse à l'arc   : There are 6 products (1 pages)\n",
      "FR: 28599 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ chasse au gibier d'eau   : There are 6 products (1 pages)\n",
      "FR: 28605 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ chasse au grand gibier battue poste   : There are 7 products (1 pages)\n",
      "FR: 28612 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ chasse au grand gibier battue traque   : There are 6 products (1 pages)\n",
      "FR: 28618 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ chasse au migrateur   : There are 7 products (1 pages)\n",
      "FR: 28625 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ chasse au petit gibier   : There are 11 products (1 pages)\n",
      "FR: 28636 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ chasse sous marine   : There are 6 products (1 pages)\n",
      "FR: 28642 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ chistera   : There are 10 products (1 pages)\n",
      "FR: 28652 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ commercial hip hop   : There are 6 products (1 pages)\n",
      "FR: 28658 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ commercial jazz   : There are 6 products (1 pages)\n",
      "FR: 28664 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ course d'endurance   : There are 0 products (0 pages)\n",
      "FR: 28664 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ course d'orientation   : There are 6 products (1 pages)\n",
      "FR: 28670 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ cricket   : There are 7 products (1 pages)\n",
      "FR: 28677 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ cross training   : There are 6 products (1 pages)\n",
      "FR: 28683 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ crossminton   : There are 7 products (1 pages)\n",
      "FR: 28690 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ cruiser   : There are 8 products (1 pages)\n",
      "FR: 28698 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ cyclosport‎   : There are 9 products (1 pages)\n",
      "FR: 28707 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ cyclotourisme   : There are 8 products (1 pages)\n",
      "FR: 28715 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ dancehall   : There are 6 products (1 pages)\n",
      "FR: 28721 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ danse classique   : There are 6 products (1 pages)\n",
      "FR: 28727 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ danse contemporaine   : There are 6 products (1 pages)\n",
      "FR: 28733 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ danse modern jazz   : There are 6 products (1 pages)\n",
      "FR: 28739 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ danse néo classique   : There are 6 products (1 pages)\n",
      "FR: 28745 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ dériveur / catamaran   : There are 9 products (1 pages)\n",
      "FR: 28754 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ disc golf   : There are 6 products (1 pages)\n",
      "FR: 28760 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ disque volant   : There are 6 products (1 pages)\n",
      "FR: 28766 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ djembel   : There are 6 products (1 pages)\n",
      "FR: 28772 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ endurance à cheval   : There are 6 products (1 pages)\n",
      "FR: 28778 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ entraînement fonctionnel   : There are 0 products (0 pages)\n",
      "FR: 28778 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ équitation   : There are 12 products (1 pages)\n",
      "FR: 28790 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ équitation concours complet   : There are 6 products (1 pages)\n",
      "FR: 28796 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ équitation dressage   : There are 7 products (1 pages)\n",
      "FR: 28803 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ équitation éthologie   : There are 6 products (1 pages)\n",
      "FR: 28809 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ équitation horse ball   : There are 6 products (1 pages)\n",
      "FR: 28815 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ équitation saut d'obstacles   : There are 6 products (1 pages)\n",
      "FR: 28821 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ équitation western   : There are 6 products (1 pages)\n",
      "FR: 28827 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ escalade en bloc   : There are 6 products (1 pages)\n",
      "FR: 28833 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ escalade en falaise   : There are 7 products (1 pages)\n",
      "FR: 28840 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ escalade en grandes voies   : There are 6 products (1 pages)\n",
      "FR: 28846 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ escalade en salle   : There are 7 products (1 pages)\n",
      "FR: 28853 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ escrime   : There are 6 products (1 pages)\n",
      "FR: 28859 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ éveil aquatique   : There are 57 products (2 pages)\n",
      "FR: 28916 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ expédition haute altitude   : There are 6 products (1 pages)\n",
      "FR: 28922 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ fast touring   : There are 14 products (1 pages)\n",
      "FR: 28936 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ fitness ballet   : There are 6 products (1 pages)\n",
      "FR: 28942 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ flag football   : There are 8 products (1 pages)\n",
      "FR: 28950 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ flamenco   : There are 6 products (1 pages)\n",
      "FR: 28956 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ fléchettes pointe acier   : There are 6 products (1 pages)\n",
      "FR: 28962 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ fléchettes pointe plastique   : There are 6 products (1 pages)\n",
      "FR: 28968 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ floorball   : There are 7 products (1 pages)\n",
      "FR: 28975 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ foot5   : There are 38 products (1 pages)\n",
      "FR: 29013 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ football à 11   : There are 39 products (1 pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 29052 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ football américain   : There are 34 products (1 pages)\n",
      "FR: 29086 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ frescobol   : There are 11 products (1 pages)\n",
      "FR: 29097 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ frontenis   : There are 13 products (1 pages)\n",
      "FR: 29110 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ full contact   : There are 6 products (1 pages)\n",
      "FR: 29116 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ funk jazz   : There are 6 products (1 pages)\n",
      "FR: 29122 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ futsal   : There are 38 products (1 pages)\n",
      "FR: 29160 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ golf   : There are 7 products (1 pages)\n",
      "FR: 29167 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ grappling   : There are 6 products (1 pages)\n",
      "FR: 29173 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ gravel‎   : There are 6 products (1 pages)\n",
      "FR: 29179 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ gym douce   : There are 6 products (1 pages)\n",
      "FR: 29185 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ gymnastique artistique   : There are 7 products (1 pages)\n",
      "FR: 29192 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ gymnastique éducative et sportive   : There are 9 products (1 pages)\n",
      "FR: 29201 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ gymnastique rythmique   : There are 8 products (1 pages)\n",
      "FR: 29209 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ gymnastique suédoise   : There are 6 products (1 pages)\n",
      "FR: 29215 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ gymnastique volontaire   : There are 6 products (1 pages)\n",
      "FR: 29221 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ handball   : There are 13 products (1 pages)\n",
      "FR: 29234 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ hip-hop   : There are 6 products (1 pages)\n",
      "FR: 29240 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ hockey balle   : There are 6 products (1 pages)\n",
      "FR: 29246 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ hockey russe   : There are 6 products (1 pages)\n",
      "FR: 29252 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ hockey sur gazon   : There are 7 products (1 pages)\n",
      "FR: 29259 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ hockey sur glace   : There are 8 products (1 pages)\n",
      "FR: 29267 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ house dance   : There are 6 products (1 pages)\n",
      "FR: 29273 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ jiu-jitsu   : There are 6 products (1 pages)\n",
      "FR: 29279 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ jogging   : There are 9 products (1 pages)\n",
      "FR: 29288 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ jokari   : There are 10 products (1 pages)\n",
      "FR: 29298 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ judo   : There are 6 products (1 pages)\n",
      "FR: 29304 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ karaté   : There are 6 products (1 pages)\n",
      "FR: 29310 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ kempo   : There are 6 products (1 pages)\n",
      "FR: 29316 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ kendo   : There are 6 products (1 pages)\n",
      "FR: 29322 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ kick boxing   : There are 6 products (1 pages)\n",
      "FR: 29328 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ kitesurf freeride   : There are 7 products (1 pages)\n",
      "FR: 29335 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ kitesurf freestyle   : There are 7 products (1 pages)\n",
      "FR: 29342 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ kitesurf race   : There are 7 products (1 pages)\n",
      "FR: 29349 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ kitesurf vagues   : There are 7 products (1 pages)\n",
      "FR: 29356 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ kitewing   : There are 6 products (1 pages)\n",
      "FR: 29362 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ kubb   : There are 6 products (1 pages)\n",
      "FR: 29368 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ kung fu   : There are 6 products (1 pages)\n",
      "FR: 29374 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ lacrosse   : There are 7 products (1 pages)\n",
      "FR: 29381 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ landkite   : There are 7 products (1 pages)\n",
      "FR: 29388 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ lia   : There are 8 products (1 pages)\n",
      "FR: 29396 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ locking   : There are 6 products (1 pages)\n",
      "FR: 29402 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ longboard skate   : There are 9 products (1 pages)\n",
      "FR: 29411 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ luge   : There are 34 products (1 pages)\n",
      "FR: 29445 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ lutte   : There are 6 products (1 pages)\n",
      "FR: 29451 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ marche athlétique de vitesse   : There are 9 products (1 pages)\n",
      "FR: 29460 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ marche athlétique longue distance   : There are 7 products (1 pages)\n",
      "FR: 29467 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ marche nordique   : There are 9 products (1 pages)\n",
      "FR: 29476 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ marche sportive   : There are 10 products (1 pages)\n",
      "FR: 29486 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ mountain touring   : There are 14 products (1 pages)\n",
      "FR: 29500 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ muay thaï   : There are 6 products (1 pages)\n",
      "FR: 29506 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ musculation   : There are 6 products (1 pages)\n",
      "FR: 29512 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ nage eau libre   : There are 10 products (1 pages)\n",
      "FR: 29522 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ natation sportive   : There are 48 products (2 pages)\n",
      "FR: 29570 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ natation synchronisée artistique   : There are 7 products (1 pages)\n",
      "FR: 29577 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ netball   : There are 7 products (1 pages)\n",
      "FR: 29584 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ observation de la faune   : There are 7 products (1 pages)\n",
      "FR: 29591 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ one wall   : There are 7 products (1 pages)\n",
      "FR: 29598 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ padel   : There are 14 products (1 pages)\n",
      "FR: 29612 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pala gomme pleine   : There are 10 products (1 pages)\n",
      "FR: 29622 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ paleta cuir   : There are 6 products (1 pages)\n",
      "FR: 29628 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ palets breton   : There are 6 products (1 pages)\n",
      "FR: 29634 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ palets vendéens   : There are 6 products (1 pages)\n",
      "FR: 29640 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ parkour   : There are 7 products (1 pages)\n",
      "FR: 29647 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pasaka   : There are 10 products (1 pages)\n",
      "FR: 29657 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ patinage artistique   : There are 6 products (1 pages)\n",
      "FR: 29663 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ patinage sur glace   : There are 6 products (1 pages)\n",
      "FR: 29669 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche a l'anglaise   : There are 6 products (1 pages)\n",
      "FR: 29675 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche a la bolognaise   : There are 6 products (1 pages)\n",
      "FR: 29681 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche a la main   : There are 6 products (1 pages)\n",
      "FR: 29687 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche a la mouche   : There are 6 products (1 pages)\n",
      "FR: 29693 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche a la traine   : There are 6 products (1 pages)\n",
      "FR: 29699 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche à pied   : There are 6 products (1 pages)\n",
      "FR: 29705 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche à soutenir bateau   : There are 6 products (1 pages)\n",
      "FR: 29711 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche au coup chinoise   : There are 6 products (1 pages)\n",
      "FR: 29717 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche au coup emmanchement   : There are 6 products (1 pages)\n",
      "FR: 29723 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche au coup télescopique   : There are 6 products (1 pages)\n",
      "FR: 29729 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche au flotteur en mer   : There are 6 products (1 pages)\n",
      "FR: 29735 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche au jigging   : There are 6 products (1 pages)\n",
      "FR: 29741 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche au leurres en mer   : There are 6 products (1 pages)\n",
      "FR: 29747 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche au poisson mort manie   : There are 6 products (1 pages)\n",
      "FR: 29753 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche au pose   : There are 6 products (1 pages)\n",
      "FR: 29759 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche au posé en bord de mer   : There are 6 products (1 pages)\n",
      "FR: 29765 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche au quiver / feeder   : There are 6 products (1 pages)\n",
      "FR: 29771 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche au vairon manié   : There are 6 products (1 pages)\n",
      "FR: 29777 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche aux leurres black bass   : There are 6 products (1 pages)\n",
      "FR: 29783 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche aux leurres brochet   : There are 6 products (1 pages)\n",
      "FR: 29789 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche aux leurres carnassier   : There are 6 products (1 pages)\n",
      "FR: 29795 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche aux leurres perche   : There are 6 products (1 pages)\n",
      "FR: 29801 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche aux leurres sandre   : There are 6 products (1 pages)\n",
      "FR: 29807 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche aux leurres truite   : There are 6 products (1 pages)\n",
      "FR: 29813 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche de l'ecrevisse   : There are 6 products (1 pages)\n",
      "FR: 29819 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche de la carpe   : There are 6 products (1 pages)\n",
      "FR: 29825 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche de la carpe au coup   : There are 6 products (1 pages)\n",
      "FR: 29831 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche de la truite a la bombette   : There are 6 products (1 pages)\n",
      "FR: 29837 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche de la truite au toc   : There are 6 products (1 pages)\n",
      "FR: 29843 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche de la truite en étang   : There are 6 products (1 pages)\n",
      "FR: 29849 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche de seiches et calamars   : There are 6 products (1 pages)\n",
      "FR: 29855 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche du silure   : There are 6 products (1 pages)\n",
      "FR: 29861 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche en surf casting   : There are 6 products (1 pages)\n",
      "FR: 29867 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pêche sous la glace   : There are 6 products (1 pages)\n",
      "FR: 29873 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pelote   : There are 8 products (1 pages)\n",
      "FR: 29881 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pelote à main nue   : There are 10 products (1 pages)\n",
      "FR: 29891 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pétanque   : There are 6 products (1 pages)\n",
      "FR: 29897 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ peteca   : There are 6 products (1 pages)\n",
      "FR: 29903 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pickleball   : There are 12 products (1 pages)\n",
      "FR: 29915 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pilates   : There are 6 products (1 pages)\n",
      "FR: 29921 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ plongée bouteille   : There are 6 products (1 pages)\n",
      "FR: 29927 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ plongeon   : There are 7 products (1 pages)\n",
      "FR: 29934 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pole dance   : There are 6 products (1 pages)\n",
      "FR: 29940 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ pony games   : There are 6 products (1 pages)\n",
      "FR: 29946 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ popping   : There are 6 products (1 pages)\n",
      "FR: 29952 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ quad agressif   : There are 6 products (1 pages)\n",
      "FR: 29958 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ quad artistique   : There are 6 products (1 pages)\n",
      "FR: 29964 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ quad fitness   : There are 6 products (1 pages)\n",
      "FR: 29970 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ quad freeride   : There are 6 products (1 pages)\n",
      "FR: 29976 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ quad vitesse   : There are 6 products (1 pages)\n",
      "FR: 29982 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ quilles finlandaises   : There are 6 products (1 pages)\n",
      "FR: 29988 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ racquetball   : There are 8 products (1 pages)\n",
      "FR: 29996 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ randonnée à cheval   : There are 6 products (1 pages)\n",
      "FR: 30002 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ randonnée glaciaire   : There are 6 products (1 pages)\n",
      "FR: 30008 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ randonnée montagne   : There are 46 products (2 pages)\n",
      "FR: 30054 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ randonnée nature   : There are 40 products (1 pages)\n",
      "FR: 30094 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ randonnée neige   : There are 19 products (1 pages)\n",
      "FR: 30113 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ randonnée rapide   : There are 10 products (1 pages)\n",
      "FR: 30123 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ rebot   : There are 10 products (1 pages)\n",
      "FR: 30133 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ régate   : There are 9 products (1 pages)\n",
      "FR: 30142 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ resort touring   : There are 14 products (1 pages)\n",
      "FR: 30156 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ rink hockey   : There are 6 products (1 pages)\n",
      "FR: 30162 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ roller agressif   : There are 6 products (1 pages)\n",
      "FR: 30168 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ roller artistique   : There are 6 products (1 pages)\n",
      "FR: 30174 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ roller derby   : There are 6 products (1 pages)\n",
      "FR: 30180 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ roller fitness   : There are 6 products (1 pages)\n",
      "FR: 30186 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ roller freeride   : There are 6 products (1 pages)\n",
      "FR: 30192 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ roller freestyle   : There are 6 products (1 pages)\n",
      "FR: 30198 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ roller hockey   : There are 7 products (1 pages)\n",
      "FR: 30205 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ roller vitesse   : There are 6 products (1 pages)\n",
      "FR: 30211 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ rpm   : There are 8 products (1 pages)\n",
      "FR: 30219 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ rugby à 13   : There are 13 products (1 pages)\n",
      "FR: 30232 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ rugby à 15   : There are 13 products (1 pages)\n",
      "FR: 30245 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ rugby à 7   : There are 13 products (1 pages)\n",
      "FR: 30258 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ running route   : There are 9 products (1 pages)\n",
      "FR: 30267 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ sambo   : There are 6 products (1 pages)\n",
      "FR: 30273 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ sauvetage sportif   : There are 7 products (1 pages)\n",
      "FR: 30280 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ self defense   : There are 6 products (1 pages)\n",
      "FR: 30286 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ sepak takraw   : There are 6 products (1 pages)\n",
      "FR: 30292 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ sh'bam   : There are 6 products (1 pages)\n",
      "FR: 30298 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ skateboard   : There are 9 products (1 pages)\n",
      "FR: 30307 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ ski de fond a roue   : There are 7 products (1 pages)\n",
      "FR: 30314 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ ski de fond alternatif   : There are 9 products (1 pages)\n",
      "FR: 30323 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ ski de fond skating   : There are 9 products (1 pages)\n",
      "FR: 30332 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ ski de piste   : There are 43 products (2 pages)\n",
      "FR: 30375 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ ski freeride   : There are 27 products (1 pages)\n",
      "FR: 30402 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ ski freestyle   : There are 27 products (1 pages)\n",
      "FR: 30429 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ ski nautique   : There are 6 products (1 pages)\n",
      "FR: 30435 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ skimboard   : There are 8 products (1 pages)\n",
      "FR: 30443 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ slackline   : There are 6 products (1 pages)\n",
      "FR: 30449 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ snooker   : There are 6 products (1 pages)\n",
      "FR: 30455 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ snorkeling   : There are 6 products (1 pages)\n",
      "FR: 30461 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ snowboard all mountain   : There are 24 products (1 pages)\n",
      "FR: 30485 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ snowboard carving   : There are 6 products (1 pages)\n",
      "FR: 30491 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ snowboard freeride   : There are 22 products (1 pages)\n",
      "FR: 30513 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ snowboard freestyle   : There are 24 products (1 pages)\n",
      "FR: 30537 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ snowboard jib   : There are 6 products (1 pages)\n",
      "FR: 30543 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ snowboard split   : There are 8 products (1 pages)\n",
      "FR: 30551 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ snowkite   : There are 7 products (1 pages)\n",
      "FR: 30558 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ softball   : There are 13 products (1 pages)\n",
      "FR: 30571 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ speedball   : There are 10 products (1 pages)\n",
      "FR: 30581 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ spéléologie   : There are 6 products (1 pages)\n",
      "FR: 30587 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ squash   : There are 14 products (1 pages)\n",
      "FR: 30601 products have been scraped!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ stand up paddle course   : There are 11 products (1 pages)\n",
      "FR: 30612 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ stand up paddle randonnée   : There are 12 products (1 pages)\n",
      "FR: 30624 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ stand up paddle vague   : There are 10 products (1 pages)\n",
      "FR: 30634 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ step   : There are 8 products (1 pages)\n",
      "FR: 30642 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ street hockey   : There are 7 products (1 pages)\n",
      "FR: 30649 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ street jazz   : There are 6 products (1 pages)\n",
      "FR: 30655 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ stretching‎   : There are 6 products (1 pages)\n",
      "FR: 30661 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ surf   : There are 10 products (1 pages)\n",
      "FR: 30671 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ swimrun   : There are 6 products (1 pages)\n",
      "FR: 30677 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ taekwondo   : There are 6 products (1 pages)\n",
      "FR: 30683 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ taï chi chuan   : There are 6 products (1 pages)\n",
      "FR: 30689 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ tennis   : There are 16 products (1 pages)\n",
      "FR: 30705 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ tennis de table   : There are 7 products (1 pages)\n",
      "FR: 30712 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ tir à l'arc   : There are 6 products (1 pages)\n",
      "FR: 30718 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ tir sportif 22 long rifle   : There are 6 products (1 pages)\n",
      "FR: 30724 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ tir sportif air comprimé   : There are 6 products (1 pages)\n",
      "FR: 30730 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ tonification   : There are 6 products (1 pages)\n",
      "FR: 30736 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ touch rugby   : There are 13 products (1 pages)\n",
      "FR: 30749 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ trail   : There are 9 products (1 pages)\n",
      "FR: 30758 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ trampoline   : There are 7 products (1 pages)\n",
      "FR: 30765 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ trekking arctique   : There are 6 products (1 pages)\n",
      "FR: 30771 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ trekking désert   : There are 6 products (1 pages)\n",
      "FR: 30777 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ trekking montagne   : There are 12 products (1 pages)\n",
      "FR: 30789 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ trekking tropical   : There are 6 products (1 pages)\n",
      "FR: 30795 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ trekking voyage   : There are 7 products (1 pages)\n",
      "FR: 30802 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ triathlon   : There are 6 products (1 pages)\n",
      "FR: 30808 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ trottinette   : There are 6 products (1 pages)\n",
      "FR: 30814 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ trottinette freestyle   : There are 6 products (1 pages)\n",
      "FR: 30820 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ ultimate   : There are 6 products (1 pages)\n",
      "FR: 30826 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ vélo tout chemin randonnée   : There are 10 products (1 pages)\n",
      "FR: 30836 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ vélo ville   : There are 8 products (1 pages)\n",
      "FR: 30844 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ vélo voyage   : There are 6 products (1 pages)\n",
      "FR: 30850 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ via ferrata   : There are 7 products (1 pages)\n",
      "FR: 30857 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ video clip   : There are 6 products (1 pages)\n",
      "FR: 30863 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ voile habitable   : There are 10 products (1 pages)\n",
      "FR: 30873 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ volleyball   : There are 13 products (1 pages)\n",
      "FR: 30886 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ vtt all mountain   : There are 6 products (1 pages)\n",
      "FR: 30892 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ vtt cross country   : There are 8 products (1 pages)\n",
      "FR: 30900 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ vtt randonnée   : There are 12 products (1 pages)\n",
      "FR: 30912 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ waacking   : There are 6 products (1 pages)\n",
      "FR: 30918 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ wakeboard   : There are 6 products (1 pages)\n",
      "FR: 30924 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ waterpolo   : There are 7 products (1 pages)\n",
      "FR: 30931 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ waveboard   : There are 7 products (1 pages)\n",
      "FR: 30938 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ windsurf free ride   : There are 8 products (1 pages)\n",
      "FR: 30946 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ windsurf freestyle   : There are 7 products (1 pages)\n",
      "FR: 30953 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ windsurf slalom   : There are 7 products (1 pages)\n",
      "FR: 30960 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ windsurf wave   : There are 7 products (1 pages)\n",
      "FR: 30967 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ xare   : There are 10 products (1 pages)\n",
      "FR: 30977 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ yoga   : There are 6 products (1 pages)\n",
      "FR: 30983 products have been scraped!\n",
      "FR_Vêtements, chaussures et matériel enfant_Accessoires_ zumba   : There are 6 products (1 pages)\n",
      "FR: 30989 products have been scraped!\n"
     ]
    }
   ],
   "source": [
    "prod_list = []\n",
    "\n",
    "for item in cat_level3:\n",
    "    country = item[\"country\"]\n",
    "    cat_url = item[\"cat_url\"]\n",
    "    cat1 = item[\"cat1\"]\n",
    "    cat2 = item[\"cat2\"]\n",
    "    cat3 = item[\"cat3\"]\n",
    "    soup = cookSoup(cat_url)\n",
    "    url_list = pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page)\n",
    "    prod_list = getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "121e9207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR: 30989 products have been saved in FR_30989_2022-05-03.csv\n"
     ]
    }
   ],
   "source": [
    "#saving product list\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "country = \"FR\"\n",
    "fetch_date=f'{datetime.date(datetime.today())}'\n",
    "filename = f'{country}_{len(prod_list)}_{fetch_date}.csv'\n",
    "\n",
    "with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter = \",\")\n",
    "    writer.writerow([\"country\",\"title\", \"sku\", \"reg_pr\", \"act_pr\", \"brand\", \"sticker\", \"cat_1\", \"cat_2\", \"cat_3\", \"url\"])\n",
    "\n",
    "    for item in prod_list:\n",
    "        writer.writerow([country, item['title'], item['sku'], item['regular price'], \n",
    "                         item['actual price'], item['brand'], item['sticker'], \n",
    "                         item['cat_1'], item['cat_2'], item['cat_3'], item['url']])\n",
    "\n",
    "print(f'{country}: {len(prod_list)} products have been saved in {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "aa45960c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.decathlon.fr/browse/c0-femme/c1-vetements/_/N-kznd1yZ1o76jocZith9kx?from=40&size=40']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debugging\n",
    "result = requests.get('https://www.decathlon.fr/browse/c0-femme/c1-chaussant/sport-foot5/_/N-1ntm14cZith9kxZ1o76joc', headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
    "soup = bts(result.text, 'html.parser')\n",
    "url_list = []\n",
    "total_prod = soup.find(\"div\",{\"class\":f\"plp-bar-info svelte-{total_page_src}\"}).find(\"span\", {\"class\":f\"svelte-{total_page_src}\"}).text\n",
    "\n",
    "#Create list of urls within the cat\n",
    "total_page = math.ceil(int(total_prod)/per_page)\n",
    "\n",
    "for i in range(1, total_page + 1):\n",
    "    page = f'{cat_url}?from={per_page * i}&size={per_page}'\n",
    "    url_list.append(page)\n",
    "\n",
    "        \n",
    "url_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ad44b98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'31'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(\"div\",{\"class\":f\"plp-bar-info svelte-{total_page_src}\"}).find(\"span\", {\"class\":f\"svelte-{total_page_src}\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0ab72592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "29511716",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = f'{cat_url}?from={per_page * 1}&size={per_page}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba27c7d",
   "metadata": {},
   "source": [
    "# to do\n",
    "\n",
    "1. Have hcange total page to (total page + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5dc2e78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/1:\n",
      "a = 1 \n",
      "b = 1\n",
      "print(a > b)\n",
      "print(a < b)\n",
      "print(a >= b)\n",
      "print(a <= b)\n",
      " 2/2: a>b\n",
      " 2/3:\n",
      "a > b\n",
      "a < b\n",
      "a >= b\n",
      "a <=b\n",
      " 2/4:\n",
      "a > b\n",
      "a < b\n",
      "a >= b\n",
      "a <=b\n",
      " 2/5:\n",
      "a > b\n",
      "a < b\n",
      "a >= b\n",
      "a <=b\n",
      " 2/6: url = \"https://www.airbnb.com/s/Amsterdam/homes?place_id=ChIJVXealLU_xkcRja_At0z9AGY&refinement_paths%5B%5D=%2Fhomes&search_type=section_navigation\"\n",
      " 2/7:\n",
      "import requests\n",
      "content = requests.get(url)\n",
      " 2/8:\n",
      "import requests\n",
      "content = requests.get(url)\n",
      " 2/9: content.text\n",
      "2/10: ! ahaha\n",
      "2/11: captured_content = \"Page 1\"\n",
      "2/12: captured_content = \"Page 1\"\n",
      "2/13: numeric_content = int(captured_content.replace( 'Page ', ''))\n",
      "2/14: urls = ['https://www.airbnb.com/rooms/30042149?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba', 'https://www.airbnb.com/rooms/14173621?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba', 'https://www.airbnb.com/rooms/13040996?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba']\n",
      "2/15: urls = ['https://www.airbnb.com/rooms/30042149?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba', 'https://www.airbnb.com/rooms/14173621?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba', 'https://www.airbnb.com/rooms/13040996?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba']\n",
      "2/16:\n",
      "#length of URLs\n",
      "len(urls)\n",
      "2/17: import pandas\n",
      "2/18: import this\n",
      "2/19:\n",
      "def scrape(url):\n",
      "    print('Im scraping' + url )\n",
      "2/20:\n",
      "def scrape(url):\n",
      "    print('Im scraping' + url )\n",
      "2/21: scrape ('airbnb.com')\n",
      " 3/1:\n",
      "# your code goes here!\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "    elif study == \"Research Master\":\n",
      "        print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "        else:\n",
      "            print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      " 3/2:\n",
      "# solution\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course.\")\n",
      " 3/3:\n",
      "# your code goes here!\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "    elif study == \"Research Master\":\n",
      "        print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "        else:\n",
      "            print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      " 3/4:\n",
      "# your code goes here!\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      " 3/5:\n",
      "# your code goes here!\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "    elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      "2/22: urls\n",
      "2/23:\n",
      "club1 = {'url':'https://www.airbnb.com/rooms/30042149?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba', \n",
      "             'country': 'england', \n",
      "             'champions_league': True}\n",
      "club2 = {'url':'https://www.airbnb.com/rooms/30042149?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba', \n",
      "             'country': 'france', \n",
      "             'champions_league': True}\n",
      "club3 = {'url':'https://www.airbnb.com/rooms/30042149?previous_page_section_name=1000&translate_ugc=false&federated_search_id=63d7f93e-924e-47b8-861e-d0f2f4d29aba', \n",
      "             'country': 'england', \n",
      "             'champions_league': True}\n",
      "2/24: list_of_dics = [club1, club2, club3]\n",
      "2/25: list_of_dics\n",
      "2/26:\n",
      "# Functions can or cannot have Arguemnts\n",
      "\n",
      "def scrape3(url):\n",
      "    print(url)\n",
      "    return(True)\n",
      "2/27: scrape3(urls[0])\n",
      "2/28: urls\n",
      "2/29: print(urls)\n",
      " 5/1:\n",
      "import os\n",
      "os.getcwd()\n",
      " 5/2:\n",
      "import os\n",
      "os.getcwd()\n",
      " 5/3: os.chdir('D:\\OneDrive\\00_Tilburg\\00_RM2021\\Unit 1\\oDCM\\W1\\python-bootcamp')\n",
      " 5/4: os.chdir('D:\\\\OneDrive\\\\00_Tilburg\\\\00_RM2021\\\\Unit 1\\\\oDCM\\\\W1\\\\python-bootcamp')\n",
      " 5/5: os.getcwd()\n",
      " 6/1:\n",
      "a = 1 \n",
      "b = 1\n",
      "print(a > b)\n",
      "print(a < b)\n",
      "print(a >= b)\n",
      "print(a <= b)\n",
      " 6/2: a > b\n",
      " 6/3:\n",
      "balance = 100\n",
      "if balance < 0: \n",
      "    print(\"You should top up your checking account to avoid paying interest\")\n",
      "elif balance == 0: \n",
      "    print(\"Your checking account balance is exactly €0.00, be careful when making new payments!\")\n",
      "else: \n",
      "    print(\"You have a positive balance\")\n",
      " 6/4:\n",
      "balance = 100\n",
      "if balance < 0: \n",
      "    \"You should top up your checking account to avoid paying interest\"\n",
      "elif balance == 0: \n",
      "    \"Your checking account balance is exactly €0.00, be careful when making new payments!\"\n",
      "else: \n",
      "    \"You have a positive balance\"\n",
      " 6/5:\n",
      "balance = 100\n",
      "if balance < 0: \n",
      "    \"You should top up your checking account to avoid paying interest\"\n",
      "elif balance == 0: \n",
      "    \"Your checking account balance is exactly €0.00, be careful when making new payments!\"\n",
      "else: \n",
      "    \"You have a positive balance\"\n",
      " 6/6:\n",
      "balance = 100\n",
      "if balance < 0: \n",
      "    print(\"You should top up your checking account to avoid paying interest\")\n",
      "elif balance == 0: \n",
      "    print(\"Your checking account balance is exactly €0.00, be careful when making new payments!\")\n",
      "else: \n",
      "    print(\"You have a positive balance\")\n",
      " 6/7:\n",
      "balance = -10\n",
      "if balance < 0: \n",
      "    print(\"You should top up your checking account to avoid paying interest\")\n",
      "elif balance == 0: \n",
      "    print(\"Your checking account balance is exactly €0.00, be careful when making new payments!\")\n",
      "else: \n",
      "    print(\"You have a positive balance\")\n",
      " 6/8:\n",
      "# your code goes here!\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      " 6/9:\n",
      "workday = False\n",
      "no_holiday = True\n",
      "\n",
      "if workday and no_holiday:\n",
      "    print(\"Go to work!\")\n",
      "6/10:\n",
      "workday = True\n",
      "no_holiday = True\n",
      "\n",
      "if workday and no_holiday:\n",
      "    print(\"Go to work!\")\n",
      "6/11:\n",
      "workday = False\n",
      "no_holiday = True\n",
      "\n",
      "if workday and no_holiday:\n",
      "    print(\"Go to work!\")\n",
      "else:\n",
      "    print(\"Back to sleep!\")\n",
      "6/12:\n",
      "workday = False\n",
      "no_holiday = False\n",
      "\n",
      "if workday and no_holiday:\n",
      "    print(\"Go to work!\")\n",
      "else:\n",
      "    print(\"Back to sleep!\")\n",
      "6/13:\n",
      "workday = True\n",
      "no_holiday = False\n",
      "\n",
      "if workday and no_holiday:\n",
      "    print(\"Go to work!\")\n",
      "else:\n",
      "    print(\"Back to sleep!\")\n",
      "6/14:\n",
      "# your code goes here!\n",
      "prior_knowledge == True\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\" and prior_knowledge:\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\" and prior_knowledge:\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      "6/15:\n",
      "# your code goes here!\n",
      "prior_knowledge = True\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\" and prior_knowledge:\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\" and prior_knowledge:\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      "6/16:\n",
      "student_dict = student_dict + {\"age\": 23}\n",
      "print(strudent_dict)\n",
      "6/17:\n",
      "student_dict = {\"name\": \"Lotte\", \n",
      "                \"study\": \"Marketing Analytics\", \n",
      "                \"prior_knowledge\": True}\n",
      "\n",
      "print(student_dict)\n",
      "6/18:\n",
      "student_dict = student_dict + {\"age\": 23}\n",
      "print(strudent_dict)\n",
      "6/19:\n",
      "student_dict = {student_dict, \"age\": 23}\n",
      "print(strudent_dict)\n",
      "6/20:\n",
      "student_dict[2]\n",
      "print(strudent_dict)\n",
      "6/21: student_dict[2]\n",
      "6/22: student_dict[\"name\"]\n",
      "6/23: student_dict[\"age\"] = 23\n",
      "6/24:\n",
      "student_dict[\"age\"] = 23\n",
      "print(student_dict)\n",
      "6/25:\n",
      "print(student_dict(\"email\"))\n",
      "print(student_dict.get(\"name\"))\n",
      "print(student_dict.get(\"email\"))\n",
      "6/26:\n",
      "print(student_dict.get(\"email\"))\n",
      "print(student_dict.get(\"name\"))\n",
      "print(student_dict.get(\"email\"))\n",
      "6/27:\n",
      "# your code goes here!\n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "6/28:\n",
      "enrollments = [\n",
      "    {\n",
      "        \"course\": \"Online Data Collection & Management\",\n",
      "        \"instructor\": \"Hannes Datta\",\n",
      "        \"students\": [\n",
      "            {\n",
      "                \"name\": \"Lotte\",\n",
      "                \"study\": \"Marketing Analytics\",\n",
      "                \"prior_knowledge\": True\n",
      "                \n",
      "            }, \n",
      "            {\n",
      "                \"name\": \"Joep\",\n",
      "                \"study\": \"Research Master\",\n",
      "                \"prior_knowledge\": True,\n",
      "                \"honors\": \"The Societal Challenge of Migration\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "6/29:\n",
      "# your code goes here!\n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "6/30:\n",
      "# your code goes here!\n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "6/31: print(enrollments[0][\"students\"][,].get[\"honors\"])\n",
      "6/32: print(enrollments[0][\"students\"][].get[\"honors\"])\n",
      "6/33: print(enrollments[0][\"students\"][0:1].get[\"honors\"])\n",
      "6/34: print(enrollments[0][\"students\"][0].get[\"honors\"])\n",
      "6/35: print(enrollments[0][\"students\"][].get(\"honors\"))\n",
      "6/36: print(enrollments[0][\"students\"][0:1].get(\"honors\"))\n",
      "6/37: print(enrollments[0][\"students\"][0].get(\"honors\"))\n",
      "6/38:\n",
      "# Question 2\n",
      "print(enrollments[0][\"students\"][0].get(\"honors\"))\n",
      "6/39: enrollments\n",
      "6/40: enrollments\n",
      "6/41:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "print(enrollments)\n",
      "6/42:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "enrollments\n",
      "6/43:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@uvt.edu\"\n",
      "enrollments\n",
      "6/44:\n",
      "enrollments = enrollments + [\n",
      "    {\"course\": \"Data Preparation and Workflow Management\",\n",
      "     \"instructor\": \"Hannes Datta\",\n",
      "     \"students\":[{\"name\": \"Sanne\",\n",
      "                  \"study\": \"Marketing Analytics\",\n",
      "                  \"prior_knowledge\": True\n",
      "         \n",
      "     }]\n",
      "        \n",
      "    }\n",
      "]\n",
      "print(enrollments)\n",
      "6/45:\n",
      "enrollments = enrollments + [\n",
      "    {\"course\": \"Data Preparation and Workflow Management\",\n",
      "     \"instructor\": \"Hannes Datta\",\n",
      "     \"students\":[{\"name\": \"Sanne\",\n",
      "                  \"study\": \"Marketing Analytics\",\n",
      "                  \"prior_knowledge\": True\n",
      "         \n",
      "     }]\n",
      "        \n",
      "    }\n",
      "]\n",
      "enrollments\n",
      "6/46:\n",
      "enrollments = enrollments + [,\n",
      "    {\"course\": \"Data Preparation and Workflow Management\",\n",
      "     \"instructor\": \"Hannes Datta\",\n",
      "     \"students\":[{\"name\": \"Sanne\",\n",
      "                  \"study\": \"Marketing Analytics\",\n",
      "                  \"prior_knowledge\": True\n",
      "         \n",
      "     }]\n",
      "        \n",
      "    }\n",
      "]\n",
      "enrollments\n",
      "6/47:\n",
      "enrollments.append([\n",
      "    {\"course\": \"Data Preparation and Workflow Management\",\n",
      "     \"instructor\": \"Hannes Datta\",\n",
      "     \"students\":[{\"name\": \"Sanne\",\n",
      "                  \"study\": \"Marketing Analytics\",\n",
      "                  \"prior_knowledge\": True\n",
      "         \n",
      "     }]\n",
      "        \n",
      "    }\n",
      "])\n",
      "enrollments\n",
      "6/48:\n",
      "enrollments.append({\"course\": \"Data Preparation and Workflow Management\",\n",
      "     \"instructor\": \"Hannes Datta\",\n",
      "     \"students\":[{\"name\": \"Sanne\",\n",
      "                  \"study\": \"Marketing Analytics\",\n",
      "                  \"prior_knowledge\": True\n",
      "         \n",
      "     })\n",
      "enrollments\n",
      "6/49:\n",
      "enrollments.append({\"course\": \"Data Preparation and Workflow Management\",\n",
      "     \"instructor\": \"Hannes Datta\",\n",
      "     \"students\":[{\"name\": \"Sanne\",\n",
      "                  \"study\": \"Marketing Analytics\",\n",
      "                  \"prior_knowledge\": True \n",
      "     })\n",
      "enrollments\n",
      "6/50:\n",
      "enrollments.append({\"course\": \"Data Preparation and Workflow Management\",\n",
      "     \"instructor\": \"Hannes Datta\",\n",
      "     \"students\":[{\"name\": \"Sanne\",\n",
      "                  \"study\": \"Marketing Analytics\",\n",
      "                  \"prior_knowledge\": True \n",
      "    }\n",
      "    }\n",
      "    )\n",
      "enrollments\n",
      "6/51:\n",
      "enrollments.append({\n",
      "    \"course\": \"Data Preparation and Workflow Management\",\n",
      "    \"instructor\": \"Hannes Datta\",\n",
      "    \"students\":[{\n",
      "        \"name\": \"Sanne\",\n",
      "        \"study\": \"Marketing Analytics\",\n",
      "        \"prior_knowledge\": True \n",
      "    }]\n",
      "    })\n",
      "enrollments\n",
      "6/52:\n",
      "enrollments = [\n",
      "    {\n",
      "        \"course\": \"Online Data Collection & Management\",\n",
      "        \"instructor\": \"Hannes Datta\",\n",
      "        \"students\": [\n",
      "            {\n",
      "                \"name\": \"Lotte\",\n",
      "                \"study\": \"Marketing Analytics\",\n",
      "                \"prior_knowledge\": True\n",
      "                \n",
      "            }, \n",
      "            {\n",
      "                \"name\": \"Joep\",\n",
      "                \"study\": \"Research Master\",\n",
      "                \"prior_knowledge\": True,\n",
      "                \"honors\": \"The Societal Challenge of Migration\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "6/53:\n",
      "# your code goes here!\n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "6/54: print(enrollments[0][\"students\"][0].get(\"honors\"))\n",
      "6/55: student_dict[\"email\"] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "6/56:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "enrollments\n",
      "6/57:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@uvt.edu\"\n",
      "enrollments\n",
      "6/58:\n",
      "enrollments.append({\n",
      "    \"course\": \"Data Preparation and Workflow Management\",\n",
      "    \"instructor\": \"Hannes Datta\",\n",
      "    \"students\":[{\n",
      "        \"name\": \"Sanne\",\n",
      "        \"study\": \"Marketing Analytics\",\n",
      "        \"prior_knowledge\": True \n",
      "    }]\n",
      "    })\n",
      "enrollments\n",
      "6/59:\n",
      "prices = [9.99, 3.95, 24.95]\n",
      "\n",
      "for price in prices:\n",
      "    print(price)\n",
      "6/60:\n",
      "prices = [9.99, 3.95, 24.95]\n",
      "\n",
      "for i in prices:\n",
      "    print(i)\n",
      "6/61:\n",
      "prices = [9.99, 3.95, 24.95, 8.95]\n",
      "\n",
      "for price in prices:\n",
      "    print(price)\n",
      "6/62:\n",
      "price_discount = []\n",
      "\n",
      "for price in prices:\n",
      "    price_dis = price * 0.85\n",
      "    price_discount.append(price_dis)\n",
      "    \n",
      "print(price_discount)\n",
      "6/63:\n",
      "price_discount = []\n",
      "\n",
      "for price in prices:\n",
      "    price_dis = price * 0.85\n",
      "    price_discount.append(price_dis)\n",
      "    \n",
      "print(round(price_discount))\n",
      "6/64:\n",
      "price_discount = []\n",
      "\n",
      "for price in prices:\n",
      "    price_dis = round(price * 0.85)\n",
      "    price_discount.append(price_dis)\n",
      "    \n",
      "print(round(price_discount))\n",
      "6/65:\n",
      "price_discount = []\n",
      "\n",
      "for price in prices:\n",
      "    price_dis = round(price * 0.85)\n",
      "    price_discount.append(price_dis)\n",
      "    \n",
      "print(price_discount)\n",
      "6/66:\n",
      "price_discount = []\n",
      "\n",
      "for price in prices:\n",
      "    price_dis = round((price * 0.85),2)\n",
      "    price_discount.append(price_dis)\n",
      "    \n",
      "print(price_discount)\n",
      "6/67: len(prices)\n",
      "6/68:\n",
      "# your code goes here!\n",
      "prices = [9.99, 3.95, 24.95, 8.95]\n",
      "sorted(prices, reverse=True)\n",
      "6/69:\n",
      "# your code goes here!\n",
      "prices = [9.99, 3.95, 24.95, 8.95]\n",
      "sorted(prices, reverse=True) \n",
      "\n",
      "total_prices = 3.95 * 60% + 8.95 * 70% + 9.99*80% + 24.95 *90%\n",
      "6/70:\n",
      "# your code goes here!\n",
      "\n",
      "total_prices = 3.95 * 60% + 8.95 * 70% + 9.99*80% + 24.95 *90%\n",
      "print(total_prices)\n",
      "6/71:\n",
      "# your code goes here!\n",
      "total_prices = 3.95 * 60% + 8.95 * 70% + 9.99 * 80% + 24.95 * 90%\n",
      "print(total_prices)\n",
      "6/72:\n",
      "# your code goes here!\n",
      "total_prices = 3.95 * 0.60 + 8.95 * 0.70 + 9.99 * 0.80 + 24.95 * 0.90\n",
      "print(total_prices)\n",
      "6/73:\n",
      "# solution (it's a bit more difficult, so no worries if you don't get it right away!)\n",
      "total_price = 0\n",
      "discount_rate = 0.10\n",
      "\n",
      "prices = sorted(prices, reverse=True)\n",
      "\n",
      "for price in prices: \n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += 0.10\n",
      "\n",
      "print(total_price)\n",
      " 7/1: prices\n",
      " 7/2:\n",
      "a = 1 \n",
      "b = 1\n",
      "print(a > b)\n",
      "print(a < b)\n",
      "print(a >= b)\n",
      "print(a <= b)\n",
      " 7/3: a > b\n",
      " 7/4:\n",
      "savings = 100 # variable assignment\n",
      "print(savings == 100) # comparison \n",
      "print(savings != 100) # comparison (False because it is 100!)\n",
      " 7/5:\n",
      "balance = -10\n",
      "if balance < 0: \n",
      "    print(\"You should top up your checking account to avoid paying interest\")\n",
      "elif balance == 0: \n",
      "    print(\"Your checking account balance is exactly €0.00, be careful when making new payments!\")\n",
      "else: \n",
      "    print(\"You have a positive balance\")\n",
      " 7/6:\n",
      "# your code goes here!\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      " 7/7:\n",
      "# solution\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course.\")\n",
      " 7/8:\n",
      "workday = True\n",
      "no_holiday = False\n",
      "\n",
      "if workday and no_holiday:\n",
      "    print(\"Go to work!\")\n",
      "else:\n",
      "    print(\"Back to sleep!\")\n",
      " 7/9:\n",
      "tired = True\n",
      "bed_time = False\n",
      "\n",
      "if tired or bed_time:\n",
      "    print(\"Go to sleep!\")\n",
      "7/10:\n",
      "weekend = False\n",
      "\n",
      "if not weekend:\n",
      "    print(\"Go to work!\")\n",
      "7/11:\n",
      "# your code goes here!\n",
      "prior_knowledge = True\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\" and prior_knowledge:\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\" and prior_knowledge:\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      "7/12:\n",
      "# solution (it's not necessary to check whether prior_knowledge == True)\n",
      "if study == \"Marketing Analytics\" and prior_knowledge:\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\" and prior_knowledge:\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course.\")\n",
      "7/13:\n",
      "final_grade = 5.4\n",
      "exam_grade = 4.3\n",
      "resit_grade = 5.6\n",
      "\n",
      "final_grade > 5.5 and exam_grade > 5.5 or resit_grade > 5.5\n",
      "7/14: final_grade > 5.5 and (exam_grade > 5.5 or resit_grade > 5.5)\n",
      "7/15:\n",
      "driver_license = False\n",
      "age = 17\n",
      "coach = True\n",
      "\n",
      "if driver_license and age >= 18 or age == 17 and coach: \n",
      "    print(\"You're allowed to drive!\")    \n",
      "else: \n",
      "    print(\"You're not allowed to drive!\")\n",
      "7/16: # your code goes here!\n",
      "7/17:\n",
      "# solution\n",
      "if driver_license and (age >= 18 or age == 17 and coach): \n",
      "    print(\"You're allowed to drive!\")    \n",
      "else: \n",
      "    print(\"You're not allowed to drive!\")\n",
      "7/18:\n",
      "students_list = [[\"Lotte\", \"Marketing Analytics\", True], \n",
      "            [\"Joep\", \"Research Master\", True]]\n",
      "7/19:\n",
      "student_dict = {\"name\": \"Lotte\", \n",
      "                \"study\": \"Marketing Analytics\", \n",
      "                \"prior_knowledge\": True}\n",
      "\n",
      "print(student_dict)\n",
      "7/20:\n",
      "student_dict[\"age\"] = 23\n",
      "print(student_dict)\n",
      "7/21: print(student_dict[\"name\"])\n",
      "7/22:\n",
      "print(student_dict.get(\"name\"))\n",
      "print(student_dict.get(\"email\"))\n",
      "7/23:\n",
      "enrollments = [\n",
      "    {\n",
      "        \"course\": \"Online Data Collection & Management\",\n",
      "        \"instructor\": \"Hannes Datta\",\n",
      "        \"students\": [\n",
      "            {\n",
      "                \"name\": \"Lotte\",\n",
      "                \"study\": \"Marketing Analytics\",\n",
      "                \"prior_knowledge\": True\n",
      "                \n",
      "            }, \n",
      "            {\n",
      "                \"name\": \"Joep\",\n",
      "                \"study\": \"Research Master\",\n",
      "                \"prior_knowledge\": True,\n",
      "                \"honors\": \"The Societal Challenge of Migration\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "7/24:\n",
      "# your code goes here!\n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "7/25: print(enrollments[0][\"students\"][0].get(\"honors\"))\n",
      "7/26:\n",
      "# Question 1 \n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "7/27:\n",
      "# Question 2\n",
      "print(enrollments[0][\"students\"][0].get(\"honors\"))\n",
      "7/28: student_dict[\"email\"] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "7/29:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "enrollments\n",
      "7/30:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@uvt.edu\"\n",
      "enrollments\n",
      "7/31:\n",
      "enrollments.append({\n",
      "    \"course\": \"Data Preparation and Workflow Management\",\n",
      "    \"instructor\": \"Hannes Datta\",\n",
      "    \"students\":[{\n",
      "        \"name\": \"Sanne\",\n",
      "        \"study\": \"Marketing Analytics\",\n",
      "        \"prior_knowledge\": True \n",
      "    }]\n",
      "    })\n",
      "enrollments\n",
      "7/32: # your code goes here!\n",
      "7/33:\n",
      "# solution\n",
      "enrollments.append({\n",
      "    \"course\": \"Data Preparation and Workflow Management\",\n",
      "    \"instructor\": \"Hannes Datta\",\n",
      "    \"students\": [{\n",
      "        \"name\": \"Sanne\",\n",
      "        \"study\": \"Marketing Analytics\", \n",
      "        \"prior_knowledge\": True\n",
      "    }]\n",
      "})\n",
      "7/34:\n",
      "prices = [9.99, 3.95, 24.95, 8.95]\n",
      "\n",
      "for price in prices:\n",
      "    print(price)\n",
      "7/35:\n",
      "price_discount = []\n",
      "\n",
      "for price in prices:\n",
      "    price_dis = round((price * 0.85),2)\n",
      "    price_discount.append(price_dis)\n",
      "    \n",
      "print(price_discount)\n",
      "7/36:\n",
      "prices_discount = []\n",
      "\n",
      "for price in prices: \n",
      "    price_discount = price * 0.85\n",
      "    prices_discount.append(price_discount)\n",
      "\n",
      "print(prices_discount) # in a future lessson we learn you how to round figures!\n",
      "7/37: prices\n",
      "7/38:\n",
      "# your code goes here!\n",
      "sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "7/39:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "7/40:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "7/41:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "\n",
      "\n",
      "a = 24.95 * 0.9 + 9.99 * 0.8 + 8.95 * 0.7 + 3.95 * 0.6\n",
      "7/42:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "\n",
      "\n",
      "a = 24.95 * 0.9 + 9.99 * 0.8 + 8.95 * 0.7 + 3.95 * 0.6\n",
      "print(a)\n",
      "7/43:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "7/44:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "7/45:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += discount_rate + 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "7/46:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "7/47:\n",
      "# first name, study, prior knowledge\n",
      "students = [[\"Lotte\", \"Marketing Analytics\", True], \n",
      "            [\"Joep\", \"Research Master\", True], \n",
      "            [\"Mirte\", \"Marketing Analytics\", False], \n",
      "            [\"Dirk\", \"Economics\", True], \n",
      "            [\"Sanne\", \"Marketing Analytics\", True], \n",
      "            [\"Roy\", \"Research Master\", False]]\n",
      "\n",
      "# your code goes here!\n",
      "names =[]\n",
      "\n",
      "for student in students:\n",
      "    if student[1] == \"Marketing Analytics\" or student[1] ==\"Research Master\" and student[2] == True:\n",
      "        names.append(student[0])\n",
      "\n",
      "print(names)\n",
      "7/48:\n",
      "# first name, study, prior knowledge\n",
      "students = [[\"Lotte\", \"Marketing Analytics\", True], \n",
      "            [\"Joep\", \"Research Master\", True], \n",
      "            [\"Mirte\", \"Marketing Analytics\", False], \n",
      "            [\"Dirk\", \"Economics\", True], \n",
      "            [\"Sanne\", \"Marketing Analytics\", True], \n",
      "            [\"Roy\", \"Research Master\", False]]\n",
      "\n",
      "# your code goes here!\n",
      "names =[]\n",
      "\n",
      "for student in students:\n",
      "    if (student[1] == \"Marketing Analytics\" or student[1] ==\"Research Master\") and student[2] == True:\n",
      "        names.append(student[0])\n",
      "\n",
      "print(names)\n",
      "7/49:\n",
      "# solution (alternative 2 - counter)\n",
      "for counter in range(len(students)): \n",
      "    if (students[counter][1] == \"Marketing Analytics\" or students[counter][1] == \"Research Master\") and students[counter][2]:\n",
      "        print(students[counter][0])\n",
      "7/50: ?range\n",
      "7/51: range(len(students))\n",
      "7/52:\n",
      "print(list(range(-10,10))) # from 1 to 9\n",
      "print(list(range(20,10))) # from 0 to 9\n",
      "7/53:\n",
      "print(list(range(1,10))) # from 1 to 9\n",
      "print(list(range(10))) # from 0 to 9\n",
      "7/54:\n",
      "print(list(range(3)))\n",
      "print(list(range(len(prices))))\n",
      "7/55:\n",
      "prices.append(10)\n",
      "print(prices)\n",
      "7/56:\n",
      "prices.append(10)\n",
      "print(list(range(len(prices))))\n",
      "7/57:\n",
      "prices.append(10)\n",
      "print(price)\n",
      "print(list(range(len(prices))))\n",
      "7/58:\n",
      "prices.append(10)\n",
      "print(prices)\n",
      "print(list(range(len(prices))))\n",
      "7/59:\n",
      "prices = [9.99, 3.95, 24.95]\n",
      "print(list(range(3)))\n",
      "print(list(range(len(prices))))\n",
      "7/60:\n",
      "prices.append(10)\n",
      "print(prices)\n",
      "print(list(range(len(prices))))\n",
      "7/61:\n",
      "for counter in range(len(prices)):\n",
      "    print(prices[counter]*.21)\n",
      "7/62:\n",
      "for counter in range(len(prices)):\n",
      "    print(prices[counter])\n",
      "7/63:\n",
      "prices = [9.99, 3.95, 24.95]\n",
      "print(list(range(3)))\n",
      "print(list(range(len(prices))))\n",
      "7/64:\n",
      "for counter in range(len(prices)):\n",
      "    print(prices[counter])\n",
      "7/65:\n",
      "for counter in range(len(prices)):\n",
      "    print(prices[counter]*.21)\n",
      "7/66:\n",
      "counter = 0\n",
      "while counter < len(prices):\n",
      "    print(prices[counter])\n",
      " 8/1:\n",
      "a = 1 \n",
      "b = 1\n",
      "print(a > b)\n",
      "print(a < b)\n",
      "print(a >= b)\n",
      "print(a <= b)\n",
      " 8/2: a > b\n",
      " 8/3:\n",
      "savings = 100 # variable assignment\n",
      "print(savings == 100) # comparison \n",
      "print(savings != 100) # comparison (False because it is 100!)\n",
      " 8/4:\n",
      "balance = -10\n",
      "if balance < 0: \n",
      "    print(\"You should top up your checking account to avoid paying interest\")\n",
      "elif balance == 0: \n",
      "    print(\"Your checking account balance is exactly €0.00, be careful when making new payments!\")\n",
      "else: \n",
      "    print(\"You have a positive balance\")\n",
      " 8/5:\n",
      "# your code goes here!\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      " 8/6:\n",
      "# solution\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\":\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\":\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course.\")\n",
      " 8/7:\n",
      "workday = True\n",
      "no_holiday = False\n",
      "\n",
      "if workday and no_holiday:\n",
      "    print(\"Go to work!\")\n",
      "else:\n",
      "    print(\"Back to sleep!\")\n",
      " 8/8:\n",
      "tired = True\n",
      "bed_time = False\n",
      "\n",
      "if tired or bed_time:\n",
      "    print(\"Go to sleep!\")\n",
      " 8/9:\n",
      "weekend = False\n",
      "\n",
      "if not weekend:\n",
      "    print(\"Go to work!\")\n",
      "8/10:\n",
      "# your code goes here!\n",
      "prior_knowledge = True\n",
      "study = \"Marketing Analytics\"\n",
      "if study == \"Marketing Analytics\" and prior_knowledge:\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\" and prior_knowledge:\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course\")\n",
      "8/11:\n",
      "# solution (it's not necessary to check whether prior_knowledge == True)\n",
      "if study == \"Marketing Analytics\" and prior_knowledge:\n",
      "    print(\"You satisfy the course requirements\")\n",
      "elif study == \"Research Master\" and prior_knowledge:\n",
      "    print(\"Please send an email with your motivation to enroll in the course to Hannes Datta\")\n",
      "else:\n",
      "    print(\"You do not satisfy the course requirements. Please contact your educational officer if you want to enroll in the course.\")\n",
      "8/12:\n",
      "final_grade = 5.4\n",
      "exam_grade = 4.3\n",
      "resit_grade = 5.6\n",
      "\n",
      "final_grade > 5.5 and exam_grade > 5.5 or resit_grade > 5.5\n",
      "8/13: final_grade > 5.5 and (exam_grade > 5.5 or resit_grade > 5.5)\n",
      "8/14:\n",
      "driver_license = False\n",
      "age = 17\n",
      "coach = True\n",
      "\n",
      "if driver_license and age >= 18 or age == 17 and coach: \n",
      "    print(\"You're allowed to drive!\")    \n",
      "else: \n",
      "    print(\"You're not allowed to drive!\")\n",
      "8/15: # your code goes here!\n",
      "8/16:\n",
      "# solution\n",
      "if driver_license and (age >= 18 or age == 17 and coach): \n",
      "    print(\"You're allowed to drive!\")    \n",
      "else: \n",
      "    print(\"You're not allowed to drive!\")\n",
      "8/17:\n",
      "students_list = [[\"Lotte\", \"Marketing Analytics\", True], \n",
      "            [\"Joep\", \"Research Master\", True]]\n",
      "8/18:\n",
      "student_dict = {\"name\": \"Lotte\", \n",
      "                \"study\": \"Marketing Analytics\", \n",
      "                \"prior_knowledge\": True}\n",
      "\n",
      "print(student_dict)\n",
      "8/19:\n",
      "student_dict[\"age\"] = 23\n",
      "print(student_dict)\n",
      "8/20: print(student_dict[\"name\"])\n",
      "8/21:\n",
      "print(student_dict.get(\"name\"))\n",
      "print(student_dict.get(\"email\"))\n",
      "8/22:\n",
      "enrollments = [\n",
      "    {\n",
      "        \"course\": \"Online Data Collection & Management\",\n",
      "        \"instructor\": \"Hannes Datta\",\n",
      "        \"students\": [\n",
      "            {\n",
      "                \"name\": \"Lotte\",\n",
      "                \"study\": \"Marketing Analytics\",\n",
      "                \"prior_knowledge\": True\n",
      "                \n",
      "            }, \n",
      "            {\n",
      "                \"name\": \"Joep\",\n",
      "                \"study\": \"Research Master\",\n",
      "                \"prior_knowledge\": True,\n",
      "                \"honors\": \"The Societal Challenge of Migration\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "8/23:\n",
      "# your code goes here!\n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "8/24: print(enrollments[0][\"students\"][0].get(\"honors\"))\n",
      "8/25:\n",
      "# Question 1 \n",
      "print(enrollments[0][\"students\"][0][\"name\"])\n",
      "8/26:\n",
      "# Question 2\n",
      "print(enrollments[0][\"students\"][0].get(\"honors\"))\n",
      "8/27: student_dict[\"email\"] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "8/28:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@tilburguniversity.edu\"\n",
      "enrollments\n",
      "8/29:\n",
      "enrollments[0]['students'][0]['email'] = \"lotte.v.veen@uvt.edu\"\n",
      "enrollments\n",
      "8/30:\n",
      "enrollments.append({\n",
      "    \"course\": \"Data Preparation and Workflow Management\",\n",
      "    \"instructor\": \"Hannes Datta\",\n",
      "    \"students\":[{\n",
      "        \"name\": \"Sanne\",\n",
      "        \"study\": \"Marketing Analytics\",\n",
      "        \"prior_knowledge\": True \n",
      "    }]\n",
      "    })\n",
      "enrollments\n",
      "8/31: # your code goes here!\n",
      "8/32:\n",
      "# solution\n",
      "enrollments.append({\n",
      "    \"course\": \"Data Preparation and Workflow Management\",\n",
      "    \"instructor\": \"Hannes Datta\",\n",
      "    \"students\": [{\n",
      "        \"name\": \"Sanne\",\n",
      "        \"study\": \"Marketing Analytics\", \n",
      "        \"prior_knowledge\": True\n",
      "    }]\n",
      "})\n",
      "8/33:\n",
      "prices = [9.99, 3.95, 24.95, 8.95]\n",
      "\n",
      "for price in prices:\n",
      "    print(price)\n",
      "8/34:\n",
      "price_discount = []\n",
      "\n",
      "for price in prices:\n",
      "    price_dis = round((price * 0.85),2)\n",
      "    price_discount.append(price_dis)\n",
      "    \n",
      "print(price_discount)\n",
      "8/35:\n",
      "prices_discount = []\n",
      "\n",
      "for price in prices: \n",
      "    price_discount = price * 0.85\n",
      "    prices_discount.append(price_discount)\n",
      "\n",
      "print(prices_discount) # in a future lessson we learn you how to round figures!\n",
      "8/36: prices\n",
      "8/37:\n",
      "# your code goes here!\n",
      "prices = sorted(prices, reverse=True)\n",
      "total_price = 0\n",
      "discount_rate = 0.1\n",
      "\n",
      "for price in prices:\n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += 0.1\n",
      "    \n",
      "print(total_price)\n",
      "\n",
      "print(prices)\n",
      "8/38:\n",
      "# solution (it's a bit more difficult, so no worries if you don't get it right away!)\n",
      "total_price = 0\n",
      "discount_rate = 0.10\n",
      "\n",
      "prices = sorted(prices, reverse=True)\n",
      "\n",
      "for price in prices: \n",
      "    total_price += price * (1-discount_rate)\n",
      "    discount_rate += 0.10\n",
      "\n",
      "print(total_price)\n",
      "8/39:\n",
      "# first name, study, prior knowledge\n",
      "students = [[\"Lotte\", \"Marketing Analytics\", True], \n",
      "            [\"Joep\", \"Research Master\", True], \n",
      "            [\"Mirte\", \"Marketing Analytics\", False], \n",
      "            [\"Dirk\", \"Economics\", True], \n",
      "            [\"Sanne\", \"Marketing Analytics\", True], \n",
      "            [\"Roy\", \"Research Master\", False]]\n",
      "\n",
      "# your code goes here!\n",
      "names =[]\n",
      "\n",
      "for student in students:\n",
      "    if (student[1] == \"Marketing Analytics\" or student[1] ==\"Research Master\") and student[2]:\n",
      "        names.append(student[0])\n",
      "\n",
      "print(names)\n",
      "8/40:\n",
      "# solution (most elegant)\n",
      "for student in students: \n",
      "    if student[1] in [\"Marketing Analytics\", \"Research Master\"] and student[2]:\n",
      "        print(student[0])\n",
      "8/41:\n",
      "# solution (alternative 1 - iterator)\n",
      "for student in students: \n",
      "    if (student[1] == \"Marketing Analytics\" or student[1] == \"Research Master\") and student[2]: # mind the brackets!\n",
      "        print(student[0])\n",
      "8/42:\n",
      "# solution (alternative 2 - counter)\n",
      "for counter in range(len(students)): \n",
      "    if (students[counter][1] == \"Marketing Analytics\" or students[counter][1] == \"Research Master\") and students[counter][2]:\n",
      "        print(students[counter][0])\n",
      "8/43:\n",
      "print(list(range(1,10))) # from 1 to 9\n",
      "print(list(range(10))) # from 0 to 9\n",
      "8/44:\n",
      "prices = [9.99, 3.95, 24.95]\n",
      "print(list(range(3)))\n",
      "print(list(range(len(prices))))\n",
      "8/45:\n",
      "prices.append(10)\n",
      "print(prices)\n",
      "print(list(range(len(prices))))\n",
      "8/46:\n",
      "for counter in range(len(prices)):\n",
      "    print(prices[counter])\n",
      "8/47:\n",
      "for counter in range(len(prices)):\n",
      "    print(prices[counter])\n",
      "8/48: print(prices)\n",
      "8/49:\n",
      "prices = prices[0:2]\n",
      "prices\n",
      "8/50:\n",
      "prices = prices[0:3]\n",
      "prices\n",
      "8/51:\n",
      "prices = prices[0:3]\n",
      "prices\n",
      "8/52:\n",
      "prices = [9.99, 3.95, 24.95]\n",
      "print(list(range(3)))\n",
      "print(list(range(len(prices))))\n",
      "8/53: prices\n",
      "8/54:\n",
      "prices.append(10)\n",
      "for counter in range(len(prices)):\n",
      "    print(prices[counter]*.21)\n",
      "8/55:\n",
      "while counter < 100:\n",
      "    print(\\U0001f600)\n",
      "8/56:\n",
      "while counter < 100:\n",
      "    print(\"\\U0001f600\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/57:\n",
      "while counter in range(10):\n",
      "    print(\"\\U0001f600\" * counter)\n",
      "8/58:\n",
      "for counter in range(10):\n",
      "    print(\"\\U0001f600\" * counter)\n",
      "8/59:\n",
      "# solution\n",
      "for counter in range(10):\n",
      "    print(\"\\U0001f600\" * 10)\n",
      "8/60:\n",
      "for counter in range(10):\n",
      "    print(\"\\U0001f600\" * 10)\n",
      "8/61: student_dict\n",
      "8/62:\n",
      "for key in student_dict.keys(): \n",
      "    print(key)\n",
      "8/63: student_dict\n",
      "8/64: student_dict.keys()\n",
      "8/65:\n",
      "student_dict.keys()\n",
      "student_dict.value()\n",
      "8/66:\n",
      "student_dict.keys()\n",
      "student_dict.values()\n",
      "8/67:\n",
      "student_dict.keys()\n",
      "print(student_dict.values())\n",
      "8/68:\n",
      "student_dict.keys()\n",
      "\n",
      "for value in student_dict.values():\n",
      "    print(student_dict.values())\n",
      "8/69:\n",
      "student_dict.keys()\n",
      "\n",
      "for value in student_dict.values():\n",
      "    print(value)\n",
      "8/70:\n",
      "student_dict.keys()\n",
      "student_dict.values()\n",
      "8/71:\n",
      "student_dict.keys()\n",
      "student_dict.values()\n",
      "student_dict.items()\n",
      "8/72:\n",
      "print(student_dict.keys())\n",
      "print(student_dict.values())\n",
      "print(student_dict.items())\n",
      "8/73:\n",
      "print(student_dict.keys())\n",
      "print(student_dict.values())\n",
      "print(student_dict.items())\n",
      "\n",
      "for item in student_dict.items():\n",
      "    print (item)\n",
      "8/74:\n",
      "print(student_dict.keys())\n",
      "print(student_dict.values())\n",
      "8/75:\n",
      "print(student_dict.items())\n",
      "\n",
      "for item in student_dict.items():\n",
      "    print (item)\n",
      "    \n",
      "for key, value in student_dict.items():\n",
      "    print (key, value)\n",
      "8/76:\n",
      "print(student_dict.items())\n",
      "\n",
      "for item in student_dict.items():\n",
      "    print (item)\n",
      "8/77:\n",
      "for key, value in student_dict.items():\n",
      "    print (key, value)\n",
      "8/78:\n",
      "students_grades = {\"Lotte\": 6.7, \"Joep\": 7.2, \"Mirte\": 9.3, \"Dirk\": 5.2, \"Sanne\": 7.5, \"Roy\": 6.9}\n",
      "# your code goes here\n",
      "8/79:\n",
      "avg = 0\n",
      "\n",
      "for counter in range(len(students_grades))\n",
      "    total += students_grades.values(counter)\n",
      "    avg = total/(counter+1)\n",
      "    counter += 1\n",
      "    print(avg)\n",
      "8/80:\n",
      "students_grades = {\"Lotte\": 6.7, \"Joep\": 7.2, \"Mirte\": 9.3, \"Dirk\": 5.2, \"Sanne\": 7.5, \"Roy\": 6.9}\n",
      "# your code goes here\n",
      "8/81:\n",
      "students_grades = {\"Lotte\": 6.7, \"Joep\": 7.2, \"Mirte\": 9.3, \"Dirk\": 5.2, \"Sanne\": 7.5, \"Roy\": 6.9}\n",
      "# your code goes here\n",
      "8/82: range(len(students_grades))\n",
      "8/83: print(range(len(students_grades)))\n",
      "8/84:\n",
      "avg = 0\n",
      "for counter in range(len(students_grades))\n",
      "    total += students_grades.values(counter)\n",
      "    avg = total/(counter+1)\n",
      "    counter += 1\n",
      "    print(avg)\n",
      "8/85:\n",
      "avg = 0\n",
      "for counter in range(len(students_grades)):\n",
      "    total += students_grades.values(counter)\n",
      "    avg = total/(counter+1)\n",
      "    counter += 1\n",
      "    print(avg)\n",
      "8/86:\n",
      "avg = 0\n",
      "total = 0\n",
      "for counter in range(len(students_grades)):\n",
      "    total += students_grades.values(counter)\n",
      "    avg = total/(counter+1)\n",
      "    counter += 1\n",
      "    print(avg)\n",
      "8/87:\n",
      "avg = 0\n",
      "total = 0\n",
      "for counter in range(len(students_grades)):\n",
      "    total += students_grades.values(counter)\n",
      "    avg = total/(counter+1)\n",
      "    counter += 1\n",
      "    print(avg)\n",
      "8/88:\n",
      "avg = 0\n",
      "total = 0\n",
      "students_grades.values(1)\n",
      "for counter in range(len(students_grades)):\n",
      "    total += students_grades.values(counter)\n",
      "    avg = total/(counter+1)\n",
      "    counter += 1\n",
      "    print(avg)\n",
      "8/89:\n",
      "avg = 0\n",
      "total = 0\n",
      "students_grades.values()\n",
      "for counter in range(len(students_grades)):\n",
      "    total += students_grades.values(counter)\n",
      "    avg = total/(counter+1)\n",
      "    counter += 1\n",
      "    print(avg)\n",
      "8/90: range(len(students_grades)))\n",
      "8/91:\n",
      "students_grades = {\"Lotte\": 6.7, \"Joep\": 7.2, \"Mirte\": 9.3, \"Dirk\": 5.2, \"Sanne\": 7.5, \"Roy\": 6.9}\n",
      "# your code goes here\n",
      "8/92: range(len(students_grades))\n",
      "8/93: students_grades.values[0]\n",
      "8/94: students_grades.values()\n",
      "8/95: students_grades.values()[0]\n",
      "8/96:\n",
      "students_grades.values()\n",
      "dict_values[0]\n",
      "8/97: students_grades.values()\n",
      "8/98: type(students_grades.values())\n",
      "8/99:\n",
      "total = 0\n",
      "\n",
      "for value in students_grades.values():\n",
      "    print(value)\n",
      "8/100:\n",
      "total = 0\n",
      "\n",
      "for value in students_grades.values():\n",
      "    total += value\n",
      "    \n",
      "avg = total/len(students_grades)\n",
      "8/101:\n",
      "total = 0\n",
      "\n",
      "for value in students_grades.values():\n",
      "    total += value\n",
      "    \n",
      "avg = total/len(students_grades)\n",
      "\n",
      "print(avg)\n",
      "8/102:\n",
      "total = 0\n",
      "\n",
      "for value in students_grades.values():\n",
      "    total += value\n",
      "    \n",
      "total/len(students_grades)\n",
      "8/103: print(enrollments)\n",
      "8/104: enrollments\n",
      "8/105: enrollments <- enrollments[0:3]\n",
      "8/106: enrollments <- enrollments[0,3]\n",
      "8/107: enrollments <- enrollments[0]\n",
      "8/108: enrollments[0]\n",
      "8/109: enrollments[1]\n",
      "8/110: enrollments[0:1]\n",
      "8/111: enrollments[0:3]\n",
      "8/112: enrollments[0:2]\n",
      "8/113: enrollments = enrollments[0:2]\n",
      "8/114:\n",
      "enrollments = enrollments[0:2]\n",
      "print(enrollments)\n",
      "8/115:\n",
      "enrollments = enrollments[0:2]\n",
      "enrollments\n",
      "8/116:\n",
      "name = []\n",
      "\n",
      "for course in enrollments:\n",
      "    for student in enrollments[course][\"students\"]:\n",
      "        name.append(enrollments[course][\"students\"][\"name\"])\n",
      "8/117:\n",
      "name = []\n",
      "\n",
      "for course in enrollments:\n",
      "    for student in course[\"students\"]:\n",
      "        name.append(student[\"name\"])\n",
      "8/118:\n",
      "name = []\n",
      "for course in enrollments:\n",
      "    for student in course[\"students\"]:\n",
      "        name.append(student[\"name\"])\n",
      "8/119:\n",
      "name = []\n",
      "for course in enrollments:\n",
      "    for student in course[\"students\"]:\n",
      "        name.append(student[\"name\"])\n",
      "8/120:\n",
      "name = []\n",
      "for course in enrollments:\n",
      "    for student in course[\"students\"]:\n",
      "        name.append(student[\"name\"])\n",
      "print(name)\n",
      "8/121:\n",
      "prices_dict = {\"beer\": 9.99, \"meat\": 3.95, \"toaster\": 24.95}\n",
      "prices_dict_discount = {}\n",
      "\n",
      "for product, price in prices_dict.items():\n",
      "    price_discount = price * 0.85\n",
      "    prices_dict_discount[product] = price_discount\n",
      "    \n",
      "prices_dict_discount\n",
      "8/122:\n",
      "prices_dict = {\"beer\": 9.99, \"meat\": 3.95, \"toaster\": 24.95}\n",
      "\n",
      "price_list = {}\n",
      "for product, price in prices_dict:\n",
      "    price_dis = price * 0.85\n",
      "    price_list[product] = price_dis\n",
      "\n",
      "price_list\n",
      "8/123:\n",
      "prices_dict = {\"beer\": 9.99, \"meat\": 3.95, \"toaster\": 24.95}\n",
      "price_list = {}\n",
      "for product, price in prices_dict.items():\n",
      "    price_dis = price * 0.85\n",
      "    price_list[product] = price_dis\n",
      "\n",
      "price_list\n",
      "8/124:\n",
      "# solution\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "    for student in course[\"students\"]:\n",
      "        students_enrolled.append(student[\"name\"])\n",
      "    \n",
      "print(students_enrolled)\n",
      "8/125: [students_enrolled.append(student[\"name\"]) for course in enrollments, student in course[\"students\"]]\n",
      "8/126: [students_enrolled.append(student[\"name\"]) for course in enrollments: for student in course[\"students\"]]\n",
      "8/127:\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "    [students_enrolled.append(student[\"name\"]) for student in course[\"students\"]]\n",
      "8/128:\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "    print[students_enrolled.append(student[\"name\"]) for student in course[\"students\"]]\n",
      "8/129:\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "    print(students_enrolled.append(student[\"name\"]) for student in course[\"students\"]])\n",
      "8/130:\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "    print(students_enrolled.append(student[\"name\"]) for student in course[\"students\"])\n",
      "8/131:\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "   students_enrolled.append(student[\"name\"]) for student in course[\"students\"]\n",
      "8/132:\n",
      "students_enrolled = []\n",
      "for course in enrollments: \n",
      "   [students_enrolled.append(student[\"name\"])for student in course[\"students\"]]\n",
      "8/133:\n",
      "students_enrolled = []\n",
      "for course in enrollments: \n",
      "   print[students_enrolled.append(student[\"name\"])for student in course[\"students\"]]\n",
      "8/134:\n",
      "students_enrolled = []\n",
      "for course in enrollments: \n",
      "   [students_enrolled.append(student[\"name\"])for student in course[\"students\"]]\n",
      "print(students_enrolled)\n",
      "8/135:\n",
      "students_enrolled = []\n",
      "[students_enrolled.append(student[\"name\"])for student in course[\"students\"] for course in enrollments]\n",
      "print(students_enrolled)\n",
      "8/136:\n",
      "enrollments = enrollments[0:2]\n",
      "enrollments\n",
      "8/137:\n",
      "students_enrolled = []\n",
      "[students_enrolled.append(student[\"name\"])for student in course[\"students\"] for course in enrollments]\n",
      "print(students_enrolled)\n",
      "8/138:\n",
      "# solution\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "    for student in course[\"students\"]:\n",
      "        students_enrolled.append(student[\"name\"])\n",
      "    \n",
      "print(students_enrolled)\n",
      "8/139: enrollments\n",
      "8/140:\n",
      "students_enrolled = []\n",
      "[students_enrolled.append(student[\"name\"])for student in course[\"students\"] for course in enrollments]\n",
      "print(students_enrolled)\n",
      "8/141:\n",
      "students_enrolled = []\n",
      "[students_enrolled.append(student[\"name\"])for student in course[\"students\"] for course in enrollments]\n",
      "8/142: enrollments\n",
      "8/143:\n",
      "students_enrolled = []\n",
      "[students_enrolled.append(student[\"name\"])for student in course[\"students\"] for course in enrollments]\n",
      "8/144:\n",
      "students_enrolled = []\n",
      "\n",
      "for course in enrollments: \n",
      "   [student[\"name\"]for student in course[\"students\"]]\n",
      "8/145:\n",
      "for course in enrollments: \n",
      "   print([student[\"name\"]for student in course[\"students\"]])\n",
      "8/146: print([student[\"name\"]for student in course[\"students\"] for course in enrollments]\n",
      "8/147: [student[\"name\"]for student in course[\"students\"] for course in enrollments]\n",
      "8/148:\n",
      "for course in enrollments:\n",
      "    [student[\"name\"]for student in course[\"students\"]]\n",
      "8/149:\n",
      "for course in enrollments:\n",
      "    print([student[\"name\"]for student in course[\"students\"]])\n",
      "8/150:\n",
      "for course in enrollments:\n",
      "    name =[student[\"name\"]for student in course[\"students\"]]\n",
      "8/151:\n",
      "for course in enrollments:\n",
      "    name =[student[\"name\"]for student in course[\"students\"]]\n",
      "    \n",
      "name\n",
      "8/152:\n",
      "for course in enrollments:\n",
      "    name +=[student[\"name\"]for student in course[\"students\"]]\n",
      "    \n",
      "name\n",
      "8/153:\n",
      "name = []\n",
      "for course in enrollments:\n",
      "    name +=[student[\"name\"]for student in course[\"students\"]]\n",
      "    \n",
      "name\n",
      "8/154:\n",
      "# solution\n",
      "total = 0\n",
      "for value in students_grades.values(): \n",
      "    total += value\n",
      "total/len(students_grades)\n",
      "8/155:\n",
      "total += value for value in students_grades.values()\n",
      "avg = total/len(students_grades)\n",
      "8/156:\n",
      "total = 0\n",
      "total+= value for value in students_grades.values()\n",
      "avg = total/len(students_grades)\n",
      "8/157:\n",
      "total = 0\n",
      "[total+= value for value in students_grades.values()]\n",
      "avg = total/len(students_grades)\n",
      "8/158:\n",
      "total = 0\n",
      "(total+= value for value in students_grades.values())\n",
      "avg = total/len(students_grades)\n",
      "8/159:\n",
      "total = 0\n",
      "total+= value for value in students_grades.values()\n",
      "avg = total/len(students_grades)\n",
      "8/160:\n",
      "total = 0\n",
      "total += [value for value in students_grades.values()]\n",
      "avg = total/len(students_grades)\n",
      "8/161:\n",
      "total = 0\n",
      "total += (value for value in students_grades.values())\n",
      "avg = total/len(students_grades)\n",
      "8/162:\n",
      "total = 0\n",
      "total += value for value in students_grades.values()\n",
      "avg = total/len(students_grades)\n",
      "8/163:\n",
      "total = 0\n",
      "[value for value in students_grades.values()]\n",
      "8/164:\n",
      "total = 0\n",
      "[value for value in students_grades.values()][0]\n",
      "8/165: total += [value for value in students_grades.values()]\n",
      "8/166: total += value for value in students_grades.values()\n",
      "8/167: value for value in students_grades.values()\n",
      "8/168: [value for value in students_grades.values()]\n",
      "8/169: [students_grades.values()]\n",
      "8/170: [value for value in students_grades.values()]\n",
      "8/171: total += [value for value in students_grades.values()]\n",
      "8/172: total += [value for value in students_grades.values()][value]\n",
      "8/173:\n",
      "name = []\n",
      "name +=[student[\"name\"]for student in course[\"students\"]] for course in enrollments\n",
      "    \n",
      "name\n",
      "8/174:\n",
      "name = []\n",
      "name +=[student[\"name\"] for course in enrollments for student in course[\"students\"]] \n",
      "    \n",
      "name\n",
      "8/175:\n",
      "name =[student[\"name\"] for course in enrollments for student in course[\"students\"]] \n",
      "    \n",
      "name\n",
      "8/176: name =[student[\"name\"] for course in enrollments for student in course[\"students\"]]\n",
      "8/177:\n",
      "name =[student[\"name\"] for course in enrollments for student in course[\"students\"]] \n",
      "name\n",
      "8/178:\n",
      "book_price_usd = 19\n",
      "book_price_eur = book_price_usd * .82 \n",
      "print(f\"The price in euros is {book_price_eur}\")\n",
      "\n",
      "tablet_price_usd = 349\n",
      "tablet_price_eur = tablet_price_usd * .82 \n",
      "print(f\"The price in euros is {tablet_price_eur}\")\n",
      "\n",
      "laptop_price_usd = 699\n",
      "laptop_price_eur = laptop_price_usd * .82 \n",
      "print(f\"The price in euros is {laptop_price_eur}\")\n",
      "8/179:\n",
      "book_price_usd = 19\n",
      "book_price_eur = book_price_usd * .82 \n",
      "print(\"The price in euros is {book_price_eur}\")\n",
      "\n",
      "tablet_price_usd = 349\n",
      "tablet_price_eur = tablet_price_usd * .82 \n",
      "print(f\"The price in euros is {tablet_price_eur}\")\n",
      "\n",
      "laptop_price_usd = 699\n",
      "laptop_price_eur = laptop_price_usd * .82 \n",
      "print(f\"The price in euros is {laptop_price_eur}\")\n",
      "8/180:\n",
      "book_price_usd = 19\n",
      "book_price_eur = book_price_usd * .82 \n",
      "print(f\"The price in euros is {book_price_eur}\")\n",
      "\n",
      "tablet_price_usd = 349\n",
      "tablet_price_eur = tablet_price_usd * .82 \n",
      "print(f\"The price in euros is {tablet_price_eur}\")\n",
      "\n",
      "laptop_price_usd = 699\n",
      "laptop_price_eur = laptop_price_usd * .82 \n",
      "print(f\"The price in euros is {laptop_price_eur}\")\n",
      "8/181:\n",
      "def sing_happy_birthday(): \n",
      "    print(\"Happy Birthday To You\")\n",
      "    print(\"Happy Birthday To You\")\n",
      "    print(\"Happy Birthday Dear You\")\n",
      "    print(\"Happy Birthday To You\")\n",
      "\n",
      "sing_happy_birthday()\n",
      "8/182:\n",
      "sing_happy_birthday()\n",
      "sing_happy_birthday()\n",
      "sing_happy_birthday()\n",
      "8/183:\n",
      "instructor = \"Hannes\"\n",
      "\n",
      "def say_hello():\n",
      "    print(\"Hello\" + instructor)\n",
      "    \n",
      "print(instructor)\n",
      "8/184:\n",
      "instructor = \"Hannes\"\n",
      "\n",
      "def say_hello():\n",
      "    print(\"Hello\" + instructor)\n",
      "8/185:\n",
      "instructor = \"Hannes\"\n",
      "\n",
      "def say_hello():\n",
      "    print(\"Hello\" + instructor)\n",
      "    \n",
      "say_hello()\n",
      "print(instructor)\n",
      "8/186:\n",
      "instructor = \"Hannes\"\n",
      "\n",
      "def say_hello():\n",
      "    print(\"Hello \" + instructor)\n",
      "    \n",
      "say_hello()\n",
      "print(instructor)\n",
      "8/187:\n",
      "def say_hello(first_name):\n",
      "    print(\"Hello \" + first_name)\n",
      "    \n",
      "say_hello(\"Hannes\")\n",
      "8/188:\n",
      "def say_hello(first_name, last_name):\n",
      "    print(\"Hello \" + first_name + last_name)\n",
      "\n",
      "say_hello(Trang, Bui)\n",
      "8/189:\n",
      "def say_hello(first_name, last_name):\n",
      "    print(\"Hello \" + first_name + last_name)\n",
      "\n",
      "say_hello(\"Trang\", \"Bui\")\n",
      "8/190:\n",
      "def say_hello(first_name, last_name):\n",
      "    print(\"Hello \" + first_name +\" \"+ last_name)\n",
      "\n",
      "say_hello(\"Trang\", \"Bui\")\n",
      "8/191:\n",
      "def add(a,b):\n",
      "    print(result = a + b)\n",
      "\n",
      "add(3,4)\n",
      "8/192:\n",
      "def add(a,b):\n",
      "    print(a + b)\n",
      "\n",
      "add(3,4)\n",
      "8/193:\n",
      "def add(a,b):\n",
      "    print(a + b)\n",
      "\n",
      "add(5,-2)\n",
      "8/194:\n",
      "def add(a,b):\n",
      "    return a + b\n",
      "\n",
      "add(5,-2)\n",
      "8/195:\n",
      "def add(a,b):\n",
      "    return a + b\n",
      "\n",
      "add(add(5,-2),4)\n",
      "8/196:\n",
      "def add(a,b):\n",
      "    return a + b\n",
      "\n",
      "add(add(add(5,-2),4),3)\n",
      "8/197:\n",
      "# your code goes here!\n",
      "day_of_the_week = [Mon, Tue, Wed, Thu, Fri, Sat, Sun]\n",
      "def return_day(x):\n",
      "    if x > 0 and x < 8:\n",
      "        return day_of_the_week[x-1]\n",
      "    else:\n",
      "        return print(\"None\")\n",
      "\n",
      "return_day(1)\n",
      "8/198:\n",
      "# your code goes here!\n",
      "day_of_the_week = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
      "def return_day(x):\n",
      "    if x > 0 and x < 8:\n",
      "        return day_of_the_week[x-1]\n",
      "    else:\n",
      "        return print(\"None\")\n",
      "\n",
      "return_day(1)\n",
      "8/199:\n",
      "# your code goes here!\n",
      "day_of_the_week = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
      "def return_day(x):\n",
      "    if x > 0 and x < 8:\n",
      "        return day_of_the_week[x-1]\n",
      "    else:\n",
      "        return print(\"None\")\n",
      "\n",
      "return_day(8)\n",
      "8/200:\n",
      "# your code goes here!\n",
      "day_of_the_week = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
      "def return_day(x):\n",
      "    if x > 0 and x < 8:\n",
      "        return day_of_the_week[x-1]\n",
      "    else:\n",
      "        return print(\"None\")\n",
      "\n",
      "return_day(7)\n",
      "8/201:\n",
      "# your code goes here!\n",
      "day_of_the_week = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
      "def return_day(x):\n",
      "    if x > 0 and x < 8:\n",
      "        return day_of_the_week[x-1]\n",
      "    else:\n",
      "        return print(\"None\")\n",
      "\n",
      "return_day(1)\n",
      "8/202:\n",
      "# your code goes here!\n",
      "day_of_the_week = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
      "def return_day(x):\n",
      "    if x > 0 and x < 8:\n",
      "        return day_of_the_week[x-1]\n",
      "    else:\n",
      "        return print(\"None\")\n",
      "\n",
      "return_day(8)\n",
      "8/203:\n",
      "# your code goes here!\n",
      "day_of_the_week = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
      "def return_day(x):\n",
      "    if x > 0 and x < 8:\n",
      "        return day_of_the_week[x-1]\n",
      "    else:\n",
      "        return print(\"None\")\n",
      "\n",
      "return_day(8)\n",
      "8/204:\n",
      "def return_day(x):\n",
      "    days={1 : \"Mon\", 2: \"Tue\", 3: \"Wed\", 4 : \"Thu\", 5 : \"Fri\", 6: \"Sat\", 7: \"Sun\"}\n",
      "    return days.get(num)\n",
      "return_day(1)\n",
      "8/205:\n",
      "def return_day(x):\n",
      "    days={1 : \"Mon\", 2: \"Tue\", 3: \"Wed\", 4 : \"Thu\", 5 : \"Fri\", 6: \"Sat\", 7: \"Sun\"}\n",
      "    return days.get(x)\n",
      "return_day(1)\n",
      "8/206:\n",
      "def return_day(x):\n",
      "    days={1 : \"Mon\", 2: \"Tue\", 3: \"Wed\", 4 : \"Thu\", 5 : \"Fri\", 6: \"Sat\", 7: \"Sun\"}\n",
      "    return days.get(x)\n",
      "return_day(8)\n",
      "8/207:\n",
      "def return_day(x):\n",
      "    days={1 : \"Mon\", 2: \"Tue\", 3: \"Wed\", 4 : \"Thu\", 5 : \"Fri\", 6: \"Sat\", 7: \"Sun\"}\n",
      "    return days.get(x)\n",
      "print(return_day(8))\n",
      "8/208:\n",
      "def convert_usd_eur(usd_amounts, currency_rate):\n",
      "    eur_amounts = []\n",
      "    for usd_amount in usd_amounts: \n",
      "        eur_amounts.append(usd_amount * currency_rate)\n",
      "        return eur_amounts\n",
      "        \n",
      "convert_usd_eur([10, 100, 1000], .82)\n",
      "8/209:\n",
      "def convert2(usd, rate):\n",
      "    eur = []\n",
      "    [eur = usd * rate for usd in usd]\n",
      "    return eur\n",
      "\n",
      "covert2([10, 100, 1000], .82)\n",
      "8/210:\n",
      "def convert2(usd, rate):\n",
      "    eur = [usd * rate for usd in usd]\n",
      "    return eur\n",
      "\n",
      "covert2([10, 100, 1000], .82)\n",
      "8/211:\n",
      "def convert2(usd, rate):\n",
      "    return [usd * rate for usd in usd]\n",
      "    \n",
      "covert2([10, 100, 1000], .82)\n",
      "8/212:\n",
      "def convert2(usd, rate):\n",
      "    return [usd * rate for usd in usd]\n",
      "    \n",
      "convert2([10, 100, 1000], .82)\n",
      "8/213:\n",
      "def convert_usd_eur(usd_amounts, currency_rate):\n",
      "    eur_amounts = []\n",
      "    for usd_amount in usd_amounts: \n",
      "        eur_amounts.append(usd_amount * currency_rate)\n",
      "    return eur_amounts\n",
      "        \n",
      "convert_usd_eur([10, 100, 1000], .82)\n",
      "8/214:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        return total += num_list[num] \n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/215:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        return total = total + num_list[num]\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/216:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        return total = total + num_list[num]\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/217:\n",
      "num_list = [3, -4, 1, 2, 0]\n",
      "num_list[0]\n",
      "8/218:\n",
      "num_list = [3, -4, 1, 2, 0]\n",
      "num_list[0] + num_list[1]\n",
      "8/219:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        return total + num_list[num]\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/220:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        total += num_list[num]\n",
      "    return total\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/221:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        total += num_list[num]\n",
      "        return total\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/222:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        return total + num\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/223:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        total += num\n",
      "        return total\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/224:\n",
      "# your code goes here!\n",
      "def add_list(num_list):\n",
      "    total = 0\n",
      "    for num in num_list:\n",
      "        total += num\n",
      "    return total\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "8/225:\n",
      "import os\n",
      "os.getcwd()\n",
      "8/226:\n",
      "# solution\n",
      "def add_list(num_list):\n",
      "    \"\"\"adding up all the numbers in the list\"\"\"\n",
      "    total = 0\n",
      "    for num in num_list: \n",
      "        total += num\n",
      "    return total\n",
      "\n",
      "add_list([3, -4, 1, 2, 0])\n",
      "10/1: import requests\n",
      "10/2: import requests\n",
      "10/3: url = \"https://www.twitch.tv/directory/game/The%20Sims%204\"\n",
      "10/4: url = \"https://www.twitch.tv/directory/game/The%20Sims%204\"\n",
      "10/5:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "10/6:\n",
      "url = \"https://www.twitch.tv/directory/game/The%20Sims%204\"\n",
      "request = request.get(url)\n",
      "10/7:\n",
      "url = \"https://www.twitch.tv/directory/game/The%20Sims%204\"\n",
      "request = requests.get(url)\n",
      "10/8: source.code = request.test\n",
      "10/9: source.code = request.test\n",
      "10/10: source.code = request.text\n",
      "10/11:\n",
      "url = \"https://www.twitch.tv/directory/game/The%20Sims%204\"\n",
      "request = requests.get(url)\n",
      "10/12: source.code = request.text\n",
      "10/13: src = request.text\n",
      "10/14:\n",
      "src = request.text\n",
      "\n",
      "soup = BeautifulSoup(src)\n",
      "10/15:\n",
      "src = request.text\n",
      "\n",
      "soup = BeautifulSoup(src)\n",
      "print(soup.find('h1'))\n",
      "10/16:\n",
      "src = request.text\n",
      "\n",
      "soup = BeautifulSoup(src)\n",
      "print(soup.find('h3'))\n",
      "10/17:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "10/18:\n",
      "url = \"https://www.twitch.tv/directory/game/The%20Sims%204\"\n",
      "request = requests.get(url)\n",
      "10/19:\n",
      "src = request.text\n",
      "\n",
      "soup = BeautifulSoup(src)\n",
      "print(soup.find('h3'))\n",
      "10/20:\n",
      "src = request.text\n",
      "\n",
      "soup = BeautifulSoup(src)\n",
      "print(soup.find('h1 class=\"CoreText-sc-cpl358-0 ScTitleText-sc-1gsen4-0 gAYdrP tw-title'))\n",
      "10/21:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "10/22:\n",
      "url = \"https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=sustainability&oq=\"\n",
      "request = requests.get(url)\n",
      "10/23:\n",
      "src = request.text\n",
      "\n",
      "soup = BeautifulSoup(src)\n",
      "print(soup.find('h3'))\n",
      "10/24:\n",
      "src = request.text\n",
      "\n",
      "soup = BeautifulSoup(src)\n",
      "print(soup.find_all('h3'))\n",
      "12/1: import requests\n",
      "12/2: import requests\n",
      "12/3: import requests\n",
      "12/4:\n",
      "url = \"https://www.hornbach.nl/shop/Planten/Tuinplanten/Vaste-planten/S5120/artikeloverzicht.html\"\n",
      "plant_request = request.get(url)\n",
      "12/5:\n",
      "url = \"https://www.hornbach.nl/shop/Planten/Tuinplanten/Vaste-planten/S5120/artikeloverzicht.html\"\n",
      "plant_request = requests.get(url)\n",
      "12/6: plant_src = plant_request.text\n",
      "12/7:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src)\n",
      "12/8:\n",
      "plant_src = plant_request.text\n",
      "type(plant_request)\n",
      "12/9: ?type\n",
      "12/10: print(plant_src[1:10])\n",
      "12/11:\n",
      "plant_src = plant_request.text\n",
      "print(plant_request)\n",
      "12/12:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src)\n",
      "12/13: print(plant_src[5000:10000])\n",
      "12/14:\n",
      "a = \"<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n",
      "<!DOCTYPE html>\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"nl\" id=\"ng-app\">\n",
      "\n",
      "\n",
      "<head>\n",
      "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=EDGE\" />\n",
      "    <meta name=\"csrf-token\" content=\"V4W7-KLVS-P9PL-WFST-LKYY-WGO6-K83E-WUQC\" />\n",
      "    <title>\"\n",
      "len(a)\n",
      "12/15:\n",
      "a = '<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n",
      "<!DOCTYPE html>\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"nl\" id=\"ng-app\">\n",
      "\n",
      "\n",
      "<head>\n",
      "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=EDGE\" />\n",
      "    <meta name=\"csrf-token\" content=\"V4W7-KLVS-P9PL-WFST-LKYY-WGO6-K83E-WUQC\" />\n",
      "    <title>'\n",
      "len(a)\n",
      "12/16:\n",
      "a = \"<?xml version=\"1.0\" encoding=\"utf-8\" ?><!DOCTYPE html><html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"nl\" id=\"ng-app\">\n",
      "<head>  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=EDGE\" />\n",
      "    <meta name=\"csrf-token\" content=\"V4W7-KLVS-P9PL-WFST-LKYY-WGO6-K83E-WUQC\" />\n",
      "    <title>\"\n",
      "len(a)\n",
      "12/17:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [2000:2500])\n",
      "12/18:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [1000:1100])\n",
      "12/19:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [500:1100])\n",
      "12/20:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:500])\n",
      "12/21:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:300])\n",
      "12/22:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:350])\n",
      "12/23:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:340])\n",
      "12/24:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:330])\n",
      "12/25:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:335])\n",
      "12/26:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:332])\n",
      "12/27:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [200:332])\n",
      "12/28:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [280:332])\n",
      "12/29:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [270:332])\n",
      "12/30:\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [278:332])\n",
      "12/31:\n",
      "#html src from the retrieved webpage\n",
      "plant_src = plant_request.text\n",
      "print(plant_src [0:332])\n",
      "12/32:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(soup.find('title'))\n",
      "12/33:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(soup.find('title')[0:60])\n",
      "12/34:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print((soup.find('title'))[0:60]))\n",
      "12/35:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print((soup.find('title'))[0:60])\n",
      "12/36:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "type(soup.find('title')\n",
      "12/37:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "type(soup.find('title'))\n",
      "12/38:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "soup.find('title')\n",
      "12/39:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "soup.find('title')[1]\n",
      "12/40:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "string(soup.find('title'))\n",
      "12/41:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "str(soup.find('title'))\n",
      "12/42:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "str(soup.find('title'))[0:60]\n",
      "12/43:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(str(soup.find('title'))[0:60])\n",
      "12/44:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(str(soup.find('title'))[8:60])\n",
      "12/45:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(str(soup.find('title'))[7:60])\n",
      "12/46:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(str(soup.find('title'))[7:61])\n",
      "12/47:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(str(soup.find('title'))[7:62])\n",
      "12/48:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(str(soup.find('title'))[7:61])\n",
      "12/49:\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(plant_src)\n",
      "print(str(soup.find('title'))[7:61])\n",
      "12/50: soup.find_all('h1')\n",
      "12/51: soup.find_all('h2')\n",
      "12/52: soup.find_all('meta')\n",
      "12/53: soup.find_all('price')\n",
      "12/54: soup.find_all('prijs')\n",
      "12/55: soup.find_all('rating')\n",
      "12/56: soup.find_all('vaste')\n",
      "12/57: soup.find_all('meta')\n",
      "12/58: soup.find_all('meta')[1]\n",
      "12/59: soup.find_all('title')[1]\n",
      "12/60: soup.find_all('title')\n",
      "12/61: soup.find_all('title').get_text()\n",
      "12/62: soup.find_all('title').[1]get_text()\n",
      "12/63: soup.find_all('title')[1].get_text()\n",
      "12/64: soup.find_all('title')[0].get_text()\n",
      "12/65: type(soup.find_all('title')[0].get_text())\n",
      "12/66: print(soup.find_all('title')[0].get_text())\n",
      "12/67:\n",
      "url2 = \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n",
      "book = requests.get(url)\n",
      "12/68:\n",
      "url2 = \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n",
      "book = requests.get(url)\n",
      "12/69:\n",
      "url2 = \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n",
      "book = requests.get(url)\n",
      "book_src = book.text\n",
      "soup2 = BeautifulSoup(book_src)\n",
      "12/70:\n",
      "url2 = \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n",
      "book = requests.get(url)\n",
      "book_src = book.text\n",
      "soup2 = BeautifulSoup(book_src)\n",
      "book\n",
      "12/71: soup2.find(table)\n",
      "12/72: soup2.find(\"table\")\n",
      "12/73: print(soup2.find(\"table\"))\n",
      "12/74: print(soup2.find('table'))\n",
      "12/75: print(soup2.find('h1'))\n",
      "12/76:\n",
      "url2 = \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n",
      "book = requests.get(url)\n",
      "book_src = book.text\n",
      "soup2 = BeautifulSoup(book_src)\n",
      "12/77: print(soup2.find('h1'))\n",
      "12/78:\n",
      "url2 = \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n",
      "book = requests.get(url2)\n",
      "book_src = book.text\n",
      "soup2 = BeautifulSoup(book_src)\n",
      "12/79: print(soup2.find('table'))\n",
      "12/80: print(soup2.find.all('table'))\n",
      "12/81: print(soup2.find_all('table'))\n",
      "12/82: print(soup2.find('table'))\n",
      "12/83: print(soup2.find('table').get_text())\n",
      "12/84: soup2.find('table').get_text()\n",
      "12/85: print(soup2.find('table'))\n",
      "12/86: print(soup2.find('table'))\n",
      "12/87: print(soup2.find('table').find_all('td'))\n",
      "12/88: print(soup2.find('table').find_all('td')[4])\n",
      "12/89: print(soup2.find('table').find_all('td')[4].get_text())\n",
      "12/90: print(soup2.find('table').find_all('td')[4].get_text())\n",
      "12/91:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find_all(\"h2\").get_text())\n",
      "12/92:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find_all(\"h2\").get_text())\n",
      "coursera\n",
      "12/93:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "coursera\n",
      "12/94:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find_all('h2').get_text())\n",
      "12/95:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find_all('h2'))\n",
      "12/96:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find_all('h2 class=\"color-primary-text card-title headline-1-text\"'))\n",
      "12/97:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find_all('color-primary-text card-title headline-1-text'))\n",
      "12/98:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find_all('color-primary-text'))\n",
      "12/99:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find('color-primary-text'))\n",
      "12/100:\n",
      "url3 = \"https://www.coursera.org/search?query=python&\"\n",
      "coursera = requests.get(url3)\n",
      "coursera_src = coursera.text\n",
      "soup3 = BeautifulSoup(coursera_src)\n",
      "print(soup3.find('color-primary-text'))\n",
      "12/101:\n",
      "url4 = \"https://icanhazdadjoke.com/\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "joke = response.json()\n",
      "print(joke)\n",
      "11/1:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "joke_request = response.json() \n",
      "print(joke_request)\n",
      "11/2:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "joke_request = response.json() \n",
      "print(joke_request)\n",
      "12/102:\n",
      "url4 = \"https://icanhazdadjoke.com/\"\n",
      "response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "joke = response.json()\n",
      "print(joke)\n",
      "12/103:\n",
      "url4 = \"https://icanhazdadjoke.com/\"\n",
      "response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "joke = response.json()\n",
      "print(joke)\n",
      "12/104:\n",
      "url4 = \"https://icanhazdadjoke.com/\"\n",
      "response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "joke = response.json()\n",
      "print(joke)\n",
      "12/105:\n",
      "url4 = \"https://icanhazdadjoke.com/\"\n",
      "response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "joke = response.json()\n",
      "print(joke)\n",
      "12/106:\n",
      "url4 = \"https://icanhazdadjoke.com/\"\n",
      "response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "joke = response.json()\n",
      "print(joke['joke'])\n",
      "12/107:\n",
      "\n",
      "for i in range(9):\n",
      "    response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "    joke = response.json()\n",
      "    print(joke['joke'])\n",
      "12/108:\n",
      "\n",
      "for i in range(10):\n",
      "    response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "    joke = response.json()\n",
      "    print(joke['joke'])\n",
      "11/3:\n",
      "# Question 3\n",
      "jokes = [] \n",
      "\n",
      "for counter in range(10):\n",
      "    url = \"https://icanhazdadjoke.com\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "    joke_request = response.json() \n",
      "    jokes.append(joke_request)\n",
      "11/4:\n",
      "# Question 3\n",
      "jokes = [] \n",
      "\n",
      "for counter in range(10):\n",
      "    url = \"https://icanhazdadjoke.com\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "    joke_request = response.json() \n",
      "    jokes.append(joke_request)\n",
      "11/5:\n",
      "# Question 3\n",
      "jokes = [] \n",
      "\n",
      "for counter in range(10):\n",
      "    url = \"https://icanhazdadjoke.com\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "    joke_request = response.json() \n",
      "    jokes.append(joke_request)\n",
      "    \n",
      "print(jokes)\n",
      "11/6:\n",
      "# Question 3\n",
      "jokes = [] \n",
      "\n",
      "for counter in range(10):\n",
      "    url = \"https://icanhazdadjoke.com\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "    joke_request = response.json() \n",
      "    jokes.append(joke_request)\n",
      "    \n",
      "print(jokes['joke'])\n",
      "11/7:\n",
      "# Question 3\n",
      "jokes = [] \n",
      "\n",
      "for counter in range(10):\n",
      "    url = \"https://icanhazdadjoke.com\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "    joke_request = response.json() \n",
      "    jokes.append(joke_request)\n",
      "    \n",
      "print(jokes_request)\n",
      "11/8:\n",
      "# Question 3\n",
      "jokes = [] \n",
      "\n",
      "for counter in range(10):\n",
      "    url = \"https://icanhazdadjoke.com\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "    joke_request = response.json() \n",
      "    jokes.append(joke_request)\n",
      "    \n",
      "print(jokes)\n",
      "12/109:\n",
      "jokes = []\n",
      "for i in range(10):\n",
      "    response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "    joke = response.json()\n",
      "    jokes.append = joke\n",
      "12/110:\n",
      "jokes = []\n",
      "for i in range(10):\n",
      "    response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "    joke = response.json()\n",
      "    jokes.append (joke)\n",
      "12/111:\n",
      "jokes = []\n",
      "for i in range(10):\n",
      "    response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "    joke = response.json()\n",
      "    jokes.append(joke)\n",
      "    print(jokes)\n",
      "12/112:\n",
      "jokes = []\n",
      "for i in range(10):\n",
      "    response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "    joke = response.json()\n",
      "    jokes.append(joke['joke'])\n",
      "    print(jokes)\n",
      "12/113:\n",
      "jokes = []\n",
      "for i in range(10):\n",
      "    response = requests.get(url4, headers={\"Accept\": \"application/json\"})\n",
      "    joke = response.json()\n",
      "    jokes.append(joke['joke'])\n",
      "print(jokes)\n",
      "17/1:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text)\n",
      "17/2: ?BeautifulSoup\n",
      "17/3:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text)\n",
      "print(soup)\n",
      "17/4:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "print(soup)\n",
      "17/5:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text)\n",
      "print(soup)\n",
      "17/6:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "print(soup)\n",
      "17/7:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "print(soup)\n",
      "17/8:\n",
      "soup2 = BeautifulSoup(res.text)\n",
      "print(soup2)\n",
      "17/9:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "print(soup)\n",
      "17/10:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "type(soup)\n",
      "17/11: soup.find_all(\"a\")\n",
      "15/1:\n",
      "# Run this code now\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find_all(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "\n",
      "soup.find(\"a\", href=True)[\"href\"]\n",
      "15/2:\n",
      "# Run this code now\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find_all(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "17/12:\n",
      "for link in soup.find_all(\"a\"):\n",
      "    print(link.attrs[\"href\"])\n",
      "17/13:\n",
      "for link in soup.find_all(\"a\"):\n",
      "    print(link.attrs[\"href\"])\n",
      "17/14:\n",
      "for link in soup.find_all(\"a\"):\n",
      "    print(link.attrs[\"href\"])\n",
      "17/15: soup.find(\"a\", href=True)[\"href\"]\n",
      "17/16: soup.find_all(\"a\", href=True)[\"href\"]\n",
      "17/17: soup.find(\"a\", href=True)\n",
      "17/18: soup.find(\"a\", href=True)[\"href\"]\n",
      "17/19: type(soup.find(\"a\", href=True))\n",
      "17/20: ?.find_all\n",
      "17/21: ?BeautifulSoup.find_all\n",
      "17/22: doc(BeautifulSoup.find_all)\n",
      "17/23: doc(BeautifulSoup)\n",
      "17/24: help(BeautifulSoup)\n",
      "17/25: help(BeautifulSoup.find_all)\n",
      "17/26:\n",
      "for link in soup.find_all(\"a\"):\n",
      "    print(link.attrs[\"href\"])\n",
      "17/27:\n",
      "for link in soup.find_all('a'):\n",
      "    print(link.attrs)\n",
      "17/28:\n",
      "for link in soup.find_all('a'):\n",
      "    print(link.attrs)\n",
      "17/29: soup.find_all('a')\n",
      "17/30: soup.find_all('a')\n",
      "17/31:\n",
      "for link in soup.find_all('a'):\n",
      "    print(link.attrs)\n",
      "17/32: soup.find_all(\"a\", class_=\"product_pod\")\n",
      "17/33: soup.find_all(class_= 'product_pod')\n",
      "17/34: soup.find_all(class_= 'product_pod')[0]\n",
      "17/35: soup.find_all(class_= 'product_pod')[0].find('a').attrs['href']\n",
      "17/36: soup.find_all(class_= 'product_pod')[0].find('a').attrs['href']\n",
      "17/37: soup.find_all(class_='product_pod')[1].find('a').attrs['href']\n",
      "17/38: soup.find_all(class_='product_pod')[1].find('a').attrs['href']\n",
      "17/39: ?slicing\n",
      "17/40: help(slicing)\n",
      "17/41: link = soup.find_all(class_='product_pod')[1].find('a').attrs['href']\n",
      "17/42: type(link)\n",
      "17/43:\n",
      "link = soup.find_all(class_='product_pod')[1].find('a').attrs['href']\n",
      "print(link)\n",
      "17/44: link[5:]\n",
      "17/45: book_url = 'https://books.toscrape.com/catalogue/' + link[5:]\n",
      "17/46:\n",
      "book_url = 'https://books.toscrape.com/catalogue/' + link[5:]\n",
      "\n",
      "print(book_url)\n",
      "17/47:\n",
      "book_url = 'https://books.toscrape.com/catalogue/' + link[6:]\n",
      "\n",
      "print(book_url)\n",
      "17/48:\n",
      "book_url = 'https://books.toscrape.com/catalogue/' + link[6:]\n",
      "\n",
      "print(book_url)\n",
      "17/49:\n",
      "book_url2 = link.replace('../../', 'https://books.toscrape.com/catalogue/')\n",
      "print(book_url2)\n",
      "17/50:\n",
      "book_url2 = link.replace('../../', 'https://books.toscrape.com/catalogue/')\n",
      "print(book_url2)\n",
      "17/51: soup.find_all(class_='product_pod')[0:5]\n",
      "17/52:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    for i in book:\n",
      "        link = book[i].find('a').attrs['href']\n",
      "        book_links.append(link)\n",
      "        print(book_links)\n",
      "17/53:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    for i in book:\n",
      "        link = book[i].find(\"a\").attrs[\"href\"]\n",
      "        book_links.append(link)\n",
      "        print(book_links)\n",
      "17/54:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    book_links.append(link)\n",
      "print(book_links)\n",
      "17/55: soup.find_all(class_='product_pod')[0]\n",
      "17/56:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    book_links.append(link)\n",
      "print(book_links\n",
      "17/57:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    book_links.append(link)\n",
      "print(book_links)\n",
      "17/58:\n",
      "book_links ={}\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links = {title : link}\n",
      "print(book_links)\n",
      "17/59:\n",
      "book_links ={}\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links += {title : link}\n",
      "print(book_links)\n",
      "17/60:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "print(book_links)\n",
      "17/61:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "print(book_links)[0]\n",
      "17/62:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "print(book_links)[0]\n",
      "17/63:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "print(book_links)\n",
      "17/64:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    book_links.append(link)\n",
      "print(book_links)[1]\n",
      "17/65:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    book_links.append(link)\n",
      "book_links[1]\n",
      "17/66:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "book_links[1]\n",
      "17/67:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "book_links[0]\n",
      "17/68:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "print(book_links[0])\n",
      "17/69:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "\n",
      "book_links.keys\n",
      "17/70:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "\n",
      "book_links[0:5]\n",
      "17/71:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "\n",
      "book_links[0:5]\n",
      "17/72:\n",
      "for book in book_links:\n",
      "    print(book.keys)\n",
      "17/73:\n",
      "for book in book_links:\n",
      "    if book.keys(\"title\")=\"A Light in the Attic\":\n",
      "        print book.values(\"link\")\n",
      "17/74:\n",
      "for book in book_links:\n",
      "    if book[\"title\"]=\"A Light in the Attic\":\n",
      "        print book(\"link\")\n",
      "17/75:\n",
      "for book in book_links:\n",
      "    if book[\"title\"]=\"A Light in the Attic\":\n",
      "        print book[\"link\"]\n",
      "17/76:\n",
      "for book in book_links:\n",
      "    if book.get[\"title\"]=\"A Light in the Attic\":\n",
      "        print book.get[\"link\"]\n",
      "17/77:\n",
      "for book in book_links:\n",
      "    if book.get[\"title\"]==\"A Light in the Attic\":\n",
      "        print book.get[\"link\"]\n",
      "17/78:\n",
      "for book in book_links:\n",
      "    if book[\"title\"] == \"A Light in the Attic\":\n",
      "        print book\n",
      "17/79:\n",
      "for book in book_links:\n",
      "    if book[\"title\"] == \"A Light in the Attic\":\n",
      "        print (book)\n",
      "17/80:\n",
      "for book in book_links:\n",
      "    if book[\"title\"] == \"A Light in the Attic\":\n",
      "        print (book)\n",
      "17/81: ?next\n",
      "15/3: next((book for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "17/82: next(book for book in book_links if book[\"title\"] == \"A Light in the Attic\")\n",
      "17/83: next(book for book in book_links if book[\"title\"] == \"A Light in the Attic\", None)\n",
      "17/84: next((book for book in book_links if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "17/85: next((book for book in book_links if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "17/86:\n",
      "full_url = []\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"].replace('../../','https://books.toscrape.com/catalogue/')\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "17/87:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "\n",
      "book_links[0:5]\n",
      "17/88:\n",
      "full_url = []\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"].replace('../../','https://books.toscrape.com/catalogue/')\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    full_url.append({'title': title, 'link': link})\n",
      "prink()\n",
      "17/89:\n",
      "full_url = []\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"].replace('../../','https://books.toscrape.com/catalogue/')\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    full_url.append({'title': title, 'link': link})\n",
      "prink(full_url)\n",
      "17/90:\n",
      "full_url = []\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"].replace('../../','https://books.toscrape.com/catalogue/')\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    full_url.append({'title': title, 'link': link})\n",
      "print(full_url)\n",
      "17/91:\n",
      "full_url = []\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"].replace('../../','https://books.toscrape.com/catalogue/')\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    full_url.append({'title': title, 'link': link})\n",
      "print(full_url)\n",
      "17/92: book_links[0:2]\n",
      "17/93: book_links[0:2]\n",
      "17/94: ?enumerate\n",
      "17/95:\n",
      "counter = 1\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/page-\"\n",
      "full_url = base_url + str(counter) + \".html\"\n",
      "print(full_url)\n",
      "17/96:\n",
      "counter = 1\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/page-\"\n",
      "full_url = base_url + str(counter) + \".html\"\n",
      "print(full_url)\n",
      "17/97: counter = range[50]\n",
      "17/98: counter = range(50)\n",
      "17/99: print(range(50))\n",
      "17/100: print(range(1,50))\n",
      "17/101:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/page-\"\n",
      "full_urls = []\n",
      "for counter in range(1,50):\n",
      "    full_urls.append(base_url + str(counter) + \".html\")\n",
      "\n",
      "print(full_urls)\n",
      "17/102:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/page-\"\n",
      "full_urls = []\n",
      "for counter in range(1,51):\n",
      "    full_urls.append(base_url + str(counter) + \".html\")\n",
      "\n",
      "print(full_urls)\n",
      "17/103:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/page-\"\n",
      "page = []\n",
      "for counter in range(1,51):\n",
      "    page.append(base_url + str(counter) + \".html\")\n",
      "\n",
      "print(pages)\n",
      "17/104:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/page-\"\n",
      "pages = []\n",
      "for counter in range(1,51):\n",
      "    pages.append(base_url + str(counter) + \".html\")\n",
      "\n",
      "print(pages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/105: print(range(1, 50))\n",
      "17/106:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in rang(1, 101):\n",
      "    quote_page_urls.append (base + str(counter) + \"/\")\n",
      "print(quote_page_urls)(0:2)\n",
      "17/107:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in rang(1, 101):\n",
      "    quote_page_urls.append (base + str(counter) + \"/\")\n",
      "print(quote_page_urls)[0:2]\n",
      "17/108:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append (base + str(counter) + \"/\")\n",
      "print(quote_page_urls)[0:2]\n",
      "17/109:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "print(quote_page_urls)(0,2)\n",
      "17/110:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "print(quote_page_urls)(0:2)\n",
      "17/111:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "type(quote_page_urls)\n",
      "17/112:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "print(quote_page_urls [0,2])\n",
      "17/113:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "print(quote_page_urls [0:2])\n",
      "17/114:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "print(quote_page_urls [0:3])\n",
      "17/115:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "print(quote_page_urls [0:3])\n",
      "17/116:\n",
      "from time import sleep\n",
      "sleep(3)\n",
      "print(\"Done\")\n",
      "17/117:\n",
      "from time import sleep\n",
      "sleep(3)\n",
      "print(\"Done\")\n",
      "17/118:\n",
      "def gen_page_urls(base, page_num):\n",
      "    page_urls =[]\n",
      "    \n",
      "    for counter in range(1, page_num + 1):\n",
      "        full_url = base + \"page-\" + str(counter) + \".html\"\n",
      "        page_urls.append(full_url)\n",
      "    \n",
      "    return page_urls\n",
      "17/119:\n",
      "def gen_page_urls(base, page_num):\n",
      "    page_urls =[]\n",
      "    \n",
      "    for counter in range(1, page_num + 1):\n",
      "        full_url = base + \"page-\" + str(counter) + \".html\"\n",
      "        page_urls.append(full_url)\n",
      "    \n",
      "    return page_urls\n",
      "\n",
      "gen_page_urls(\"https://books.toscrape.com/catalogue/category/books_1/\", 3)\n",
      "17/120:\n",
      "def gen_page_urls(base, page_num):\n",
      "    page_urls =[]\n",
      "    \n",
      "    for counter in range(1, page_num + 1):\n",
      "        full_url = base + \"page-\" + str(counter) + \".html\"\n",
      "        page_urls.append(full_url)\n",
      "    \n",
      "    return page_urls\n",
      "\n",
      "gen_page_urls(\"https://books.toscrape.com/catalogue/category/books_1/\", 3)\n",
      "17/121:\n",
      "def extr_book_urls(page_urls):\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = Beautiful.Soup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class =\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "17/122:\n",
      "def extr_book_urls(page_urls):\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = Beautiful.Soup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "17/123:\n",
      "def extr_book_urls(page_urls):\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = Beautiful.Soup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "17/124:\n",
      "def extr_book_urls(page_urls):\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = Beautiful.Soup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "17/125:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = generate_page_urls(base_url, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extract_book_urls(page_urls)\n",
      "17/126:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extr_book_urls(page_urls)\n",
      "17/127:\n",
      "def extr_book_urls(page_urls):\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "17/128:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extr_book_urls(page_urls)\n",
      "17/129:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extr_book_urls(page_urls)\n",
      "book_list[0:2]\n",
      "20/1:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extr_book_urls(page_urls)\n",
      "book_list[0:2]\n",
      "20/2:\n",
      "def gen_page_urls(base, page_num):\n",
      "    page_urls =[]\n",
      "    \n",
      "    for counter in range(1, page_num + 1):\n",
      "        full_url = base + \"page-\" + str(counter) + \".html\"\n",
      "        page_urls.append(full_url)\n",
      "    \n",
      "    return page_urls\n",
      "\n",
      "gen_page_urls(\"https://books.toscrape.com/catalogue/category/books_1/\", 3)\n",
      "20/3:\n",
      "def extr_book_urls(page_urls):\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "20/4:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extr_book_urls(page_urls)\n",
      "book_list[0:2]\n",
      "20/5:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://books.toscrape.com/catalogue/category/books_1/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "20/6:\n",
      "for link in soup.find_all(\"a\"):\n",
      "    print(link.attrs[\"href\"])\n",
      "20/7:\n",
      "for link in soup.find_all('a'):\n",
      "    print(link.attrs)\n",
      "20/8: soup.find_all('a')\n",
      "20/9: soup.find_all(class_= 'product_pod')[0].find('a').attrs['href']\n",
      "20/10:\n",
      "link = soup.find_all(class_='product_pod')[1].find('a').attrs['href']\n",
      "print(link)\n",
      "20/11:\n",
      "book_url = 'https://books.toscrape.com/catalogue/' + link[6:]\n",
      "\n",
      "print(book_url)\n",
      "20/12:\n",
      "book_url2 = link.replace('../../', 'https://books.toscrape.com/catalogue/')\n",
      "print(book_url2)\n",
      "20/13: soup.find_all(class_='product_pod')[0]\n",
      "20/14:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    book_links.append(link)\n",
      "book_links[1]\n",
      "20/15:\n",
      "book_links =[]\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"]\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    book_links.append({'title': title, 'link': link})\n",
      "\n",
      "book_links[0:5]\n",
      "20/16:\n",
      "for book in book_links:\n",
      "    if book[\"title\"] == \"A Light in the Attic\":\n",
      "        print (book)\n",
      "20/17: next((book for book in book_links if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "20/18:\n",
      "full_url = []\n",
      "for book in soup.find_all(class_='product_pod'):\n",
      "    link = book.find(\"a\").attrs[\"href\"].replace('../../','https://books.toscrape.com/catalogue/')\n",
      "    title = book.find(\"img\").attrs[\"alt\"]\n",
      "    full_url.append({'title': title, 'link': link})\n",
      "print(full_url)\n",
      "20/19: book_links[0:2]\n",
      "20/20:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/page-\"\n",
      "pages = []\n",
      "for counter in range(1,51):\n",
      "    pages.append(base_url + str(counter) + \".html\")\n",
      "\n",
      "print(pages)\n",
      "20/21: print(range(1, 50))\n",
      "20/22:\n",
      "base = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "for counter in range(1, 101):\n",
      "    quote_page_urls.append(base + str(counter) + \"/\")\n",
      "print(quote_page_urls [0:3])\n",
      "20/23:\n",
      "from time import sleep\n",
      "sleep(3)\n",
      "print(\"Done\")\n",
      "20/24:\n",
      "def gen_page_urls(base, page_num):\n",
      "    page_urls =[]\n",
      "    \n",
      "    for counter in range(1, page_num + 1):\n",
      "        full_url = base + \"page-\" + str(counter) + \".html\"\n",
      "        page_urls.append(full_url)\n",
      "    \n",
      "    return page_urls\n",
      "\n",
      "gen_page_urls(\"https://books.toscrape.com/catalogue/category/books_1/\", 3)\n",
      "20/25:\n",
      "def extr_book_urls(page_urls):\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "20/26:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extr_book_urls(page_urls)\n",
      "book_list[0:2]\n",
      "20/27:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book_urls =[]\n",
      "20/28:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book_urls =[]\n",
      "\n",
      "books[0]\n",
      "20/29:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base,5)\n",
      "20/30:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base,5)\n",
      "page_urls\n",
      "20/31:\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base,5)\n",
      "book_list = extr_book_urls(page_urls)\n",
      "20/32: book_list[0]\n",
      "20/33: books = soup.find_all(class_=\"product_pod\")\n",
      "20/34: books = soup.find_all(class_=\"product_pod\")\n",
      "20/35:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "books[0]\n",
      "20/36:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book.find(\"p\", class_=\"instock availability\")\n",
      "20/37:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book.find(\"a\")\n",
      "20/38:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book.find(\"p\", class_=\"instock availability\")\n",
      "20/39:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book.find(\"p\", class_=\"instock availability\")==\"In stock\"\n",
      "20/40:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book.find(\"p\", class_=\"instock availability\")== \"In stock\"\n",
      "20/41:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book.find(\"p\", class_=\"instock availability\")\n",
      "20/42:\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book.find(\"p\", class_=\"instock availability\").text\n",
      "20/43:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url})\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "        sleep(1)\n",
      "    return book_list\n",
      "20/44:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url})\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "        sleep(1)\n",
      "    return book_list\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list = extr_book_urls2(page_urls)\n",
      "book_list[0:2]\n",
      "20/45:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list = extr_book_urls2(page_urls)\n",
      "book_list[0:2]\n",
      "20/46:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url, \"avail\": book_instock})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list = extr_book_urls2(page_urls)\n",
      "book_list[0:2]\n",
      "20/47:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "            book_list2.append({\"title\": book_title,\n",
      "                             \"url\": book_url, \n",
      "                              \"avail\": book_instock})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list = extr_book_urls2(page_urls)\n",
      "book_list[0:2]\n",
      "20/48:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "            book_list2.append({\"title\": book_title,\n",
      "                             \"url\": book_url, \n",
      "                              \"avail\": book_instock})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list2 = extr_book_urls2(page_urls)\n",
      "book_list2[0:2]\n",
      "20/49:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "            book_list2.append({\"title\": book_title,\n",
      "                             \"url\": book_url, \n",
      "                              \"avail\": book_instock})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "extr_book_urls2(page_urls)\n",
      "book_list2[0:2]\n",
      "20/50:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "            book_list2.append({\"title\": book_title,\n",
      "                             \"url\": book_url, \n",
      "                              \"avail\": book_instock})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list2\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list2 = extr_book_urls2(page_urls)\n",
      "book_list2[0:2]\n",
      "20/51:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text.replace(\"\\n\",\"\").replace(\"  \",\"\")\n",
      "            book_list2.append({\"title\": book_title,\n",
      "                             \"url\": book_url, \n",
      "                              \"avail\": book_instock})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list2\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list2 = extr_book_urls2(page_urls)\n",
      "book_list2[0:2]\n",
      "20/52:\n",
      "def extr_book_urls2(page_urls):\n",
      "    book_list2 = []\n",
      "    \n",
      "    for page_url in page_urls:\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "       \n",
      "        for book in books:\n",
      "            book_title = book.find(\"img\").attrs[\"alt\"]\n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text.replace(\"\\n\",\"\").replace(\"  \",\"\")\n",
      "            book_list2.append({\"title\": book_title,\n",
      "                             \"url\": book_url, \n",
      "                              \"avail\": book_instock})\n",
      "           \n",
      "        sleep(1)\n",
      "    return book_list2\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = gen_page_urls(base, 2) \n",
      "book_list2 = extr_book_urls2(page_urls)\n",
      "book_list2[0:2]\n",
      "20/53:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, 'html.parser')\n",
      "    next_btn = soup.find(class_=\"next\")\n",
      "    return next_btn.find('a').attrs['href'] if next_btn else None\n",
      "\n",
      "page_1 = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "print(\"The next page is: \" + str(check_next_page(page_1)))\n",
      "20/54:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, 'html.parser')\n",
      "    next_btn = soup.find(class_=\"next\")\n",
      "    return next_btn.find('a').attrs['href'] if next_btn else None\n",
      "\n",
      "page_1 = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "print(\"The next page is: \" + str(check_next_page(page_1)))\n",
      "\n",
      "type(check_next_page(page_1))\n",
      "20/55:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, 'html.parser')\n",
      "    next_btn = soup.find(class_=\"next\")\n",
      "    return next_btn.find('a').attrs['href'] if next_btn else None\n",
      "\n",
      "page_1 = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "print(\"The next page is: \" + check_next_page(page_1))\n",
      "\n",
      "type(check_next_page(page_1))\n",
      "20/56: check_next_page(page_50)\n",
      "20/57: check_next_page(page_50)\n",
      "20/58:\n",
      "check_next_page(page_50)\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "def next_page_url(next_btn):\n",
      "    if next_btn != None:\n",
      "        page_url = base + next_btn\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "20/59:\n",
      "check_next_page(https://books.toscrape.com/catalogue/page-1.html)\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "def next_page_url(next_btn):\n",
      "    if next_btn != None:\n",
      "        page_url = base + next_btn\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "  \n",
      "next_page_url\n",
      "20/60:\n",
      "check_next_page(\"https://books.toscrape.com/catalogue/page-1.html\")\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "def next_page_url(next_btn):\n",
      "    if next_btn != None:\n",
      "        page_url = base + next_btn\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "  \n",
      "next_page_url()\n",
      "20/61:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "def next_page_url(check_next_page(url)):\n",
      "    if next_btn != None:\n",
      "        page_url = base + next_btn\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "  \n",
      "next_page_url()\n",
      "20/62:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "check_next_page(url)\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "def next_page_url(check_next_page(url)):\n",
      "    if next_btn != None:\n",
      "        page_url = base + next_btn\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "20/63:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "check_next_page(url)\n",
      "20/64:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "link = check_next_page(url)\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "\n",
      "def next_page_url(link):\n",
      "    if link != None:\n",
      "        page_url = base + link\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "        \n",
      "next_page_url(link)\n",
      "20/65:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "link = check_next_page(url)\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "\n",
      "def next_page_url(link):\n",
      "    if link != None:\n",
      "        return page_url = base + link\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "        \n",
      "next_page_url(link)\n",
      "20/66:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "link = check_next_page(url)\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "\n",
      "def next_page_url(link):\n",
      "    if link != None:\n",
      "        page_url = base + link\n",
      "        print(page_url)\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "        \n",
      "next_page_url(link)\n",
      "20/67:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "link = check_next_page(url)\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "\n",
      "def next_page_url(link):\n",
      "    if link != None:\n",
      "        page_url = base + link\n",
      "        print(\"Next page link:\" + page_url)\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "        \n",
      "next_page_url(link)\n",
      "20/68:\n",
      "url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "link = check_next_page(url)\n",
      "\n",
      "base = \"https://books.toscrape.com/catalogue/\"\n",
      "\n",
      "def next_page_url(link):\n",
      "    if link != None:\n",
      "        page_url = base + link\n",
      "        print(\"Next page link: \" + page_url)\n",
      "    else:\n",
      "        print(\"This is already the last page!\")\n",
      "        \n",
      "next_page_url(link)\n",
      "23/1:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "joke_request = response.json() \n",
      "print(joke_request)\n",
      "23/2:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "#get the resquests using the url using the header\n",
      "joke_request = response.json() #convert data into json file\n",
      "print(joke_request)\n",
      "23/3:\n",
      "import requests\n",
      "import time\n",
      "\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "\n",
      "i = 0\n",
      "while i<10:\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "    #get the resquests using the url using the header\n",
      "    joke_request = response.json() #convert data into json file\n",
      "    print(i)\n",
      "    print(joke_request [\"id\"])\n",
      "    print(joke_request [\"joke\"])\n",
      "    i = i+1\n",
      "    time.sleep(2)\n",
      "23/4:\n",
      "# Question 2 \n",
      "import time\n",
      "\n",
      "i = 1\n",
      "while i <= 3:\n",
      "    url = 'https://www.reddit.com/r/marketing/about/.json'\n",
      "    headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "    response = requests.get(url, headers=headers)\n",
      "    json_response = response.json()\n",
      "    \n",
      "    print(json_response['data']['active_user_count'])\n",
      "    i += 1\n",
      "    time.sleep(5)\n",
      "24/1:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "#get the resquests using the url using the header\n",
      "joke_request = response.json() #convert data into json file\n",
      "print(joke_request)\n",
      "24/2:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "#get the resquests using the url using the header\n",
      "joke_request = response.json() #convert data into json file\n",
      "print(joke_request)\n",
      "24/3:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "#get the resquests using the url using the header\n",
      "joke_request = response.json() #convert data into json file\n",
      "print(joke_request)\n",
      "24/4:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "\n",
      "print(response)\n",
      "24/5:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "\n",
      "print(response.url)\n",
      "24/6:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "\n",
      "response\n",
      "24/7:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "\n",
      "type(response)\n",
      "24/8:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "\n",
      "view(response)\n",
      "24/9:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "\n",
      "dir(response)\n",
      "24/10:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "response.url\n",
      "24/11:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "response.text\n",
      "24/12:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "response.text\n",
      "24/13:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "BeautifulSoup(response.text)\n",
      "24/14:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "BeautifulSoup(response)\n",
      "24/15:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "dir(response)\n",
      "24/16:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "response.json\n",
      "24/17:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "dir(response.json)\n",
      "24/18:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "response.json()\n",
      "24/19:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "response.text()\n",
      "24/20:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "response.json()\n",
      "24/21:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "24/22: dir(response)\n",
      "24/23: response.links()\n",
      "24/24: response.links\n",
      "24/25: print(response.links)\n",
      "24/26: print(response.raw)\n",
      "24/27: dir(response.raw)\n",
      "24/28: dir(response.raw)\n",
      "24/29: a = ['head', 'tail']\n",
      "24/30:\n",
      "a = ['head', 'tail']\n",
      "a.head\n",
      "24/31: dir(response.__attrs__)\n",
      "24/32: type(response)\n",
      "24/33: ?requests.models.Response\n",
      "24/34: help(requests.models.Response)\n",
      "24/35: dir(response)\n",
      "24/36:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "response.url\n",
      "24/37:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "24/38:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request\n",
      "24/39:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "print(joke_request)\n",
      "24/40:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request.results\n",
      "24/41:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request[results]\n",
      "24/42:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "24/43:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "type(joke_request)\n",
      "24/44:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "dir(joke_request)\n",
      "24/45: ?dir\n",
      "24/46: help(dir)\n",
      "24/47:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "dir(joke_request)\n",
      "joke_request.class\n",
      "24/48:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "dir(joke_request)\n",
      "24/49:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "joke_request.get()\n",
      "24/50:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "joke_request.get\n",
      "24/51:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "joke_request.get('results')\n",
      "24/52:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in joke_request.get('results'):\n",
      "    print joke.get['joke']\n",
      "24/53:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in joke_request.get('results'):\n",
      "    print (joke.get['joke'])\n",
      "24/54:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in joke_request.get('results'):\n",
      "    print (joke.get('joke')\n",
      "24/55:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in joke_request.get('results'):\n",
      "    print (joke.get('joke'))\n",
      "24/56:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in joke_request.get('results'):\n",
      "    print (joke['joke'])\n",
      "24/57:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in joke_request['results']:\n",
      "    print (joke['joke'])\n",
      "24/58:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in range(5):\n",
      "    for joke in joke_request['results']:\n",
      "    print (joke['joke'])\n",
      "24/59:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in range(5):\n",
      "    for joke in joke_request['results']:\n",
      "    print (joke['joke'])\n",
      "24/60:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in range(5):\n",
      "    for joke in joke_request['results']:\n",
      "        print (joke['joke'])\n",
      "24/61:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "for joke in joke_request['results']:\n",
      "        if joke < 5:\n",
      "            print (joke['joke'])\n",
      "24/62:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "24/63:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results'](0:4)\n",
      "24/64:\n",
      "import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results'][0:4]\n",
      "24/65:\n",
      "##### import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results'][0:5]\n",
      "24/66:\n",
      "##### import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results'][0:5]\n",
      "for joke in joke_request['results'] [0:5]:\n",
      "        print (joke['joke'])\n",
      "24/67:\n",
      "##### import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"dog\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results'][0:5]\n",
      "for joke in joke_request['results'] [0:5]:\n",
      "        print (joke['joke'])\n",
      "24/68:\n",
      "##### import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"dog\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request['results'][0:5]\n",
      "for joke in joke_request['results'] [0:5]:\n",
      "        print (joke['joke'])\n",
      "24/69:\n",
      "##### import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"dog\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "24/70:\n",
      "##### import requests\n",
      "\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"dog\"}) #passing parameter term=cat to the url\n",
      "# response.url is now 'https://icanhazdadjoke.com/search?term=cat'\n",
      "\n",
      "joke_request = response.json()\n",
      "joke_request\n",
      "24/71:\n",
      "def find_joke(q):\n",
      "    response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": q})\n",
      "    joke_request = response.json()\n",
      "    print (\"There are\" + str(joke_request['total_jokes']) + \"jokes\")\n",
      "q = \"dog\"\n",
      "find_joke(q)\n",
      "24/72:\n",
      "def find_joke(q):\n",
      "    response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": q})\n",
      "    joke_request = response.json()\n",
      "    print (\"There are \" + str(joke_request['total_jokes']) + \" jokes\")\n",
      "q = \"dog\"\n",
      "find_joke(q)\n",
      "24/73:\n",
      "def find_joke(q):\n",
      "    response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": q})\n",
      "    joke_request = response.json()\n",
      "    print (\"There are \" + str(joke_request['total_jokes']) + \" jokes about\" + q)\n",
      "q = \"dog\"\n",
      "find_joke(q)\n",
      "24/74:\n",
      "def find_joke(q):\n",
      "    response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": q})\n",
      "    joke_request = response.json()\n",
      "    print (\"There are \" + str(joke_request['total_jokes']) + \" jokes about \" + q)\n",
      "q = \"dog\"\n",
      "find_joke(q)\n",
      "24/75:\n",
      "def find_joke(q):\n",
      "    response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": q})\n",
      "    joke_request = response.json()\n",
      "    print (\"There are \" + str(joke_request['total_jokes']) + \" jokes containing \" + q)\n",
      "q = \"dog\"\n",
      "find_joke(q)\n",
      "24/76:\n",
      "def find_joke(q):\n",
      "    response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": q})\n",
      "    joke_request = response.json()\n",
      "    print (f\"There are \" + str(joke_request['total_jokes']) + \" jokes containing \" + q)\n",
      "q = \"dog\"\n",
      "find_joke(q)\n",
      "24/77:\n",
      "def find_joke(q):\n",
      "    response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": q})\n",
      "    joke_request = response.json()\n",
      "    print (\"There are \" + str(joke_request['total_jokes']) + \" jokes containing \" + q)\n",
      "q = \"dog\"\n",
      "find_joke(q)\n",
      "24/78: help(requests.models.Response)\n",
      "24/79:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"})\n",
      "joke_request = response.json()\n",
      "24/80:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"})\n",
      "joke_request = response.json()\n",
      "joke_request\n",
      "24/81:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"})\n",
      "joke_request = response.json()\n",
      "joke_request['results']\n",
      "24/82:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"})\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = ''\n",
      "24/83:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"})\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = ''\n",
      "joke_request\n",
      "24/84:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"}) #return all jokes\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = ''\n",
      "joke_request.url\n",
      "24/85:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"}) #return all jokes\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = ''\n",
      "response.url\n",
      "24/86:\n",
      "search_url = \"https://icanhazdadjoke.com\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}) #return all jokes\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = ''\n",
      "response.url\n",
      "24/87:\n",
      "search_url = \"https://icanhazdadjoke.com\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}) #return all jokes\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = ''\n",
      "joke_request\n",
      "24/88:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "for limit in range (10, 31, 10)\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": limit}) #return all jokes with limits = 10, 20 or 30\n",
      "joke_request = response.json()\n",
      "print(f\"Limit {limit} gives {joke_request['total_pages']} pages\")\n",
      "24/89:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "for limit in range (10, 31, 10):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": limit}) #return all jokes with limits = 10, 20 or 30\n",
      "joke_request = response.json()\n",
      "print(f\"Limit {limit} gives {joke_request['total_pages']} pages\")\n",
      "24/90:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "for limit in range (10, 31, 10):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": limit}) #return all jokes with limits = 10, 20 or 30\n",
      "    joke_request = response.json()\n",
      "    print(f\"Limit {limit} gives {joke_request['total_pages']} pages\")\n",
      "24/91:\n",
      "response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\"}) #return all jokes with limits = 10, 20 or 30\n",
      "joke_request = response.json()\n",
      "24/92:\n",
      "response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\"}) #return all jokes with limits = 10, 20 or 30\n",
      "joke_request = response.json()\n",
      "joke_request\n",
      "24/93:\n",
      "response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": 5,\n",
      "                                   \"page\": 2})\n",
      "response.json()\n",
      "24/94:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "        response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "        total_pages = response.json()[\"total_pages\"]\n",
      "        for page in range (1, total_pages + 1):\n",
      "                response = requests.get(search_url, \n",
      "                                        headers={\"Accept\": \"application/json\"}, \n",
      "                                        params={\"term\": term,\n",
      "                                               \"page\": page})\n",
      "                joke_request = response.json()\n",
      "                for joke in joke_request[\"results\"]:\n",
      "                    joke_list.append = joke\n",
      "                \n",
      "    return joke_list\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "24/96:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "        response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "        total_pages = response.json()[\"total_pages\"]\n",
      "        for page in range (1, total_pages + 1):\n",
      "                response = requests.get(search_url, \n",
      "                                        headers={\"Accept\": \"application/json\"}, \n",
      "                                        params={\"term\": term,\n",
      "                                               \"page\": page})\n",
      "                joke_request = response.json()\n",
      "                for joke in joke_request[\"results\"]:\n",
      "                    joke_list.append = joke\n",
      "                \n",
      "    print(joke_list)\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "24/98:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "        response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "        total_pages = response.json()[\"total_pages\"]\n",
      "        for page in range (1, total_pages + 1):\n",
      "                response = requests.get(search_url, \n",
      "                                        headers={\"Accept\": \"application/json\"}, \n",
      "                                        params={\"term\": term,\n",
      "                                               \"page\": page})\n",
      "                joke_request = response.json()\n",
      "                for joke in joke_request[\"results\"]:\n",
      "                    joke_list.append(joke)\n",
      "                \n",
      "    print(joke_list)\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "24/100:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "        response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "        total_pages = response.json()[\"total_pages\"]\n",
      "        for page in range (1, total_pages + 1):\n",
      "                response = requests.get(search_url, \n",
      "                                        headers={\"Accept\": \"application/json\"}, \n",
      "                                        params={\"term\": term,\n",
      "                                               \"page\": page})\n",
      "                joke_request = response.json()\n",
      "                for joke in joke_request[\"results\"]:\n",
      "                    joke_list.append(joke)\n",
      "                \n",
      "        print(joke_list)\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "24/101:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "        response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "        total_pages = response.json()[\"total_pages\"]\n",
      "        for page in range (1, total_pages + 1):\n",
      "                response = requests.get(search_url, \n",
      "                                        headers={\"Accept\": \"application/json\"}, \n",
      "                                        params={\"term\": term,\n",
      "                                               \"page\": page})\n",
      "                joke_request = response.json()\n",
      "                for joke in joke_request[\"results\"]:\n",
      "                    joke_list.append(joke)\n",
      "                \n",
      "                print(joke_list)\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "24/102:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "        response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "        total_pages = response.json()[\"total_pages\"]\n",
      "        for page in range (1, total_pages + 1):\n",
      "                response = requests.get(search_url, \n",
      "                                        headers={\"Accept\": \"application/json\"}, \n",
      "                                        params={\"term\": term,\n",
      "                                               \"page\": page})\n",
      "                joke_request = response.json()\n",
      "                for joke in joke_request[\"results\"]:\n",
      "                    joke_list.append(joke)\n",
      "                print(joke_list)\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "24/103:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "        response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "        total_pages = response.json()[\"total_pages\"]\n",
      "        for page in range (1, total_pages + 1):\n",
      "                response = requests.get(search_url, \n",
      "                                        headers={\"Accept\": \"application/json\"}, \n",
      "                                        params={\"term\": term,\n",
      "                                               \"page\": page})\n",
      "                joke_request = response.json()\n",
      "                for joke in joke_request[\"results\"]:\n",
      "                    joke_list.append(joke)\n",
      "        print(joke_list)\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "24/104:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    total_pages = response.json()[\"total_pages\"]\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        for joke in joke_request[\"results\"]:\n",
      "            joke_list.append(joke)\n",
      "    print(joke_list)\n",
      "term = \"rain\"\n",
      "find_joke(term)\n",
      "26/1:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "joke_request = response.json() \n",
      "print(joke_request)\n",
      "26/2:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "joke_request = response.json()\n",
      "print(joke_request)\n",
      "26/3: # your answer goes here!\n",
      "26/4:\n",
      "# Question 1 \n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"dog\"})\n",
      "joke_request = response.json()\n",
      "print(f\"The number of dog jokes is: {joke_request['total_jokes']}\")\n",
      "26/5:\n",
      "# Question 2\n",
      "def find_jokes(term):\n",
      "    search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    joke_request = response.json()\n",
      "    num_results = joke_request['total_jokes']\n",
      "    return num_results\n",
      "\n",
      "find_jokes(\"some-searchterm-you-would-like-to-try-out\")\n",
      "26/6:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"})\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = '' # let's remove all jokes, and only look at the other attributes in the JSON response\n",
      "joke_request\n",
      "26/7:\n",
      "for limit in range(10, 31, 10):  # note that range(a, b) runs from a to b-1; so the last value is exclusive (so from 10 to 30 with steps of 10)\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": limit})\n",
      "    joke_request = response.json()\n",
      "    print(f\"Limit {limit} gives {joke_request['total_pages']} pages\")\n",
      "26/8:\n",
      "response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": 5,\n",
      "                                   \"page\": 2})\n",
      "response.json()\n",
      "26/9: # your answer goes here!\n",
      "26/10:\n",
      "def find_jokes(term):\n",
      "    search_url = \"https://icanhazdadjoke.com/search\"\n",
      "    page = 1\n",
      "    jokes = []\n",
      "\n",
      "    while True:  # alternatively you can also use a for-loop that goes from page 1 to total_results / 20 (rounded up)\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,  # optionally you can add \"limit\": 20 but that's already the default so it doesn't change anything\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        jokes.extend(joke_request['results'])\n",
      "        if joke_request['current_page'] <= joke_request['total_pages']:\n",
      "            page += 1\n",
      "        else: \n",
      "            return jokes\n",
      "\n",
      "output = find_jokes(\"cat\") # try running it with \"\", too!\n",
      "26/11: print(f\"You've collected {len(output)} jokes\")\n",
      "24/105:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    total_pages = response.json()[\"total_pages\"]\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        for joke in joke_request[\"results\"]:\n",
      "            joke_list.append(joke)\n",
      "    print(joke_list)\n",
      "term = \"cat\"\n",
      "find_joke(term)\n",
      "24/106:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    total_pages = response.json()[\"total_pages\"]\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        for joke in joke_request[\"results\"]:\n",
      "            joke_list.append(joke)\n",
      "    print(joke_list)\n",
      "term = \"cat\"\n",
      "len(find_joke(term))\n",
      "24/107:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    total_pages = response.json()[\"total_pages\"]\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        for joke in joke_request[\"results\"]:\n",
      "            joke_list.append(joke)\n",
      "    return joke_list\n",
      "term = \"cat\"\n",
      "len(find_joke(term))\n",
      "24/108:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    total_pages = response.json()[\"total_pages\"]\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        for joke in joke_request[\"results\"]:\n",
      "            joke_list.append(joke)\n",
      "    return joke_list\n",
      "term = \"dog\"\n",
      "len(find_joke(term))\n",
      "24/109:\n",
      "joke_list =[]\n",
      "def find_joke(term):\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    total_pages = response.json()[\"total_pages\"]\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        for joke in joke_request[\"results\"]:\n",
      "            joke_list.append(joke)\n",
      "    return joke_list\n",
      "term = \"rain\"\n",
      "len(find_joke(term))\n",
      "26/12:\n",
      "def find_jokes(term):\n",
      "    search_url = \"https://icanhazdadjoke.com/search\"\n",
      "    page = 1\n",
      "    jokes = []\n",
      "\n",
      "    while True:  # alternatively you can also use a for-loop that goes from page 1 to total_results / 20 (rounded up)\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,  # optionally you can add \"limit\": 20 but that's already the default so it doesn't change anything\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        jokes.extend(joke_request['results'])\n",
      "        if joke_request['current_page'] <= joke_request['total_pages']:\n",
      "            page += 1\n",
      "        else: \n",
      "            return jokes\n",
      "\n",
      "output = find_jokes(\"rain\") # try running it with \"\", too!\n",
      "26/13: print(f\"You've collected {len(output)} jokes\")\n",
      "28/1:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://github.com/search?q=open+education\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/2:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories?q=open+education\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "27/1:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "joke_request = response.json() \n",
      "print(joke_request)\n",
      "28/3:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"term\": \"open education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/4:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": \"open education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/5:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"term\": \"open education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/6:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": \"open education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/7:\n",
      "# request JSON output from github search API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": \"open education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/8:\n",
      "# request JSON output from github search API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": \"open+education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/9:\n",
      "# request JSON output from github search API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": \"open education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "28/10: dir(git_request)\n",
      "28/11: git_request['total_count']\n",
      "28/12: git_request['total_count']\n",
      "28/13: git_request['total_count']\n",
      "28/14:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {response_request['total_git']} respositories that contains the term {term})\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/15:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_git']} respositories that contains the term {term})\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/16:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_git']} respositories that contains the term {term}\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/17:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contains the term {term}\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/18:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term {term}\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/19:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/20:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term \"{term}\"\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/21:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*)\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/22:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\"\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/23:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*)\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/24:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/25:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"consumer behavior\")\n",
      "28/26: git_request\n",
      "28/27: git_request['items'] = ''\n",
      "28/28: git_request['items'] = ''\n",
      "28/29:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "28/30: git_request['items']\n",
      "28/31: git_request\n",
      "28/32:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "28/33: git_request\n",
      "28/34:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "28/35:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "print (f\"There are {git_request['total_count']} respositories\")\n",
      "28/36: git_request\n",
      "28/37: len(git_request['items'])\n",
      "28/38: git_request['items'] = \"\"\n",
      "28/39:\n",
      "git_request['items'] = \"\"\n",
      "git_request\n",
      "28/40:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "print (f\"There are {git_request['total_count']} respositories\")\n",
      "28/41:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"limit\": 50})\n",
      "git_request = response.json()\n",
      "print (f\"There are {git_request['total_count']} respositories\")\n",
      "28/42: len(git_request['items'])\n",
      "28/43:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"limit\": 10,\n",
      "                                 \"page\": 2})\n",
      "git_request = response.json()\n",
      "print (f\"There are {git_request['total_count']} respositories\")\n",
      "28/44: len(git_request['items'])\n",
      "28/45:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"page\": 2})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "28/46:\n",
      "# url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "28/47:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"python\")\n",
      "28/48:\n",
      "# url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "28/49: len(git_request['items'])\n",
      "28/50:\n",
      "total_page = git_request['total_count']/len(git_request['items']\n",
      "total_page\n",
      "28/51: total_page = git_request['total_count']/len(git_request['items']\n",
      "28/52: git_request['total_count']\n",
      "28/53: git_request['total_count']/(len(git_request['items']))\n",
      "28/54: total_page = git_request['total_count']/(len(git_request['items']))\n",
      "28/55:\n",
      "total_page = git_request['total_count']/(len(git_request['items']))\n",
      "total_page\n",
      "28/56:\n",
      "total_page = git_request['total_count']/(len(git_request['items']))\n",
      "math.ceil(total_page)\n",
      "28/57:\n",
      "import math\n",
      "total_page = git_request['total_count']/(len(git_request['items']))\n",
      "math.ceil(total_page)\n",
      "28/58:\n",
      "import math\n",
      "total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "28/59:\n",
      "import math\n",
      "total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "total_page\n",
      "28/60:\n",
      "import math\n",
      "\n",
      "for page in range(1, total_page + 1):\n",
      "\n",
      "repo_list =[]\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 100})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "28/61:\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "    for page in range (1, total_pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 100})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "28/62:\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 100})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "28/63:\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "28/64:\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/1:\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/2:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    total_page = math.ceil(git_request['total_count']/(len(git_request['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/3:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    total_page = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/4:\n",
      "#using parameter per_page to set the number of results for each request\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"page\": 2})\n",
      "git_request = response.json()\n",
      "git_request['items']\n",
      "30/5:\n",
      "#using parameter per_page to set the number of results for each request\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"page\": 2})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "30/6:\n",
      "#using parameter per_page to set the number of results for each request\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"page\": 2})\n",
      "git_request = response.json()\n",
      "git_request['items']\n",
      "30/7:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    total_page = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/8:\n",
      "#using parameter per_page to set the number of results for each request\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"page\": 2})\n",
      "git_request = response.json()\n",
      "git_request['items']\n",
      "30/9:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"python\")\n",
      "30/10:\n",
      "#using parameter per_page to set the number of results for each request\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"page\": 1})\n",
      "git_request = response.json()\n",
      "git_request['items']\n",
      "30/11:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    total_page = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/12:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    search_url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "\n",
      "    while True:\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 50,\n",
      "                                        \"page\": page})\n",
      "        git_request = response.json()\n",
      "        total_page = math.ceil(git_request['total_count']/50)\n",
      "        repo_list.extend(git_request['items'])\n",
      "        if page <= total_page:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "30/13:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    search_url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "\n",
      "    while True:\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 50,\n",
      "                                        \"page\": page})\n",
      "        repo_request = response.json()\n",
      "        total_page = math.ceil(repo_request['total_count']/50)\n",
      "        repo_list.extend(repo_request['items'])\n",
      "        if page <= total_page:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "30/14:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "\n",
      "    while True:\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 50,\n",
      "                                        \"page\": page})\n",
      "        repo_request = response.json()\n",
      "        total_page = math.ceil(repo_request['total_count']/50)\n",
      "        repo_list.extend(repo_request['items'])\n",
      "        if page <= total_page:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "30/15:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    total_page = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    for page in range (1, 50):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/16:\n",
      "#using parameter per_page to set the number of results for each request\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                 \"page\": 1})\n",
      "git_request = response.json()\n",
      "git_request['items']\n",
      "30/17:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    total_page = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "\n",
      "def find_repo(term):\n",
      "    total_page(term)\n",
      "    for page in range (1, total_page+1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/18:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "\n",
      "def find_repo(term):\n",
      "    total_page(term)\n",
      "    for page in range (1, pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/19:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "def find_repo(term):\n",
      "    total_page(term)\n",
      "    for page in range (1, pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/20:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "def find_repo(term):\n",
      "    total_page(term)\n",
      "    for page in range (1, pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "30/21:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "term = \"open education\"\n",
      "total_page(term)\n",
      "30/22:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "term = \"open education\"\n",
      "total_page(term)\n",
      "30/23:\n",
      "repo_list =[]\n",
      "def find_repo(term):\n",
      "    total_page(term)\n",
      "    for page in range (1, pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "30/24:\n",
      "repo_list =[]\n",
      "def find_repo(term):\n",
      "    total_page(term)\n",
      "    for page in range (1, pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "30/25:\n",
      "repo_list =[]\n",
      "def find_repo(term):\n",
      "    pages = total_page(term)\n",
      "    for page in range (1, pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "30/26:\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    for page in range (1, pages + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "pages = total_page(term)\n",
      "find_repo(term)\n",
      "30/27:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"open education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "git_request = response.json() \n",
      "while 'next' in response.links.keys():\n",
      "  response = requests.get(response.links['next']['url'],headers={\"Authorization\": git_token})\n",
      "  git_request.extend(response.json())\n",
      "30/28:\n",
      "git_token = \"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\"\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "res=requests.get(url,\n",
      "                 headers={\"Authorization\": git_token},\n",
      "                 params = {\"q\": \"open education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "repos=res.json()\n",
      "while 'next' in res.links.keys():\n",
      "  res=requests.get(res.links['next']['url'],headers={\"Authorization\": git_token})\n",
      "  repos.extend(res.json())\n",
      "30/29:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"open education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "git_request = response.json() \n",
      "git_request\n",
      "30/30:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"open education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "git_request = response.json() \n",
      "response.headers\n",
      "git_request\n",
      "30/31:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"open education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "git_request = response.json() \n",
      "response.headers\n",
      "30/32:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"open education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "git_request = response.json() \n",
      "response.headers['Link']\n",
      "30/33: type(response.headers['Link'])\n",
      "30/34:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"python\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "git_request = response.json() \n",
      "response.headers['Link']\n",
      "30/35:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"python\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":15})\n",
      "git_request = response.json() \n",
      "response.headers['Link']\n",
      "30/36:\n",
      "import requests\n",
      "git_token = \"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\"\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                     \"Authorization\": git_token},\n",
      "                       params = {\"q\": \"python\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":15})\n",
      "git_request = response.json() \n",
      "git_request\n",
      "33/1: runfile('D:/OneDrive/00_Tilburg/00_RM2021/Unit 1/oDCM/github-api/scraping/scap.py', wdir='D:/OneDrive/00_Tilburg/00_RM2021/Unit 1/oDCM/github-api/scraping')\n",
      "33/2: runfile('D:/OneDrive/00_Tilburg/00_RM2021/Unit 1/oDCM/github-api/scraping/scap.py', wdir='D:/OneDrive/00_Tilburg/00_RM2021/Unit 1/oDCM/github-api/scraping')\n",
      "35/1:\n",
      "# Run this code now\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find_all(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "35/2:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "soup.find_all(class_=\"product_pod\")[0].find(\"a\").attrs[\"href\"]\n",
      "35/3: # your answer goes here!\n",
      "35/4:\n",
      "# Question 1\n",
      "url_book = soup.find_all(class_=\"product_pod\")[1].find(\"a\").attrs[\"href\"]\n",
      "print(url_book)\n",
      "35/5:\n",
      "# Question 2 \n",
      "base_url = \"https://books.toscrape.com/catalogue/\" # gives a 403 error if you run the URL separately but works as expected once combined with the book url\n",
      "book_url = base_url + url_book[6:] # so we skip characters with index 0, 1, 2, 3, 4, 5: \"../../\"\n",
      "print(book_url)\n",
      "35/6:\n",
      "# Question 3\n",
      "base_url = \"https://books.toscrape.com/catalogue/\"\n",
      "book_url = base_url + url_book\n",
      "book_url = book_url.replace('../', '')\n",
      "print(book_url)\n",
      "35/7:\n",
      "# list of all books on the overview page\n",
      "books = soup.find_all(class_=\"product_pod\")\n",
      "book_urls = []\n",
      "\n",
      "for book in books: \n",
      "    book_url = book.find(\"a\").attrs[\"href\"]\n",
      "    book_urls.append(book_url)\n",
      "    \n",
      "# print the first five urls\n",
      "print(book_urls[0:5])\n",
      "35/8:\n",
      "book_list = []\n",
      "\n",
      "for book in books: \n",
      "    book_title = book.find(\"img\").attrs[\"alt\"] \n",
      "    book_url = book.find(\"a\").attrs[\"href\"]\n",
      "    book_list.append({'title': book_title,\n",
      "                      'url': book_url})\n",
      "35/9: next((book for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "35/10: # your answer goes here!\n",
      "35/11:\n",
      "# Question 1\n",
      "for id, book in enumerate(book_list):\n",
      "    book[\"full_url\"] = (base_url + book[\"url\"]).replace('../','')\n",
      "\n",
      "# show the first five elements\n",
      "book_list[0:5]\n",
      "35/12:\n",
      "# Question 2 \n",
      "next((book for book in book_list if book[\"title\"] == \"Black Dust\"), None)\n",
      "\n",
      "# it does not return any result because the book does not exist (this book is on shown on the 2nd page and we only scraped the first one!)\n",
      "35/13:\n",
      "counter = 1\n",
      "full_url = base_url + \"page-\" + str(counter) + \".html\" \n",
      "print(full_url)\n",
      "35/14:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = []\n",
      "\n",
      "for counter in range(1, 51):\n",
      "    full_url = base_url + \"page-\" + str(counter) + \".html\" \n",
      "    page_urls.append(full_url)\n",
      "35/15:\n",
      "# print the last five page urls (btw, run print(page_urls) for yourself to see all page URLs!)\n",
      "print(\"The number of page urls in the list is: \" + str(len(page_urls)))\n",
      "35/16: # your answer goes here!\n",
      "35/17:\n",
      "# Question 2\n",
      "base_url = \"https://quotes.toscrape.com/page/\"\n",
      "quote_page_urls = []\n",
      "\n",
      "for counter in range(1, 11):\n",
      "    full_url = base_url + str(counter)\n",
      "    quote_page_urls.append(full_url)\n",
      "\n",
      "print(quote_page_urls)\n",
      "35/18:\n",
      "# run this cell again to see the timer in action yourself!\n",
      "from time import sleep\n",
      "sleep(5)\n",
      "print(\"I'll be printed to the console after 5 seconds!\")\n",
      "35/19: # your answer goes here!\n",
      "35/20:\n",
      "sleep(2*60)\n",
      "print(\"Done!\")\n",
      "35/21:\n",
      "def generate_page_urls(base_url, num_pages):\n",
      "    '''generate a list of full page urls from a base url and counter that has takes on the values between 1 and num_pages'''\n",
      "    page_urls = []\n",
      "    \n",
      "    for counter in range(1, num_pages + 1):\n",
      "        full_url = base_url + \"page-\" + str(counter) + \".html\"\n",
      "        page_urls.append(full_url)\n",
      "        \n",
      "    return page_urls\n",
      "35/22: generate_page_urls(\"https://books.toscrape.com/catalogue/category/books_1/\", 5)\n",
      "35/23:\n",
      "def extract_book_urls(page_urls):\n",
      "    '''collect the book title and url for every book on all page urls'''\n",
      "    book_list = []\n",
      "    \n",
      "    # collect all books on page_url\n",
      "    for page_url in page_urls: \n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "        \n",
      "        # for each book on that page look up the title and url and store it in a list\n",
      "        for book in books: \n",
      "            book_title = book.find(\"img\").attrs[\"alt\"] \n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url}) \n",
      "            \n",
      "        sleep(1)  # pause 1 second after each request\n",
      "            \n",
      "    return book_list\n",
      "35/24:\n",
      "# this cell references functions in other cells, therefore make sure you have loaded all cells above first! (Cell > Run All Above)\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = generate_page_urls(base_url, 2) # to save time and resources we only scrape the first 2 pages\n",
      "book_list = extract_book_urls(page_urls)\n",
      "35/25:\n",
      "# Preview the results\n",
      "book_list[0:5]\n",
      "35/26: # Your answer goes here\n",
      "35/27:\n",
      "# Question 1\n",
      "\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = generate_page_urls(base_url, 5) \n",
      "book_list = extract_book_urls(page_urls)\n",
      "book_list\n",
      "35/28:\n",
      "# Question 2\n",
      "def extract_book_urls(page_urls):\n",
      "    '''collect the book title and url for every book on all page urls'''\n",
      "    book_list = []\n",
      "    \n",
      "    # this part is the same as above\n",
      "    for page_url in page_urls: \n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "\n",
      "        for book in books: \n",
      "            book_title = book.find(\"img\").attrs[\"alt\"] \n",
      "            book_url = (\"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"]).replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text # only this changed!\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url,\n",
      "                             \"instock\": book_instock}) # and this line!\n",
      "            \n",
      "        sleep(1)  \n",
      "            \n",
      "    return book_list\n",
      "35/29:\n",
      "# Question 3\n",
      "def extract_book_urls(page_urls):\n",
      "    '''collect the book title and url for every book on all page urls'''\n",
      "    book_list = []\n",
      "    \n",
      "    for page_url in page_urls: \n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "        books = soup.find_all(class_=\"product_pod\")\n",
      "\n",
      "        for book in books: \n",
      "            book_title = book.find(\"img\").attrs[\"alt\"] \n",
      "            book_url = \"https://books.toscrape.com/catalogue/\" + book.find(\"a\").attrs[\"href\"].replace('../','')\n",
      "            book_instock = book.find(\"p\", class_=\"instock availability\").text\n",
      "            \n",
      "            # addition to clean up the text (the rest remains the same!)\n",
      "            book_instock = book_instock.replace('\\n','').replace(' ','') # first replace a line-break (`\\n`) by an empty space, then replace a space (' ') by an empty space\n",
      "            \n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": book_url,\n",
      "                             \"instock\": book_instock})\n",
      "            \n",
      "        sleep(1) \n",
      "            \n",
      "    return book_list\n",
      "\n",
      "# test function!\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = generate_page_urls(base_url, 2) \n",
      "book_list = extract_book_urls(page_urls)\n",
      "book_list\n",
      "35/30:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_btn = soup.find(class_= \"next\") # observe the similarity with the code snippet used above\n",
      "    return next_btn.find(\"a\").attrs[\"href\"] if next_btn else None\n",
      "\n",
      "page_1 = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
      "print(\"The next page is: \" + check_next_page(page_1))\n",
      "35/31: # your answer goes here!\n",
      "35/32:\n",
      "# Question 1 \n",
      "output = check_next_page(\"https://books.toscrape.com/catalogue/page-50.html\")\n",
      "print(output) # the output is None because page 50 is the last one\n",
      "35/33:\n",
      "# Question 2 \n",
      "def next_page_url(url):\n",
      "    base_url = \"https://books.toscrape.com/catalogue/\"\n",
      "    if url != None: \n",
      "        page_url = base_url + url \n",
      "        return page_url \n",
      "    else: \n",
      "        print(\"This is already the last page!\")\n",
      "        \n",
      "next_page_url(check_next_page(\"https://books.toscrape.com/catalogue/page-50.html\"))\n",
      "35/34:\n",
      "def extract_all_books(page_url):\n",
      "    books = []\n",
      "    while page_url:\n",
      "        print(page_url)\n",
      "        for book in extract_book_urls([page_url]):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(page_url) != None: \n",
      "            page_url = \"https://books.toscrape.com/catalogue/category/books_1/\" + check_next_page(page_url)\n",
      "        else: \n",
      "            break\n",
      "        \n",
      "        # if \"page-4\" in page_url: break # (activate this if you don't want to run the entire loop)\n",
      "    return books\n",
      "35/35: book_list = extract_all_books(\"https://books.toscrape.com/catalogue/page-1.html\")\n",
      "35/36: book_list\n",
      "35/37: # your answer goes here!\n",
      "35/38:\n",
      "# Question 1\n",
      "# There are 1000 books \n",
      "\n",
      "len(books)\n",
      "\n",
      "#That's 50 pages into 20 products, which matches our expectations.\n",
      "35/39: books[0]\n",
      "35/40:\n",
      "# Question 2\n",
      "# we use one of the code snippets from above to search for the title\n",
      "\n",
      "next((book for book in book_list if book[\"title\"] == 'The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics'), None)\n",
      "\n",
      "# we can view the URL and open it in the browser.\n",
      "35/41:\n",
      "# Question 3\n",
      "books_instock = [book for book in book_list if book[\"instock\"] == \"Instock\"]\n",
      "len(books_instock)\n",
      "\n",
      "# All books are in stock!\n",
      "35/42:\n",
      "# Question 4\n",
      "len([book for book in book_list if \"boat\" in book[\"title\"].lower()])\n",
      "\n",
      "# here, we're checking for the appearence of the word \"boat\" in the title.\n",
      "35/43: [book for book in book_list if book[\"title\"] == \"Black Dust\"]\n",
      "35/44:\n",
      "res = requests.get('https://books.toscrape.com/catalogue/black-dust_976/index.html')\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "len(soup.find(id=\"content_inner\").find(\"p\", class_ = \"star-rating\").find_all(class_ = \"icon-star\"))\n",
      "35/45: soup\n",
      "35/46: soup.find(id=\"content_inner\")\n",
      "35/47: soup.find(id=\"content_inner\").find_all(\"p\")\n",
      "35/48: soup.find(id=\"content_inner\").find_all(\"p\", class_ = \"star-rating\")\n",
      "35/49: soup.find(id=\"content_inner\").find(\"p\", class_ = \"star-rating\")\n",
      "35/50: soup.find(id=\"content_inner\").find(\"p\", class_ = \"star-rating\").find_all(class_ = \"icon-star\")\n",
      "35/51: len(soup.find(id=\"content_inner\").find(\"p\", class_ = \"star-rating\").find_all(class_ = \"icon-star\"))\n",
      "35/52: # your answer goes here!\n",
      "35/53:\n",
      "# Question 1\n",
      "def get_book_description(books):\n",
      "    book_descriptions = []\n",
      "    \n",
      "    for book in books: \n",
      "        page_url = book[\"url\"]\n",
      "\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        # tip: look at the Google Inspector screenshot below \n",
      "        description = soup.find(id=\"content_inner\").find_all(\"p\")[3].get_text()\n",
      "        title = soup.find(id=\"content_inner\").find('img')['alt']\n",
      "        book_descriptions.append({'url': page_url,\n",
      "                                  'title': title,\n",
      "                                  'description': description})\n",
      "    return book_descriptions\n",
      "\n",
      "book_descriptions = get_book_description(book_list[0:5])\n",
      "book_descriptions\n",
      "\n",
      "# Question 2\n",
      "# tÃ©gÃ© (or similarly encoded strings) are characters from languages other than English, which use an extended character space.\n",
      "35/54:\n",
      "from datetime import datetime\n",
      "\n",
      "now = datetime.now()\n",
      "print(now)\n",
      "35/55:\n",
      "import csv \n",
      "\n",
      "with open(\"book_descriptions.csv\", \"w\") as csv_file: # <<- this is the line with the \"flag\"l see exercises below\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"description\", \"date_time\"])\n",
      "    now = datetime.now()\n",
      "    for book in book_descriptions: # here we reference the book_descriptions list - make sure it's loaded otherwise you get an error! (Cell > Run All Above)\n",
      "        writer.writerow([book['title'], book['description'], now])\n",
      "print('done!')\n",
      "36/1:\n",
      "# request JSON output from github search API\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": \"open education\"})\n",
      "git_request = response.json() \n",
      "print(git_request)\n",
      "36/2:\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\",                                     },\n",
      "                       params = {\"q\": \"python\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":15})\n",
      "git_request = response.json() \n",
      "git_request\n",
      "36/3:\n",
      "import requests\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\",                                     },\n",
      "                       params = {\"q\": \"python\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "git_request = response.json() \n",
      "git_request\n",
      "36/4:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"python\")\n",
      "36/5:\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "36/6:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} r\"espositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "36/7:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "36/8:\n",
      "# Function to calculate the total pages\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "term = \"open education\"\n",
      "total_page(term)\n",
      "36/9:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "36/10:\n",
      "# Function to calculate the total pages\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "term = \"open education\"\n",
      "total_page(term)\n",
      "36/11:\n",
      "# Building a function to request more pages but didn't succeed yet\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "\n",
      "    while True: #while page <= total page, keep requesting and adding repository data to repo_list\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 50,\n",
      "                                        \"page\": page}) #setting 50 items per page\n",
      "        repo_request = response.json()\n",
      "        total_page = math.ceil(repo_request['total_count']/50) #total pages = total items/items per page\n",
      "        repo_list.extend(repo_request['items'])\n",
      "        if page <= total_page:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "36/12:\n",
      "# Tried with personal authentication token but it didn't work (limited results to only 1000 first items)\n",
      "git_token = \"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\" \n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "res=requests.get(url,\n",
      "                 headers={\"Authorization\": git_token},\n",
      "                 params = {\"q\": \"open education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":1})\n",
      "repos=res.json()\n",
      "while 'next' in res.links.keys():\n",
      "  res=requests.get(res.links['next']['url'],headers={\"Authorization\": git_token})\n",
      "  repos.extend(res.json())\n",
      "36/13:\n",
      "# Tried with personal authentication token but it didn't work (limited results to only 1000 first items)\n",
      "git_token = \"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\" \n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "res=requests.get(url,\n",
      "                 headers={\"Authorization\": git_token},\n",
      "                 params = {\"q\": \"python\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":15})\n",
      "repos=res.json()\n",
      "36/14:\n",
      "# Tried with personal authentication token but it didn't work (limited results to only 1000 first items)\n",
      "git_token = \"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\" \n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "res=requests.get(url,\n",
      "                 headers={\"Authorization\": git_token},\n",
      "                 params = {\"q\": \"python\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\":15})\n",
      "repos=res.json()\n",
      "38/1:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "38/2:\n",
      "# Function to calculate the total pages\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "term = \"open education\"\n",
      "total_page(term)\n",
      "38/3:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "\n",
      "git_request.links\n",
      "38/4: git_request['headers']\n",
      "38/5:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    print (f\"There are {git_request['total_count']} respositories that contain the term *{term}*\")\n",
      "\n",
      "find_repo(\"open education\")\n",
      "38/6: git_request\n",
      "38/7:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    returns (git_request)\n",
      "   \n",
      "find_repo(\"open education\")\n",
      "38/8:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    return git_request\n",
      "   \n",
      "find_repo(\"open education\")\n",
      "38/9:\n",
      "# Building a function to request more pages but didn't succeed yet\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "\n",
      "    while True: #while page <= total page, keep requesting and adding repository data to repo_list\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 50,\n",
      "                                        \"page\": page}) #setting 50 items per page\n",
      "        repo_request = response.json()\n",
      "        total_page = math.ceil(repo_request['total_count']/50) #total pages = total items/items per page\n",
      "        repo_list.extend(repo_request['items'])\n",
      "        if page <= total_page:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "38/10: git_request['headers']\n",
      "38/11: git_request\n",
      "38/12:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    return git_request\n",
      "   \n",
      "find_repo(\"open education\")\n",
      "38/13: git_request[\"Headers\"]\n",
      "38/14: git_request\n",
      "38/15:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    return git_request\n",
      "   \n",
      "git_request = find_repo(\"open education\")\n",
      "38/16:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    return git_request\n",
      "   \n",
      "git_request = find_repo(\"open education\")\n",
      "38/17: git_request\n",
      "38/18:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    git_request = find_repo(\"open education\")\n",
      "   \n",
      "git_request\n",
      "38/19:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "       \n",
      "git_request\n",
      "38/20:\n",
      "# Building a function to request more pages but didn't succeed yet\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "\n",
      "    while True: #while page <= total page, keep requesting and adding repository data to repo_list\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 50,\n",
      "                                        \"page\": page}) #setting 50 items per page\n",
      "        repo_request = response.json()\n",
      "        total_page = math.ceil(repo_request['total_count']/50) #total pages = total items/items per page\n",
      "        repo_list.extend(repo_request['items'])\n",
      "        if page <= total_page:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "\n",
      "find_repo(\"open education\")\n",
      "41/1:\n",
      "# Make selenium and chromedriver work for github.com\n",
      "# install also - pip install selenium , pip install webdriver_manager\n",
      "\n",
      "import selenium.webdriver\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.chrome.options import Options\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "\n",
      "#driver = webdriver.Chrome()\n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "base_url = \"https://github.com/search?\"\n",
      "driver.get(base_url)\n",
      "43/1: ?HTMLParser\n",
      "43/2: ??HTMLParser\n",
      "43/3:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, params = {\"q\": term, \"per_page\":100, \"page\": 1})\n",
      "soup = BeautifulSoup(response.text, \"html.parser\")\n",
      "response=soup.find_all(class_=\"repo-list-item hx_hit-repo d-flex flex-justify-start py-4 public source\")\n",
      "43/4:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, params = {\"q\": \"education\", \"per_page\":100, \"page\": 1})\n",
      "soup = BeautifulSoup(response.text, \"html.parser\")\n",
      "response=soup.find_all(class_=\"repo-list-item hx_hit-repo d-flex flex-justify-start py-4 public source\")\n",
      "43/5: response\n",
      "43/6: soup\n",
      "43/7:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://github.com/search/repositories\"\n",
      "response = requests.get(url, params = {\"q\": \"education\", \"per_page\":100, \"page\": 1})\n",
      "soup = BeautifulSoup(response.text, \"html.parser\")\n",
      "response=soup.find_all(class_=\"repo-list-item hx_hit-repo d-flex flex-justify-start py-4 public source\")\n",
      "43/8: soup\n",
      "43/9: response\n",
      "43/10: soup\n",
      "43/11:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://github.com/search\"\n",
      "response = requests.get(url, params = {\"q\": \"education\", \"per_page\":100, \"page\": 1, \"type\": Repositories})\n",
      "soup = BeautifulSoup(response.text, \"html.parser\")\n",
      "response=soup.find_all(class_=\"repo-list-item hx_hit-repo d-flex flex-justify-start py-4 public source\")\n",
      "43/12:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"https://github.com/search\"\n",
      "response = requests.get(url, params = {\"q\": \"education\", \n",
      "                                       \"per_page\":100, \n",
      "                                       \"page\": 1, \n",
      "                                       \"type\": \"Repositories\"})\n",
      "soup = BeautifulSoup(response.text, \"html.parser\")\n",
      "response=soup.find_all(class_=\"repo-list-item hx_hit-repo d-flex flex-justify-start py-4 public source\")\n",
      "43/13: response\n",
      "43/14: soup\n",
      "45/1:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    total_items = git_request['total_count']\n",
      "    return total_items\n",
      "\n",
      "find_repo(\"open education\")\n",
      "45/2:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    total_pages = math.ceil(git_request['total_count'])/len(git_request['items'])\n",
      "    return total_pages\n",
      "\n",
      "find_repo(\"open education\")\n",
      "45/3:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    total_pages = math.ceil((git_request['total_count'])/len(git_request['items']))\n",
      "    return total_pages\n",
      "\n",
      "find_repo(\"open education\")\n",
      "45/4:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    total_pages = math.ceil((git_request['total_count'])/len(git_request['items']))\n",
      "    return total_pages\n",
      "\n",
      "find_repo(\"python\")\n",
      "45/5: find_repo(\"python\")\n",
      "45/6:\n",
      "# Creating a function to search for repositories with a given keyword\n",
      "# per_page: extract 100 items each page\n",
      "import requests\n",
      "import math\n",
      "\n",
      "def total_pages(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term,\n",
      "                                \"per_page\":100})\n",
      "    git_request = response.json()\n",
      "    total_pages = math.ceil((git_request['total_count'])/len(git_request['items']))\n",
      "    return total_pages\n",
      "45/7: total_pages(\"python\")\n",
      "45/8:\n",
      "def find_repo(term):\n",
      "    no_of_pages = total_pages(term)\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "    while True:\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 100,\n",
      "                                        \"page\": page})\n",
      "        repo_request = response.json()\n",
      "        repo_list.extend(repo_request['items'])\n",
      "        if page < no_of_pages:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "45/9:\n",
      "def find_repo(term):\n",
      "    no_of_pages = total_pages(term)\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    page = 1\n",
      "    repo_list =[]\n",
      "    while True:\n",
      "        response = requests.get(url,\n",
      "                                headers={\"Accept\": \"application/json\"},\n",
      "                                params={\"q\": term,\n",
      "                                        \"per_page\": 100,\n",
      "                                        \"page\": page})\n",
      "        repo_request = response.json()\n",
      "        repo_list.extend(repo_request['items'])\n",
      "        if page < no_of_pages:\n",
      "            page += 1\n",
      "        else: \n",
      "            return repo_list\n",
      "\n",
      "find_repo(\"education\")\n",
      "45/10:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "page = 1\n",
      "repo_list =[]\n",
      "while True:\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": term,\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": page})\n",
      "    repo_request = response.json()\n",
      "    repo_list.extend(repo_request['items'])\n",
      "    if page < 50:\n",
      "        page += 1\n",
      "    else: \n",
      "        return repo_list\n",
      "45/11:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "page = 1\n",
      "repo_list =[]\n",
      "while True:\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": term,\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": page})\n",
      "    repo_request = response.json()\n",
      "    repo_list.extend(repo_request['items'])\n",
      "    if page < 50:\n",
      "        page += 1\n",
      "    else: \n",
      "        repo_list\n",
      "45/12:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "page = 1\n",
      "repo_list =[]\n",
      "while True:\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": education,\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": page})\n",
      "    repo_request = response.json()\n",
      "    repo_list.extend(repo_request['items'])\n",
      "    if page < 50:\n",
      "        page += 1\n",
      "    else: \n",
      "        repo_list\n",
      "45/13:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "page = 1\n",
      "repo_list =[]\n",
      "while True:\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": \"education\",\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": page})\n",
      "    repo_request = response.json()\n",
      "    repo_list.extend(repo_request['items'])\n",
      "    if page < 50:\n",
      "        page += 1\n",
      "    else: \n",
      "        repo_list\n",
      "45/14:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url,\n",
      "                        headers={\"Accept\": \"application/json\"},\n",
      "                        params={\"q\": \"education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\": 1})\n",
      "repo_request = response.json()\n",
      "repo_request['items']\n",
      "45/15:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "repo_list[]\n",
      "i = 1\n",
      "for i in range (1: (no_of_pages + 1)):\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": \"education\",\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": i})\n",
      "    repo_request = response.json()\n",
      "    repo_list.append(repo_request['items'])\n",
      "45/16:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "repo_list = []\n",
      "i = 1\n",
      "for i in range (1: (no_of_pages + 1)):\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": \"education\",\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": i})\n",
      "    repo_request = response.json()\n",
      "    repo_list.append(repo_request['items'])\n",
      "45/17:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "repo_list = []\n",
      "i = 1\n",
      "for i in range (1, (no_of_pages + 1)):\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": \"education\",\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": i})\n",
      "    repo_request = response.json()\n",
      "    repo_list.append(repo_request['items'])\n",
      "45/18:\n",
      "no_of_pages = total_pages(\"education\")\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "repo_list = []\n",
      "i = 1\n",
      "for i in range (1, 10):\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": \"education\",\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": i})\n",
      "    repo_request = response.json()\n",
      "    repo_list.append(repo_request['items'])\n",
      "45/19:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "repo_list = []\n",
      "i = 1\n",
      "for i in range (1, 10):\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": \"education\",\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": i})\n",
      "    repo_request = response.json()\n",
      "    repo_list.append(repo_request['items'])\n",
      "45/20:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "repo_list = []\n",
      "i = 1\n",
      "for i in range (1, 10):\n",
      "    response = requests.get(url,\n",
      "                            headers={\"Accept\": \"application/json\"},\n",
      "                            params={\"q\": \"education\",\n",
      "                                    \"per_page\": 100,\n",
      "                                    \"page\": i})\n",
      "    repo_request = response.json()\n",
      "    repo_list.append(repo_request['items'])\n",
      "    i+=1\n",
      "repo_list\n",
      "45/21:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url,\n",
      "                        headers={\"Accept\": \"application/json\"},\n",
      "                        params={\"q\": \"education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\": i})\n",
      "repo_request = response.json()\n",
      "repo_request['items']\n",
      "45/22:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url,\n",
      "                        headers={\"Accept\": \"application/json\"},\n",
      "                        params={\"q\": \"education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\": 1})\n",
      "repo_request = response.json()\n",
      "repo_request['items']\n",
      "45/23:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url,\n",
      "                        headers={\"Accept\": \"application/json\"},\n",
      "                        params={\"q\": \"education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\": 15})\n",
      "repo_request = response.json()\n",
      "repo_request['items']\n",
      "45/24:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url,\n",
      "                        headers={\"Accept\": \"application/json\"},\n",
      "                        params={\"q\": \"education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\": 10})\n",
      "repo_request = response.json()\n",
      "repo_request['items']\n",
      "45/25:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url,\n",
      "                        headers={\"Accept\": \"application/json\"},\n",
      "                        params={\"q\": \"education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\": 11})\n",
      "repo_request = response.json()\n",
      "repo_request['items']\n",
      "45/26:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url,\n",
      "                        headers={\"Accept\": \"application/json\"},\n",
      "                        params={\"q\": \"education\",\n",
      "                                \"per_page\": 100,\n",
      "                                \"page\": 11})\n",
      "repo_request = response.json()\n",
      "repo_request['items']\n",
      "\n",
      "total_pages(\"education\")\n",
      "45/27: total_pages(\"education\")\n",
      "45/28: total_pages(\"open education\")\n",
      "45/29: total_pages(\"python\")\n",
      "45/30:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": term})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/31:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/32:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "response.headers\n",
      "45/33:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "soup = BeautifulSoup(response.headers, \"html.parser\")\n",
      "45/34:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\"})\n",
      "git_request = response.json()\n",
      "\n",
      "response.headers\n",
      "45/35:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\";\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "\n",
      "response.headers\n",
      "45/36:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "\n",
      "response.headers\n",
      "45/37:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "\n",
      "response.headers\n",
      "45/38:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python\" + \"created: 2020-01-01T00:00:00..2021-01-01T00:00:00\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "45/39:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python\" + \"created: 2020-01-01T00:00:00..2021-01-01T00:00:00\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/40:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python\" + \"created: 2020-01-01..2021-01-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/41:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"cats created:2016-01-01T00:00:00Z..2017-01-01T00:00:00Z\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/42:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2016-01-01T00:00:00Z..2017-01-01T00:00:00Z\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/43:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2016-01-01T00:00:00Z..2016-01-08T00:00:00Z\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/44:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2016-01-01T00:00:00..2016-01-07T00:00:00\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/45:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2016-01-01T00:00:00..2016-01-05T00:00:00\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/46:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2020-01-01T00:00:00..2020-01-05T00:00:00\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/47:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2020-01-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/48:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2020-10-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/49:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2020-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/50:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2016-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/51:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2021-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/52:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2015-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/53:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2021-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/54:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2021-01-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/55:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education created:2021-01-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/56:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education created:2021-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/57:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"open education pushed:>2021-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/58:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python pushed:>2021-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/59:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"customer behavior pushed:>2021-02-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/60:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning created:>2021-01-01\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/61:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/62:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2021-01-01T00:00:00Z..2021-01-02T00:00:00Z\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/63:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2021-05-01T00:00:00Z..2021-05-02T00:00:00Z\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/64:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2021-05-01T00:00:00Z..2021-05-01T00:12:00Z\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/65:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"python created:2021-05-01T00:00:00Z..2021-05-01T12:00:00Z\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/66:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today()\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/67:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today()\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/68:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today()\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/69:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today()\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/70: ?datetime.today\n",
      "45/71: ?datetime\n",
      "45/72:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "45/73:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today()\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/74:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today()\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/75:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today()\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/76: datetime.today()\n",
      "45/77:\n",
      "datetime.today()\n",
      "until\n",
      "45/78:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime.today()\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)\n",
      "45/79: since\n",
      "45/80: until\n",
      "45/81: until < datetime.today()\n",
      "45/82:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/83:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {git_request[\"total_count\"]}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/84:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {git_request(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/85:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f'Repositories created between {since} and {until}: {git_request['total_count']}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/86:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/87:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request['total_count']\n",
      "45/88:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "print(f'Repositories created: {git_request['total_count']}')\n",
      "45/89:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    print(f\"Repositories created between {since} and {until}: {git_request['total_count']}\")\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/90:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    git_request['total_count']\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/91:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "count = ()\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    count.append(git_request['total_count']) \n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/92:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "count = []\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    count.append(git_request['total_count']) \n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/93:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"open education\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "count = []\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    \n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/94: git_request\n",
      "45/95:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"tilburguniversity\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "count = []\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    \n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/96: git_request\n",
      "45/97:\n",
      "# Function to calculate the total pages\n",
      "\n",
      "def total_page(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                            params = {\"q\": term})\n",
      "    pages = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    return pages\n",
      "\n",
      "term = \"open education\"\n",
      "total_page(term)\n",
      "45/98:\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "term = \"tilburguniversity\"\n",
      "url = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL'\n",
      "count = []\n",
      "while until < datetime.today():\n",
      "    day_url = url.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\", \n",
      "                                         \"Authorization\": \"token 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\"})\n",
      "    git_request = response.json()\n",
      "    git_request['total_count']\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/99:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "URL = f'https://api.github.com/search/repositories?q=open education created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=30)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/100:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"Tilburg University\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=3)  # Since 3 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/101:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=3)  # Since 3 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/102:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=3)  # Since 3 days ago\n",
      "until = since + timedelta(days=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/103:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 3 days ago\n",
      "until = since + timedelta(hour=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/104:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 3 days ago\n",
      "until = since + timedelta(hours=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/105:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=2)  # Since 3 days ago\n",
      "until = since + timedelta(hours=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/106:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(hours=24)  # Since 3 days ago\n",
      "until = since + timedelta(hours=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/107:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=24)  # Since 3 days ago\n",
      "until = since + timedelta(hours=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(days=1)\n",
      "45/108:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 3 days ago\n",
      "until = since + timedelta(hours=1)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=1)\n",
      "45/109:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 3 days ago\n",
      "until = since + timedelta(hours=12)   # Until 29 days ago \n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    r = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/110:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "45/111:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request\n",
      "45/112:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request(\"items\")\n",
      "45/113:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "git_request.get(\"items\")\n",
      "45/114:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    repo_list.append(repo_request.json().get(\"items\"))\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/115: len(repo_list)\n",
      "45/116: repo_list\n",
      "45/117:\n",
      "url = \"https://api.github.com/search/repositories\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                        params = {\"q\": \"machine learning\",\n",
      "                                  \"per_page\": 100})\n",
      "git_request = response.json()\n",
      "len(git_request.get(\"items\"))\n",
      "45/118: git_request.get(\"items\")\n",
      "45/119: git_request.get(\"items\")[1]\n",
      "45/120:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    repo_list.extend(repo_request.json().get(\"items\"))\n",
      "\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/121: repo_list\n",
      "45/122: len(repo_list)\n",
      "45/123:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    repo_list.extend(repo_request.json().get(\"items\"))\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/124: repo_request\n",
      "45/125: repo_request(\"items\")\n",
      "45/126: repo_request.json[\"items\"]\n",
      "45/127: repo_request.json().get(\"items\")\n",
      "45/128: len(repo_request.json().get(\"items\"))\n",
      "45/129:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    repo_list.extend(repo_request.json().get(\"items\"))\n",
      "    print(f'Repositories created between {since} and {until}: {r.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/130:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    repo_list.extend(repo_request.json().get(\"items\"))\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/131:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    repo_list.extend(repo_request.json().get(\"items\"))\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/132:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'No. of pages between {since} and {until}: {repo_request.json().get(\"total_count\")/100}')\n",
      "    \n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/133:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'No. of pages between {since} and {until}: {math.ceil(repo_request.json().get(\"total_count\")/100)}')\n",
      "    \n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/134:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/135: len(repo_list)\n",
      "45/136: repo_list\n",
      "45/137: repo_list[1]\n",
      "45/138: class(repo_list[1])\n",
      "45/139: len(repo_list[1])\n",
      "45/140: len(repo_list[15])\n",
      "45/141: len(repo_list)\n",
      "45/142: len(repo_list[2])\n",
      "45/143: len(repo_list[10])\n",
      "45/144: len(repo_list[11])\n",
      "45/145: len(repo_list[14])\n",
      "45/146: len(repo_list[15])\n",
      "45/147: len(repo_list[0])\n",
      "45/148: len(repo_list[15])\n",
      "45/149: len(repo_list[14])\n",
      "45/150:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/151:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"consumer\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/152: len(repo_list[14])\n",
      "45/153: len(repo_list)\n",
      "45/154: len(repo_list(1))\n",
      "45/155: len(repo_list[1])\n",
      "45/156: len(repo_list[2])\n",
      "45/157: len(repo_list[0])\n",
      "45/158:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/159: len(repo_list)\n",
      "45/160: len(repo_list[0])\n",
      "45/161: len(repo_list[1])\n",
      "45/162: len(repo_list[2])\n",
      "45/163:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "45/164: len(repo_list[7])\n",
      "45/165: len(repo_list[6])\n",
      "45/166: len(repo_list[14])\n",
      "48/1:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(months=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/2:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(months=2)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/3:\n",
      "import requests\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=2)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/4:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=2)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/5: len(repo_list)\n",
      "48/6: repo_list\n",
      "48/7: repo_list[1]\n",
      "48/8:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(days=2)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/9: len(repo_list)\n",
      "48/10: len(repo_list[1])\n",
      "48/11: repo_list[1]\n",
      "48/12:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - timedelta(years=1)  # Since 1 day ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/13:\n",
      "datetime.today()\n",
      "since = datetime.today() - timedelta(years=1)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)\n",
      "48/14:\n",
      "datetime.today()\n",
      "since = datetime.today() - timedelta(month=1)  # Since 30 days ago\n",
      "until = since + timedelta(days=1)\n",
      "48/15:\n",
      "from dateutil.relativedelta import relativedelta\n",
      "datetime.today()\n",
      "since = datetime.today() + relativedelta(months=-6)  # Since 6 months ago\n",
      "until = since + timedelta(days=1)\n",
      "48/16:\n",
      "from dateutil.relativedelta import relativedelta\n",
      "datetime.today()\n",
      "since = datetime.today() + relativedelta(months=-6)  # Since 6 months ago\n",
      "until = since + timedelta(days=1) \n",
      "since\n",
      "48/17:\n",
      "from dateutil.relativedelta import relativedelta\n",
      "datetime.today()\n",
      "since = datetime.today() + relativedelta(months=-6)  # Since 6 months ago\n",
      "until = since + timedelta(days=1) \n",
      "since\n",
      "until\n",
      "48/18:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(years=-1)  # Since 1 year ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/19:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Since 1 month ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/20:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Since 1 month ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/21:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Since 1 month ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/22:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Since 1 month ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/23:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Since 1 month ago\n",
      "until = since + timedelta(hours=12)   # Until half day ago \n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100)\n",
      "    for i in range(1, no_page + 1):\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.append(page_request.json().get(\"items\"))\n",
      "    # Update dates for the next search\n",
      "    since = until\n",
      "    until = since + timedelta(hours=12)\n",
      "48/24: len(repo_list[])\n",
      "48/25: len(repo_list())\n",
      "48/26: len(repo_list)\n",
      "48/27: repo_list\n",
      "48/28: repo_list[101]\n",
      "48/29: len(repo_list)\n",
      "48/30: repo_list[100]\n",
      "48/31: len(repo_list[1])\n",
      "48/32: len(repo_list[0])\n",
      "48/33:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "48/34:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "48/35: len(repo_list)\n",
      "48/36:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "48/37:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "48/38:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/1:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/2:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"python\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/3:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/4:\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/5:\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/6:\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/7:\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/8:\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/9:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(months=-1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/10: math.ceil(122/100)\n",
      "50/11:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(days=-1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/12: len(repo_list)\n",
      "50/13:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(days=-30)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/14:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() + relativedelta(days=-5)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/15: len(repo_list)\n",
      "50/16:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    return repo_list\n",
      "50/17: find_repo(\"machine learning\",2)\n",
      "50/18: len(find_repo(\"machine learning\",2))\n",
      "50/19:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"machine learning\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=2)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/20: repo_list = find_repo(\"education\",1))\n",
      "50/21: repo_list = find_repo(\"python\",1))\n",
      "50/22: repo_list = find_repo(\"python\",1)\n",
      "50/23: len(repo_list)\n",
      "50/24: repo_list[1]\n",
      "50/25: repo_list[1].get(\"id\")\n",
      "50/26: repo_list[1].get(\"id\",\"name\")\n",
      "50/27: repo_list[1].get(\"id\",\"name\")\n",
      "50/28: repo_list[1].get(\"id\").(\"name\")\n",
      "50/29: repo_list[1].get(\"id\").get(\"name\")\n",
      "50/30: repo_list[1].get(\"name\")\n",
      "50/31: repo_list[1]\n",
      "50/32: repo_list[1].items['id']\n",
      "50/33: repo_list[1].items('id')\n",
      "50/34:\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/35:\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\", default=0)}')\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/36:\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(months=1)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\", 0)}')\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/37:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=30)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\",0)/100) #calculating the total No. of pages\n",
      "    for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "        page_url = f'{day_url}&page={i}'\n",
      "        page_request = requests.get(page_url, headers=HEADERS)\n",
      "        #update list of repositories\n",
      "        repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/38:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=30)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\",0)/100) #calculating the total No. of pages\n",
      "    while no_page > 0:\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\",0)) #adding the fetched page to the list\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/39:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=30)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\",0)/100) #calculating the total No. of pages\n",
      "    if no_page > 0:\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    else:\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/40:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=30)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\",0)/100) #calculating the total No. of pages\n",
      "    if no_page > 0:\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    else:\n",
      "    # Update dates for the next search\n",
      "    since = until #move start-date and end-date up 12hours\n",
      "    until = since + timedelta(hours=12)\n",
      "50/41:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "\n",
      "term = \"education\"\n",
      "URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "since = datetime.today() - relativedelta(days=30)  # Start fetching repo created 1 month ago\n",
      "until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "repo_list = []\n",
      "\n",
      "while until < datetime.today():\n",
      "    day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "    repo_request = requests.get(day_url, headers=HEADERS)\n",
      "    print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "    no_page = math.ceil(repo_request.json().get(\"total_count\",0)/100) #calculating the total No. of pages\n",
      "    if no_page > 0:\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "    else:\n",
      "    # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "50/42: repo_list[1](1)\n",
      "50/43: repo_list[1]\n",
      "50/44: repo_list[1][1]\n",
      "50/45: repo_list[1].items(\"id\")\n",
      "50/46: repo_list[1].items(1)\n",
      "50/47: repo_list[1].items\n",
      "50/48: repo_list[1].items()\n",
      "50/49:\n",
      "col = ('id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count')\n",
      "col(1)\n",
      "50/50:\n",
      "col = ('id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count')\n",
      "col[0]\n",
      "50/51:\n",
      "col = ('id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count')\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    for col in col:\n",
      "        col = item.get(col)\n",
      "        repo.append(col)\n",
      "    dt.append(repo)\n",
      "dt\n",
      "50/52: type(col)\n",
      "50/53:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    for col in col:\n",
      "        col = item.get(col)\n",
      "        repo.append(col)\n",
      "    dt.append(repo)\n",
      "dt\n",
      "50/54: type(col)\n",
      "50/55:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": fork})\n",
      "dt[1]\n",
      "50/56:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt[1]\n",
      "50/57:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt[2]\n",
      "50/58:\n",
      "repo_list = find_repo(\"python\",1)\n",
      "len(repo_list)\n",
      "50/59:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "len(dt)\n",
      "50/60:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt[14]\n",
      "50/61:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt[100]\n",
      "50/62:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt[1000]\n",
      "50/63:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt[50]\n",
      "50/64:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt\n",
      "50/65:\n",
      "col = ['id', 'name', 'html_url', 'created_at', 'stargazers_count', 'watchers_count', 'language', 'forks_count']\n",
      "dt = []\n",
      "repo = []\n",
      "for item in repo_list:\n",
      "    id = item.get(\"id\")\n",
      "    name = item.get(\"name\")\n",
      "    url = item.get(\"html_url\")\n",
      "    created = item.get(\"created_at\")\n",
      "    stars = item.get(\"stargazers_count\")\n",
      "    watch = item.get(\"watchers_count\")\n",
      "    language = item.get(\"language\")\n",
      "    forks = item.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name, \n",
      "               \"url\": url, \n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt[0]\n",
      "50/66:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "    return dt\n",
      "50/67: dt = find_repo(\"python\",1)\n",
      "50/68: len(dt)\n",
      "50/69:\n",
      "#function to search for a specific \"term\" and fletch all repositories created in the last {day} days\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    return repo_list\n",
      "50/70:\n",
      "repo_list = find_repo(\"python\",1)\n",
      "len(repo_list)\n",
      "50/71:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "    return dt\n",
      "50/72: dt = find_repo(\"python\",1)\n",
      "50/73: len(dt)\n",
      "50/74: dt[1]\n",
      "50/75: datetime.today()\n",
      "50/76: datetime.date()\n",
      "50/77: datetime.datetime.now()\n",
      "50/78: datetime.now()\n",
      "50/79: datetime.now().date\n",
      "50/80: datetime.now().date()\n",
      "50/81: print(f\"today's date is: {datetime.now().date()}\")\n",
      "50/82:\n",
      "import csv\n",
      "\n",
      "with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "    for repo in dt:\n",
      "        writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "print('done!')\n",
      "50/83: pwd()\n",
      "50/84:\n",
      "import csv\n",
      "import pandas as pd   \n",
      "\n",
      "with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "    for repo in dt:\n",
      "        writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\")\n",
      "\n",
      "rep\n",
      "50/85:\n",
      "import csv\n",
      "import pandas as pd   \n",
      "\n",
      "with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "    for repo in dt:\n",
      "        writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= ’;’)\n",
      "\n",
      "rep\n",
      "50/86:\n",
      "import csv\n",
      "import pandas as pd   \n",
      "\n",
      "with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "    for repo in dt:\n",
      "        writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "\n",
      "rep\n",
      "50/87:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "50/88: find_repo(\"python\",1)\n",
      "55/1:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today:\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables into a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "55/2: find_repo(\"python\",1)\n",
      "58/1:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables into a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "58/2:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables into a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "58/3:\n",
      "#Give it a try:\n",
      "find_repo(\"education\",30)\n",
      "58/4:\n",
      "#Give it a try:\n",
      "find_repo(\"education\",1)\n",
      "58/5:\n",
      "#Give it a try:\n",
      "find_repo(\"education\",10)\n",
      "58/6:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables into a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "58/7:\n",
      "#Give it a try:\n",
      "find_repo(\"education\",5)\n",
      "58/8:\n",
      "#Give it a try:\n",
      "find_repo(\"machine learning\",5)\n",
      "58/9:\n",
      "#Give it a try:\n",
      "find_repo(\"python\",1)\n",
      "58/10:\n",
      "#Give it a try:\n",
      "find_repo(\"education\",10)\n",
      "58/11:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to fetch each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the fetched page to the list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables into a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "58/12:\n",
      "#Give it a try:\n",
      "find_repo(\"machine learning\",5)\n",
      "58/13:\n",
      "#Give it a try:\n",
      "find_repo(\"education\",30)\n",
      "58/14:\n",
      "def print_total_repo(term, day):\n",
      "    URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days=1)  # Start fetching repo created 1 month ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\", 0)}')\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "58/15: print_total_repo(\"education\",30)\n",
      "58/16:\n",
      "def print_total_repo(term, day):\n",
      "    URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created 1 month ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\", 0)}')\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "58/17: print_total_repo(\"education\",30)\n",
      "58/18:\n",
      "def print_total_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term} created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created 1 month ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\", 0)}')\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "58/19: print_total_repo(\"education\",30)\n",
      "58/20: print_total_repo(\"education\",5)\n",
      "58/21:\n",
      "#Give it a try:\n",
      "find_repo(\"python\",1)\n",
      "60/1:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created N days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the retrieved repositories to a list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables to a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "60/2: find_repo(\"education\",5)\n",
      "60/3: find_repo(\"education\",30)\n",
      "60/4:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created N days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            print(i)\n",
      "            print(requests.status_code)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the retrieved repositories to a list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables to a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "60/5:\n",
      "#Give it a try:\n",
      "find_repo(\"python\",1)\n",
      "60/6:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created N days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            print(i)\n",
      "            print(page_request.status_code)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the retrieved repositories to a list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables to a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "60/7:\n",
      "#Give it a try:\n",
      "find_repo(\"python\",1)\n",
      "60/8:\n",
      "#Function to fletch repositories by \"term\" within a certain number of \"days\" from today. \n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization': 'token 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'}\n",
      "\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created N days ago\n",
      "    until = since + timedelta(hours=12) # dividing the total No.of repo into segments of 12 hours each\n",
      "    repo_list = []\n",
      "    dt = []\n",
      "    repo = []\n",
      "\n",
      "    #Fletching repositories:\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        repo_request = requests.get(day_url, headers=HEADERS)\n",
      "        #print(f'Repositories created between {since} and {until}: {repo_request.json().get(\"total_count\")}')\n",
      "        no_page = math.ceil(repo_request.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_request = requests.get(page_url, headers=HEADERS)\n",
      "            print(i)\n",
      "            print(page_request.status_code)\n",
      "            if (page_request.status_code!=200): print(page_request.text)\n",
      "            #update list of repositories\n",
      "            repo_list.extend(page_request.json().get(\"items\")) #adding the retrieved repositories to a list\n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=12)\n",
      "    \n",
      "    #Saving relevant variables to a list:\n",
      "    for item in repo_list:\n",
      "        id = item.get(\"id\")\n",
      "        name = item.get(\"name\")\n",
      "        url = item.get(\"html_url\")\n",
      "        created = item.get(\"created_at\")\n",
      "        stars = item.get(\"stargazers_count\")\n",
      "        watch = item.get(\"watchers_count\")\n",
      "        language = item.get(\"language\")\n",
      "        forks = item.get(\"forks_count\")\n",
      "        dt.append({\"id\": id, \n",
      "                   \"name\": name, \n",
      "                   \"url\": url, \n",
      "                   \"created\": created,\n",
      "                   \"stars\": stars,\n",
      "                   \"watch\": watch,\n",
      "                   \"language\": language,\n",
      "                   \"forks\": forks})\n",
      "        \n",
      "    #Writing data into .csv file and returning the table:\n",
      "    with open(f\"{term}_{datetime.now().date()}.csv\", \"w\") as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "        for repo in dt:\n",
      "            writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks']])\n",
      "    rep = pd.read_csv(f\"{term}_{datetime.now().date()}.csv\", delimiter= \";\")\n",
      "    \n",
      "    return rep\n",
      "60/9:\n",
      "#Give it a try:\n",
      "find_repo(\"python\",1)\n",
      "63/1:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GithubToken']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    #HEADERS = {'Authorization': 'token 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'}\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 12 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        a=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{a}.json','a',encoding='utf-8')\n",
      "        \n",
      "        file_list.append(f'{term}_{a}.json')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "    # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=4)\n",
      "    return file_list\n",
      "63/2: find_repo(\"education\",2)\n",
      "63/3: os.environ\n",
      "63/4:\n",
      "import os\n",
      "os.environ\n",
      "64/1:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GithubToken']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    #HEADERS = {'Authorization': 'token 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'}\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 12 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        a=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{a}.json','a',encoding='utf-8')\n",
      "        \n",
      "        file_list.append(f'{term}_{a}.json')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "    # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=4)\n",
      "    return file_list\n",
      "64/2:\n",
      "import os\n",
      "os.environ\n",
      "64/3:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    #HEADERS = {'Authorization': 'token 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'}\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 12 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        a=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{a}.json','a',encoding='utf-8')\n",
      "        \n",
      "        file_list.append(f'{term}_{a}.json')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "    # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until = since + timedelta(hours=4)\n",
      "    return file_list\n",
      "64/4: find_repo(\"education\",2)\n",
      "64/5:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{fetch_time}.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        file_list.append(f'{term}_{fetch_time}.json')\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "    return file_list\n",
      "64/6:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{fetch_time}.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        file_list.append(f'{term}_{fetch_time}.json')\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "    return file_list\n",
      "64/7: file_list = find_repo(\"education\",2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/8: file_list\n",
      "66/1:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "66/2:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "66/3:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content['items']\n",
      "66/4:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "66/5:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "len(content)\n",
      "66/6:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[]('items')\n",
      "66/7:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "66/8:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[1]\n",
      "66/9:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content(0)\n",
      "66/10:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[0]\n",
      "66/11:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[0]['items']\n",
      "66/12:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[0]('items')\n",
      "66/13:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[0]['items']\n",
      "66/14:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[0].get('items')\n",
      "66/15:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content[0]\n",
      "66/16:\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "json.loads(content)\n",
      "66/17:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "json.loads(content)\n",
      "66/18:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "66/19:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "json.loads(content[1])\n",
      "66/20:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "json.loads(content[0])\n",
      "66/21:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "json.loads(content[0])('items')\n",
      "66/22:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "json.loads(content[0])['items']\n",
      "66/23:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "json.loads(f.readlines()[0])['items']\n",
      "66/24:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo_list.append(json.loads(f.readlines()[0])['items'])\n",
      "repo_list\n",
      "66/25: file_list = find_repo(\"education\",2)\n",
      "66/26:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{fetch_time}.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        file_list.append(f'{term}_{fetch_time}.json')\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "    return file_list\n",
      "66/27: file_list = find_repo(\"education\",2)\n",
      "66/28:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo_list.append(json.loads(f.readlines()[0])['items'])\n",
      "repo_list\n",
      "66/29:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "\n",
      "json.loads(f.readlines()[0])['items']\n",
      "66/30:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.append(repo)\n",
      "repo_list\n",
      "66/31: file_list\n",
      "66/32:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "66/33:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/34: file_list = find_repo(\"education\",1)\n",
      "66/35:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.append(repo)\n",
      "repo_list\n",
      "66/36: file_list = find_repo(\"education\",2)\n",
      "66/37:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.append(repo)\n",
      "repo_list\n",
      "66/38:\n",
      "import json\n",
      "f=open('education_2021-10-05 11_09_54.327837.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/39:\n",
      "import json\n",
      "f=open('education_2021-10-05 12_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/40:\n",
      "import json\n",
      "f=open('education_2021-10-06 20_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/41:\n",
      "import json\n",
      "f=open('education_2021-10-06 16_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/42:\n",
      "import json\n",
      "f=open('education_2021-10-06 12_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/43:\n",
      "import json\n",
      "f=open('education_2021-10-06 08_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/44:\n",
      "import json\n",
      "f=open('education_2021-10-06 04_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/45:\n",
      "import json\n",
      "f=open('education_2021-10-06 00_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/46:\n",
      "import json\n",
      "f=open('education_2021-10-05 20_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/47:\n",
      "import json\n",
      "f=open('education_2021-10-05 16_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/48:\n",
      "import json\n",
      "f=open('education_2021-10-05 12_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/49:\n",
      "import json\n",
      "f=open('education_2021-10-07 00_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/50:\n",
      "import json\n",
      "f=open('education_2021-10-07 04_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "66/51:\n",
      "import json\n",
      "f=open('education_2021-10-07 04_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])\n",
      "repo\n",
      "66/52:\n",
      "import json\n",
      "f=open('education_2021-10-07 08_33_45.030074.json','r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])\n",
      "repo\n",
      "66/53: file_list = find_repo(\"education\",1)\n",
      "66/54:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.append(repo)\n",
      "repo_list\n",
      "66/55:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.extend(repo)\n",
      "repo_list\n",
      "66/56:\n",
      "#for each json file it works . How will we create a loop for all json files ? \n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.extend(repo)\n",
      "len(repo_list)\n",
      "66/57:\n",
      "dt=[]\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name = repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "    dt.append({\"id\": id, \n",
      "               \"name\": name,\n",
      "               \"url\": url,\n",
      "               \"created\": created,\n",
      "               \"stars\": stars,\n",
      "               \"watch\": watch,\n",
      "               \"language\": language,\n",
      "               \"forks\": forks})\n",
      "dt\n",
      "66/58:\n",
      "#Sometimes this doesn't work because of rate-limit.\n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.extend(repo)\n",
      "repo_list\n",
      "66/59:\n",
      "#try the function\n",
      "file_list = find_repo(\"education\",3)\n",
      "66/60:\n",
      "#Sometimes this doesn't work because of rate-limit.\n",
      "repo_list = []\n",
      "for file in file_list:  \n",
      "    f=open(file,'r',encoding='utf-8')\n",
      "    repo = json.loads(f.readlines()[0])['items']\n",
      "    repo_list.extend(repo)\n",
      "repo_list\n",
      "66/61:\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{fetch_time}.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        file_list.append(f'{term}_{fetch_time}.json')\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        \n",
      "    return file_list #return the list of file names\n",
      "66/62:\n",
      "#try the function\n",
      "file_list = find_repo(\"education\",3)\n",
      "66/63:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(f'{term}_{fetch_time}.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        file_list.append(f'{term}_{fetch_time}.json')\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "        \n",
      "    return file_list #return the list of file names\n",
      "66/64:\n",
      "#try the function\n",
      "file_list = find_repo(\"education\",3)\n",
      "66/65: file_list[11]\n",
      "66/66: file_list\n",
      "66/67:\n",
      "f=open('education_2021-10-07 09_07_08.537482.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "66/68:\n",
      "f=open('education_2021-10-07 09_07_08.537482.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "con[1]\n",
      "66/69:\n",
      "f=open('education_2021-10-07 09_07_08.537482.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "con\n",
      "66/70:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    # clean file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        file_list.append(filename)\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "        \n",
      "    return file_list #return the list of file names\n",
      "66/71:\n",
      "#try the function\n",
      "file_list = find_repo(\"education\",3)\n",
      "66/72: file_list\n",
      "66/73:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json, 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "con\n",
      "66/74:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "con\n",
      "66/75:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "for item in con:\n",
      "    jsonobj = json.loads(item)\n",
      "    print(jsonobj['total_count'])\n",
      "66/76: con[11]\n",
      "66/77: con[11]['total_count']\n",
      "66/78:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "for item in con:\n",
      "    jsonobj = json.loads(item)\n",
      "    try:\n",
      "        jsonobj['total_count']\n",
      "    except:\n",
      "        next\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    print(jsonobj['total_count'])\n",
      "66/79:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "for item in con:\n",
      "    jsonobj = json.loads(item)\n",
      "    try:\n",
      "        print(jsonobj['total_count'])\n",
      "    except:\n",
      "        next\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    print(jsonobj['total_count'])\n",
      "66/80:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "for item in con:\n",
      "    try:\n",
      "        jsonobj = json.loads(item)\n",
      "        print(jsonobj['total_count'])\n",
      "    except:\n",
      "        next\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    print(jsonobj['total_count'])\n",
      "66/81:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "for item in con:\n",
      "    try:\n",
      "        jsonobj = json.loads(item)\n",
      "        print(jsonobj['total_count'])\n",
      "    except:\n",
      "        next\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    print(jsonobj['total_count'])\n",
      "66/82:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "cntr=0\n",
      "for item in con:\n",
      "    print(cntr)\n",
      "    try:\n",
      "        jsonobj = json.loads(item)\n",
      "        print(jsonobj['total_count'])\n",
      "    except:\n",
      "        next\n",
      "    cnt+=1\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    print(jsonobj['total_count'])\n",
      "66/83:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "cntr=0\n",
      "for item in con:\n",
      "    print(cntr)\n",
      "    try:\n",
      "        jsonobj = json.loads(item)\n",
      "        #print(jsonobj['total_count'])\n",
      "    except:\n",
      "        next\n",
      "    cnt+=1\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    #print(jsonobj['total_count'])\n",
      "66/84:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "cntr=0\n",
      "for item in con:\n",
      "    print(cntr)\n",
      "    try:\n",
      "        jsonobj = json.loads(item)\n",
      "        #print(jsonobj['total_count'])\n",
      "    except:\n",
      "        next\n",
      "    cntr+=1\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    #print(jsonobj['total_count'])\n",
      "66/85:\n",
      "f=open('education_3_2021-10-04 13_13_00.612737.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "cntr=0\n",
      "for item in con:\n",
      "    print(cntr)\n",
      "    try:\n",
      "        jsonobj = json.loads(item)\n",
      "        #print(jsonobj['total_count'])\n",
      "    except:\n",
      "        next\n",
      "    cntr+=1\n",
      "    # these lines will only be eecuted if the EXCEPT clause above didn't run\n",
      "    print(jsonobj['total_count'])\n",
      "69/1:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token = os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    # clean file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return file_list #return the list of file names\n",
      "69/2:\n",
      "#try the function\n",
      "file_list = find_repo(\"education\",3)\n",
      "69/3: file_list\n",
      "69/4:\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "69/5:\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "\n",
      "con\n",
      "69/6:\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "\n",
      "len(con)\n",
      "69/7:\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "\n",
      "con\n",
      "69/8:\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "\n",
      "con[0]\n",
      "69/9:\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "con=f.readlines()\n",
      "\n",
      "con[1]\n",
      "69/10:\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt=f.readlines()\n",
      "repo = json.loads(dt[0])\n",
      "69/11:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt=f.readlines()\n",
      "repo = json.loads(dt[0])\n",
      "69/12:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt=f.readlines()\n",
      "repo = json.loads(dt[0])\n",
      "repo\n",
      "69/13:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "69/14:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "repo\n",
      "69/15:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "repo = json.loads(f.readlines()[0])['items']\n",
      "len(repo)\n",
      "69/16:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = json.loads(f.readlines()[0])\n",
      "69/17:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = json.loads(f.readlines()[0])\n",
      "69/18:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = json.loads(f.readlines()[0])\n",
      "dt\n",
      "69/19:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = json.loads(f.readlines()[1])\n",
      "dt\n",
      "69/20:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = json.loads(f.readlines())\n",
      "\n",
      "dt\n",
      "69/21:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = json.loads(f.readlines()[0])\n",
      "len(repo)\n",
      "69/22:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[0])\n",
      "69/23:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[0])\n",
      "len(dt)\n",
      "69/24:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[0])\n",
      "dt\n",
      "69/25:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[0])\n",
      "dt[1]\n",
      "69/26:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[0])\n",
      "dt[2]\n",
      "69/27:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[0])\n",
      "len(dt)\n",
      "69/28:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[1])\n",
      "len(dt)\n",
      "69/29:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "\n",
      "len\n",
      "dt = json.loads(f.readlines()[4])\n",
      "len(dt)\n",
      "69/30:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = json.loads(f.readlines())\n",
      "len(dt)\n",
      "69/31:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "dt\n",
      "69/32:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "dt[1]\n",
      "69/33:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "dt[0]\n",
      "69/34:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "len(dt)\n",
      "69/35:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "repo=json.loads(dt)\n",
      "repo\n",
      "69/36:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "repo=json.loads(dt)\n",
      "repo\n",
      "dt\n",
      "69/37:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "\n",
      "dt\n",
      "69/38:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "repo = []\n",
      "for dt in dt:\n",
      "    repo.append(json.loads(dt)[\"items\"])\n",
      "repo\n",
      "69/39:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "dt[0]\n",
      "69/40:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "json.loads(dt[0])\n",
      "69/41:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "json.loads(dt[0])['items']\n",
      "69/42:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "repo = []\n",
      "for counter in range(len(dt))\n",
      "    repo.extend(json.loads(dt[counter])['items'])\n",
      "69/43:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "repo = []\n",
      "for counter in range(0,len(dt)+1)\n",
      "    repo.extend(json.loads(dt[counter])['items'])\n",
      "69/44:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "repo = []\n",
      "for dt in dt\n",
      "    repo.extend(json.loads(dt)['items'])\n",
      "69/45:\n",
      "import json\n",
      "f=open('education_3_2021-10-04 17_58_48.114423.json', 'r',encoding='utf-8')\n",
      "dt = f.readlines()\n",
      "repo = []\n",
      "for dt in dt:\n",
      "    repo.extend(json.loads(dt)['items'])\n",
      "72/1:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    # clean file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return file_list #return the list of file names\n",
      "72/2:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    # clean file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        #creating a list of json files'names\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return file_list #return the list of file names\n",
      "72/3:\n",
      "#try the function\n",
      "file_list = find_repo(\"education\",2)\n",
      "72/4: file_list\n",
      "72/5: filename\n",
      "72/6:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return filename\n",
      "72/7:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",2)\n",
      "72/8: filename\n",
      "72/9:\n",
      "#run the below \n",
      "#copy paste the json file below\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "73/1:\n",
      "#if there is an error-continue in the next line \n",
      "repo =[]\n",
      "import json \n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list=jsonobj[\"items\"]\n",
      "    \n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "        \n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "        repo.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "73/2: repo\n",
      "73/3:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return filename\n",
      "73/4:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "73/5: filename\n",
      "73/6:\n",
      "#run the below \n",
      "#copy paste the json file below\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "73/7:\n",
      "#if there is an error-continue in the next line \n",
      "repo =[]\n",
      "import json \n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list=jsonobj[\"items\"]\n",
      "    \n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "        \n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "        repo.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "73/8: repo\n",
      "73/9: len(repo)\n",
      "73/10:\n",
      "#Writing data into .csv file and returning the table:\n",
      "#error here\n",
      "import csv\n",
      "import pandas as pd \n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"repo.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\",\"readme\"])\n",
      "    for repo in repo:\n",
      "        writer.writerow([repo['id'], repo['name'], repo['url'], repo['language'], repo['created'], repo['stars'], repo['watch'], repo['forks'],repo['readme']])\n",
      "data = pd.read_csv(\"repo.csv\", delimiter= \";\")\n",
      "73/11: type(repo)\n",
      "73/12: repo\n",
      "73/13: len(content)\n",
      "73/14:\n",
      "#if there is an error-continue in the next line \n",
      "repo =[]\n",
      "repo_list = []\n",
      "import json \n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "        \n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "        \n",
      "        repo.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "73/15: repo_list\n",
      "73/16: len(repo_list)\n",
      "73/17: repo_list[0]\n",
      "73/18: content\n",
      "73/19: content[1]\n",
      "73/20: print(len(content))\n",
      "73/21: content[0]\n",
      "73/22: content[1]\n",
      "73/23: content[2]\n",
      "73/24: content[3]\n",
      "73/25: content[4]\n",
      "73/26: content[5]\n",
      "73/27:\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "73/28:\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "\n",
      "repo_list\n",
      "73/29:\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "\n",
      "len(repo_list)\n",
      "73/30:\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "\n",
      "repo_list\n",
      "73/31:\n",
      "#if there is an error-continue in the next line \n",
      "repo =[]\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    repo.extend({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "73/32:\n",
      "#if there is an error-continue in the next line \n",
      "repo =[]\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    repo.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "73/33:\n",
      "#taking list of repo\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "73/34:\n",
      "#taking list of repo\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "repo_list\n",
      "73/35:\n",
      "#taking list of repo\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "repo_list[0]\n",
      "73/36:\n",
      "repo =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    repo.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "73/37:\n",
      "repo =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    repo.extend([{\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme}])\n",
      "73/38:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "73/39: dt\n",
      "73/40: len(dt)\n",
      "73/41:\n",
      "#Writing data into .csv file and returning the table:\n",
      "#error here\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\",\"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([dt['id'], dt['name'], dt['url'], dt['language'], dt['created'], dt['stars'], dt['watch'], dt['forks'],dt['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "73/42: dt[0]\n",
      "73/43: dt[0]['id']\n",
      "73/44:\n",
      "#error here\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\",\"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "73/45:\n",
      "#error here\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\",\"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "73/46:\n",
      "#error here\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "73/47: data\n",
      "73/48:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\",\"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "75/1:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return filename\n",
      "75/2:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "75/3: filename\n",
      "75/4:\n",
      "#run the below \n",
      "#copy paste the json file below\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "75/5: len(content)\n",
      "75/6:\n",
      "#taking list of repo\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "75/7:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "75/8: dt[0]\n",
      "75/9: dt[0]\n",
      "75/10: data\n",
      "75/11: data\n",
      "75/12:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "#if we remove the readme, then file can be saved and the following works well\n",
      "\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "75/13: data\n",
      "75/14:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "#if we remove the readme, then file can be saved and the following works well\n",
      "\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "75/15:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text() #.replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "75/16: dt[0]\n",
      "75/17:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "#if we remove the readme, then file can be saved and the following works well\n",
      "\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "75/18:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "#if we remove the readme, then file can be saved and the following works well\n",
      "\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "81/1:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    \n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return filename\n",
      "81/2:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    file_list=[]\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    \n",
      "    file_list.append(filename)\n",
      "          \n",
      "    return filename\n",
      "81/3:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "81/4: filename\n",
      "81/5: file_list\n",
      "81/6:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(fetch_time+day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    \n",
      "    return filename\n",
      "81/7:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "81/8: filename\n",
      "81/9:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(since+day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    \n",
      "    return filename\n",
      "81/10:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "81/11:\n",
      "import time\n",
      "\n",
      "log_file = 'log.txt'\n",
      "# clean the log\n",
      "f = open(log_file, 'w')\n",
      "f.close()\n",
      "\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json'\n",
      "    \n",
      "    # writing json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(str(since)+'\\t'+day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #saving json files:\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # Update dates for the next search\n",
      "        since = until #move start-date and end-date up 12hours\n",
      "        until += timedelta(hours=4)\n",
      "        time.sleep(2)\n",
      "    \n",
      "    return filename\n",
      "81/12:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "81/13:\n",
      "#run the below \n",
      "#copy paste the json file below\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "81/14: len(content)\n",
      "81/15:\n",
      "#taking list of repo\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "81/16:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "81/17: dt[0]\n",
      "81/18: dt[1]\n",
      "81/19: dt[3]\n",
      "81/20:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "#if we remove the readme, then file can be saved and the following works well\n",
      "\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "81/21:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "#if we remove the readme, then file can be saved and the following works well\n",
      "\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\", encoding='utf-8') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "81/22: data\n",
      "81/23:\n",
      "#the problem here is that readme contains special characters, it can't be written in the csv file\n",
      "#if we remove the readme, then file can be saved and the following works well\n",
      "\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "81/24: data\n",
      "82/1:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(str(since)+'\\t'+day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "        repo_request=response.json()\n",
      "        \n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "82/2:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "82/3: filename\n",
      "82/4:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "content\n",
      "82/5: len(content)\n",
      "82/6:\n",
      "#taking list of repo\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "82/7: content[1]\n",
      "82/8:\n",
      "total_count = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    total_count.extend(jsonobj[\"total_count\"])\n",
      "\n",
      "total_count\n",
      "82/9:\n",
      "no_of_repo = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    no_of_repo.extend(jsonobj[\"total_count\"])\n",
      "\n",
      "no_of_repo\n",
      "82/10:\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "\n",
      "no_of_repo\n",
      "82/11: URL\n",
      "83/1:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=python+created:2021-10-10T20:09:08Z..2021-10-11T00:09:08Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(day_url, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "repo_request\n",
      "83/2:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=python+created:2021-10-10T20:09:08Z..2021-10-11T00:09:08Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "repo_request\n",
      "83/3:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=python+created:2021-10-10T20:09:08Z..2021-10-11T00:09:08Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "len(repo_request['items'])\n",
      "83/4:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "        \n",
      "        #pagination:\n",
      "        for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "            page_url = f'{day_url}&page={i}'\n",
      "            page_response = requests.get(page_url, headers=HEADERS)\n",
      "            \n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            \n",
      "            #writing all request in the json file (filename):\n",
      "            repo_request=page_response.json()\n",
      "            converted_to_string=json.dumps(repo_request)\n",
      "            f=open(filename,'a',encoding='utf-8')\n",
      "            f.write(converted_to_string + '\\n')\n",
      "            f.close()\n",
      "        \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/5:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/6:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        if search_response.json().get(\"total_count\") == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/7:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/8:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        if search_response.json().get(\"total_count\") == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/9:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/10:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "len(repo_request['items'])\n",
      "83/11:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "repo_request\n",
      "83/12:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "repo_request['items']\n",
      "83/13:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "len(repo_request['items'])\n",
      "83/14: repo_request\n",
      "83/15:\n",
      "no_page = math.ceil(repo_request.get(\"total_count\")/100)\n",
      "no_page\n",
      "83/16:\n",
      "no_page = math.ceil(repo_request.get(\"total_count\")/100)\n",
      "for i in range(1, no_page + 1):\n",
      "    print(i)\n",
      "83/17:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100&page=1'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "len(repo_request['items'])\n",
      "83/18:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "len(repo_request['items'])\n",
      "83/19:\n",
      "no_page = math.ceil(repo_request.get(\"total_count\")/100)\n",
      "for i in range(1, no_page + 1):\n",
      "    page_url = f'{URL}&page={i}'\n",
      "    page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "page_response\n",
      "83/20: repo_request['total_count']\n",
      "83/21:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        if search_response.json().get(\"total_count\") == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/22:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/23: repo_request.get('total_count')\n",
      "83/24:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "\n",
      "response.json().get('total_count')\n",
      "83/25:\n",
      "import requests\n",
      "import math\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import pandas as pd \n",
      "import json\n",
      "import os\n",
      "import time\n",
      "\n",
      "token=os.environ['GITHUBTOKEN']\n",
      "URL = 'https://api.github.com/search/repositories?q=education+created:2021-10-11T16:49:48Z..2021-10-11T20:49:48Z&per_page=100&page=1'\n",
      "HEADERS = {'Authorization':f'{token}'}\n",
      "\n",
      "response = requests.get(URL, headers=HEADERS)\n",
      "repo_request=response.json()\n",
      "\n",
      "repo_request\n",
      "83/26: repo_request.get('total_count')\n",
      "83/27:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(search_response.json().get('total_count'))\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/28:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/29:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No. of repositories created between {since} and {until}:{search_response.json().get('total_count')})\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/30:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No. of repositories created between {since} and {until}:{search_response.json().get('total_count')}'')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/31:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No. of repositories created between {since} and {until}:{search_response.json().get('total_count')}')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/32:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/33:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/34:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "83/35:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            if (page_response.status_code!=200): print(page_response.text)\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/36:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/37:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "83/38:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",3)\n",
      "83/39:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",3)\n",
      "82/12:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(str(since)+'\\t'+day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "            \n",
      "        #writing all request in the json file (filename):\n",
      "        repo_request=response.json()\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "82/13:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "82/14: filename\n",
      "82/15:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "82/16:\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "\n",
      "no_of_repo\n",
      "82/17:\n",
      "#taking list of repo\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "83/40:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            if (page_response.status_code!=200): print(page_response.text)\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/41:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "83/42:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/43:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(page_response.text)\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/44:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/45:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(page_response.text)\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/46:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",3)\n",
      "83/47:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "83/48:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            #writing log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(page_response.text)\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination:\n",
      "            for i in range(1, no_page + 1): #running a loop to retrieve each page\n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #writing log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/49:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "83/50:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "83/51:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/52:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "83/53:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",2)\n",
      "83/54:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/55:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "82/18:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "82/19:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "82/20: filename\n",
      "82/21:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "82/22:\n",
      "#getting the number of repositories in each search\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "\n",
      "no_of_repo\n",
      "82/23:\n",
      "#taking list of repo and the number of the repos\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "82/24:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "82/25: filename\n",
      "82/26:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "82/27:\n",
      "#getting the number of repositories in each search\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "\n",
      "no_of_repo\n",
      "82/28:\n",
      "#taking list of repo and the number of the repos\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "82/29:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "83/56:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/57:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/58:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    #import os\n",
      "    import time\n",
      "    \n",
      "    #token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':'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'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/59:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/60:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #token=os.environ['GITHUBTOKEN']\n",
      "    token='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'\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':token}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/61:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/62:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    #HEADERS = {'Authorization':f'{token}'}\n",
      "    HEADERS = {'Authorization':'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'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/63:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    #HEADERS = {'Authorization':f'{token}'}\n",
      "    HEADERS = {'Authorization':'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'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/64:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/65:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    #HEADERS = {'Authorization':f'{token}'}\n",
      "    HEADERS = {'Authorization':'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'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/66:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/67:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/68:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/69:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/70:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'% self.api_token}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/71:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/72:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/73:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/74:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100&access_token={token}'\n",
      "    #HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/75:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/76:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/77:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100&access_token={token}'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/78:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/79:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/80:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Something is wrong in here: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/81:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/82:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/83:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "83/84:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "83/85:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "87/1:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #writing log file:\n",
      "        f = open(log_file, 'a')\n",
      "        f.write(str(since)+'\\t'+day_url+'\\t'+str(response.status_code)+'\\n')\n",
      "        f.close()\n",
      "            \n",
      "        #writing all request in the json file (filename):\n",
      "        repo_request=response.json()\n",
      "        converted_to_string=json.dumps(repo_request)\n",
      "        f=open(filename,'a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "87/2:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "87/3: filename\n",
      "87/4:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "87/5:\n",
      "#getting the number of repositories in each search\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "\n",
      "no_of_repo\n",
      "87/6:\n",
      "#getting the number of repositories in each search\n",
      "import json\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "\n",
      "no_of_repo\n",
      "87/7:\n",
      "#taking list of repo and the number of the repos\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "87/8:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "87/9:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "87/10:\n",
      "#taking list of repo and the number of the repositories\n",
      "\n",
      "repo_list = []\n",
      "import json \n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "87/11:\n",
      "#getting No. of repositories in each search\n",
      "import json\n",
      "\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj = json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "    \n",
      "no_of_repo\n",
      "88/1:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers)\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "88/2:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "87/12: content[0]\n",
      "87/13: len(content)\n",
      "87/14:\n",
      "#taking list of repo and the number of the repositories\n",
      "import json \n",
      "\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    print(len(jsonobj['items']))\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "88/3:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'} #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\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(2)\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "88/4:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "88/5:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'} #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\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7)\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "88/6:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "88/7:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'} #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\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7)\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "        #time.sleep(2) #sleep for 2s before the next request\n",
      "    \n",
      "    return filename\n",
      "88/8:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "88/9:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'} #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\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7)\n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7)\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "       \n",
      "    return filename\n",
      "88/10:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "88/11:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "88/12: filename\n",
      "88/13:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "88/14: content\n",
      "88/15:\n",
      "import json \n",
      "\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    print(len(jsonobj['items']))\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "88/16:\n",
      "#getting No. of repositories in each search\n",
      "import json\n",
      "\n",
      "no_of_repo = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj = json.loads(item)\n",
      "    no_of_repo.append(jsonobj[\"total_count\"])\n",
      "    \n",
      "no_of_repo\n",
      "88/17:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "89/1: filename\n",
      "89/2:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'#&access_token={token}\n",
      "    HEADERS = {'Authorization':f'{token}'} #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\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7)\n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7)\n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "       \n",
      "    return filename\n",
      "89/3:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",3)\n",
      "89/4: filename\n",
      "89/5:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "89/6:\n",
      "import json \n",
      "\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    print(len(jsonobj['items']))\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "89/7:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "89/8: dt[0]\n",
      "89/9: len(dt)\n",
      "89/10:\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "89/11: data\n",
      "89/12:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",1)\n",
      "89/13: filename\n",
      "89/14:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "89/15:\n",
      "import json \n",
      "\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    print(len(jsonobj['items']))\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "89/16:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "89/17: len(dt)\n",
      "89/18:\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "89/19: data\n",
      "89/20:\n",
      "#try the function\n",
      "filename = find_repo(\"machine learning\",1)\n",
      "89/21: filename\n",
      "89/22:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "89/23:\n",
      "import json \n",
      "\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    print(len(jsonobj['items']))\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "89/24:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "89/25: len(dt)\n",
      "89/26:\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "89/27: data\n",
      "89/28:\n",
      "#try the function\n",
      "filename = find_repo(\"python\",3)\n",
      "89/29: filename\n",
      "89/30:\n",
      "#loading the json file\n",
      "\n",
      "f=open(filename,'r',encoding='utf-8')\n",
      "content=f.readlines()\n",
      "89/31:\n",
      "import json \n",
      "\n",
      "repo_list = []\n",
      "\n",
      "for item in content:\n",
      "    jsonobj=json.loads(item)\n",
      "    print(len(jsonobj['items']))\n",
      "    repo_list.extend(jsonobj[\"items\"])\n",
      "    \n",
      "len(repo_list)\n",
      "89/32:\n",
      "dt =[]\n",
      "\n",
      "for repo in repo_list:\n",
      "    id = repo.get(\"id\")\n",
      "    name =repo.get(\"name\")\n",
      "    url = repo.get(\"html_url\")\n",
      "    created = repo.get(\"created_at\")\n",
      "    stars = repo.get(\"stargazers_count\")\n",
      "    watch = repo.get(\"watchers_count\")\n",
      "    language = repo.get(\"language\")\n",
      "    forks = repo.get(\"forks_count\")\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    readme=requests.get(url)\n",
      "    soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "    readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "    if readme == None:\n",
      "        readme =(\"\")\n",
      "    else: \n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "    dt.append({\"id\": id,\n",
      "                 \"name\": name,\n",
      "                 \"url\": url,\n",
      "                 \"created\": created,\n",
      "                 \"stars\": stars,\n",
      "                 \"watch\": watch,\n",
      "                 \"language\": language,\n",
      "                 \"forks\": forks,\n",
      "                 \"readme\":readme})\n",
      "89/33: len(dt)\n",
      "89/34:\n",
      "import csv\n",
      "import pandas as pd \n",
      "\n",
      "#Writing data into .csv file and returning the table:\n",
      "with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "    \n",
      "    for item in dt:\n",
      "        writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "89/35: data\n",
      "94/1:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "       \n",
      "    return filename\n",
      "94/2:\n",
      "#try the function\n",
      "filename = find_repo(\"open education\",3)\n",
      "97/1:\n",
      "#Function1 find_repo: Looking for respositories containing a specific \"term\"\n",
      "#created within a specific timeframe of [day] days\n",
      "\n",
      "def find_repo(term, day):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the fetching URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##building URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{datetime.today()}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "       \n",
      "    return filename\n",
      "97/2:\n",
      "#Function 2:  Extracting the list of repositories from the json file\n",
      "\n",
      "def extract_repo_list(filename):\n",
      "    import json \n",
      "    f=open(filename,'r',encoding='utf-8')\n",
      "    content=f.readlines()\n",
      "    repo_list = []\n",
      "\n",
      "    for item in content:\n",
      "        jsonobj=json.loads(item)\n",
      "        print(len(jsonobj['items']))\n",
      "        repo_list.extend(jsonobj[\"items\"])\n",
      "    return repo_list\n",
      "97/3:\n",
      "#Function 3: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    \n",
      "    dt =[]\n",
      "\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "        return dt\n",
      "97/4:\n",
      "#Function 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "97/5:\n",
      "# Search for repositories:\n",
      "filename = find_repo(\"education\",1)\n",
      "# Extract repository lists:\n",
      "repo_list = extract_repo_list(filename)\n",
      "# Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "# Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/6:\n",
      "#Function1 find_repo: Looking for respositories containing a specific \"term\"\n",
      "#created within a specific timeframe of [day] days\n",
      "\n",
      "def find_repo(term, day):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the fetching URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##building URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=4)\n",
      "       \n",
      "    return filename\n",
      "97/7:\n",
      "#Function 2:  Extracting the list of repositories from the json file\n",
      "\n",
      "def extract_repo_list(filename):\n",
      "    import json \n",
      "    f=open(filename,'r',encoding='utf-8')\n",
      "    content=f.readlines()\n",
      "    repo_list = []\n",
      "\n",
      "    for item in content:\n",
      "        jsonobj=json.loads(item)\n",
      "        print(len(jsonobj['items']))\n",
      "        repo_list.extend(jsonobj[\"items\"])\n",
      "    return repo_list\n",
      "97/8:\n",
      "#Function 3: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    \n",
      "    dt =[]\n",
      "\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "        return dt\n",
      "97/9:\n",
      "#Function 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/10:\n",
      "# Search for repositories:\n",
      "filename = find_repo(\"education\",1)\n",
      "# Extract repository lists:\n",
      "repo_list = extract_repo_list(filename)\n",
      "# Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "# Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/11: len(dt)\n",
      "97/12: len(repo_list)\n",
      "97/13:\n",
      "#Function 3: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    \n",
      "    dt =[]\n",
      "\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "    return dt\n",
      "97/14:\n",
      "# Search for repositories:\n",
      "filename = find_repo(\"education\",1)\n",
      "# Extract repository lists:\n",
      "repo_list = extract_repo_list(filename)\n",
      "# Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "# Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/15: len(repo_list)\n",
      "97/16: len(dt)\n",
      "97/17:\n",
      "#Function1 find_repo: Looking for respositories containing a specific \"term\"\n",
      "#created within a specific timeframe of [day] days\n",
      "\n",
      "def find_repo(term, day):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the fetching URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##building URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=12)\n",
      "       \n",
      "    return filename\n",
      "97/18:\n",
      "#Function1 find_repo: Looking for respositories containing a specific \"term\"\n",
      "#created within a specific timeframe of [day] days\n",
      "\n",
      "def find_repo(term, day, h):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the fetching URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##building URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours=h)\n",
      "       \n",
      "    return filename\n",
      "97/19:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"python\",1,10)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/20:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"python\",3,5)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/21:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"python\",3,8)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/22:\n",
      "#Function1 grf - GitHub Repository Finder: \n",
      "#Looking for respositories containing a specific \"term\"\n",
      "#created within a specific timeframe of [day] days\n",
      "#search results are crawled on every h-hour segment\n",
      "\n",
      "def grf(term, day, h):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the API URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##building URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file to monitor error\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file to combine all json files returned from each call:\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until <= datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every h-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours=h)\n",
      "       \n",
      "    return filename\n",
      "97/23:\n",
      "#Function 2:  Extracting the list of repositories from the json file\n",
      "\n",
      "def extract_repo_list(filename):\n",
      "    import json \n",
      "    f=open(filename,'r',encoding='utf-8')\n",
      "    content=f.readlines()\n",
      "    repo_list = []\n",
      "\n",
      "    for item in content:\n",
      "        jsonobj=json.loads(item)\n",
      "        repo_list.extend(jsonobj[\"items\"])\n",
      "    return repo_list\n",
      "97/24:\n",
      "#Function 3: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    \n",
      "    dt =[]\n",
      "\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "    return dt\n",
      "97/25:\n",
      "#Function 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "97/26:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"python\",3,8)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/27:\n",
      "#Function1 grf - GitHub Repository Finder: 3 parameters are defined:\n",
      "#term: specifies the keyword to search for respositories\n",
      "#day: specifies the timeframe to search for repositories. If t is the moment extraction is started then the function will return all repositories created between (t-d) and t.\n",
      "#search results are crawled on every h-hour segment\n",
      "\n",
      "def grf(term, day, h):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the API URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##building URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=4) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file to monitor error\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file to combine all json files returned from each call:\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until <= datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every h-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours=h)\n",
      "       \n",
      "    return filename\n",
      "97/28:\n",
      "#Function1 grf - GitHub Repository Finder: 3 parameters are defined:\n",
      "#>term: specifies the keyword to search for respositories. Type: str\n",
      "#>d: specifies the timeframe to search for repositories. Type: int \n",
      "##If [now] is the moment when extraction is started, function will return all repositories \n",
      "##created between (now-d) days and now.\n",
      "#>h: specifies the time duration (hours) of each query. Type: int\n",
      "\n",
      "def grf(term, d, h):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the API URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##building URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days = d)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours = h) # dividing the total No.of repo into segments of h hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file to monitor error\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file to combine all json files returned from each call:\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until <= datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every h-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours=h)\n",
      "       \n",
      "    return filename\n",
      "97/29:\n",
      "#Function 2:  Extracting the list of repositories from the json file\n",
      "\n",
      "def extract_repo_list(filename):\n",
      "    import json \n",
      "    f=open(filename,'r',encoding='utf-8')\n",
      "    content=f.readlines()\n",
      "    repo_list = []\n",
      "\n",
      "    for item in content:\n",
      "        jsonobj=json.loads(item)\n",
      "        repo_list.extend(jsonobj[\"items\"])\n",
      "    return repo_list\n",
      "97/30:\n",
      "#Function 3: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    \n",
      "    dt =[]\n",
      "\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "    return dt\n",
      "97/31:\n",
      "#Function 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "97/32:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"education\",1,8)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/33:\n",
      "#Function1 find_repo - Looking for repositories defined by 3 parameters:\n",
      "#>term: specifies the keyword to search for respositories. Type: str\n",
      "#>d: specifies the timeframe to search for repositories. Type: int \n",
      "##If [now] is the moment when extraction is started, function will return all repositories \n",
      "##created between (now-d) days and now.\n",
      "#>h: specifies the time duration (hours) of each query. Type: int\n",
      "\n",
      "def grf(term, d, h):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the API URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    ##constructing API URL with term: the keywords - SINCE...UNTIL: determining the timeframe to extract data\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days = d)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours = h) # dividing the total No.of repo into segments of h hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file to monitor error\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file to combine all json files returned from each call:\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until <= datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every h-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours=h)\n",
      "       \n",
      "    return filename\n",
      "97/34:\n",
      "#Function 2:  Extracting the list of repositories from the json file\n",
      "\n",
      "def extract_repo_list(filename):\n",
      "    import json \n",
      "    f=open(filename,'r',encoding='utf-8')\n",
      "    content=f.readlines()\n",
      "    repo_list = []\n",
      "\n",
      "    for item in content:\n",
      "        jsonobj=json.loads(item)\n",
      "        repo_list.extend(jsonobj[\"items\"])\n",
      "    return repo_list\n",
      "97/35:\n",
      "#Function 3: save_column: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    \n",
      "    dt =[]\n",
      "\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "        \n",
      "        #scraping readme by going through each repository url\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "    return dt\n",
      "97/36:\n",
      "#Function 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "97/37:\n",
      "#Function 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "97/38:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"education\",1,8)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "96/1:\n",
      "def find_repo(term, day):\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days= day)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours=8) # dividing the total No.of repo into segments of 4 hours each\n",
      "    \n",
      "    fetch_time=f'{until}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every 4-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if things go well, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all request in the json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up 4hours\n",
      "        since = until \n",
      "        until += timedelta(hours=8)\n",
      "       \n",
      "    return filename\n",
      "96/2:\n",
      "#try the function\n",
      "filename = find_repo(\"education\",1)\n",
      "96/3: filename\n",
      "97/39:\n",
      "#Function1 find_repo - Looking for repositories defined by 3 parameters:\n",
      "#>term: specifies the keyword to search for respositories. Type: str\n",
      "#>d: specifies the timeframe to search for repositories. Type: int \n",
      "##If [now] is the moment when extraction is started, function will return all repositories \n",
      "##created between (now-d) days and now.\n",
      "#>h: specifies the time duration (hours) of each query. Type: int\n",
      "\n",
      "def find_repo(term, d, h):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the API URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days = d)  # Start fetching repo created {day} days ago\n",
      "    until = since + timedelta(hours = h) # dividing the total No.of repo into segments of h hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{day}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file to monitor error\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file to combine all json files returned from each call:\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until <= datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every h-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request=page_response.json()\n",
      "                converted_to_string=json.dumps(repo_request)\n",
      "                f=open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours = h)\n",
      "       \n",
      "    return filename\n",
      "97/40: find_repo(\"education\", 1, 8)\n",
      "97/41:\n",
      "#Function1 find_repo - Looking for repositories defined by 3 parameters:\n",
      "#>term: specifies the keyword to search for respositories. Type: str\n",
      "#>d: specifies the timeframe to search for repositories. Type: int \n",
      "##If [now] is the moment when extraction is started, function will return all repositories \n",
      "##created between (now-d) days and now.\n",
      "#>h: specifies the time duration (hours) of each query. Type: int\n",
      "\n",
      "def find_repo(term, d, h):\n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #setting variables to construct the API URL\n",
      "    \n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    \n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    since = datetime.today() - relativedelta(days = d)  # Start fetching repo created {d} days ago\n",
      "    until = since + timedelta(hours = h) # dividing the total No.of repo into segments of h hours each\n",
      "    \n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{d}days_{fetch_time}.json' # name of the final json file\n",
      "    \n",
      "    # create the log file to monitor error\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # create an empty json file to combine all json files returned from each call:\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until <= datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7) #sleep 7s after each request\n",
      "        \n",
      "        #print out No of repos in every h-hour call\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #in case there is no Total_count, print out the failed link and the text of the failed request\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) #printing headers to view the rate limit\n",
      "        \n",
      "        #if fetching successful, do pagination:\n",
      "        else:\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) #calculating the total No. of pages\n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7) #sleep 7s after each request\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request = page_response.json()\n",
      "                converted_to_string = json.dumps(repo_request)\n",
      "                f = open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        # update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours = h)\n",
      "       \n",
      "    return filename\n",
      "97/42: find_repo(\"education\", 1, 8)\n",
      "97/43:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"education\",1,8)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/44:\n",
      "#Wrapping up into one single function:\n",
      "def grf(term,d,h):\n",
      "    filename = find_repo(\"education\",1,8)\n",
      "    repo_list = extract_repo_list(filename)\n",
      "    dt = save_column(repo_list)\n",
      "    data = save_dt(dt)\n",
      "    return data\n",
      "97/45: grf(\"education\", 1, 8)\n",
      "98/1:\n",
      "import requests\n",
      "url = 'https://www.reddit.com/r/politics.json'\n",
      "\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "politics_response = response.json()\n",
      "98/2: politics_response\n",
      "98/3: politics_response['data'][1]\n",
      "98/4: politics_response('data')\n",
      "98/5: politics_response['data']\n",
      "98/6: politics_response['data']['children']\n",
      "98/7: len(politics_response['data']['children'])\n",
      "98/8: politics_response['data']['children'][0]\n",
      "98/9: politics_response['data']['children'][0]['data']\n",
      "98/10: politics_response['data']['children'][0]['data']['selftext']\n",
      "98/11: len(politics_response['data']['children'])\n",
      "98/12: politics_response['data']\n",
      "98/13:\n",
      "mod = 'politics'\n",
      "after = None\n",
      "item_type = []\n",
      "\n",
      "for counter in range(5): \n",
      "    url = f'https://www.reddit.com/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['kind'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/14:\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": t3_q9dtmj})\n",
      "json_response = response.json()\n",
      "after = json_response['data']['after']\n",
      "98/15:\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": t3_q9dtmj})\n",
      "json_response = response.json()\n",
      "98/16:\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": 't3_q9dtmj'})\n",
      "json_response = response.json()\n",
      "98/17:\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": 't3_q9dtmj'})\n",
      "json_response = response.json()\n",
      "after = json_response['data']['after']\n",
      "98/18: json_response\n",
      "98/19:\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "after = t3_q9dtmj\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": after})\n",
      "json_response = response.json()\n",
      "after = json_response['data']['after']\n",
      "98/20:\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "after = 't3_q9dtmj'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": after})\n",
      "json_response = response.json()\n",
      "after = json_response['data']['after']\n",
      "98/21:\n",
      "url = f'https://www.reddit.com/{mod}.json?after=t3_1jg35w'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers)\n",
      "json_response = response.json()\n",
      "after = json_response['data']['after']\n",
      "98/22:\n",
      "url = f'https://www.reddit.com/{mod}.json?after=t3_1jg35w'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers)\n",
      "json_response = response.json()\n",
      "98/23: json_response\n",
      "99/1:\n",
      "after = None\n",
      "item_type = []\n",
      "mod = \"sixwaystop313\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/user/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['kind'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/24:\n",
      "import json\n",
      "import requests\n",
      "\n",
      "mod = 'politics'\n",
      "after = None\n",
      "item_type = []\n",
      "\n",
      "for counter in range(5): \n",
      "    url = f'https://www.reddit.com/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['kind'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/25:\n",
      "import json\n",
      "import requests\n",
      "\n",
      "mod = 'politics'\n",
      "after = None\n",
      "item_type = []\n",
      "\n",
      "for counter in range(5): \n",
      "    url = f'https://www.reddit.com/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after']\n",
      "98/26:\n",
      "import json\n",
      "import requests\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "after = 't3_q9dtmj'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": after})\n",
      "json_response = response.json()\n",
      "after = json_response['data']['after']\n",
      "99/2:\n",
      "# request JSON output from icanhazdadjoke API\n",
      "import requests\n",
      "url = \"https://icanhazdadjoke.com\"\n",
      "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
      "joke_request = response.json() \n",
      "print(joke_request)\n",
      "99/3:\n",
      "import requests\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"cat\"})\n",
      "joke_request = response.json()\n",
      "print(joke_request)\n",
      "99/4: # your answer goes here!\n",
      "99/5:\n",
      "# Question 1 \n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"dog\"})\n",
      "joke_request = response.json()\n",
      "print(f\"The number of dog jokes is: {joke_request['total_jokes']}\")\n",
      "99/6:\n",
      "# Question 2\n",
      "def find_jokes(term):\n",
      "    search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": term})\n",
      "    joke_request = response.json()\n",
      "    num_results = joke_request['total_jokes']\n",
      "    return num_results\n",
      "\n",
      "find_jokes(\"some-searchterm-you-would-like-to-try-out\")\n",
      "99/7:\n",
      "search_url = \"https://icanhazdadjoke.com/search\"\n",
      "\n",
      "response = requests.get(search_url, \n",
      "                        headers={\"Accept\": \"application/json\"}, \n",
      "                        params={\"term\": \"\"})\n",
      "joke_request = response.json()\n",
      "joke_request['results'] = '' # let's remove all jokes, and only look at the other attributes in the JSON response\n",
      "joke_request\n",
      "99/8:\n",
      "for limit in range(10, 31, 10):  # note that range(a, b) runs from a to b-1; so the last value is exclusive (so from 10 to 30 with steps of 10)\n",
      "    response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": limit})\n",
      "    joke_request = response.json()\n",
      "    print(f\"Limit {limit} gives {joke_request['total_pages']} pages\")\n",
      "99/9:\n",
      "response = requests.get(search_url, \n",
      "                            headers={\"Accept\": \"application/json\"}, \n",
      "                            params={\"term\": \"\", \n",
      "                                   \"limit\": 5,\n",
      "                                   \"page\": 2})\n",
      "response.json()\n",
      "99/10: # your answer goes here!\n",
      "99/11:\n",
      "def find_jokes(term):\n",
      "    search_url = \"https://icanhazdadjoke.com/search\"\n",
      "    page = 1\n",
      "    jokes = []\n",
      "\n",
      "    while True:  # alternatively you can also use a for-loop that goes from page 1 to total_results / 20 (rounded up)\n",
      "        response = requests.get(search_url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"term\": term,  # optionally you can add \"limit\": 20 but that's already the default so it doesn't change anything\n",
      "                                        \"page\": page})\n",
      "        joke_request = response.json()\n",
      "        jokes.extend(joke_request['results'])\n",
      "        if joke_request['current_page'] <= joke_request['total_pages']:\n",
      "            page += 1\n",
      "        else: \n",
      "            return jokes\n",
      "\n",
      "output = find_jokes(\"rain\") # try running it with \"\", too!\n",
      "99/12: print(f\"You've collected {len(output)} jokes\")\n",
      "99/13:\n",
      "import requests\n",
      "import math\n",
      "\n",
      "repo_list =[]\n",
      "\n",
      "def find_repo(term):\n",
      "    url = \"https://api.github.com/search/repositories\"\n",
      "    response = requests.get(url, headers={\"Accept\": \"application/json\"},\n",
      "                       params = {\"q\": term})\n",
      "    total_page = math.ceil(response.json()['total_count']/(len(response.json()['items'])))\n",
      "    for page in range (1, total_page + 1):\n",
      "        response = requests.get(url, \n",
      "                                headers={\"Accept\": \"application/json\"}, \n",
      "                                params={\"q\": term,\n",
      "                                       \"page\": page})\n",
      "        git_request = response.json()\n",
      "        for repo in git_request['items']:\n",
      "            repo_list.append(repo)\n",
      "    return repo_list\n",
      "term = \"open education\"\n",
      "find_repo(term)\n",
      "99/14:\n",
      "import requests\n",
      "url = 'https://www.reddit.com/r/marketing/about/.json'\n",
      "\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "json_response = response.json()\n",
      "99/15:\n",
      "# Question 2 \n",
      "import time\n",
      "\n",
      "i = 1\n",
      "while i <= 3:\n",
      "    url = 'https://www.reddit.com/r/marketing/about/.json'\n",
      "    headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "    response = requests.get(url, headers=headers)\n",
      "    json_response = response.json()\n",
      "    \n",
      "    print(json_response['data']['active_user_count'])\n",
      "    i += 1\n",
      "    time.sleep(5)\n",
      "99/16:\n",
      "# Question 3\n",
      "def get_usercount(subreddit):\n",
      "    response = response = requests.get(f'https://www.reddit.com/r/{subreddit}/about/.json', headers=headers)\n",
      "    json_response = response.json()\n",
      "    out = {}\n",
      "    out['subreddit'] = subreddit\n",
      "    out['total_users'] = json_response['data']['subscribers']\n",
      "    out['active_users'] = json_response['data']['active_user_count']\n",
      "    return out\n",
      "    \n",
      "get_usercount('science')\n",
      "99/17:\n",
      "mod = \"sixwaystop313\"\n",
      "response = requests.get(f'https://www.reddit.com/user/{mod}.json', headers=headers)\n",
      "json_response = response.json()\n",
      "json_response\n",
      "99/18:\n",
      "import json\n",
      "json_response = json.loads(response.text.replace('null', '\"None\"').replace('True','\"True\"').replace('False','\"False\"'))\n",
      "json_response\n",
      "99/19: json_response['data']['after']\n",
      "99/20:\n",
      "after = json_response['data']['after']\n",
      "url = f'https://www.reddit.com/user/{mod}.json'\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": after})\n",
      "json_response_after = response.json()\n",
      "json_response_after\n",
      "99/21:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "item_type = []\n",
      "mod = \"sixwaystop313\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/user/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['kind'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/27:\n",
      "import json\n",
      "import requests\n",
      "mod = \"politics\"\n",
      "url = f'https://www.reddit.com/{mod}.json'\n",
      "after = 't3_q9dtmj'\n",
      "print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": after})\n",
      "json_response = response.json()\n",
      "after = json_response['data']['after']\n",
      "98/28:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "item_type = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['kind'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/29:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "item_type = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['kind'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/30:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "self_text = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['selftext'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/31: politics_response['data']['children']\n",
      "98/32:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "self_text = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['selftext'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/33: politics_response['data']['children']['selftext']\n",
      "98/34: politics_response['data']['children']('selftext')\n",
      "98/35: politics_response['data']['children'][0]('selftext')\n",
      "98/36: politics_response['data']['children'][0]['selftext']\n",
      "98/37: politics_response['data']['children'][0]\n",
      "98/38: politics_response['data']['children'][0]['data']['selftext']\n",
      "98/39:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "self_text = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['data']['selftext'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "98/40:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "self_text = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        self_text.append(item['data']['selftext'])\n",
      "\n",
      "# Let's view the item types: \n",
      "self_text\n",
      "98/41: politics_response['data']['children'][1]['data']['selftext']\n",
      "98/42: politics_response['data']['children'][3]['data']['selftext']\n",
      "98/43: politics_response['data']['children'][3]['data']\n",
      "98/44: politics_response['data']['children'][3]\n",
      "98/45:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']['data']:\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append(\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups)\n",
      "\n",
      "post_list\n",
      "98/46:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']['data']:\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/47: politics_response['data']['children'][3]['data']\n",
      "98/48:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "    politics = json_response['data']['children']['data']\n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in politics:\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/49: politics_response['data']['children'][3]['data'].get(\"ups\")\n",
      "98/50: politics_response['data']['children'][3]['data'][\"ups\"]\n",
      "98/51:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "    json_response['data']['children']['data']\n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i['data']\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/52:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "    \n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i['data']\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/53:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "selftext = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "    \n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        selftext.append(item['data'][\"selftext\"])\n",
      "    \n",
      "\n",
      "\n",
      "selftext\n",
      "98/54:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "    json_response['data']['children']['data']\n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/55:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i['data']\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/56: len(post_list)\n",
      "98/57:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "selftext = []\n",
      "mod = \"politics\"\n",
      "for counter in range(100): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "    \n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        selftext.append(item['data'][\"selftext\"])\n",
      "    \n",
      "selftext\n",
      "98/58:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "for counter in range(100): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i['data']\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/59: len(post_list)\n",
      "99/22:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "item_type = []\n",
      "mod = \"sixwaystop313\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/user/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        item_type.append(item['kind'])\n",
      "\n",
      "# Let's view the item types: \n",
      "item_type\n",
      "99/23: len(item_type)\n",
      "98/60:\n",
      "import json\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        id = item['data'][\"id\"]\n",
      "        selftext = item['data'][\"selftext\"]\n",
      "        downs = item['data']['downs']\n",
      "        ups = item['data']['ups']\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/61:\n",
      "import json\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        id = item['data'][\"id\"]\n",
      "        selftext = item['data'][\"selftext\"]\n",
      "        downs = item['data']['downs']\n",
      "        ups = item['data']['ups']\n",
      "    post_list.append({\"id\":id,\n",
      "                    \"selftext\":selftext,\n",
      "                    \"downs\": downs,\n",
      "                    \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/62: len(post_list)\n",
      "98/63:\n",
      "import json\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        id = item['data'][\"id\"]\n",
      "        selftext = item['data'][\"selftext\"]\n",
      "        downs = item['data']['downs']\n",
      "        ups = item['data']['ups']\n",
      "        post_list.append({\"id\":id,\n",
      "                        \"selftext\":selftext,\n",
      "                        \"downs\": downs,\n",
      "                        \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/64: len(post_list)\n",
      "98/65: post_list[2]\n",
      "98/66: post_list[6]\n",
      "98/67: post_list[10]\n",
      "98/68: post_list\n",
      "98/69:\n",
      "for i in post_list:\n",
      "    print(i['selftext'])\n",
      "98/70:\n",
      "import json\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i['data']\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "        subreddit = item.get('subreddit')\n",
      "        post_list.append({\"id\":id,\n",
      "                        \"selftext\":selftext,\n",
      "                        \"downs\": downs,\n",
      "                        \"ups\":ups,\n",
      "                         \"community\":subreddit})\n",
      "\n",
      "post_list\n",
      "98/71:\n",
      "def post_list(mod)\n",
      "    import json\n",
      "    for counter in range(10): \n",
      "        url = f'https://www.reddit.com/r/{mod}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            subreddit = item.get('subreddit')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups,\n",
      "                             \"community\":subreddit})\n",
      "\n",
      "    return post_list\n",
      "98/72:\n",
      "def post_list(mod):\n",
      "    import json\n",
      "    for counter in range(10): \n",
      "        url = f'https://www.reddit.com/r/{mod}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            subreddit = item.get('subreddit')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups,\n",
      "                             \"community\":subreddit})\n",
      "\n",
      "    return post_list\n",
      "98/73:\n",
      "after = None\n",
      "post_list = []\n",
      "post_list = post_list(\"science\")\n",
      "post_list(\"politics\")\n",
      "98/74:\n",
      "after = None\n",
      "post_list = []\n",
      "post_list = export_post(\"science\")\n",
      "export_post(\"politics\")\n",
      "98/75:\n",
      "def export_post(com):\n",
      "    import json\n",
      "    for counter in range(10): \n",
      "        url = f'https://www.reddit.com/r/{com}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            subreddit = item.get('subreddit')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups,\n",
      "                             \"community\":subreddit})\n",
      "\n",
      "    return post_list\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/76:\n",
      "after = None\n",
      "post_list = []\n",
      "post_list = export_post(\"science\")\n",
      "export_post(\"politics\")\n",
      "98/77:\n",
      "def export_post(com):\n",
      "    import json\n",
      "    after = None\n",
      "    for counter in range(10): \n",
      "        url = f'https://www.reddit.com/r/{com}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            subreddit = item.get('subreddit')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups,\n",
      "                             \"community\":subreddit})\n",
      "\n",
      "    return post_list\n",
      "98/78:\n",
      "post_list = []\n",
      "post_list = export_post(\"science\")\n",
      "export_post(\"politics\")\n",
      "98/79:\n",
      "post_list = []\n",
      "post_list = export_post(\"science\")\n",
      "data = export_post(\"politics\")\n",
      "98/80: data\n",
      "98/81: len(data)\n",
      "98/82: post_list\n",
      "98/83:\n",
      "post_list = []\n",
      "post_list = export_post(\"science\")\n",
      "post_list = export_post(\"politics\")\n",
      "98/84: len(post_list)\n",
      "98/85: post_list\n",
      "98/86:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"marketing\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i['data']\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "        post_list.append({\"id\":id,\n",
      "                        \"selftext\":selftext,\n",
      "                        \"downs\": downs,\n",
      "                        \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/87:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "mod = \"politics\"\n",
      "for counter in range(10): \n",
      "    url = f'https://www.reddit.com/r/{mod}.json'\n",
      "    print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "    response = requests.get(url, \n",
      "                            headers=headers, \n",
      "                            params={\"after\": after})\n",
      "    json_response = response.json()\n",
      "    after = json_response['data']['after'] \n",
      "\n",
      "    # loop over all items in a request\n",
      "    for i in json_response['data']['children']:\n",
      "        item = i['data']\n",
      "        id = item.get(\"id\")\n",
      "        selftext = item.get(\"selftext\")\n",
      "        downs = item.get('downs')\n",
      "        ups = item.get('ups')\n",
      "        post_list.append({\"id\":id,\n",
      "                        \"selftext\":selftext,\n",
      "                        \"downs\": downs,\n",
      "                        \"ups\":ups})\n",
      "\n",
      "post_list\n",
      "98/88:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(filename,'w',encoding='utf-8')\n",
      "f.close()\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{mod}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups})\n",
      "98/89:\n",
      "import json\n",
      "\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{mod}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups})\n",
      "97/46:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"python\",3,8)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "data\n",
      "97/47:\n",
      "# Step1: Search for repositories\n",
      "filename = find_repo(\"python\",3,8)\n",
      "\n",
      "# Step2: Extract repository lists\n",
      "repo_list = extract_repo_list(filename)\n",
      "\n",
      "# Step3: Saving relevant information\n",
      "dt = save_column(repo_list)\n",
      "\n",
      "# Step4: Saving the dataset\n",
      "data = save_dt(dt)\n",
      "97/48:\n",
      "#Wrapping up into one single function: Github Repository Finder (grf)\n",
      "def grf(term, d, h):\n",
      "    filename = find_repo(term, d, h)\n",
      "    repo_list = extract_repo_list(filename)\n",
      "    dt = save_column(repo_list)\n",
      "    data = save_dt(dt)\n",
      "    return data\n",
      "97/49: grf(\"python\", 3, 8)\n",
      "100/1:\n",
      "#Step 2:  Extracting the list of repositories from the json file\n",
      "\n",
      "def extract_repo_list(filename):\n",
      "    '''\n",
      "    Reads .json file of repository data, returns a list of repositories.\n",
      "\n",
      "            Parameters:\n",
      "                    filename (str): Name of the .json file obtained from find_repo()\n",
      "                    \n",
      "            Returns:\n",
      "                    repo_list (list): List of all repositories found in the search queries \n",
      "    ''' \n",
      "    import json \n",
      "    f=open(filename,'r',encoding='utf-8')\n",
      "    content=f.readlines()\n",
      "    repo_list = []\n",
      "\n",
      "    for item in content:\n",
      "        jsonobj=json.loads(item)\n",
      "        repo_list.extend(jsonobj[\"items\"])\n",
      "    return repo_list\n",
      "100/2:\n",
      "#Step1: Searching for repositories, crawling data, storing responses in a .json file\n",
      "\n",
      "def find_repo(term, d, h):\n",
      "    '''\n",
      "    Searches for repositories on GitHub, returns name of a .json file of original data.\n",
      "\n",
      "            Parameters:\n",
      "                    term (str): A string defining the search term\n",
      "                    d (int): An integer defining the timeframe to search (in days)\n",
      "                    h (int): An integer defining the duration (in hours) of each query\n",
      "\n",
      "            Returns:\n",
      "                    filename (str): Name of a single .json file which stores the original response data\n",
      "                    .json file: A file stored in working directory\n",
      "                    log.txt: Log file recording all API calls and their status\n",
      "    \n",
      "    Check GRF documentation for detailed function explanation. \n",
      "    ''' \n",
      "    #importing required libraries\n",
      "    import requests\n",
      "    import math\n",
      "    from datetime import datetime, timedelta\n",
      "    from dateutil.relativedelta import relativedelta\n",
      "    import csv\n",
      "    import pandas as pd \n",
      "    import json\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    #constructing URLs\n",
      "    token=os.environ['GITHUBTOKEN']\n",
      "    URL = f'https://api.github.com/search/repositories?q={term}+created:SINCE..UNTIL&per_page=100'\n",
      "    HEADERS = {'Authorization':f'{token}'}\n",
      "    \n",
      "    #setting timeframe and search segments\n",
      "    since = datetime.today() - relativedelta(days = d)  \n",
      "    until = since + timedelta(hours = h)\n",
      "    \n",
      "    #setting name for the final json file\n",
      "    fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "    filename = f'{term}_{d}days_{fetch_time}.json' \n",
      "    \n",
      "    # creating the log file to monitor error\n",
      "    log_file = 'log.txt'\n",
      "    f = open(log_file, 'w')\n",
      "    f.close()\n",
      "    \n",
      "    # creating an empty json file to combine all json files returned from each call:\n",
      "    f= open(filename,'w',encoding='utf-8')\n",
      "    f.close()\n",
      "        \n",
      "    while until < datetime.today():\n",
      "        day_url = URL.replace('SINCE', since.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('UNTIL', until.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
      "        search_response = requests.get(day_url, headers=HEADERS)\n",
      "        time.sleep(7)\n",
      "        \n",
      "        #print out No of repos in every h-hour request\n",
      "        print(f'No of repositories created between {since} and {until}:{search_response.json().get(\"total_count\")}')\n",
      "        \n",
      "        #If no Total_count found, print out the failed link and additional information\n",
      "        if search_response.json().get('total_count') == None:\n",
      "            print(f'Failed request: {day_url}')\n",
      "            \n",
      "            #saving status in log file:\n",
      "            f = open(log_file, 'a')\n",
      "            f.write(str(since)+'\\t'+page_url+'\\t'+str(search_response.status_code)+'\\n')\n",
      "            f.close()\n",
      "            \n",
      "            print(search_response.text)\n",
      "            print(search_response.headers) \n",
      "        \n",
      "        #if extraction successful, do pagination:\n",
      "        else:\n",
      "            #calculating the total No. of pages\n",
      "            no_page = math.ceil(search_response.json().get(\"total_count\")/100) \n",
      "            \n",
      "            #pagination: running a loop to retrieve each page\n",
      "            for i in range(1, no_page + 1): \n",
      "                page_url = f'{day_url}&page={i}'\n",
      "                page_response = requests.get(page_url, headers=HEADERS)\n",
      "                time.sleep(7)\n",
      "                \n",
      "                #writing all requests in a single json file (filename):\n",
      "                repo_request = page_response.json()\n",
      "                converted_to_string = json.dumps(repo_request)\n",
      "                f = open(filename,'a',encoding='utf-8')\n",
      "                f.write(converted_to_string + '\\n')\n",
      "                f.close()\n",
      "        \n",
      "                #saving status in log file:\n",
      "                f = open(log_file, 'a')\n",
      "                f.write(str(since)+'\\t'+page_url+'\\t'+str(page_response.status_code)+'\\n')\n",
      "                f.close()\n",
      "                \n",
      "        #update dates for the next search - moving {since} and {until} up /h/ hours\n",
      "        since = until \n",
      "        until += timedelta(hours = h)\n",
      "       \n",
      "    return filename\n",
      "100/3:\n",
      "#Step 3: save_column: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    '''\n",
      "    Extracts relevant repository information, saves it in a list.\n",
      "\n",
      "            Parameters:\n",
      "                    repo_list (list): List obtained from export_repo_list()\n",
      "                    \n",
      "            Returns:\n",
      "                    dt (list): List of repositories with only desired information \n",
      "    ''' \n",
      "    dt =[]\n",
      "    \n",
      "    #extracting data directly from the list\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "        \n",
      "        #scraping readme by going through each repository url\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        #if readme is empty, recording as blank (NaN)\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "    return dt\n",
      "100/4:\n",
      "#Step 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    '''\n",
      "    Saves desired data in a .csv file, returns a preview of the final dataset.\n",
      "\n",
      "            Parameters:\n",
      "                    dt (list): List obtained from save_column()\n",
      "                    \n",
      "            Returns:\n",
      "                    data.csv: Final dataset stored in working directory\n",
      "    ''' \n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "100/5: ?find_repo\n",
      "100/6: ?export_repo_list\n",
      "100/7:\n",
      "#Step 2:  Extracting the list of repositories from the json file\n",
      "\n",
      "def extract_repo_list(filename):\n",
      "    '''\n",
      "    Reads .json file of repository data, returns a list of repositories.\n",
      "\n",
      "            Parameters:\n",
      "                    filename (str): Name of the .json file obtained from find_repo()\n",
      "                    \n",
      "            Returns:\n",
      "                    repo_list (list): List of all repositories found in the search queries \n",
      "    ''' \n",
      "    import json \n",
      "    f=open(filename,'r',encoding='utf-8')\n",
      "    content=f.readlines()\n",
      "    repo_list = []\n",
      "\n",
      "    for item in content:\n",
      "        jsonobj=json.loads(item)\n",
      "        repo_list.extend(jsonobj[\"items\"])\n",
      "    return repo_list\n",
      "100/8:\n",
      "#Step 3: save_column: Extracting relevant information from the repository list\n",
      "\n",
      "def save_column(repo_list):\n",
      "    '''\n",
      "    Extracts relevant repository information, saves it in a list.\n",
      "\n",
      "            Parameters:\n",
      "                    repo_list (list): List obtained from extract_repo_list()\n",
      "                    \n",
      "            Returns:\n",
      "                    dt (list): List of repositories with only desired information \n",
      "    ''' \n",
      "    dt =[]\n",
      "    \n",
      "    #extracting data directly from the list\n",
      "    for repo in repo_list:\n",
      "        id = repo.get(\"id\")\n",
      "        name =repo.get(\"name\")\n",
      "        url = repo.get(\"html_url\")\n",
      "        created = repo.get(\"created_at\")\n",
      "        stars = repo.get(\"stargazers_count\")\n",
      "        watch = repo.get(\"watchers_count\")\n",
      "        language = repo.get(\"language\")\n",
      "        forks = repo.get(\"forks_count\")\n",
      "        \n",
      "        #scraping readme by going through each repository url\n",
      "        import requests\n",
      "        from bs4 import BeautifulSoup\n",
      "        readme=requests.get(url)\n",
      "        soup =BeautifulSoup(readme.text,\"html.parser\")\n",
      "        readme=soup.find(class_=\"markdown-body entry-content container-lg\")\n",
      "        #if readme is empty, recording as blank (NaN)\n",
      "        if readme == None:\n",
      "            readme =(\"\")\n",
      "        else: \n",
      "            readme=soup.find(class_=\"markdown-body entry-content container-lg\").get_text().replace('\\n',\" \")\n",
      "\n",
      "        dt.append({\"id\": id,\n",
      "                     \"name\": name,\n",
      "                     \"url\": url,\n",
      "                     \"created\": created,\n",
      "                     \"stars\": stars,\n",
      "                     \"watch\": watch,\n",
      "                     \"language\": language,\n",
      "                     \"forks\": forks,\n",
      "                     \"readme\":readme})\n",
      "    return dt\n",
      "100/9:\n",
      "#Step 4: Saving dataset in .csv file and displaying the dataset:\n",
      "\n",
      "def save_dt(dt):\n",
      "    '''\n",
      "    Saves desired data in a .csv file, returns a preview of the final dataset.\n",
      "\n",
      "            Parameters:\n",
      "                    dt (list): List obtained from save_column()\n",
      "                    \n",
      "            Returns:\n",
      "                    data.csv: Final dataset stored in working directory\n",
      "    ''' \n",
      "    import csv\n",
      "    import pandas as pd \n",
      "\n",
      "    with open(\"dt.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"id\", \"name\", \"url\", \"language\", \"created\", \"stars\", \"watch\", \"forks\", \"readme\"])\n",
      "\n",
      "        for item in dt:\n",
      "            writer.writerow([item['id'], item['name'], item['url'], item['language'], item['created'], item['stars'], item['watch'], item['forks'], item['readme']])\n",
      "\n",
      "    data = pd.read_csv(\"dt.csv\", delimiter= \";\")\n",
      "    return data\n",
      "100/10: ?extract_repo_list\n",
      "100/11: ?save_column\n",
      "100/12: ?save_dt\n",
      "100/13:\n",
      "#Wrapping up into one single function: Github Repository Finder (grf)\n",
      "def grf(term, d, h):\n",
      "    '''\n",
      "    Searches for repositories on GitHub, returns a .csv file of final dataset.\n",
      "\n",
      "            Parameters:\n",
      "                    term (str): A string defining the search term\n",
      "                    d (int): An integer defining the timeframe to search (in days)\n",
      "                    h (int): An integer defining the duration (in hours) of each query\n",
      "\n",
      "            Returns:\n",
      "                    data.csv: Final dataset stored in working directory\n",
      "                    .json file: A file of raw data stored in working directory\n",
      "                    log.txt: Log file recording all API calls and their status\n",
      "    \n",
      "    Check GRF documenation for more details on the function and its components.\n",
      "    ''' \n",
      "    filename = find_repo(term, d, h)\n",
      "    repo_list = extract_repo_list(filename)\n",
      "    dt = save_column(repo_list)\n",
      "    data = save_dt(dt)\n",
      "    return data\n",
      "100/14: ?grf\n",
      "100/15: grf(\"python\", 3, 8)\n",
      "100/16: grf(\"python\", 3, 8)\n",
      "100/17:\n",
      "import pandas as pd\n",
      "pd.read_csv(\"data/dt.csv\", delimiter= \";\",nrows=100)\n",
      "100/18:\n",
      "import pandas as pd\n",
      "pd.read_csv(\"data/dt.csv\", delimiter= \";\",nrows=10)\n",
      "102/1:\n",
      "import pandas as pd\n",
      "pd.read_csv(\"../data/dt.csv\", delimiter= \";\",nrows=10)\n",
      "106/1:\n",
      "# extracts all links (the a tag!), and prints the URL (href) to the screen;\n",
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find_all(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "106/2: soup\n",
      "106/3: res\n",
      "106/4: res.text\n",
      "106/5: soup\n",
      "106/6: res.text\n",
      "106/7: soup.find_all(\"a\")\n",
      "106/8: soup.find_all(\"a\")[0].attrs[\"href\"]\n",
      "106/9: soup.find_all(\"a\")[0]\n",
      "106/10: soup.find_all(\"a\")[0].attrs[\"href\"]\n",
      "106/11: soup.find_all(\"a\")\n",
      "106/12: soup.find_all(\"product_pod\")\n",
      "106/13: soup.find_all(class_ = \"product_pod\")\n",
      "106/14: len(soup.find_all(class_ = \"product_pod\"))\n",
      "106/15: soup.find_all(class_ = \"product_pod\")[0]\n",
      "106/16: soup.find_all(class_ = \"product_pod\")[0].find_all[\"h3\"]\n",
      "106/17: soup.find_all(class_ = \"product_pod\")[0].find_all(\"h3\")\n",
      "106/18: soup.find_all(class_ = \"product_pod\")[0].find_all(\"a\").attrs[\"href\"]\n",
      "106/19: soup.find_all(class_ = \"product_pod\")[0].find_all(\"a\")\n",
      "106/20: soup.find_all(class_ = \"product_pod\")[0].find(\"a\").attrs[\"href\"]\n",
      "106/21:\n",
      "# extracts all links (the a tag!), and prints the URL (href) to the screen;\n",
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "106/22:\n",
      "# extracts all links (the a tag!), and prints the URL (href) to the screen;\n",
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find_all(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "106/23: soup.find(\"a\")\n",
      "106/24: soup.find_all(\"a\")\n",
      "106/25: soup.findl(\"a\").link.attrs[\"href\"]\n",
      "106/26: soup.findl(\"a\").attrs[\"href\"]\n",
      "106/27: soup.findl(\"a\")\n",
      "106/28: soup.find(\"a\").attrs[\"href\"]\n",
      "106/29: soup.find_all(class_ = \"product_pod\")[1].find(\"a\").attrs[\"href\"]\n",
      "106/30:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "book_url = base_url + url_book[6:]\n",
      "book_url\n",
      "106/31: url_book = soup.find_all(class_ = \"product_pod\")[1].find(\"a\").attrs[\"href\"]\n",
      "106/32:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "book_url = base_url + url_book[6:]\n",
      "book_url\n",
      "106/33: url_book.replace(\"../../\",base_url)\n",
      "106/34: soup.find_all(class_ = \"product_pod\")[1]\n",
      "106/35: soup.find_all(class_ = \"product_pod\")\n",
      "106/36:\n",
      "books = soup.find_all(class_ = \"product_pod\")\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url_list = []\n",
      "for book in books:\n",
      "    url_book = book.find(\"a\").attrs[\"href\"]\n",
      "    url = url_book.replace(\"../../\", base_url)\n",
      "    url_list.append(url)\n",
      "url_list\n",
      "106/37:\n",
      "books = soup.find_all(class_ = \"product_pod\")\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url_list = []\n",
      "for book in books:\n",
      "    url_book = book.find(\"a\").attrs[\"href\"]\n",
      "    url = url_book.replace(\"../../\", base_url)\n",
      "    url_list.append(url)\n",
      "url_list\n",
      "106/38: soup.find_all(class_ = \"product_pod\")\n",
      "106/39: soup.find_all(class_ = \"product_pod\").find(\"img\")\n",
      "106/40: soup.find_all(class_ = \"product_pod\")[0].find(\"img\")\n",
      "106/41: soup.find_all(class_ = \"product_pod\")[0].find(\"img\").attrs['alt']\n",
      "106/42:\n",
      "books = soup.find_all(class_ = \"product_pod\")\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url_list = []\n",
      "\n",
      "for book in books:\n",
      "    url_book = book.find(\"a\").attrs[\"href\"]\n",
      "    url = url_book.replace(\"../../\", base_url)\n",
      "    book_title = book.find(\"img\").attrs['alt']\n",
      "    url_list.append({\"title\": book_title,\n",
      "                     \"url\": url})\n",
      "url_list\n",
      "106/43: next((book for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "106/44:\n",
      "books = soup.find_all(class_ = \"product_pod\")\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "book_list = []\n",
      "\n",
      "for book in books:\n",
      "    url_book = book.find(\"a\").attrs[\"href\"]\n",
      "    url = url_book.replace(\"../../\", base_url)\n",
      "    book_title = book.find(\"img\").attrs['alt']\n",
      "    book_list.append({\"title\": book_title,\n",
      "                     \"url\": url})\n",
      "book_list\n",
      "106/45: next((book for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "106/46: next((book[\"url\"] for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "106/47:\n",
      "next((book for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "\n",
      "#or:\n",
      "next((book[\"url\"] for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "106/48:\n",
      "next((book for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "\n",
      "#or:\n",
      "next((book[\"url\"] for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "106/49: ?enumerate\n",
      "106/50: enumerate(book_list)\n",
      "106/51: ?enumerate\n",
      "106/52:\n",
      "# iterate over the dictionaries and update URLs accordingly:\n",
      "\n",
      "for id, book in enumerate(book_list):\n",
      "    book[\"org_url\"] = (book[\"url\"]).replace('https://books.toscrape.com/catalogue/','../../')\n",
      "\n",
      "# show the first five elements\n",
      "book_list[0:5]\n",
      "106/53:\n",
      "# creating a list of page_url:\n",
      "\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = []\n",
      "\n",
      "for counter in range(1, 51):\n",
      "    full_url = f'{base_url}page-{counter}.html' \n",
      "    page_urls.append(full_url)\n",
      "106/54:\n",
      "# creating a list of page_url:\n",
      "\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "page_urls = []\n",
      "\n",
      "for counter in range(1, 51):\n",
      "    full_url = f'{base_url}page-{counter}.html' \n",
      "    page_urls.append(full_url)\n",
      "\n",
      "page_urls[0:5]\n",
      "106/55:\n",
      "quote_base = \"https://quotes.toscrape.com/page/\"\n",
      "url_list =[]\n",
      "\n",
      "for counter in range(1:11):\n",
      "    quote_url = f'{quote_base}{counter}/'\n",
      "    url_list.append(quote_url)\n",
      "\n",
      "url_list\n",
      "106/56:\n",
      "quote_base = \"https://quotes.toscrape.com/page/\"\n",
      "url_list =[]\n",
      "\n",
      "for counter in range(11):\n",
      "    quote_url = f'{quote_base}{counter}/'\n",
      "    url_list.append(quote_url)\n",
      "\n",
      "url_list\n",
      "106/57:\n",
      "quote_base = \"https://quotes.toscrape.com/page/\"\n",
      "url_list =[]\n",
      "\n",
      "for counter in range(1,11):\n",
      "    quote_url = f'{quote_base}{counter}/'\n",
      "    url_list.append(quote_url)\n",
      "\n",
      "url_list\n",
      "106/58:\n",
      "quote_base = \"https://quotes.toscrape.com/page/\"\n",
      "url_list =[]\n",
      "\n",
      "for counter in range(1,11):\n",
      "    quote_url = f'{quote_base}{counter}/'\n",
      "    url_list.append(quote_url)\n",
      "\n",
      "url_list\n",
      "106/59:\n",
      "# creating a list of page_url:\n",
      "def generate_url_list(base_url, num_page):\n",
      "    \n",
      "    page_urls = []\n",
      "\n",
      "    for counter in range(1, num_page + 1):\n",
      "        full_url = f'{base_url}page-{counter}.html' \n",
      "        page_urls.append(full_url)\n",
      "\n",
      "    return page_urls\n",
      "106/60:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 15\n",
      "generate_url_list(base_url, num_page)\n",
      "106/61:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 5\n",
      "generate_url_list(base_url, num_page)\n",
      "106/62:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 5\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "106/63:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    for page in pages\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #extract book information from the list of books\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"]\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url})\n",
      "    return book_list\n",
      "106/64:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #extract book information from the list of books\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"]\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url})\n",
      "    return book_list\n",
      "106/65:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 5\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "extract_books(pages)\n",
      "106/66:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 5\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "books = extract_books(pages)\n",
      "106/67: len(books)\n",
      "106/68: books[1]\n",
      "106/69:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 2\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "books = extract_books(pages)\n",
      "106/70: books[0:5]\n",
      "106/71:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    # collect all books on page_url\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #for each book on that page look up the title and url and store it in a list\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"] #search for a particular tag and its attribute\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            availability = book.find(\"p\", class_= \"instock availability\") #search for a particular class\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url,\n",
      "                             \"availability\": availability})\n",
      "    return book_list\n",
      "106/72:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 2\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "books = extract_books(pages)\n",
      "106/73: books[0:5]\n",
      "106/74:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    # collect all books on page_url\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #for each book on that page look up the title and url and store it in a list\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"] #search for a particular tag and its attribute\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            availability = book.find(\"p\", class_= \"icon-ok\") #search for a particular class\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url,\n",
      "                             \"availability\": availability})\n",
      "    return book_list\n",
      "106/75:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 2\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "books = extract_books(pages)\n",
      "106/76: books[0:5]\n",
      "106/77:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    # collect all books on page_url\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #for each book on that page look up the title and url and store it in a list\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"] #search for a particular tag and its attribute\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            availability = book.find(\"p\", class_= \"instock availability\") #search for a particular class\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url,\n",
      "                             \"availability\": availability})\n",
      "    return book_list\n",
      "106/78:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "soup\n",
      "106/79:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "soup.find_all(class_ = \"product_pod\")\n",
      "106/80:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "soup.find_all(class_ = \"product_pod\").find(class_= \"instock availability\")\n",
      "106/81:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "book = soup.find_all(class_ = \"product_pod\")\n",
      "106/82: book.find(class_= \"instock availability\")\n",
      "106/83: book\n",
      "106/84: len(book)\n",
      "106/85: book[0]\n",
      "106/86: book[0].find(class_= \"instock availability\")\n",
      "106/87: book[0].find(\"p\", class_= \"instock availability\")\n",
      "106/88: book[0]\n",
      "106/89: book[0]find(\"p\", class_= \"instock availability\").text\n",
      "106/90: book[0]find(\"p\", class_= \"instock availability\")\n",
      "106/91: book[0].find(\"p\", class_= \"instock availability\").text\n",
      "106/92:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    # collect all books on page_url\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #for each book on that page look up the title and url and store it in a list\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"] #search for a particular tag and its attribute\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            #search for a particular class\n",
      "            availability = book.find(\"p\", class_= \"instock availability\").text.replace(\"\\n\", \"\")\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url,\n",
      "                             \"availability\": availability})\n",
      "    return book_list\n",
      "106/93:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 5\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "books = extract_books(pages)\n",
      "106/94: books[0:5]\n",
      "106/95:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    # collect all books on page_url\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #for each book on that page look up the title and url and store it in a list\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"] #search for a particular tag and its attribute\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            #search for a particular class\n",
      "            availability = book.find(\"p\", class_= \"instock availability\").text.replace(\"\\n\", \"\").replace(\" \",\"\")\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url,\n",
      "                             \"availability\": availability})\n",
      "    return book_list\n",
      "106/96:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 5\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "books = extract_books(pages)\n",
      "106/97: books[0:5]\n",
      "107/1:\n",
      "# extracts all links (the a tag!), and prints the URL (href) to the screen;\n",
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find_all(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "107/2: soup.find_all(class_ = \"product_pod\")[0].find(\"a\").attrs[\"href\"]\n",
      "107/3: url_book = soup.find_all(class_ = \"product_pod\")[1].find(\"a\").attrs[\"href\"]\n",
      "107/4:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "book_url = base_url + url_book[6:]\n",
      "book_url\n",
      "107/5: url_book.replace(\"../../\",base_url)\n",
      "107/6:\n",
      "books = soup.find_all(class_ = \"product_pod\")\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url_list = []\n",
      "for book in books:\n",
      "    url_book = book.find(\"a\").attrs[\"href\"]\n",
      "    url = url_book.replace(\"../../\", base_url)\n",
      "    url_list.append(url)\n",
      "url_list\n",
      "107/7:\n",
      "books = soup.find_all(class_ = \"product_pod\")\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "book_list = []\n",
      "\n",
      "for book in books:\n",
      "    url_book = book.find(\"a\").attrs[\"href\"]\n",
      "    url = url_book.replace(\"../../\", base_url)\n",
      "    book_title = book.find(\"img\").attrs['alt']\n",
      "    book_list.append({\"title\": book_title,\n",
      "                     \"url\": url})\n",
      "book_list\n",
      "107/8:\n",
      "next((book for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "#or:\n",
      "next((book[\"url\"] for book in book_list if book[\"title\"] == \"A Light in the Attic\"), None)\n",
      "107/9:\n",
      "# iterate over the dictionaries and update URLs accordingly:\n",
      "\n",
      "for id, book in enumerate(book_list):\n",
      "    book[\"org_url\"] = (book[\"url\"]).replace('https://books.toscrape.com/catalogue/','../../')\n",
      "\n",
      "# show the first five elements\n",
      "book_list[0:5]\n",
      "107/10:\n",
      "# creating a list of page_url:\n",
      "def generate_url_list(base_url, num_page):\n",
      "    \n",
      "    page_urls = []\n",
      "\n",
      "    for counter in range(1, num_page + 1):\n",
      "        full_url = f'{base_url}page-{counter}.html' \n",
      "        page_urls.append(full_url)\n",
      "\n",
      "    return page_urls\n",
      "107/11:\n",
      "from time import sleep\n",
      "sleep(5)\n",
      "#sleep (2*60) --> sleep for 2 minutes\n",
      "107/12:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    # collect all books on page_url\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #for each book on that page look up the title and url and store it in a list\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"] #search for a particular tag and its attribute\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            #search for a particular class\n",
      "            availability = book.find(\"p\", class_= \"instock availability\").text.replace(\"\\n\", \"\").replace(\" \",\"\")\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url,\n",
      "                             \"availability\": availability})\n",
      "    return book_list\n",
      "107/13:\n",
      "base_url = \"https://books.toscrape.com/catalogue/category/books_1/\"\n",
      "num_page = 5\n",
      "pages = generate_url_list(base_url, num_page)\n",
      "books = extract_books(pages)\n",
      "107/14: books[0:5]\n",
      "107/15:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books_1/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "book = soup.find_all(class_ = \"product_pod\")\n",
      "107/16:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1'\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\").attrs[\"href\"].text == \"next\"\n",
      "107/17:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\").attrs[\"href\"].text == \"next\"\n",
      "107/18:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\").attrs[\"href\"]\n",
      "107/19:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup\n",
      "107/20:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup\n",
      "107/21:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\").attrs['href'].text\n",
      "107/22:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\").attrs['href']\n",
      "107/23:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\")\n",
      "107/24:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\").attrs['href']\n",
      "107/25:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\")[0]\n",
      "107/26:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\")[0].attrs['href']\n",
      "107/27:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\")[0].attrs['href'].text\n",
      "107/28:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\")[0].text\n",
      "107/29:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"a\")[19].text\n",
      "107/30:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_button = soup.find(\"p\", class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "\n",
      "check_next_page(url)\n",
      "107/31:\n",
      "url = 'https://books.toscrape.com/catalogue/page-1.html'\n",
      "\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "\n",
      "check_next_page(url)\n",
      "107/32:\n",
      "url = 'https://books.toscrape.com/catalogue/page-50.html'\n",
      "\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "\n",
      "check_next_page(url)\n",
      "107/33:\n",
      "url = 'https://books.toscrape.com/catalogue/page-50.html'\n",
      "\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else print(\"This is the last page\")\n",
      "\n",
      "check_next_page(url)\n",
      "107/34:\n",
      "def next_page_url(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser)\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    if next_button.find('a').attrs['href']!= None:\n",
      "                         next_page = next_button.find('a').attrs['href']\n",
      "                         return next_url = next_page.replace(\"../../\",base_url)              \n",
      "    else print(\"This is the last page\")\n",
      "107/35:\n",
      "def next_page_url(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    if next_button.find('a').attrs['href']!= None:\n",
      "                         next_page = next_button.find('a').attrs['href']\n",
      "                         return next_url = next_page.replace(\"../../\",base_url)              \n",
      "    else print(\"This is the last page\")\n",
      "107/36:\n",
      "def next_page_url(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    if next_button.find('a').attrs['href']!= None:\n",
      "        next_page = next_button.find('a').attrs['href']\n",
      "        next_url = next_page.replace(\"../../\",base_url)\n",
      "        return next_url\n",
      "    else print(\"This is the last page\")\n",
      "107/37:\n",
      "def next_page_url(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    if next_button.find('a').attrs['href']!= None:\n",
      "        next_page = next_button.find('a').attrs['href']\n",
      "        next_url = next_page.replace(\"../../\",base_url)\n",
      "        return next_url\n",
      "    else:\n",
      "        print(\"This is the last page\")\n",
      "107/38:\n",
      "url = 'https://books.toscrape.com/catalogue/page-10.html'\n",
      "next_page_url(url)\n",
      "107/39:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url = 'https://books.toscrape.com/catalogue/page-10.html'\n",
      "next_page_url(url)\n",
      "107/40:\n",
      "def next_page_url(url):\n",
      "    if url!= None:\n",
      "        next_page = url.replace(\"../../\", base_url)\n",
      "        return next_url\n",
      "    else:\n",
      "        print(\"This is the last page\")\n",
      "107/41:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    \n",
      "    #check next page:\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "\n",
      "check_next_page('https://books.toscrape.com/catalogue/page-10.html')\n",
      "107/42:\n",
      "def next_page_url(url):\n",
      "    if url!= None:\n",
      "        next_page = f'{base_url}{url}'\n",
      "        return next_url\n",
      "    else:\n",
      "        print(\"This is the last page\")\n",
      "107/43:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url = 'https://books.toscrape.com/catalogue/page-10.html'\n",
      "next_page_url(url)\n",
      "107/44:\n",
      "def next_page_url(url):\n",
      "    if url!= None:\n",
      "        next_page = f'{base_url}{url}'\n",
      "        return next_page\n",
      "    else:\n",
      "        print(\"This is the last page\")\n",
      "107/45:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url = 'https://books.toscrape.com/catalogue/page-10.html'\n",
      "next_page_url(url)\n",
      "107/46:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url = check_next_page('https://books.toscrape.com/catalogue/page-10.html')\n",
      "next_page_url(url)\n",
      "107/47:\n",
      "def next_page_url(url):\n",
      "    if url!= None:\n",
      "        next_page = base_url + url\n",
      "        return next_page\n",
      "    else:\n",
      "        print(\"This is the last page\")\n",
      "107/48:\n",
      "base_url = 'https://books.toscrape.com/catalogue/'\n",
      "url = check_next_page('https://books.toscrape.com/catalogue/page-10.html')\n",
      "next_page_url(url)\n",
      "107/49:\n",
      "def extract_all_books(page_url):\n",
      "    books = []\n",
      "    while page_url:\n",
      "        print(page_url)\n",
      "        for book in extract_book_urls([page_url]):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(page_url) != None: \n",
      "            page_url = \"https://books.toscrape.com/catalogue/category/books_1/\" + check_next_page(page_url)\n",
      "        else: \n",
      "            break\n",
      "        \n",
      "        # if \"page-4\" in page_url: break # (activate this if you don't want to run the entire loop)\n",
      "    return books\n",
      "107/50: extract_all_books(\"https://books.toscrape.com/catalogue/page-1.html\")\n",
      "107/51:\n",
      "def extract_all_books(page_url):\n",
      "    books = []\n",
      "    while page_url:\n",
      "        print(page_url)\n",
      "        for book in extract_books([page_url]):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(page_url) != None: \n",
      "            page_url = \"https://books.toscrape.com/catalogue/category/books_1/\" + check_next_page(page_url)\n",
      "        else: \n",
      "            break\n",
      "        \n",
      "        # if \"page-4\" in page_url: break # (activate this if you don't want to run the entire loop)\n",
      "    return books\n",
      "107/52: extract_all_books(\"https://books.toscrape.com/catalogue/page-1.html\")\n",
      "107/53: len(book_list)\n",
      "107/54: len(books)\n",
      "107/55: books = extract_all_books(\"https://books.toscrape.com/catalogue/page-1.html\")\n",
      "107/56: len(books)\n",
      "107/57: next((book for book in book_list if book[\"title\"] == 'The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics'), None)\n",
      "107/58:\n",
      "# extracting books from list of url\n",
      "\n",
      "def extract_books(pages):\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "\n",
      "    book_list = []\n",
      "\n",
      "    base_url = 'https://books.toscrape.com/catalogue/'\n",
      "\n",
      "    # collect all books on page_url\n",
      "    for page in pages:\n",
      "        res = requests.get(page)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        #extract all book information that is wrapped inside class \"product_pod\"\n",
      "        books = soup.find_all(class_ = \"product_pod\")\n",
      "\n",
      "        #for each book on that page look up the title and url and store it in a list\n",
      "        for book in books:\n",
      "            url_book = book.find(\"a\").attrs[\"href\"] #search for a particular tag and its attribute\n",
      "            url = url_book.replace(\"../../\", base_url)\n",
      "            book_title = book.find(\"img\").attrs['alt']\n",
      "            #search for a particular class\n",
      "            availability = book.find(\"p\", class_= \"instock availability\").text.replace(\"\\n\", \"\").replace(\" \",\"\")\n",
      "            book_list.append({\"title\": book_title,\n",
      "                             \"url\": url,\n",
      "                             \"availability\": availability})\n",
      "    return book_list\n",
      "107/59:\n",
      "def extract_all_books(page_url):\n",
      "    books = []\n",
      "    while page_url:\n",
      "        print(page_url)\n",
      "        for book in extract_books([page_url]):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(page_url) != None: \n",
      "            page_url = \"https://books.toscrape.com/catalogue/category/books_1/\" + check_next_page(page_url)\n",
      "        else: \n",
      "            break\n",
      "        \n",
      "        # if \"page-4\" in page_url: break # (activate this if you don't want to run the entire loop)\n",
      "    return books\n",
      "107/60: books = extract_all_books(\"https://books.toscrape.com/catalogue/page-49.html\")\n",
      "107/61: len(books)\n",
      "107/62: books[0]\n",
      "107/63:\n",
      "books_instock = [book for book in book_list if book[\"Availability\"] == \"Instock\"]\n",
      "len(books_instock)\n",
      "107/64:\n",
      "books_instock = [book for book in book_list if book[\"availability\"] == \"Instock\"]\n",
      "len(books_instock)\n",
      "107/65:\n",
      "books_instock = [book for book in books if book[\"availability\"] == \"Instock\"]\n",
      "len(books_instock)\n",
      "107/66: next((book for book in book_list if book[\"title\"] == 'On the Road (Duluoz Legend)'), None)\n",
      "107/67: next((book for book in books if book[\"title\"] == 'On the Road (Duluoz Legend)'), None)\n",
      "107/68: books[9]\n",
      "107/69:\n",
      "#### Checking if there is a specific word in title:\n",
      "len([book for book in books if \"boat\" in book[\"title\"].lower()])\n",
      "107/70:\n",
      "#### Checking if there is a specific word in title:\n",
      "len([book for book in books if \"road\" in book[\"title\"].lower()])\n",
      "107/71:\n",
      "#### Checking if there is a specific word in title:\n",
      "next[book for book in books if \"road\" in book[\"title\"].lower()])\n",
      "107/72:\n",
      "#### Checking if there is a specific word in title:\n",
      "next((book for book in books if \"road\" in book[\"title\"].lower()]), None)\n",
      "107/73:\n",
      "#### Checking if there is a specific word in title:\n",
      "next((book for book in books if \"road\" in book[\"title\"].lower()), None)\n",
      "107/74: len([book for book in books if \"boat\" in book[\"title\"].lower()])\n",
      "107/75: len([book for book in books if \"love\" in book[\"title\"].lower()])\n",
      "107/76:\n",
      "url = \"https://books.toscrape.com/catalogue/black-dust_976/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "107/77: soup\n",
      "107/78: soup.find_all(\"p\", class_=\"star-rating\")\n",
      "107/79: soup.find(\"p\", class_=\"star-rating\")\n",
      "107/80: soup.find(\"p\", class_=\"star-rating\").text\n",
      "107/81: soup.find(\"p\", class_=\"star-rating\")\n",
      "107/82: soup.find(\"p\", class_=\"star-rating\").find(\"i\")\n",
      "107/83: soup.find(\"p\", class_=\"star-rating\").find_all(\"i\")\n",
      "107/84: len(soup.find(\"p\", class_=\"star-rating\").find_all(\"i\"))\n",
      "107/85: soup.find(id=\"content_inner\")\n",
      "107/86: soup.find(id=\"content_inner\").find(class_=\"product_page\")\n",
      "107/87: soup.find(id=\"content_inner\").find(\"p\", class_=\"product_page\")\n",
      "107/88: soup.find(id=\"content_inner\").find(\"p\", class_=\"product_page\")\n",
      "107/89: soup.find(id=\"content_inner\").find(\"p\", class_=\"star-rating\")\n",
      "107/90: soup.find(id=\"content_inner\").find_all(\"p\", class_=\"star-rating\")\n",
      "107/91: soup.find(id=\"content_inner\").find_all(class_=\"star-rating\")\n",
      "107/92: soup.find(id=\"content_inner\").find(class_=\"star-rating\")\n",
      "107/93: soup.find(id=\"content_inner\").find(\"i\", class_=\"star-rating\")\n",
      "107/94: soup.find(id=\"content_inner\").find_all(\"i\", class_=\"star-rating\")\n",
      "107/95: soup.find(id=\"content_inner\").find_all(\"p\", class_=\"star-rating\")\n",
      "107/96: soup.find(id=\"content_inner\").find(\"p\", class_=\"star-rating\")\n",
      "107/97: soup.find(id=\"content_inner\").find_all(\"i\", class_=\"icon-star\")\n",
      "107/98: soup.find(id=\"content_inner\").find(\"i\", class_=\"icon-star\")\n",
      "107/99:\n",
      "url = \"https://books.toscrape.com/catalogue/black-dust_976/index.html\"\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "107/100: soup.find(\"article\")\n",
      "107/101: soup.find(\"article\").find_all(\"p\")\n",
      "107/102: soup.find(id=\"content_inner\")\n",
      "107/103: soup.find(id=\"content_inner\").find_all(\"p\")\n",
      "107/104: soup.find(\"article\").find_all(\"p\")\n",
      "107/105: soup.find(\"article\").find_all(\"p\")[3]\n",
      "107/106: soup.find(\"article\").find_all(\"p\")[3].get_text()\n",
      "107/107:\n",
      "def get_book_description(books):\n",
      "    book_descriptions = []\n",
      "    \n",
      "    for book in books: \n",
      "        page_url = book[\"url\"]\n",
      "\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        # tip: look at the Google Inspector screenshot below \n",
      "        description = soup.find(id=\"content_inner\").find_all(\"p\")[3].get_text()\n",
      "        title = soup.find(id=\"content_inner\").find('img')['alt']\n",
      "        book_descriptions.append({'url': page_url,\n",
      "                                  'title': title,\n",
      "                                  'description': description})\n",
      "    return book_descriptions\n",
      "\n",
      "book_descriptions = get_book_description(book_list[0:2])\n",
      "book_descriptions\n",
      "107/108:\n",
      "def get_book_description(books):\n",
      "    book_descriptions = []\n",
      "    \n",
      "    for book in books: \n",
      "        page_url = book[\"url\"]\n",
      "\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        # tip: look at the Google Inspector screenshot below \n",
      "        description = soup.find(id=\"content_inner\").find_all(\"p\")[3].get_text()\n",
      "        title = soup.find(id=\"content_inner\").find('img')['alt']\n",
      "        book_descriptions.append({'url': page_url,\n",
      "                                  'title': title,\n",
      "                                  'description': description})\n",
      "    return book_descriptions\n",
      "\n",
      "book_descriptions = get_book_description(book_list[0:1])\n",
      "book_descriptions\n",
      "107/109:\n",
      "def get_book_description(books):\n",
      "    book_descriptions = []\n",
      "    \n",
      "    for book in books: \n",
      "        page_url = book[\"url\"]\n",
      "\n",
      "        res = requests.get(page_url)\n",
      "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "        # tip: look at the Google Inspector screenshot below \n",
      "        description = soup.find(id=\"content_inner\").find_all(\"p\")[3].get_text()\n",
      "        title = soup.find(id=\"content_inner\").find('img')['alt']\n",
      "        book_descriptions.append({'url': page_url,\n",
      "                                  'title': title,\n",
      "                                  'description': description})\n",
      "    return book_descriptions\n",
      "\n",
      "book_descriptions = get_book_description(book_list[0:2])\n",
      "book_descriptions\n",
      "107/110:\n",
      "from datetime import datetime\n",
      "now = datetime.now()\n",
      "\n",
      "import csv \n",
      "\n",
      "with open(\"book_descriptions.csv\", \"a\") as csv_file: # <<- this is the line with the \"flag\"l see exercises below\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"description\", \"date_time\"])\n",
      "    now = datetime.now()\n",
      "    for book in book_descriptions: # here we reference the book_descriptions list - make sure it's loaded otherwise you get an error! (Cell > Run All Above)\n",
      "        writer.writerow([book['title'], book['description'], now])\n",
      "print('done!')\n",
      "110/1:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    \n",
      "    #check next page:\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "\n",
      "check_next_page(url)\n",
      "110/2:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "110/3:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    \n",
      "    #check next page:\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "110/4:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "check_next_page(url)\n",
      "110/5:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "check_next_page(url)\n",
      "110/6:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        for book in extract_books([url]):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(page_url) != None: \n",
      "            page_url = \"https://books.toscrape.com/catalogue/category/books/nonfiction_13/\" + check_next_page(page_url)\n",
      "        else: \n",
      "            break\n",
      "110/7:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "extract_all_books(url)\n",
      "110/8:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "books = soup.find_all(\"h3\")\n",
      "110/9:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "books = soup.find_all(\"h3\")\n",
      "110/10:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"h3\")\n",
      "110/11:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"h3\").text\n",
      "110/12:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"h3\")[0]\n",
      "110/13:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(\"h3\")[0].text\n",
      "110/14: soup.find(\"img\").attrs['alt']\n",
      "110/15: soup.find_all(\"img\").attrs['alt']\n",
      "110/16: soup.find_all(\"img\")\n",
      "110/17: soup.find_all(\"img\")[0].attrs['alt']\n",
      "110/18: soup.find_all(\"img\")\n",
      "110/19: soup.find_all(\"img\")['alt']\n",
      "110/20: soup.find_all(\"img\").text\n",
      "110/21: soup.find_all(\"img\")\n",
      "110/22: soup.find_all(class_=\"product_pod\")\n",
      "110/23: soup.find_all(class_=\"product_pod\")[0]\n",
      "110/24: soup.find_all(class_=\"product_pod\")[0].find(\"a\")['alt']\n",
      "110/25: soup.find_all(class_=\"product_pod\")[0].find(\"img\")['alt']\n",
      "110/26: soup.find_all(class_=\"product_pod\")[0]\n",
      "110/27: soup.find_all(class_=\"product_pod\")[0].find(class_=\"product_price\").text\n",
      "110/28: soup.find_all(class_=\"product_pod\")[0].find(class_=\"product_price\")\n",
      "110/29: soup.find_all(class_=\"product_pod\")[0].find(class_=\"price_color\")\n",
      "110/30: soup.find_all(class_=\"product_pod\")[0].find(class_=\"price_color\").text\n",
      "110/31:\n",
      "def extract_nonfiction_book(url):\n",
      "    nonfic_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for books in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text\n",
      "        nonfic_book.append({'title': title,\n",
      "                            'price': price})\n",
      "110/32:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "nonfic_book = extract_nonfiction_book(url)\n",
      "110/33:\n",
      "def extract_nonfiction_book(url):\n",
      "    nonfic_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text\n",
      "        nonfic_book.append({'title': title,\n",
      "                            'price': price})\n",
      "110/34:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "nonfic_book = extract_nonfiction_book(url)\n",
      "110/35: nonfic_book\n",
      "110/36:\n",
      "def extract_nonfiction_book(url):\n",
      "    nonfic_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text\n",
      "        nonfic_book.append({'title': title,\n",
      "                            'price': price})\n",
      "    return nonfic_book\n",
      "110/37:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "nonfic_book = extract_nonfiction_book(url)\n",
      "110/38: nonfic_book\n",
      "110/39:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        for book in extract_nonfiction_book([url]):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/nonfiction_13/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "110/40:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "extract_all_books(url)\n",
      "110/41:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        for book in extract_nonfiction_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/nonfiction_13/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "110/42:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "extract_all_books(url)\n",
      "110/43:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "nonfic_book_list = extract_all_books(url)\n",
      "110/44: nonfic_book_list\n",
      "110/45: len(nonfic_book_list)\n",
      "110/46:\n",
      "from datetime import datetime\n",
      "now = datetime.now()\n",
      "\n",
      "import csv \n",
      "\n",
      "with open(\"nonfic_book.csv\", \"a\") as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"date_time\"])\n",
      "    now = datetime.now()\n",
      "    for book in nonfic_book_list:\n",
      "        writer.writerow([book['title'], book['price'], now])\n",
      "        \n",
      "pd.read_csv(\"nonfic_book.csv\", delimiter= \";\")\n",
      "110/47:\n",
      "from datetime import datetime\n",
      "now = datetime.now()\n",
      "\n",
      "import csv \n",
      "\n",
      "with open(\"nonfic_book.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"date_time\"])\n",
      "    now = datetime.now()\n",
      "    for book in nonfic_book_list:\n",
      "        writer.writerow([book['title'], book['price'], now])\n",
      "        \n",
      "pd.read_csv(\"nonfic_book.csv\", delimiter= \";\")\n",
      "110/48:\n",
      "from datetime import datetime\n",
      "now = datetime.now()\n",
      "\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"nonfic_book.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"date_time\"])\n",
      "    now = datetime.now()\n",
      "    for book in nonfic_book_list:\n",
      "        writer.writerow([book['title'], book['price'], now])\n",
      "        \n",
      "pd.read_csv(\"nonfic_book.csv\", delimiter= \";\")\n",
      "110/49:\n",
      "def extract_nonfiction_book(url):\n",
      "    nonfic_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text.replace(\"Â\",\"\")\n",
      "        nonfic_book.append({'title': title,\n",
      "                            'price': price})\n",
      "    return nonfic_book\n",
      "110/50:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html'\n",
      "nonfic_book_list = extract_all_books(url)\n",
      "110/51: nonfic_book_list[0]\n",
      "110/52:\n",
      "from datetime import datetime\n",
      "now = datetime.now()\n",
      "\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"nonfic_book.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"date_time\"])\n",
      "    now = datetime.now()\n",
      "    for book in nonfic_book_list:\n",
      "        writer.writerow([book['title'], book['price'], now])\n",
      "        \n",
      "pd.read_csv(\"nonfic_book.csv\", delimiter= \";\")\n",
      "110/53: pd.read_csv(\"nonfic_book.csv\", delimiter= \";\")[0:6]\n",
      "110/54: pd.read_csv(\"nonfic_book.csv\", delimiter= \";\")[0:5]\n",
      "110/55:\n",
      "from datetime import datetime\n",
      "now = datetime.now()\n",
      "\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"nonfic_book.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"date_time\"])\n",
      "    now = datetime.now()\n",
      "    for book in nonfic_book_list:\n",
      "        writer.writerow([book['title'], book['price'], now])\n",
      "\n",
      "pd.read_csv(\"nonfic_book.csv\", delimiter= \";\")[0:5]\n",
      "114/1:\n",
      "import requests\n",
      "import json\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{mod}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups})\n",
      "114/2:\n",
      "import requests\n",
      "import json\n",
      "\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups})\n",
      "114/3:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item.get(\"id\")\n",
      "            selftext = item.get(\"selftext\")\n",
      "            downs = item.get('downs')\n",
      "            ups = item.get('ups')\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups})\n",
      "114/4: post_list\n",
      "114/5: post_list[10]\n",
      "114/6: post_list[0:5]\n",
      "114/7:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            url = item['url']\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups})\n",
      "114/8:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            url = item['url']\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups,\n",
      "                             \"url\":url})\n",
      "114/9: post_list[0:5]\n",
      "114/10:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            ext_url = item['url_overridden_by_dest']\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups,\n",
      "                             \"ext_url\":ext_url})\n",
      "114/11:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups)\n",
      "114/12:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            post_list.append({\"id\":id,\n",
      "                            \"selftext\":selftext,\n",
      "                            \"downs\": downs,\n",
      "                            \"ups\":ups})\n",
      "114/13: post_list[0:5]\n",
      "114/14:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"reddit.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"selftext\", \"downs\", \"ups\"])\n",
      "    now = datetime.now()\n",
      "    for post in post_list:\n",
      "        writer.writerow([post['id'], post['selftext'], post['downs'], post['ups'])\n",
      "\n",
      "pd.read_csv(\"reddit.csv\", delimiter= \";\")[0:5]\n",
      "114/15:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"reddit.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"selftext\", \"downs\", \"ups\"])\n",
      "    now = datetime.now()\n",
      "    for post in post_list:\n",
      "        writer.writerow([post['id'], post['selftext'], post['downs'], post['ups']])\n",
      "\n",
      "pd.read_csv(\"reddit.csv\", delimiter= \";\")[0:5]\n",
      "114/16:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"reddit.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"selftext\", \"downs\", \"ups\"])\n",
      "    for post in post_list:\n",
      "        writer.writerow([post['id'], post['selftext'], post['downs'], post['ups']])\n",
      "\n",
      "pd.read_csv(\"reddit.csv\", delimiter= \";\")[0:5]\n",
      "114/17: len(post_list)\n",
      "114/18:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"reddit.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"selftext\", \"downs\", \"ups\"])\n",
      "    for post in post_list:\n",
      "        writer.writerow([post['id'], post['selftext'], post['downs'], post['ups']])\n",
      "\n",
      "pd.read_csv(\"reddit.csv\", delimiter= \";\")[250:300]\n",
      "114/19:\n",
      "url = 'https://www.reddit.com/r/politics.json'\n",
      "response = requests.get(url, \n",
      "                        headers=headers, \n",
      "                        params={\"after\": after})\n",
      "json_response = response.json()\n",
      "114/20: json_response\n",
      "114/21: len(json_response)\n",
      "114/22:\n",
      "url = 'https://www.reddit.com/r/politics.json'\n",
      "response = requests.get(url, \n",
      "                        headers=headers)\n",
      "json_response = response.json()\n",
      "114/23: json_response\n",
      "114/24: json_response['children']\n",
      "114/25: json_response[0]['children']\n",
      "114/26: json_response[0]\n",
      "114/27: json_response\n",
      "114/28: json_response['data']\n",
      "114/29: json_response['data'][children]\n",
      "114/30: json_response['data']['children']\n",
      "114/31: len(json_response['data']['children'])\n",
      "114/32: json_response['data']['children']\n",
      "114/33: json_response['data']['children']['title']\n",
      "114/34: json_response['data']['children'][0]\n",
      "114/35: json_response['data']['children'][0]['title']\n",
      "114/36: json_response['data']['children'][0]\n",
      "114/37: json_response['data']['children'][0]['data']\n",
      "114/38: json_response['data']['children'][0]['data']['title']\n",
      "114/39:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            title = item['data']['title']\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            post_list.append({\"id\":id,\n",
      "                              \"title\": title,\n",
      "                              \"selftext\":selftext,\n",
      "                              \"downs\": downs,\n",
      "                              \"ups\":ups})\n",
      "114/40:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            item_data = item['data']\n",
      "            title = item_data['title']\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            post_list.append({\"id\":id,\n",
      "                              \"title\": title,\n",
      "                              \"selftext\":selftext,\n",
      "                              \"downs\": downs,\n",
      "                              \"ups\":ups})\n",
      "114/41: json_response['data']['children'][0]['data']\n",
      "114/42: json_response['data']['children']\n",
      "114/43: json_response['data']['children'][0]\n",
      "114/44: json_response['data']['children'][0]['data']\n",
      "114/45: json_response['data']['children'][0]['selftext']\n",
      "114/46:\n",
      "import requests\n",
      "import json\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "response = requests.get(url, headers=headers)\n",
      "after = None\n",
      "post_list = []\n",
      "term = [\"politics\",\"science\"]\n",
      "\n",
      "# create an empty json file\n",
      "f= open(\"reddit.json\",'w',encoding='utf-8')\n",
      "f.close()\n",
      "\n",
      "for term in term:\n",
      "    for counter in range(10):\n",
      "        url = f'https://www.reddit.com/r/{term}.json'\n",
      "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
      "        response = requests.get(url, \n",
      "                                headers=headers, \n",
      "                                params={\"after\": after})\n",
      "        json_response = response.json()\n",
      "        after = json_response['data']['after'] \n",
      "\n",
      "        #writing all request in the json file (filename):\n",
      "        converted_to_string=json.dumps(json_response)\n",
      "        f=open('reddit.json','a',encoding='utf-8')\n",
      "        f.write(converted_to_string + '\\n')\n",
      "        f.close()\n",
      "        \n",
      "        # loop over all items in a request\n",
      "        for i in json_response['data']['children']:\n",
      "            item = i['data']\n",
      "            id = item[\"id\"]\n",
      "            title = item['title']\n",
      "            selftext = item[\"selftext\"]\n",
      "            downs = item['downs']\n",
      "            ups = item['ups']\n",
      "            post_list.append({\"id\":id,\n",
      "                              \"title\": title,\n",
      "                              \"selftext\":selftext,\n",
      "                              \"downs\": downs,\n",
      "                              \"ups\":ups})\n",
      "114/47:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"reddit.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\", \"selftext\", \"downs\", \"ups\"])\n",
      "    for post in post_list:\n",
      "        writer.writerow([post['id'], post['selftext'], post['downs'], post['ups']])\n",
      "\n",
      "pd.read_csv(\"reddit.csv\", delimiter= \";\")[250:260]\n",
      "114/48: post_list[0]\n",
      "114/49:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"reddit.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"id\",\"title\", \"selftext\", \"downs\", \"ups\"])\n",
      "    for post in post_list:\n",
      "        writer.writerow([post['id'], post['title'], post['selftext'], post['downs'], post['ups']])\n",
      "\n",
      "pd.read_csv(\"reddit.csv\", delimiter= \";\")[250:260]\n",
      "118/1:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append(item['data']['author'])\n",
      "    return users\n",
      "\n",
      "users = get_users('marketing')\n",
      "users\n",
      "118/2:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append({\"name\": item['data']['author'],\n",
      "                     \"community\": subreddit})\n",
      "    return users\n",
      "118/3: get_users('marketing')\n",
      "118/4:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append({\"name\": item['data']['author'],\n",
      "                     \"subreddit\": subreddit})\n",
      "    return users\n",
      "118/5: get_users('marketing')\n",
      "118/6:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "\n",
      "user_list =[]\n",
      "for sub in subreddits:\n",
      "    user_list.append(get_users(subreddit))\n",
      "118/7:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "\n",
      "user_list =[]\n",
      "for sub in subreddits:\n",
      "    user_list.append(get_users(sub))\n",
      "118/8: user_list.append(get_users('marketing'))\n",
      "118/9: user_list\n",
      "118/10: user_list = get_users('marketing')\n",
      "118/11: user_list\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/12:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append({\"name\": item['data']['author'])\n",
      "    return users\n",
      "118/13:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append(item['data']['author'])\n",
      "    return users\n",
      "118/14: get_users('marketing')\n",
      "118/15:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "\n",
      "user_list =[]\n",
      "for sub in subreddits:\n",
      "    user_list.append(get_users(sub))\n",
      "118/16:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "\n",
      "user_list =[]\n",
      "for sub in subreddits:\n",
      "    get_users(sub)\n",
      "118/17: user_list = get_users('surfing')\n",
      "118/18: user_list\n",
      "118/19:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "\n",
      "for subreddit in subreddits:\n",
      "    get_users(subreddit)\n",
      "121/1:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "121/2:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    \n",
      "    #check next page:\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "121/3:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "121/4: soup.find_all(class_=\"product_pod\")\n",
      "121/5: soup.find_all(class_=\"product_pod\").find(\"p\", class_=\"star-rating\")\n",
      "121/6: soup.find_all(class_=\"product_pod\")\n",
      "121/7: soup.find_all(class_=\"product_pod\").find(\"p\")\n",
      "121/8: soup.find_all(class_=\"product_pod\")\n",
      "121/9: soup.find_all(class_=\"product_pod\")[0]\n",
      "121/10: soup.find_all(class_=\"product_pod\")[0].find(\"p\")\n",
      "121/11: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"star-rating\")\n",
      "121/12: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"star-rating\").find_all(\"i\")\n",
      "121/13: len(soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"star-rating\").find_all(\"i\"))\n",
      "121/14: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"star-rating\").find_all(\"i\")\n",
      "121/15: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"star-rating\")\n",
      "121/16:\n",
      "def extract_mystery_book(url):\n",
      "    mystery_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text.replace(\"Â\",\"\")\n",
      "        rating = book.find(\"p\", class_=\"star-rating\")\n",
      "        mystery_book.append({'title': title,\n",
      "                            'price': price})\n",
      "    return mistery_book\n",
      "121/17:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "mystery_book_list = extract_all_books(url)\n",
      "121/18:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/19:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "mystery_book_list = extract_all_books(url)\n",
      "121/20:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    \n",
      "    #check next page:\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "121/21:\n",
      "def extract_mystery_book(url):\n",
      "    mystery_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text.replace(\"Â\",\"\")\n",
      "        rating = book.find(\"p\", class_=\"star-rating\")\n",
      "        mystery_book.append({'title': title,\n",
      "                            'price': price,\n",
      "                            'rating': rating})\n",
      "    return mistery_book\n",
      "121/22:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/23:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "mystery_book_list = extract_all_books(url)\n",
      "121/24:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "extract_mystery_book(url)\n",
      "121/25:\n",
      "def extract_mystery_book(url):\n",
      "    mystery_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text.replace(\"Â\",\"\")\n",
      "        rating = book.find(\"p\", class_=\"star-rating\")\n",
      "        mystery_book.append({'title': title,\n",
      "                            'price': price,\n",
      "                            'rating': rating})\n",
      "    return mystery_book\n",
      "121/26:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/27:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "extract_all_books(url)\n",
      "121/28:\n",
      "def extract_mystery_book(url):\n",
      "    mystery_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text.replace(\"Â\",\"\")\n",
      "        rating = len(book.find(\"p\", class_=\"star-rating\"))\n",
      "        mystery_book.append({'title': title,\n",
      "                            'price': price,\n",
      "                            'rating': rating})\n",
      "    return mystery_book\n",
      "121/29: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"star-rating\")\n",
      "121/30: soup.find_all(class_=\"product_pod\")[0].find(class_).\n",
      "121/31: soup.find_all(class_=\"product_pod\")[0].find(class_='').\n",
      "121/32: soup.find_all(class_=\"product_pod\")[0].find(class_='icon-star').\n",
      "121/33: soup.find_all(class_=\"product_pod\")[0].find(class_='icon-star')\n",
      "121/34: soup.find_all(class_=\"product_pod\")[0].find(class_='star-rating')\n",
      "121/35: soup.find_all(class_=\"product_pod\")[6].find(class_='star-rating')\n",
      "121/36: soup.find_all(class_=\"product_pod\")[6].find(class_='star-rating')[0]\n",
      "121/37: soup.find_all(class_=\"product_pod\")[6].find(class_='star-rating')\n",
      "121/38:\n",
      "def extract_mystery_book(url):\n",
      "    mystery_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text.replace(\"Â\",\"\")\n",
      "        rating = len(book.find(\"p\", class_=\"star-rating\").find_all(\"i\"))\n",
      "        mystery_book.append({'title': title,\n",
      "                            'price': price,\n",
      "                            'rating': rating})\n",
      "    return mystery_book\n",
      "121/39:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/40:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "mystery_book = extract_all_books(url)\n",
      "121/41: mystery_book[0:5]\n",
      "121/42:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "121/43: soup.find_all(class_=\"product_pod\")\n",
      "121/44: soup.find_all(class_=\"product_pod\")[0]\n",
      "121/45: soup.find_all(class_=\"product_pod\")[0]find(\"p\", class_=\"star-rating\")\n",
      "121/46: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"star-rating\")\n",
      "121/47: soup.find_all(class_=\"product_pod\")[0].find(\"p\")\n",
      "121/48: soup.find_all(class_=\"product_pod\")[0].find(\"p\").get_text()\n",
      "121/49: soup.find_all(class_=\"product_pod\")[0].find(\"p\")\n",
      "121/50: soup.find_all(class_=\"product_pod\")[0].find(\"Four\")\n",
      "121/51: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"Four\")\n",
      "121/52: soup.find_all(class_=\"product_pod\")[0].find(\"p\", class_=\"One\")\n",
      "121/53:\n",
      "from datetime import datetime\n",
      "now = datetime.now()\n",
      "\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"books.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"rating\"])\n",
      "    now = datetime.now()\n",
      "    for book in mystery_book:\n",
      "        writer.writerow([book['title'], book['price'], book['rating']])\n",
      "\n",
      "pd.read_csv(\"books.csv\", delimiter= \";\")[0:5]\n",
      "121/54:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"books.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"rating\"])\n",
      "    for book in mystery_book:\n",
      "        writer.writerow([book['title'], book['price'], book['rating']])\n",
      "\n",
      "pd.read_csv(\"books.csv\", delimiter= \";\")[0:5]\n",
      "121/55:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        \n",
      "        with open(\"seeds.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/56:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "mystery_book = extract_all_books(url)\n",
      "121/57:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        \n",
      "        with open(\"seeds.csv\", \"a\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/58:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "mystery_book = extract_all_books(url)\n",
      "121/59:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        \n",
      "        with open(\"seeds.csv\", \"a\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/60:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        \n",
      "        with open(\"seeds.csv\", \"a\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/61:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "\n",
      "mystery_book = extract_all_books(url)\n",
      "121/62:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "121/63:\n",
      "def check_next_page(url):\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    \n",
      "    #check next page:\n",
      "    next_button = soup.find(class_= \"next\")\n",
      "    return next_button.find('a').attrs['href'] if next_button else None\n",
      "121/64:\n",
      "def extract_mystery_book(url):\n",
      "    mystery_book = []\n",
      "    res = requests.get(url)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "    for book in soup.find_all(class_=\"product_pod\"):\n",
      "        title = book.find(\"img\")['alt']\n",
      "        price = book.find(class_=\"price_color\").text.replace(\"Â\",\"\")\n",
      "        rating = len(book.find(\"p\", class_=\"star-rating\").find_all(\"i\"))\n",
      "        mystery_book.append({'title': title,\n",
      "                            'price': price,\n",
      "                            'rating': rating})\n",
      "    return mystery_book\n",
      "121/65:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        \n",
      "        with open(\"seeds.csv\", \"a\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/66:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "mystery_book = extract_all_books(url)\n",
      "121/67:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"books.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"rating\"])\n",
      "    for book in mystery_book:\n",
      "        writer.writerow([book['title'], book['price'], book['rating']])\n",
      "\n",
      "pd.read_csv(\"books.csv\", delimiter= \";\")[0:5]\n",
      "121/68:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        \n",
      "        with open(\"seeds.csv\", \"a\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/69:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "mystery_book = extract_all_books(url)\n",
      "121/70:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"books.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"rating\"])\n",
      "    for book in mystery_book:\n",
      "        writer.writerow([book['title'], book['price'], book['rating']])\n",
      "\n",
      "pd.read_csv(\"books.csv\", delimiter= \";\")[0:5]\n",
      "121/71: len(mystery_book)\n",
      "121/72:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    with open(\"seeds.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \";\")\n",
      "        writer.writerow([\"seeds\"])\n",
      "    while url:\n",
      "        print(url)\n",
      "        with open(\"seeds.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/73:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "mystery_book = extract_all_books(url)\n",
      "121/74:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"books.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"rating\"])\n",
      "    for book in mystery_book:\n",
      "        writer.writerow([book['title'], book['price'], book['rating']])\n",
      "\n",
      "pd.read_csv(\"books.csv\", delimiter= \";\")[0:5]\n",
      "121/75:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        with open(\"seeds.csv\", \"a\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/76:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "mystery_book = extract_all_books(url)\n",
      "121/77:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "121/78:\n",
      "def extract_all_books(url):\n",
      "    books = []\n",
      "    while url:\n",
      "        print(url)\n",
      "        with open(\"seeds.csv\", \"a\", encoding='utf-8', newline='') as csv_file:\n",
      "            writer = csv.writer(csv_file, delimiter = \";\")\n",
      "            writer.writerow([url])\n",
      "        \n",
      "        for book in extract_mystery_book(url):\n",
      "            books.append(book)\n",
      "        \n",
      "        if check_next_page(url) != None: \n",
      "            url = \"https://books.toscrape.com/catalogue/category/books/mystery_3/\" + check_next_page(url)\n",
      "        else: \n",
      "            break\n",
      "    return books\n",
      "121/79:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "mystery_book = extract_all_books(url)\n",
      "121/80:\n",
      "import csv \n",
      "import pandas as pd \n",
      "\n",
      "with open(\"books.csv\", \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"price\", \"rating\"])\n",
      "    for book in mystery_book:\n",
      "        writer.writerow([book['title'], book['price'], book['rating']])\n",
      "\n",
      "pd.read_csv(\"books.csv\", delimiter= \";\")[0:5]\n",
      "118/20:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append(item['data']['author'])\n",
      "    return users\n",
      "\n",
      "users = get_users('marketing')\n",
      "users\n",
      "118/21:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "\n",
      "for subreddit in subreddits:\n",
      "    get_users(subreddit)\n",
      "118/22:\n",
      "users = get_users('skating')\n",
      "users\n",
      "118/23:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "users = []\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append(item['data']['author'])\n",
      "    return users\n",
      "118/24:\n",
      "users = get_users('skating')\n",
      "users\n",
      "118/25:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append(item['data']['author'])\n",
      "    return users\n",
      "118/26:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "users = []\n",
      "\n",
      "for subreddit in subreddits:\n",
      "    users.append = get_users('skating')\n",
      "118/27:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "users = []\n",
      "\n",
      "for subreddit in subreddits:\n",
      "    users.append (get_users('skating'))\n",
      "118/28: users[1]\n",
      "118/29: len(users)\n",
      "118/30:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "\n",
      "for subreddit in subreddits:\n",
      "    users.append(get_users(subreddit))\n",
      "118/31:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "users = []\n",
      "for subreddit in subreddits:\n",
      "    users.append(get_users(subreddit))\n",
      "118/32:\n",
      "url = 'https://www.reddit.com/r/surfing.json'\n",
      "response = requests.get(url,headers=headers)\n",
      "json_response = response.json()\n",
      "118/33: json_response\n",
      "118/34: json_response['data']\n",
      "118/35:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append(item['data']['author'])\n",
      "    return users\n",
      "118/36:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        users.append(item['data']['author'])\n",
      "    return users\n",
      "118/37:\n",
      "users = get_users('skating')\n",
      "users\n",
      "118/38:\n",
      "subreddits = ['surfing','skating','horseriding', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "users = []\n",
      "for item in subreddits:\n",
      "    print(get_users(item))\n",
      "118/39:\n",
      "url = 'https://www.reddit.com/r/horseriding.json'\n",
      "response = requests.get(url,headers=headers)\n",
      "json_response = response.json()\n",
      "118/40: json_response['data']\n",
      "118/41: response\n",
      "118/42:\n",
      "subreddits = ['surfing','skating', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "users = []\n",
      "for item in subreddits:\n",
      "    users.append(get_users(item))\n",
      "118/43: users\n",
      "118/44: len(users)\n",
      "118/45:\n",
      "import requests\n",
      "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
      "\n",
      "def get_users(subreddit):\n",
      "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
      "    response = requests.get(url,\n",
      "                            headers=headers)\n",
      "    json_response = response.json()\n",
      "    users = []\n",
      "    # loop over all items in a request\n",
      "    for item in json_response['data']['children']:\n",
      "        user = item['data']['author']\n",
      "        subred = subreddit\n",
      "        users.append({\"user\":user,\n",
      "                      \"subred\":subreddit})\n",
      "    return users\n",
      "118/46:\n",
      "users = get_users('skating')\n",
      "users\n",
      "118/47:\n",
      "subreddits = ['surfing','skating', 'tennis', 'soccer', 'cooking', 'gaming']\n",
      "users = []\n",
      "for item in subreddits:\n",
      "    users.append(get_users(item))\n",
      "121/81:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "soup.find_all(class_=\"product_pod\")\n",
      "121/82:\n",
      "url = 'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "soup.find_all(class_=\"product_pod\")[0]\n",
      "121/83: soup.find_all(class_=\"product_pod\")[0].find(\"p\").attrs['class']\n",
      "121/84: soup.find_all(class_=\"product_pod\")[0].find(\"p\").attrs['class'].[1]\n",
      "121/85: soup.find_all(class_=\"product_pod\")[0].find(\"p\").attrs['class']\n",
      "121/86: soup.find_all(class_=\"product_pod\")[0].find(\"p\").attrs['class'][0]\n",
      "121/87: soup.find_all(class_=\"product_pod\")[0].find(\"p\").attrs['class'][1]\n",
      "123/1:\n",
      "import requests\n",
      "import lxml.html as lh\n",
      "import pandas as pd\n",
      "123/2:\n",
      "import requests\n",
      "import lxml.html as lh\n",
      "import pandas as pd\n",
      "123/3:\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "#Create a handle, page, to handle the contents of the website\n",
      "page = requests.get(url)\n",
      "#Store the contents of the website under doc\n",
      "doc = lh.fromstring(page.content)\n",
      "#Parse data that are stored between <tr>..</tr> of HTML\n",
      "tr_elements = doc.xpath('//tr')\n",
      "123/4:\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "#Create a handle, page, to handle the contents of the website\n",
      "page = requests.get(url)\n",
      "#Store the contents of the website under doc\n",
      "doc = lh.fromstring(page.content)\n",
      "#Parse data that are stored between <tr>..</tr> of HTML\n",
      "tr_elements = doc.xpath('//tr')\n",
      "123/5:\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "#Create a handle, page, to handle the contents of the website\n",
      "page = requests.get(url)\n",
      "#Store the contents of the website under doc\n",
      "doc = lh.fromstring(page.content)\n",
      "#Parse data that are stored between <tr>..</tr> of HTML\n",
      "tr_elements = doc.xpath('//tr')\n",
      "123/6: tr_elements\n",
      "123/7: page\n",
      "123/8: doc\n",
      "123/9: page.content\n",
      "123/10: page.header\n",
      "123/11:\n",
      "tr_elements = doc.xpath('//tr')\n",
      "#Create empty list\n",
      "col=[]\n",
      "i=0\n",
      "#For each row, store each first element (header) and an empty list\n",
      "for t in tr_elements[0]:\n",
      "    i+=1\n",
      "    name=t.text_content()\n",
      "    print '%d:\"%s\"'%(i,name)\n",
      "    col.append((name,[]))\n",
      "123/12:\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "#Create a handle, page, to handle the contents of the website\n",
      "page = requests.get(url)\n",
      "#Store the contents of the website under doc\n",
      "doc = lh.fromstring(page.content)\n",
      "#Parse data that are stored between <tr>..</tr> of HTML\n",
      "tr_elements = doc.xpath('//tr')\n",
      "123/13: [len(T) for T in tr_elements[:12]]\n",
      "123/14: tr_elements = doc.xpath('//tr')\n",
      "123/15: tr_elements = doc.xpath('//tr')\n",
      "123/16: tr_elements\n",
      "123/17: tr_elements[0]\n",
      "123/18:\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "#Create a handle, page, to handle the contents of the website\n",
      "page = requests.get(url)\n",
      "#Store the contents of the website under doc\n",
      "doc = lh.fromstring(page.content)\n",
      "#Parse data that are stored between <tr>..</tr> of HTML\n",
      "tr_elements = doc.xpath('//tr')\n",
      "123/19: tr_elements\n",
      "123/20: tr_elements[0]\n",
      "123/21: tr_elements[0].content\n",
      "123/22: tr_elements[0].content()\n",
      "123/23: tr_elements[0].text\n",
      "123/24: tr_elements[0].text_content()\n",
      "123/25: tr_elements[0]\n",
      "123/26: tr_elements[0].text_content()\n",
      "123/27: len(tr_elements[0])\n",
      "123/28: tr_elements[0][0]\n",
      "123/29: tr_elements[0][1]\n",
      "123/30:\n",
      "tr_elements = doc.xpath('//tr')\n",
      "#Create empty list\n",
      "col=[]\n",
      "i=0\n",
      "#For each row, store each first element (header) and an empty list\n",
      "for t in tr_elements[0]:\n",
      "    i+=1\n",
      "    name=t.text_content()\n",
      "    print '%d:\"%s\"'%(i,name)\n",
      "    col.append((name,[]))\n",
      "123/31:\n",
      "tr_elements = doc.xpath('//tr')\n",
      "#Create empty list\n",
      "col=[]\n",
      "i=0\n",
      "#For each row, store each first element (header) and an empty list\n",
      "for t in tr_elements[0]:\n",
      "    i+=1\n",
      "    name=t.text_content()\n",
      "    col.append((name,[]))\n",
      "123/32: col\n",
      "123/33: col[0]\n",
      "123/34: col[1]\n",
      "123/35: col\n",
      "123/36:\n",
      "#Since out first row is the header, data is stored on the second row onwards\n",
      "for j in range(1,len(tr_elements)):\n",
      "    #T is our j'th row\n",
      "    T=tr_elements[j]\n",
      "    \n",
      "    #If row is not of size 10, the //tr data is not from our table \n",
      "    if len(T)!=5:\n",
      "        break\n",
      "    \n",
      "    #i is the index of our column\n",
      "    i=0\n",
      "    \n",
      "    #Iterate through each element of the row\n",
      "    for t in T.iterchildren():\n",
      "        data=t.text_content() \n",
      "        #Check if row is empty\n",
      "        if i>0:\n",
      "        #Convert any numerical value to integers\n",
      "            try:\n",
      "                data=int(data)\n",
      "            except:\n",
      "                pass\n",
      "        #Append the data to the empty list of the i'th column\n",
      "        col[i][1].append(data)\n",
      "        #Increment i for the next column\n",
      "        i+=1\n",
      "123/37:\n",
      "Dict={title:column for (title,column) in col}\n",
      "df=pd.DataFrame(Dict)\n",
      "123/38: df.head()\n",
      "123/39: df.head(nrows = 100)\n",
      "123/40: df.head(100)\n",
      "123/41:\n",
      "tr_elements = doc.xpath('//tr')\n",
      "#Create empty list\n",
      "col=[]\n",
      "i=0\n",
      "#For each row, store each first element (header) and an empty list\n",
      "for t in tr_elements[0]:\n",
      "    i+=1\n",
      "    name=t.text_content().replace(\"\\n\",\"\")\n",
      "    col.append((name,[]))\n",
      "123/42: col\n",
      "123/43:\n",
      "#Since out first row is the header, data is stored on the second row onwards\n",
      "for j in range(1,len(tr_elements)):\n",
      "    #T is our j'th row\n",
      "    T=tr_elements[j]\n",
      "    \n",
      "    #If row is not of size 10, the //tr data is not from our table \n",
      "    if len(T)!=5:\n",
      "        break\n",
      "    \n",
      "    #i is the index of our column\n",
      "    i=0\n",
      "    \n",
      "    #Iterate through each element of the row\n",
      "    for t in T.iterchildren():\n",
      "        data=t.text_content() \n",
      "        #Check if row is empty\n",
      "        if i>0:\n",
      "        #Convert any numerical value to integers\n",
      "            try:\n",
      "                data=int(data)\n",
      "            except:\n",
      "                pass\n",
      "        #Append the data to the empty list of the i'th column\n",
      "        col[i][1].append(data)\n",
      "        #Increment i for the next column\n",
      "        i+=1\n",
      "123/44:\n",
      "Dict={title:column for (title,column) in col}\n",
      "df=pd.DataFrame(Dict)\n",
      "123/45: df.head(100)\n",
      "123/46: len(df)\n",
      "123/47:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "# return the href attribute in the <a> tag nested within the first product class element\n",
      "for link in soup.find_all(\"a\"): \n",
      "    print(link.attrs[\"href\"])\n",
      "123/48:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "\n",
      "soup.find_all(class_=\"f3-widget-paginator\")[0].find(\"a\").attrs[\"href\"]\n",
      "123/49:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "123/50:\n",
      "url_list = []\n",
      "for i in range(104):\n",
      "    url_list.append(soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"])\n",
      "123/51:\n",
      "url_list = []\n",
      "for i in range(1:105):\n",
      "    url_list.append(soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"])\n",
      "123/52:\n",
      "url_list = []\n",
      "for i in range(1,105):\n",
      "    url_list.append(soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"])\n",
      "123/53:\n",
      "url_list = []\n",
      "for i in range(0,105):\n",
      "    url_list.append(soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"])\n",
      "123/54:\n",
      "url_list = []\n",
      "for i in range(0,104):\n",
      "    url_list.append(soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"])\n",
      "123/55:\n",
      "url_list = []\n",
      "for i in range(0,104):\n",
      "    url = soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"])\n",
      "    url_list.append(url)\n",
      "123/56:\n",
      "url_list = []\n",
      "for i in range(0,104):\n",
      "    url = soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"]\n",
      "    url_list.append(url)\n",
      "123/57:\n",
      "### url_list = []\n",
      "for i in range(0,103):\n",
      "    url = soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"]\n",
      "    url_list.append(url)\n",
      "123/58:\n",
      "### url_list = []\n",
      "for i in range(1,103):\n",
      "    url = soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"]\n",
      "    url_list.append(url)\n",
      "123/59:\n",
      "### url_list = []\n",
      "for i in range(10):\n",
      "    url = soup.find_all(class_=\"f3-widget-paginator\")[i].find(\"a\").attrs[\"href\"]\n",
      "    url_list.append(url)\n",
      "123/60: soup.find_all(class_=\"f3-widget-paginator\")\n",
      "123/61: len(soup.find_all(class_=\"f3-widget-paginator\"))\n",
      "123/62: soup.find_all(class_=\"f3-widget-paginator\").find_all(\"a\").attrs[\"href\"]\n",
      "123/63: soup.find_all(class_=\"f3-widget-paginator\").find(\"a\").attrs[\"href\"]\n",
      "123/64: soup.find_all(class_=\"f3-widget-paginator\")\n",
      "123/65: soup.find_all(class_=\"f3-widget-paginator\")[0]\n",
      "123/66: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\").attrs[\"href\"]\n",
      "123/67: soup.find_all(class_=\"f3-widget-paginator\")[0].find(\"a\").attrs[\"href\"]\n",
      "123/68: soup.find_all(class_=\"f3-widget-paginator\")[1].find(\"a\").attrs[\"href\"]\n",
      "123/69: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\").attrs[\"href\"]\n",
      "123/70: soup.find_all(class_=\"f3-widget-paginator\")[0]\n",
      "123/71: soup.find_all(class_=\"pagerLink\")\n",
      "123/72: len(soup.find_all(class_=\"pagerLink\"))\n",
      "123/73: soup.find_all(class_=\"pagerLink\")\n",
      "125/1:\n",
      "import requests\n",
      "import lxml.html as lh\n",
      "import pandas as pd\n",
      "125/2:\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "#Create a handle, page, to handle the contents of the website\n",
      "page = requests.get(url)\n",
      "#Store the contents of the website under doc\n",
      "doc = lh.fromstring(page.content)\n",
      "#Parse data that are stored between <tr>..</tr> of HTML\n",
      "tr_elements = doc.xpath('//tr')\n",
      "125/3:\n",
      "tr_elements = doc.xpath('//tr')\n",
      "#Create empty list\n",
      "col=[]\n",
      "i=0\n",
      "#For each row, store each first element (header) and an empty list\n",
      "for t in tr_elements[0]:\n",
      "    i+=1\n",
      "    name=t.text_content().replace(\"\\n\",\"\")\n",
      "    col.append((name,[]))\n",
      "125/4: col\n",
      "125/5:\n",
      "#Since out first row is the header, data is stored on the second row onwards\n",
      "for j in range(1,len(tr_elements)):\n",
      "    #T is our j'th row\n",
      "    T=tr_elements[j]\n",
      "    \n",
      "    #If row is not of size 10, the //tr data is not from our table \n",
      "    if len(T)!=5:\n",
      "        break\n",
      "    \n",
      "    #i is the index of our column\n",
      "    i=0\n",
      "    \n",
      "    #Iterate through each element of the row\n",
      "    for t in T.iterchildren():\n",
      "        data=t.text_content() \n",
      "        #Check if row is empty\n",
      "        if i>0:\n",
      "        #Convert any numerical value to integers\n",
      "            try:\n",
      "                data=int(data)\n",
      "            except:\n",
      "                pass\n",
      "        #Append the data to the empty list of the i'th column\n",
      "        col[i][1].append(data)\n",
      "        #Increment i for the next column\n",
      "        i+=1\n",
      "125/6:\n",
      "Dict={title:column for (title,column) in col}\n",
      "df=pd.DataFrame(Dict)\n",
      "125/7: df\n",
      "125/8:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/9: len(soup.find_all(class_=\"pagerLink\"))\n",
      "125/10: soup.find_all(class_=\"pagerLink\")[19]\n",
      "125/11: soup.find_all(class_=\"pagerLink\")[18]\n",
      "125/12: soup.find_all(class_=\"pagerLink\")[19]\n",
      "125/13: soup.find_all(class_=\"pagerLink\")[19].find(\"a\").attrs(\"href\")\n",
      "125/14: soup.find_all(class_=\"pagerLink\")[19].find(\"a\").attrs[\"href\"]\n",
      "125/15: soup.find_all(class_=\"pagerLink\").find(\"a\").attrs[\"href\"]\n",
      "125/16: soup.find_all(class_=\"pagerLink\")\n",
      "125/17: soup.find_all(class_=\"pagerLink\").attrs[\"href\"]\n",
      "125/18: soup.find_all(class_=\"pagerLink\").text\n",
      "125/19: soup.find_all(class_=\"pagerLink\")[0].text\n",
      "125/20: soup.find_all(class_=\"pagerLink\")[0]\n",
      "125/21: soup.find_all(class_=\"pagerLink\")[0].find(\"href\")\n",
      "125/22: soup.find_all(class_=\"pagerLink\")[0].find(\"a\")\n",
      "125/23: soup.find_all(class_=\"a\")\n",
      "125/24: soup.find_all(\"a\")\n",
      "125/25: soup.find_all(class_=\"f3-widget-paginator\")\n",
      "125/26: soup.find_all(class_=\"f3-widget-paginator\").find_all(\"a\")\n",
      "125/27: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "125/28: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\").attrs['href']\n",
      "125/29: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[1].attrs['href']\n",
      "125/30: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[0].attrs['href']\n",
      "125/31: len(soup.find_all(class_=\"f3-widget-paginator\")[0])\n",
      "125/32: len(soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[0])\n",
      "125/33: len(soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\"))\n",
      "125/34:\n",
      "url_list = []\n",
      "for item in soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\"):\n",
      "    url = item.attrs[\"href\"]\n",
      "    url_list.append(url)\n",
      "125/35:\n",
      "url_list = []\n",
      "for item in soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\"):\n",
      "    url = f'https://www.ohnegentechnik.org{item.attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "125/36: url_list\n",
      "125/37:\n",
      "url_list = []\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "for i in range(20):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "125/38: url_list\n",
      "125/39:\n",
      "url_list = []\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "for i in range(19):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "125/40: url_list\n",
      "125/41: len(url_list)\n",
      "125/42: page = url[len(url_list)-1]\n",
      "125/43:\n",
      "page = url[len(url_list)-1]\n",
      "page\n",
      "125/44:\n",
      "page = url_list[len(url_list)-1]\n",
      "page\n",
      "125/45:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/46:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/47: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "125/48: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[20]\n",
      "125/49: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[0]\n",
      "125/50: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[1]\n",
      "125/51: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[2]\n",
      "125/52: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "125/53: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11]\n",
      "125/54: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:18]\n",
      "125/55: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:19]\n",
      "125/56: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "125/57: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:21]\n",
      "125/58: soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "125/59:\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "for i in range(10):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "    page = url_list[len(url_list)-1]\n",
      "125/60: all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "125/61:\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "len(all_url)\n",
      "125/62:\n",
      "for i in range(9):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "    page = url_list[len(url_list)-1]\n",
      "125/63: url_list\n",
      "125/64: len(url_list)\n",
      "125/65: url_list\n",
      "125/66:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/67:\n",
      "url_list = []\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "for i in range(19):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "    page = url_list[len(url_list)-1]\n",
      "125/68: url_list\n",
      "125/69:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/70:\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "len(all_url)\n",
      "125/71:\n",
      "for i in range(9):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "    page = url_list[len(url_list)-1]\n",
      "125/72: url_list\n",
      "125/73: len(url_list)\n",
      "125/74: url_list\n",
      "125/75: page\n",
      "125/76:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "all_url\n",
      "125/77:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "all_url\n",
      "125/78:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "all_url\n",
      "125/79:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "for i in range(9):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "    page = url_list[len(url_list)-1]\n",
      "125/80: url_list\n",
      "125/81: len(url_list)\n",
      "125/82:\n",
      "while len(url_list) < 105:\n",
      "    res = requests.get(page)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "    for i in range(9):\n",
      "        url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "        url_list.append(url)\n",
      "        page = url_list[len(url_list)-1]\n",
      "125/83: len(url_list)\n",
      "125/84: url_list\n",
      "125/85:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/86:\n",
      "url_list = []\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "for i in range(19):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "    page = url_list[len(url_list)-1]\n",
      "125/87:\n",
      "while len(url_list) < 100:\n",
      "    res = requests.get(page)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "    for i in range(9):\n",
      "        url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "        url_list.append(url)\n",
      "        page = url_list[len(url_list)-1]\n",
      "125/88: url_list\n",
      "125/89: page\n",
      "125/90:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/91:\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "all_url\n",
      "125/92:\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "len(all_url)\n",
      "125/93: all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[17:19]\n",
      "125/94:\n",
      "\n",
      "all_url\n",
      "125/95:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[16:19]\n",
      "125/96: all_url\n",
      "125/97:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[17:19]\n",
      "125/98: all_url\n",
      "125/99:\n",
      "for i in range(2):\n",
      "        url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "        url_list.append(url)\n",
      "125/100: url_list\n",
      "125/101:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[17:20]\n",
      "for i in range(2):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "125/102: url_list\n",
      "125/103:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "125/104:\n",
      "url_list = []\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "for i in range(19):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "    page = url_list[len(url_list)-1]\n",
      "125/105:\n",
      "while len(url_list) < 100:\n",
      "    res = requests.get(page)\n",
      "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "    all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[11:20]\n",
      "    for i in range(9):\n",
      "        url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "        url_list.append(url)\n",
      "        page = url_list[len(url_list)-1]\n",
      "125/106:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")\n",
      "125/107: all_url[17:21]\n",
      "125/108: url_list\n",
      "125/109:\n",
      "res = requests.get(page)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "all_url = soup.find_all(class_=\"f3-widget-paginator\")[0].find_all(\"a\")[17:21]\n",
      "125/110:\n",
      "for i in range(3):\n",
      "    url = f'https://www.ohnegentechnik.org{all_url[i].attrs[\"href\"]}'\n",
      "    url_list.append(url)\n",
      "125/111: url_list\n",
      "125/112: len(url_list)\n",
      "125/113: url\n",
      "125/114: page1 = [\"https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche\"]\n",
      "125/115: page1.append(url_list)\n",
      "125/116: page1\n",
      "125/117: urls = [\"https://www.ohnegentechnik.org/fuer-verbraucher/wofuer-steht-das-ohne-gentechnik-siegel/wo-finde-ich-ohne-gentechnik-produkte/produktrecherche\"]\n",
      "125/118: urls.extend(url_list)\n",
      "125/119: urls\n",
      "125/120: len(urls)\n",
      "125/121:\n",
      "#Create a handle, page, to handle the contents of the website\n",
      "url= url_list[0]\n",
      "page = requests.get(url)\n",
      "#Store the contents of the website under doc\n",
      "doc = lh.fromstring(page.content)\n",
      "#Parse data that are stored between <tr>..</tr> of HTML\n",
      "tr_elements = doc.xpath('//tr')\n",
      "\n",
      "#Create empty list\n",
      "col=[]\n",
      "i=0\n",
      "#For each row, store each first element (header) and an empty list\n",
      "for t in tr_elements[0]:\n",
      "    i+=1\n",
      "    name=t.text_content().replace(\"\\n\",\"\")\n",
      "    col.append((name,[]))\n",
      "\n",
      "#Since out first row is the header, data is stored on the second row onwards\n",
      "for url in url_list:\n",
      "    page = requests.get(url)\n",
      "    doc = lh.fromstring(page.content)\n",
      "    tr_elements = doc.xpath('//tr')\n",
      "    for j in range(1,len(tr_elements)):\n",
      "        #T is our j'th row\n",
      "        T=tr_elements[j]\n",
      "\n",
      "        #If row is not of size 10, the //tr data is not from our table \n",
      "        if len(T)!=5:\n",
      "            break\n",
      "\n",
      "        #i is the index of our column\n",
      "        i=0\n",
      "\n",
      "        #Iterate through each element of the row\n",
      "        for t in T.iterchildren():\n",
      "            data=t.text_content() \n",
      "            #Check if row is empty\n",
      "            if i>0:\n",
      "            #Convert any numerical value to integers\n",
      "                try:\n",
      "                    data=int(data)\n",
      "                except:\n",
      "                    pass\n",
      "            #Append the data to the empty list of the i'th column\n",
      "            col[i][1].append(data)\n",
      "            #Increment i for the next column\n",
      "            i+=1\n",
      "125/122:\n",
      "Dict={title:column for (title,column) in col}\n",
      "df=pd.DataFrame(Dict)\n",
      "125/123: df\n",
      "125/124: df.to_csv(\"nonGMO.csv\")\n",
      "129/1:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://info.lidl/en'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "129/2:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://info.lidl/en'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "129/3: soup.find_all(class_=\"lidl-m-svg-map-marker__text\").text\n",
      "129/4: soup.find_all(class_=\"lidl-m-svg-map-marker__text\")\n",
      "129/5: soup.find_all(class_=\"lidl-m-svg-map-marker__text\")[1]\n",
      "129/6: soup.find_all(class_=\"lidl-m-svg-map-marker__text\")[1].text\n",
      "129/7:\n",
      "for i in soup.find_all(class_=\"lidl-m-svg-map-marker__text\"):\n",
      "    print i.text\n",
      "129/8:\n",
      "for i in soup.find_all(class_=\"lidl-m-svg-map-marker__text\"):\n",
      "    print (i.text)\n",
      "130/1:\n",
      "url='https://www.international.tiffany.com/jewelry-stores/store-list/'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "130/2:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "url='https://www.international.tiffany.com/jewelry-stores/store-list/'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "130/3: soup.find_all(class_=\"stores-filter__regions-content-dropdown-list\")\n",
      "131/1:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url='https://storelocator.yves-rocher.com/en/'\n",
      "res = requests.get(url)\n",
      "soup = BeautifulSoup(res.text, \"html.parser\")\n",
      "131/2: soup.find_all(\"option\")\n",
      "131/3:\n",
      "for i in soup.find_all(\"option\"):\n",
      "    print(i.text)\n",
      "133/1:\n",
      "import pandas as pd\n",
      "import requests\n",
      "import json\n",
      "133/2:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup\n",
      "import pandas as pd\n",
      "import requests\n",
      "import json\n",
      "133/3:\n",
      "#DE - Woman sportwear\n",
      "url1 = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "de1 = requests.get(url)\n",
      "soup_de1 = BeautifulSoup(res.text, \"html.parser\")\n",
      "133/4:\n",
      "#DE - Woman sportwear\n",
      "url1 = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "de1 = requests.get(url1)\n",
      "soup_de1 = BeautifulSoup(res.text, \"html.parser\")\n",
      "133/5:\n",
      "#DE - Woman sportwear\n",
      "url1 = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "de1 = requests.get(url1)\n",
      "soup_de1 = BeautifulSoup(de1.text, \"html.parser\")\n",
      "133/6:\n",
      "#DE - Woman sportwear\n",
      "url1 = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "de1 = requests.get(url1)\n",
      "soup_de1 = BeautifulSoup(de1.text, \"html.parser\")\n",
      "\n",
      "soup_de1\n",
      "133/7:\n",
      "#DE - Woman sportwear\n",
      "url1 = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "de1 = requests.get(url1)\n",
      "soup_de1 = BeautifulSoup(de1.text, \"html.parser\")\n",
      "\n",
      "de1\n",
      "133/8:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup\n",
      "import pandas as pd\n",
      "import requests\n",
      "from urllib.request import urlopen\n",
      "133/9:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "from urllib.request import urlopen\n",
      "133/10:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "from urllib.request import urlopen\n",
      "133/11:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "def getAndParseURL(url): \n",
      "result = requests.get(url)\n",
      "soup = bts(result.text, 'html.parser')\n",
      "return soup\n",
      "133/12:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url)\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "133/13:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url)\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "\n",
      "getAndParseURL(url)\n",
      "133/14:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "\n",
      "getAndParseURL(url)\n",
      "133/15:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/98.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "\n",
      "getAndParseURL(url)\n",
      "133/16:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "\n",
      "getAndParseURL(url)\n",
      "133/17:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 <- getAndParseURL(url)\n",
      "\n",
      "de1\n",
      "133/18:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "133/19:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 <- getAndParseURL(url)\n",
      "\n",
      "de1\n",
      "133/20:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "from urllib.request import urlopen\n",
      "133/21:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "de1\n",
      "133/22: print(de1.find('product-list')\n",
      "133/23:\n",
      "for i in de1.findAll(\"h2\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "133/24:\n",
      "for i in de1.findAll(\"strong\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "133/25:\n",
      "# Getting product category\n",
      "de1.find(\"h1\")\n",
      "\n",
      "# Getting product name\n",
      "for i in de1.findAll(\"h2\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "    \n",
      "# Getting brand name\n",
      "\n",
      "for i in de1.findAll(\"strong\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "133/26: de1.find(\"h1\")\n",
      "133/27: de1.find(\"h1\").text\n",
      "133/28: print(de1.find(\"h1\").text)\n",
      "133/29: print(de1.find(\"div\", {\"class\": \"plp-bar-info svelte-1uqvrhu\"}).text)\n",
      "133/30: print(de1.find(\"span\", {\"class\": \"svelte-1uqvrhu\"}).text)\n",
      "133/31: 4096/40\n",
      "136/1:\n",
      "# Getting product category\n",
      "print(de1.find(\"h1\").text)\n",
      "\n",
      "# Getting product name\n",
      "for i in de1.findAll(\"h2\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "    \n",
      "# Getting brand name\n",
      "\n",
      "for i in de1.findAll(\"strong\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "    \n",
      "help(findAll)\n",
      "136/2: help(findAll)\n",
      "136/3: ?findAll\n",
      "136/4:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "from urllib.request import urlopen\n",
      "136/5:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "136/6:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "136/7:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "de1\n",
      "136/8:\n",
      "# Getting product category\n",
      "print(de1.find(\"h1\").text)\n",
      "136/9:\n",
      "# Getting product name\n",
      "for i in de1.findAll(\"h2\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "136/10:\n",
      "# Getting product name\n",
      "for i in de1.find_all(\"h2\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "136/11:\n",
      "# Getting product name\n",
      "for i in de1.find_all(\"h2\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "136/12:\n",
      "# Getting brand name\n",
      "\n",
      "for i in de1.find_All(\"strong\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "136/13:\n",
      "# Getting brand name\n",
      "for i in de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "136/14:\n",
      "# Getting brand name\n",
      "for i in de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    i.text\n",
      "136/15:\n",
      "# Getting brand name\n",
      "for i in de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"}):\n",
      "    print(i.text)\n",
      "136/16:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "136/17:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "brand_name\n",
      "136/18:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "brand_name[1]\n",
      "136/19:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "brand_name[0]\n",
      "136/20:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "brand_name[0].text\n",
      "136/21:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "length(brand_name)\n",
      "136/22:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "len(brand_name)\n",
      "136/23:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "for name in brand_name:\n",
      "    print(brand_name.text)\n",
      "136/24:\n",
      "# Getting brand name\n",
      "brand_name = de1.find_all(\"strong\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "for name in brand_name:\n",
      "    print(name.text)\n",
      "136/25:\n",
      "# Getting product name\n",
      "prod_name = de1.find_all(\"h2\",{\"class\":\"svelte-wmr4s2\"})\n",
      "prod_name_list =[]\n",
      "\n",
      "for name in prod_name:\n",
      "    prod_name_list.append(name.text)\n",
      "136/26:\n",
      "# Getting product name\n",
      "prod_name = de1.find_all(\"h2\",{\"class\":\"svelte-wmr4s2\"})\n",
      "prod_name_list =[]\n",
      "\n",
      "for name in prod_name:\n",
      "    prod_name_list.append(name.text)\n",
      "    \n",
      "prod_name_list\n",
      "136/27:\n",
      "# Getting product name\n",
      "prod_name = de1.find_all(\"h2\",{\"class\":\"svelte-wmr4s2\"})\n",
      "prod_name_list =[]\n",
      "\n",
      "for name in prod_name:\n",
      "    prod_name_list.append(name.text)\n",
      "    \n",
      "len(prod_name_list)\n",
      "136/28:\n",
      "#Getting product name\n",
      "prod_name = de1.find_all(\"h2\",{\"class\":\"svelte-wmr4s2\"})\n",
      "\n",
      "#creating an empty array of product names\n",
      "prod_name_list =[]\n",
      "\n",
      "for name in prod_name:\n",
      "    prod_name_list.append(name.text)\n",
      "    \n",
      "len(prod_name_list)\n",
      "136/29: prod_name\n",
      "136/30: sport = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "136/31:\n",
      "sport = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "sport\n",
      "136/32:\n",
      "sport = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "len(sport)\n",
      "136/33:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "len(prod)\n",
      "\n",
      "type(prod)\n",
      "136/34:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "len(prod)\n",
      "136/35:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "len(prod)\n",
      "136/36:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[0]\n",
      "136/37:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[0].find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "136/38:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[0].find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "136/39:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[3].find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "136/40:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[3].find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "136/41:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[2].find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "136/42:\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = product.find(\"a\").attrs[\"href\"].text\n",
      "    prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'price': prod_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker})\n",
      "136/43:\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'price': prod_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker})\n",
      "136/44:\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'price': prod_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker})\n",
      "136/45:\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'price': prod_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker})\n",
      "136/46:\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'price': prod_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker})\n",
      "136/47: de1_list\n",
      "136/48: de1[3].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "136/49: prod[3].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "136/50: prod[3].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "136/51: prod[1].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "136/52: prod[1].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "136/53: prod[2].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "136/54: prod[3].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "136/55: prod[3].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text == None\n",
      "136/56:\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = product.find(\"a\").attrs[\"href\"]\n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'regular price': prod_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker})\n",
      "136/57: de1_list\n",
      "136/58:\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = product.find(\"a\").attrs[\"href\"]\n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "136/59: de1_list\n",
      "136/60:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}product.find(\"a\").attrs[\"href\"]'\n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "136/61: de1_list\n",
      "136/62:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{product.find(\"a\").attrs[\"href\"]}'\n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "136/63: de1_list\n",
      "140/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "140/2:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "140/3:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "de1\n",
      "140/4:\n",
      "# Getting product category\n",
      "print(de1.find(\"h1\").text)\n",
      "140/5:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{product.find(\"a\").attrs[\"href\"]}'\n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/6:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[2].find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "140/7:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{product.find(\"a\").attrs[\"href\"]}'\n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/8:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{product.find(\"a\").attrs[\"href\"]}'\n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/9: de1_list\n",
      "140/10: prod[1].find(\"div\").attrs[\"href\"]\n",
      "140/11: text = 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ'\n",
      "140/12:\n",
      "string = 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ'\n",
      "\n",
      "print(string.partition(\"?mc\")[2])\n",
      "140/13:\n",
      "string = 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ'\n",
      "\n",
      "print(string.partition(\"?mc=\")[2])\n",
      "140/14:\n",
      "string = 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ'\n",
      "\n",
      "print(string.partition(\"?mc=\")[2].rpartition('&')[0])\n",
      "140/15:\n",
      "string = 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ'\n",
      "\n",
      "print(string.partition(\"?mc=\")[2].rpartition('&')[0])\n",
      "140/16:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "print(string.partition(\"?mc=\")[2].rpartition('&')[0])\n",
      "140/17:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "string.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "140/18:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "string.partition(\"?mc=\")[2].rpartition('&')[0] == None\n",
      "140/19:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "print(string.partition(\"?mc=\")[2].rpartition('&')[0]) == None\n",
      "140/20:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "print(string.partition(\"?mc=\")[7])\n",
      "140/21:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "print(string.partition(\"?mc=\")[2].rpartition('&')[0]) == None\n",
      "140/22:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "print(string.partition(\"?mc=\")[2].rpartition('&')[0]) == None\n",
      "140/23:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2]\n",
      "140/24:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:6]\n",
      "140/25:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2]\n",
      "140/26:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:7]\n",
      "140/27:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][1:7]\n",
      "140/28:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:6]\n",
      "140/29:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0]\n",
      "140/30:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:1]\n",
      "140/31:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:2]\n",
      "140/32:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:7]\n",
      "140/33:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0]\n",
      "140/34:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][1]\n",
      "140/35:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][2]\n",
      "140/36:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][3]\n",
      "140/37:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][4]\n",
      "140/38:\n",
      "string = 'https://www.decathlon.de/p/top-fitness-baumwolle-damen-rosa/_/R-p-X8742703?mc=8742703'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:7]\n",
      "140/39:\n",
      "string = 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:7]\n",
      "140/40:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{product.find(\"a\").attrs[\"href\"]}'\n",
      "    prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/41: de1_list\n",
      "140/42:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    #prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0]) == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0])\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/43:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    #prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    \n",
      "    if print(link.partition(\"?mc=\")[2].rpartition('&')[0])) == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0])\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/44:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    #prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0]) == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0])\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/45:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    #prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0])\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/46:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    #prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/47: de1_list\n",
      "140/48:\n",
      "string = 'https://www.decathlon.de/p/t-shirt-slim-fitness/_/R-p-160976?mc=8380104&c=SCHWARZ'\n",
      "\n",
      "string.partition(\"?mc=\")[2][0:7]\n",
      "140/49: string.replace('\\n',\" \")\n",
      "140/50: '8380104'.replace('\\n',\" \")\n",
      "140/51: '8380104\\n'.replace('\\n',\" \")\n",
      "140/52: '8380104\\n'.replace('\\n',\"\")\n",
      "140/53:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n',\"\")\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n',\"\")\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n',\"\")\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/54: de1_list\n",
      "140/55: '8380104\\n'.replace('\\n'&'*',\"\")\n",
      "140/56: '8380104\\n'.replace('\\n'|'*',\"\")\n",
      "140/57: '8380104\\n'.replace('\\n',\"\").replace('*',\"\")\n",
      "140/58: '8380104\\n**'.replace('\\n',\"\").replace('*',\"\")\n",
      "140/59:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n',\"\").replace('*',\"\")\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n',\"\").replace('*',\"\")\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n',\"\").replace('*',\"\")\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/60: de1_list\n",
      "140/61:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n',\"\").replace('*',\"\")\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n',\"\").replace('*',\"\")\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n',\"\").replace('*',\"\")\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n',\"\").replace('*',\"\")\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/62: de1_list\n",
      "140/63:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "140/64: de1_list\n",
      "143/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "143/2:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "143/3:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "de1\n",
      "143/4:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "143/5:\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "\n",
      "prod[2].find(\"div\", {\"class\": \"sticker svelte-15lojui\"})\n",
      "143/6:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "143/7: de1_list\n",
      "143/8: prod[0].find(\"a\").attrs[\"href\"]\n",
      "143/9:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        id1 = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        id1 = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "143/11:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        id1 = link.partition(\"?mc=\")[2]\n",
      "else:\n",
      "    id1 = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "143/12:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        id1 = link.partition(\"?mc=\")[2]\n",
      "else:\n",
      "    id1 = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "id1\n",
      "143/13:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        id1 = link.partition(\"?mc=\")[2]\n",
      "else:\n",
      "    id1 = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "143/14:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        id1 = link.partition(\"?mc=\")[2]\n",
      "else:\n",
      "    id1 = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "link\n",
      "143/15:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        id1 = link.partition(\"?mc=\")[2]\n",
      "else:\n",
      "    id1 = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "link.partition(\"?mc=\")[2].rpartition('&')[0] == None\n",
      "\n",
      "link\n",
      "143/16:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "if link.partition(\"?mc=\")[2].rpartition('&')[0] == None:\n",
      "        id1 = link.partition(\"?mc=\")[2]\n",
      "else:\n",
      "    id1 = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "link.partition(\"?mc=\")[2].rpartition('&')[0] == None\n",
      "143/17:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "143/18:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "143/19:\n",
      "link = prod[0].find(\"a\").attrs[\"href\"]\n",
      "\n",
      "link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "link.partition(\"?mc=\")[2]\n",
      "143/20:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if print(link.partition(\"?mc=\")[2].rpartition('&')[0]) == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "143/21:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if print(link.partition(\"?mc=\")[2].rpartition('&')[0]) == None:\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "143/22: de1_list\n",
      "143/23:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "143/24: de1_list\n",
      "143/25:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"span\",{\"class\":\"svelte-1uqvrhu\"})\n",
      "143/26:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"span\",{\"class\":\"svelte-1uqvrhu\"})\n",
      "\n",
      "total_prod\n",
      "143/27:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"span\",{\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "total_prod\n",
      "143/28:\n",
      "#Getting total products in the category\n",
      "de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"})\n",
      "143/29:\n",
      "#Getting total products in the category\n",
      "de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "143/30:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "total_prod/40\n",
      "143/31:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "int(total_prod)/40\n",
      "143/32:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "# No of pages\n",
      "round(int(total_prod)/40)\n",
      "143/33:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "# No of pages\n",
      "math_ceil(int(total_prod)/40)\n",
      "143/34:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "143/35:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "# No of pages\n",
      "math_ceil(int(total_prod)/40)\n",
      "143/36:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "# No of pages\n",
      "math.ceil(int(total_prod)/40)\n",
      "143/37:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "# No of pages\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "math.ceil(1.2)\n",
      "143/38:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "# No of pages\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "math.ceil(1.1)\n",
      "143/39:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "# No of pages\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "math.ceil(1.0)\n",
      "143/40:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "[1:10]\n",
      "143/41:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "print([1:10])\n",
      "143/42:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "print(1:10)\n",
      "143/43:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in 1:10:\n",
      "        print(i)\n",
      "143/44:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in (1:10):\n",
      "        print(i)\n",
      "143/45:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1:10):\n",
      "        print(i)\n",
      "143/46:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1,10):\n",
      "        print(i)\n",
      "143/47:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1, math.ceil(int(total_prod)/40)):\n",
      "        print(i)\n",
      "143/48:\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1, math.ceil(int(total_prod)/40)):\n",
      "        page = f'{page1}?from={40 * i}&size=40'\n",
      "        print(f'page{i + 1}: {page}')\n",
      "143/49:\n",
      "#Pagination\n",
      "page1 = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1, math.ceil(int(total_prod)/40)):\n",
      "        page = f'{page1}?from={40 * i}&size=40'\n",
      "        print(f'page{i + 1}: {page}')\n",
      "144/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "144/2:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "144/3:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "de1\n",
      "144/4:\n",
      "#Pagination\n",
      "page1 = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(0, math.ceil(int(total_prod)/40)):\n",
      "        page = f'{page1}?from={40 * i}&size=40'\n",
      "        print(f'page{i + 1}: {page}')\n",
      "144/5:\n",
      "#Pagination\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "url_list = [url]\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(0, math.ceil(int(total_prod)/40)):\n",
      "        url = f'{url}?from={40 * i}&size=40'\n",
      "        url_list.append(url)\n",
      "144/6:\n",
      "#Pagination\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "url_list = [url]\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(0, math.ceil(int(total_prod)/40)):\n",
      "        url = f'{url}?from={40 * i}&size=40'\n",
      "        url_list.append(url)\n",
      "        \n",
      "url_list\n",
      "144/7:\n",
      "#Pagination\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "url_list = [url]\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1, math.ceil(int(total_prod)/40)):\n",
      "        url = f'{url}?from={40 * i}&size=40'\n",
      "        url_list.append(url)\n",
      "        \n",
      "url_list\n",
      "144/8:\n",
      "#Pagination\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "url_list = [url]\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1, math.ceil(int(total_prod)/40)):\n",
      "        page = f'{url}?from={40 * i}&size=40'\n",
      "        url_list.append(page)\n",
      "        \n",
      "url_list\n",
      "144/9:\n",
      "#Pagination\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "url_list = [url]\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1, math.ceil(int(total_prod)/40)):\n",
      "        page = f'{url}?from={40 * i}&size=40'\n",
      "        url_list.append(page)\n",
      "144/10:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "145/1: %history -g\n",
      "145/2:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "145/3:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "145/4:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "145/5:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "145/6:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "145/7:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "145/8:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "145/9:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "145/10: de1_list\n",
      "144/11:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "144/12:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "144/13:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "144/14:\n",
      "#Pagination\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "url_list = [url]\n",
      "#Getting total products in the category\n",
      "total_prod = de1.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "\n",
      "#No of pages (rounding up all numbers)\n",
      "math.ceil(int(total_prod)/40)\n",
      "\n",
      "for i in range(1, math.ceil(int(total_prod)/40)):\n",
      "        page = f'{url}?from={40 * i}&size=40'\n",
      "        url_list.append(page)\n",
      "144/15:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for url in url_list:\n",
      "    de1 = getAndParseURL(url)\n",
      "    prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "    for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "144/16:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for url in url_list:\n",
      "    de1 = getAndParseURL(url)\n",
      "    prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "    for product in prod:\n",
      "        link = product.find(\"a\").attrs[\"href\"]\n",
      "        prod_cat = de1.find(\"h1\").text\n",
      "        prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "        brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "        prod_url = f'{de_base}{link}'\n",
      "\n",
      "        #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "        #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "        if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "            prod_id = link.partition(\"?mc=\")[2]\n",
      "        else:\n",
      "            prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "        if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "            reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "            act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "        else:\n",
      "            reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "            act_price = None\n",
      "\n",
      "        if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "            prod_sticker = None\n",
      "        else:\n",
      "            prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "        de1_list.append({'title': prod_title,\n",
      "                         'sku': prod_id,\n",
      "                         'regular price': reg_price,\n",
      "                         'actual price' : act_price,\n",
      "                         'brand': brand_name,\n",
      "                         'url' : prod_url,\n",
      "                         'sticker' : prod_sticker,\n",
      "                         'category' : prod_cat})\n",
      "144/17: de1_list\n",
      "144/18: len(de1_list)\n",
      "144/19:\n",
      "len(de1_list)\n",
      "\n",
      "total_prod\n",
      "144/20:\n",
      "len(de1_list)\n",
      "\n",
      "total_prod\n",
      "144/21:\n",
      "len(de1_list)\n",
      "\n",
      "total_prod\n",
      "\n",
      "prod_cat\n",
      "144/22:\n",
      "#Saving into csv:\n",
      "\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import time\n",
      "\n",
      "fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "filename = f'{prod_cat}_{total_prod}_{fetch_time}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "    for item in de1_list:\n",
      "        writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "\n",
      "pd.read_csv(filename, delimiter= \";\")\n",
      "144/23:\n",
      "#Saving into csv:\n",
      "\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import time\n",
      "\n",
      "fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "filename = f'{prod_cat}_{total_prod}_{fetch_time}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "    for item in de1_list:\n",
      "        writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "\n",
      "pd.read_csv(filename, delimiter= \";\", 10)\n",
      "144/24:\n",
      "#Saving into csv:\n",
      "\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import time\n",
      "\n",
      "fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "filename = f'{prod_cat}_{total_prod}_{fetch_time}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "    for item in de1_list:\n",
      "        writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "\n",
      "pd.read_csv(filename, delimiter= \";\", nrows=10)\n",
      "144/25:\n",
      "#Saving into csv:\n",
      "\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import time\n",
      "\n",
      "fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "filename = f'{prod_cat}_{total_prod}_{fetch_time}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "    for item in de1_list:\n",
      "        writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "\n",
      "pd.read_csv(filename, delimiter= \";\", nrows=20)\n",
      "144/26:\n",
      "len(de1_list)\n",
      "\n",
      "total_prod\n",
      "\n",
      "de1_list[17]\n",
      "144/27:\n",
      "#Saving into csv:\n",
      "\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import time\n",
      "\n",
      "country = 'DE'\n",
      "fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "filename = f'{country}_{prod_cat}_{total_prod}_{fetch_time}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \";\")\n",
      "    writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "    for item in de1_list:\n",
      "        writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "\n",
      "pd.read_csv(filename, delimiter= \";\", nrows=20)\n",
      "144/28:\n",
      "#Saving into csv:\n",
      "\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import time\n",
      "\n",
      "country = 'DE'\n",
      "fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "filename = f'{country}_{prod_cat}_{total_prod}_{fetch_time}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "    for item in de1_list:\n",
      "        writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "\n",
      "pd.read_csv(filename, delimiter= \",\", nrows=20)\n",
      "144/29:\n",
      "#Saving into csv:\n",
      "\n",
      "from datetime import datetime, timedelta\n",
      "from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import time\n",
      "\n",
      "country = 'DE'\n",
      "fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "filename = f'{country}_{prod_cat}_{total_prod}_{fetch_time}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "    for item in de1_list:\n",
      "        writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "\n",
      "pd.read_csv(filename, delimiter= \",\", nrows=20)\n",
      "147/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "147/2:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "147/3:\n",
      "# Function to get main data\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "\n",
      "def getDecathlonData(country_url, url_list):\n",
      "    prod_list = []\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. abse)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat})\n",
      "            \n",
      "    print(f'{len(prod_list)} products in the {prod_cat} category have been saved!)\n",
      "        \n",
      "    return prod_list\n",
      "147/4:\n",
      "# Function to get main data\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "\n",
      "def getDecathlonData(country_url, url_list):\n",
      "    prod_list = []\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. abse)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat})\n",
      "            \n",
      "    print(f'{len(prod_list)} products in the {prod_cat} category have been saved!')\n",
      "        \n",
      "    return prod_list\n",
      "147/5: datetime.date()\n",
      "147/6:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "import csv\n",
      "#import time\n",
      "147/7: datetime.date()\n",
      "147/8:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "#from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import time\n",
      "147/9: datetime.date()\n",
      "147/10: datetime.today()\n",
      "147/11: datetime.today().date\n",
      "147/12: datetime.date(datetime.today())\n",
      "147/13:\n",
      "datetime.date(datetime.today())\n",
      "\n",
      "fetch_time=f'{datetime.today()}'.replace(\":\",\"_\")\n",
      "147/14:\n",
      "fetch_date=f'{datetime.date(datetime.today())}'.replace(\":\",\"_\")\n",
      "fetch_date\n",
      "147/15:\n",
      "fetch_date=f'{datetime.date(datetime.today())}'\n",
      "fetch_date\n",
      "147/16:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "#from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import time\n",
      "147/17:\n",
      "#Function for parsing the URLs\n",
      "def cookSoup(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "147/18:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(cat_url, soup):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "    #No of pages (rounding up all numbers) = math.ceil(int(total_prod)/40)\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/40\n",
      "    for i in range(1, total_page)):\n",
      "            page = f'{url}?from={40 * i}&size=40'\n",
      "            url_list.append(page)\n",
      "    print (f'There are {total_prod} products ({total_page} pages) in the category')\n",
      "    return url_list\n",
      "147/19:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(cat_url, soup):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "    #No of pages (rounding up all numbers) = math.ceil(int(total_prod)/40)\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/40\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{url}?from={40 * i}&size=40'\n",
      "            url_list.append(page)\n",
      "    print (f'There are {total_prod} products ({total_page} pages) in the category')\n",
      "    return url_list\n",
      "147/20:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(cat_url, soup):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "    #No of pages (rounding up all numbers) = math.ceil(int(total_prod)/40)\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/40)\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{url}?from={40 * i}&size=40'\n",
      "            url_list.append(page)\n",
      "    print (f'There are {total_prod} products ({total_page} pages) in the category')\n",
      "    return url_list\n",
      "147/21:\n",
      "# Function to get main data\n",
      "def getDecathlonData(country_url, url_list):\n",
      "    prod_list = []\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. abse)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat})\n",
      "            \n",
      "    print(f'{len(prod_list)} products in the {prod_cat} category have been saved!')\n",
      "        \n",
      "    return prod_list\n",
      "147/22:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, prod_list)\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{prod_cat}_{total_prod}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{filename} saved')\n",
      "147/23:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{prod_cat}_{total_prod}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{filename} saved')\n",
      "147/24:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(cat_url, soup)\n",
      "prod_list = getDecathlonData(country_url, url_list)\n",
      "saveDecathlonData(country, prod_list)\n",
      "147/25:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(cat_url, soup):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "    #No of pages (rounding up all numbers) = math.ceil(int(total_prod)/40)\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/40)\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{cat_url}?from={40 * i}&size=40'\n",
      "            url_list.append(page)\n",
      "    print (f'There are {total_prod} products ({total_page} pages) in the category')\n",
      "    return url_list\n",
      "147/26:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(cat_url, soup)\n",
      "prod_list = getDecathlonData(country_url, url_list)\n",
      "saveDecathlonData(country, prod_list)\n",
      "147/27:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(cat_url, soup)\n",
      "prod_list = getDecathlonData(country_url, url_list)\n",
      "\n",
      "filename = f'{country}_{prod_cat}_{total_prod}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "filename\n",
      "#saveDecathlonData(country, prod_list)\n",
      "147/28:\n",
      "# Function to get main data\n",
      "def getDecathlonData(country_url, url_list):\n",
      "    prod_list = []\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. abse)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat})\n",
      "            \n",
      "    print(f'{len(prod_list)} products in the {prod_cat} category have been saved!')\n",
      "        \n",
      "    return prod_list, prod_cat\n",
      "147/29:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(cat_url, soup)\n",
      "prod_list = getDecathlonData(country_url, url_list)\n",
      "saveDecathlonData(country, prod_list, prod_cat)\n",
      "\n",
      "#saveDecathlonData(country, prod_list)\n",
      "147/30:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, prod_list, prod_cat):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{prod_cat}_{total_prod}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{filename} saved')\n",
      "147/31:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(cat_url, soup)\n",
      "prod_list = getDecathlonData(country_url, url_list)\n",
      "saveDecathlonData(country, prod_list, prod_cat)\n",
      "\n",
      "#saveDecathlonData(country, prod_list)\n",
      "147/32:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "cat = 'Sportbekleidung Damen'\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(cat_url, soup)\n",
      "prod_list = getDecathlonData(country_url, url_list)\n",
      "saveDecathlonData(country, prod_list)\n",
      "147/33:\n",
      "# Function to get main data\n",
      "def getDecathlonData(country_url, url_list):\n",
      "    prod_list = []\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. abse)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat})\n",
      "            \n",
      "    print(f'{len(prod_list)} products in the {prod_cat} category have been saved!')\n",
      "        \n",
      "    return prod_list\n",
      "147/34:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{cat}_{total_prod}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{filename} saved')\n",
      "147/35:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "cat = 'Sportbekleidung Damen'\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(cat_url, soup)\n",
      "prod_list = getDecathlonData(country_url, url_list)\n",
      "saveDecathlonData(country, prod_list)\n",
      "147/36:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{cat}_{total_prod}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{filename} saved')\n",
      "147/37:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "prod_cat = 'Sportbekleidung Damen'\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(cat_url, soup)\n",
      "prod_list = getDecathlonData(country_url, url_list)\n",
      "saveDecathlonData(country, prod_list)\n",
      "147/38:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "cat = 'Sportbekleidung Damen'\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(cat_url, soup)\n",
      "prod_list = getDecathlonData(country_url, url_list)\n",
      "saveDecathlonData(country, prod_list)\n",
      "147/39:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, prod_list, total_prod):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{cat}_{total_prod}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{filename} saved')\n",
      "147/40:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "cat = 'Sportbekleidung Damen'\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(cat_url, soup)\n",
      "prod_list = getDecathlonData(country_url, url_list)\n",
      "saveDecathlonData(country, prod_list, total_prod)\n",
      "147/41:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{cat}_{len(prod_list)}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{filename} saved')\n",
      "147/42:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, cat, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{cat}_{len(prod_list)}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{filename} saved')\n",
      "147/43:\n",
      "# Function to get main data\n",
      "def getDecathlonData(country_url, url_list):\n",
      "    prod_list = []\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. abse)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat})\n",
      "            \n",
      "    print(f'{len(prod_list)} products in the {prod_cat} category have been saved!')\n",
      "        \n",
      "    return prod_list\n",
      "147/44:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, cat, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{cat}_{len(prod_list)}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{filename} saved')\n",
      "147/45:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "cat = 'Sportbekleidung Damen'\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(cat_url, soup)\n",
      "prod_list = getDecathlonData(country_url, url_list)\n",
      "saveDecathlonData(country, cat, prod_list)\n",
      "147/46:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "cat = 'Sportbekleidung Damen'\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(soup, cat_url, country, cat)\n",
      "prod_list = getDecathlonData(country_url, url_list, country, cat)\n",
      "saveDecathlonData(country, cat, prod_list)\n",
      "147/47:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "#from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import time\n",
      "147/48:\n",
      "#Function for parsing the URLs\n",
      "def cookSoup(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "147/49:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(soup, cat_url, country, cat):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "    #No of pages (rounding up all numbers) = math.ceil(int(total_prod)/40)\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/40)\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{cat_url}?from={40 * i}&size=40'\n",
      "            url_list.append(page)\n",
      "    print (f'{country}_{cat}: There are {total_prod} products ({total_page} pages) in the category')\n",
      "    return url_list\n",
      "147/50:\n",
      "# Function to get main data\n",
      "def getDecathlonData(country_url, url_list, country, cat):\n",
      "    prod_list = []\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. abse)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat})\n",
      "            \n",
      "    print(f'{country}_{cat}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      "147/51:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, cat, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{cat}_{len(prod_list)}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{country}_{cat}: Data has been saved in {filename}')\n",
      "147/52:\n",
      "#DE - Woman sportwear\n",
      "cat_url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "country = 'DE'\n",
      "country_url = \"https://www.decathlon.de\"\n",
      "cat = 'Sportbekleidung Damen'\n",
      "\n",
      "soup = cookSoup(cat_url)\n",
      "url_list = pageCreation(soup, cat_url, country, cat)\n",
      "prod_list = getDecathlonData(country_url, url_list, country, cat)\n",
      "saveDecathlonData(country, cat, prod_list)\n",
      "147/53:\n",
      "# Importing country - category data\n",
      "de = pd.read_csv('cat_list.csv', header=None, index_col=0, squeeze=True).to_dict()\n",
      "\n",
      "print(de)\n",
      "147/54:\n",
      "# Importing country - category data\n",
      "de = pd.read_csv('cat_list.csv', header= TRUE, index_col=0, squeeze=True).to_dict()\n",
      "147/55:\n",
      "# Importing country - category data\n",
      "de = pd.read_csv('cat_list.csv', header=True, index_col=0, squeeze=True).to_dict()\n",
      "147/56:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open(cat_list.csv, 'r') as data:\n",
      "    for line in csv.DictReader(data):\n",
      "    print(line)\n",
      "147/57:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open(cat_list.csv, 'r') as data:\n",
      "    for line in csv.DictReader(data):\n",
      "        print(line)\n",
      "147/58:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as data:\n",
      "    for line in csv.DictReader(data):\n",
      "        print(line)\n",
      "147/59:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as data:\n",
      "    for line in csv.DictReader(data):\n",
      "        print(line)\n",
      "\n",
      "line\n",
      "147/60:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as data:\n",
      "    for line in csv.DictReader(data):\n",
      "        print(line)\n",
      "\n",
      "type(line)\n",
      "147/61:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as data:\n",
      "    for line in csv.DictReader(data):\n",
      "        print(line)\n",
      "\n",
      "types(line)\n",
      "147/62:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as data:\n",
      "    for line in csv.DictReader(data):\n",
      "        print(line)\n",
      "\n",
      "len(line)\n",
      "147/63:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as data:\n",
      "    for line in csv.DictReader(data):\n",
      "        print(line)\n",
      "\n",
      "type(line)\n",
      "147/64:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as data:\n",
      "    for line in csv.DictReader(data):\n",
      "        print(line)\n",
      "\n",
      "len(line)\n",
      "147/65:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        print(cat)\n",
      "147/66:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        print(cat)\n",
      "147/67: cat\n",
      "147/68: cat_data\n",
      "147/69:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "cat_list\n",
      "147/70: len(cat_list)\n",
      "147/71:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_list\n",
      "147/72:\n",
      "for category in cat_list:\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat)\n",
      "    saveDecathlonData(country, cat, prod_list)\n",
      "147/73:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "import csv\n",
      "import time\n",
      "147/74:\n",
      "#Function for parsing the URLs\n",
      "def cookSoup(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "147/75:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(soup, cat_url, country, cat):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "    #No of pages (rounding up all numbers) = math.ceil(int(total_prod)/40)\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/40)\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{cat_url}?from={40 * i}&size=40'\n",
      "            url_list.append(page)\n",
      "    print (f'{country}_{cat}: There are {total_prod} products ({total_page} pages) in the category')\n",
      "    return url_list\n",
      "147/76:\n",
      "# Function to get main data\n",
      "def getDecathlonData(country_url, url_list, country, cat):\n",
      "    prod_list = []\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. abse)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat})\n",
      "            \n",
      "    print(f'{country}_{cat}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      "147/77:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, cat, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{cat}_{len(prod_list)}_{fetch_date}.csv'.replace(\" \",\"_\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{country}_{cat}: Data has been saved in {filename}')\n",
      "147/78:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "cat_list\n",
      "147/79:\n",
      "# Importing country - category data\n",
      "import csv\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "147/80:\n",
      "for category in cat_list:\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat)\n",
      "    saveDecathlonData(country, cat, prod_list)\n",
      "147/81:\n",
      "for category in cat_list:\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat)\n",
      "    saveDecathlonData(country, cat, prod_list)\n",
      "147/82: cat_list[1]\n",
      "147/83: cat_list[1].country\n",
      "147/84: cat_list[1](country)\n",
      "147/85: cat_list[1]['country']\n",
      "147/86: cat_list[1]\n",
      "147/87:\n",
      "for category in cat_list:\n",
      "    country = category['country']\n",
      "    country_url = category['country url']\n",
      "    cat = category['cat']\n",
      "    cat_url = category['cat_url']\n",
      "        soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat)\n",
      "    saveDecathlonData(country, cat, prod_list)\n",
      "147/88:\n",
      "for category in cat_list:\n",
      "    country = category['country']\n",
      "    country_url = category['country url']\n",
      "    cat = category['cat']\n",
      "    cat_url = category['cat_url']\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat)\n",
      "    saveDecathlonData(country, cat, prod_list)\n",
      "147/89:\n",
      "for category in cat_list:\n",
      "    country = category['country']\n",
      "    country_url = category['country_url']\n",
      "    cat = category['cat']\n",
      "    cat_url = category['cat_url']\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat)\n",
      "    saveDecathlonData(country, cat, prod_list)\n",
      "147/90:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, cat, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{cat}_{len(prod_list)}_{fetch_date}.csv'.replace(\" \",\"_\").replace(\",\",\"\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['url']])\n",
      "    \n",
      "    return print(f'{country}_{cat}: Data has been saved in {filename}')\n",
      "148/1:\n",
      "# Opening the html file\n",
      "HTMLFile = open(\"Factiva.html\", \"r\")\n",
      "  \n",
      "# Reading the file\n",
      "index = HTMLFile.read()\n",
      "  \n",
      "# Creating a BeautifulSoup object and specifying the parser\n",
      "soup = bts(index, 'lxml')\n",
      "148/2:\n",
      "# Opening the html file\n",
      "HTMLFile = open(\"amz.html\", \"r\")\n",
      "  \n",
      "# Reading the file\n",
      "index = HTMLFile.read()\n",
      "  \n",
      "# Creating a BeautifulSoup object and specifying the parser\n",
      "soup = bts(index, 'lxml')\n",
      "148/3:\n",
      "# Opening the html file\n",
      "HTMLFile = open(\"amz.html\", \"r\", encoding='Latin1')\n",
      "  \n",
      "# Reading the file\n",
      "index = HTMLFile.read()\n",
      "  \n",
      "# Creating a BeautifulSoup object and specifying the parser\n",
      "soup = bts(index, 'lxml')\n",
      "148/4:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "148/5:\n",
      "# Opening the html file\n",
      "HTMLFile = open(\"amz.html\", \"r\", encoding='Latin1')\n",
      "  \n",
      "# Reading the file\n",
      "index = HTMLFile.read()\n",
      "  \n",
      "# Creating a BeautifulSoup object and specifying the parser\n",
      "soup = bts(index, 'lxml')\n",
      "148/6:\n",
      "# Opening the html file\n",
      "HTMLFile = open(\"amz.html\", \"r\", encoding='Latin1')\n",
      "  \n",
      "# Reading the file\n",
      "index = HTMLFile.read()\n",
      "  \n",
      "# Creating a BeautifulSoup object and specifying the parser\n",
      "soup = bts(index, 'lxml')\n",
      "\n",
      "soup\n",
      "148/7:\n",
      "# Opening the html file\n",
      "HTMLFile = open(\"amz.html\", \"r\", encoding='Latin1')\n",
      "  \n",
      "# Reading the file\n",
      "index = HTMLFile.read()\n",
      "  \n",
      "# Creating a BeautifulSoup object and specifying the parser\n",
      "soup = bts(index, 'lxml')\n",
      "148/8: factiva.find_all(class_=\"clsSplitter\")\n",
      "148/9:\n",
      "# Opening the html file\n",
      "HTMLFile = open(\"amz.html\", \"r\", encoding='Latin1')\n",
      "  \n",
      "# Reading the file\n",
      "index = HTMLFile.read()\n",
      "  \n",
      "# Creating a BeautifulSoup object and specifying the parser\n",
      "factiva = bts(index, 'lxml')\n",
      "148/10: factiva.find_all(class_=\"clsSplitter\")\n",
      "148/11:\n",
      "art <- factiva.find_all(class_=\"clsSplitter\")\n",
      "len(art)\n",
      "148/12:\n",
      "art <- factiva.find_all(class_=\"clsSplitter\")\n",
      "len(art)\n",
      "148/13:\n",
      "art = factiva.find_all(class_=\"clsSplitter\")\n",
      "len(art)\n",
      "148/14:\n",
      "art = factiva.find_all(class_=\"headline\")\n",
      "len(art)\n",
      "148/15:\n",
      "art = factiva.find(\"tbody\")\n",
      "len(art)\n",
      "148/16:\n",
      "art = factiva.find_all(\"tbody\")\n",
      "len(art)\n",
      "148/17:\n",
      "art = factiva.find(\"tbody\")\n",
      "len(art)\n",
      "148/18:\n",
      "art = factiva.find(\"tbody\")\n",
      "art\n",
      "148/19: art = factiva.find(\"tbody\")\n",
      "148/20: art = factiva.find(\"div\",{\"class\":\"headlines\"})\n",
      "148/21:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\"})\n",
      "len(art)\n",
      "148/22:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "len(art)\n",
      "148/23:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "art\n",
      "148/24:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", style:\"height: 416px;\"})\n",
      "art\n",
      "148/25:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", 'style':\"height: 416px;\"})\n",
      "art\n",
      "148/26:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", 'style':\"height: 416px;\"})\n",
      "len(art)\n",
      "148/27: art = factiva.find(\"div\",{\"class\":\"headlines\", \"style\":\"height: 416px;\"})\n",
      "148/28:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"style\":\"height: 416px;\"})\n",
      "art\n",
      "148/29:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"style\":\"height: 416px\"})\n",
      "art\n",
      "148/30:\n",
      "art = factiva.find_all(\"div\",{\"class\":\"headlines\", \"style\":\"height: 416px\"})\n",
      "art\n",
      "148/31:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "len(art)\n",
      "148/32:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "art[1]\n",
      "148/33:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "art[0]\n",
      "148/34:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "art\n",
      "148/35:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "art.find_all(\"tr\", {\"class\":'headline'})\n",
      "148/36:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "len(art.find_all(\"tr\", {\"class\":'headline'}))\n",
      "148/37:\n",
      "for article in art:\n",
      "    if article.find('div', {'class':'dedupHeadlines'}) != None:\n",
      "        art.remove(article)\n",
      "    else:\n",
      "        print(len(art))\n",
      "148/38:\n",
      "for article in art:\n",
      "    if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "        print(len(art))\n",
      "    else:\n",
      "        art.remove(article)\n",
      "148/39:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "len(art.find_all(\"tr\", {\"class\":'headline'}))\n",
      "\n",
      "art[1]\n",
      "148/40:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "len(art.find_all(\"tr\", {\"class\":'headline'}))\n",
      "\n",
      "art\n",
      "148/41:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "len(art.find_all(\"tr\", {\"class\":'headline'}))\n",
      "148/42:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "articles = art.find_all(\"tr\", {\"class\":'headline'})\n",
      "\n",
      "articles\n",
      "148/43:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "        print(len(art))\n",
      "    else:\n",
      "        art.remove(article)\n",
      "148/44:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "        print(len(art))\n",
      "    else:\n",
      "        article.remove(article)\n",
      "148/45:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "        print(len(art))\n",
      "    else:\n",
      "        articles.remove(article)\n",
      "148/46:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "        print(len(articles))\n",
      "    else:\n",
      "        articles.remove(article)\n",
      "148/47:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "articles = art.find_all(\"tr\", {\"class\":'headline'})\n",
      "\n",
      "len(articles)\n",
      "148/48:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "        print(len(articles))\n",
      "    else:\n",
      "        articles.remove(article)\n",
      "\n",
      "len(articles)\n",
      "148/49:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHeadlines'}) != None:\n",
      "        articles.remove(article)\n",
      "    else: \n",
      "        print(len(articles))\n",
      "148/50:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHeadlines'}) != None:\n",
      "        articles.remove(article)\n",
      "    else: \n",
      "        print(len(articles))\n",
      "\n",
      "len(articles)\n",
      "148/51:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "articles = art.find_all(\"tr\", {\"class\":'headline'})\n",
      "\n",
      "len(articles)\n",
      "148/52:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHeadlines'}) != None:\n",
      "        articles.remove(article)\n",
      "    else: \n",
      "        print(len(articles))\n",
      "\n",
      "len(articles)\n",
      "148/53: articles\n",
      "148/54: articles[1]\n",
      "148/55: articles[1].find(\"tr\", {\"class\":\"headline\"})\n",
      "148/56: articles[1]\n",
      "148/57: articles[1].find(\"a\").attrs[\"href\"]\n",
      "148/58: articles[1].find(\"a\").text\n",
      "148/59:\n",
      "#title\n",
      "for article in articles:\n",
      "    print(article.find(\"a\").text)\n",
      "148/60:\n",
      "#title\n",
      "for article in articles:\n",
      "    print(article: article.find(\"a\").text)\n",
      "148/61:\n",
      "#title\n",
      "for article in articles:\n",
      "    print(f'{article}. {article.find(\"a\").text'})\n",
      "148/62:\n",
      "#title\n",
      "for article in articles:\n",
      "    print(f'{article}. {article.find(\"a\").text}')\n",
      "148/63:\n",
      "#title\n",
      "for article in articles:\n",
      "    i=1\n",
      "    print(f'{i}. {article.find(\"a\").text}')\n",
      "    i = i+1\n",
      "148/64:\n",
      "#title\n",
      "i=1\n",
      "for article in articles:\n",
      "    print(f'{i}. {article.find(\"a\").text}')\n",
      "    i = i+1\n",
      "148/65:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "articles = art.find_all(\"tr\", {\"class\":'headline'})\n",
      "\n",
      "len(articles)\n",
      "148/66:\n",
      "#title\n",
      "i=1\n",
      "for article in articles:\n",
      "    print(f'{i}. {article.find(\"a\").text}')\n",
      "    i = i+1\n",
      "148/67:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHd'}) != None:\n",
      "        articles.remove(article)\n",
      "    else: \n",
      "        print(len(articles))\n",
      "\n",
      "len(articles)\n",
      "148/68:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHd'}) == None:\n",
      "        articles.remove(article)\n",
      "    else: \n",
      "        print(len(articles))\n",
      "\n",
      "len(articles)\n",
      "148/69:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHd'}) != None:\n",
      "        articles.remove(article)\n",
      "    else: \n",
      "        print(len(articles))\n",
      "\n",
      "len(articles)\n",
      "148/70:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "articles = art.find_all(\"tr\", {\"class\":'headline'})\n",
      "\n",
      "len(articles)\n",
      "148/71:\n",
      "for article in articles:\n",
      "    if article.find('div', {'class':'dedupHd'}) != None:\n",
      "        articles.remove(article)\n",
      "    else: \n",
      "        print(len(articles))\n",
      "\n",
      "len(articles)\n",
      "148/72:\n",
      "#title\n",
      "i=1\n",
      "for article in articles:\n",
      "    print(f'{i}. {article.find(\"a\").text}')\n",
      "    i = i+1\n",
      "148/73: articles[0].find(\"td\")\n",
      "148/74: articles[0].find_all(\"td\")\n",
      "148/75: articles[0].find_all(\"td\")[1]\n",
      "148/76: articles[0].find_all(\"td\")[2]\n",
      "148/77: articles[0].find_all(\"td\")[2].text\n",
      "148/78: articles[0].find_all(\"td\")[2]\n",
      "148/79: articles[0].find_all(\"td\")[2].find_all(\"a\")\n",
      "148/80: articles[0].find_all(\"td\")[2]\n",
      "148/81: articles[0].find_all(\"td\")[2].find(\"div\")\n",
      "148/82: articles[0].find_all(\"td\")[2].find(\"div\").text\n",
      "148/83: articles[0].find_all(\"td\")[2].find(\"div\").text\n",
      "148/84: articles[0].find_all(\"td\")[1].find(\"div\").text\n",
      "148/85: articles[0].find_all(\"td\")[2].find(\"div\").text\n",
      "148/86: articles[0].find_all(\"td\")\n",
      "148/87: articles[0]\n",
      "148/88: articles[0].find('td')\n",
      "148/89: articles[0].find('tr')\n",
      "148/90: articles[0]\n",
      "148/91:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "articles = art.find_all(\"tr\", {\"class\":'headline'})\n",
      "\n",
      "len(articles)\n",
      "148/92:\n",
      "#title\n",
      "amz =[]\n",
      "for article in articles:\n",
      "    title = article.find(\"a\").text\n",
      "    date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "148/93: amz\n",
      "148/94: articles[0].find(\"a\").text\n",
      "148/95: articles[0]\n",
      "148/96: articles[1].\n",
      "148/97: articles[1]\n",
      "148/98: articles[0]\n",
      "148/99: articles[1]\n",
      "148/100: articles[0:1]\n",
      "148/101: articles[0:2]\n",
      "148/102: articles[0]\n",
      "148/103: articles[0].find('td', {'class':'count'})\n",
      "148/104:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "articles = art.find_all(\"td\", {\"class\":'count'})\n",
      "\n",
      "len(articles)\n",
      "148/105: articles[0]\n",
      "148/106:\n",
      "art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "articles = art.find_all(\"tr\", {\"class\":'headline'})\n",
      "\n",
      "len(articles)\n",
      "148/107: articles[0]\n",
      "148/108: articles[0]\n",
      "148/109: articles[1]\n",
      "148/110: len(articles)\n",
      "148/111: articles[0]\n",
      "148/112: articles[0].find(\"a\").text\n",
      "148/113: articles[0].find_all(\"td\")[2].find(\"div\").text\n",
      "148/114: articles[0]\n",
      "148/115: articles[0].find('div', {'class':'dedupHeadlines'}).text\n",
      "148/116:\n",
      "#title\n",
      "factiva_list =[]\n",
      "for article in articles:\n",
      "    title = article.find(\"a\").text\n",
      "    date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "    \n",
      "    if articles.find('div', {'class':'dedupHeadlines'}).text == None:\n",
      "        dup = None\n",
      "    else:\n",
      "        dup = articles.find('div', {'class':'dedupHeadlines'}).text\n",
      "        \n",
      "    factiva_list.append({'title': title,\n",
      "                 'date': date,\n",
      "                 'dup': dup})\n",
      "148/117:\n",
      "#title\n",
      "factiva_list =[]\n",
      "for article in articles:\n",
      "    title = article.find(\"a\").text\n",
      "    date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "    \n",
      "    if articles.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "        dup = None\n",
      "    else:\n",
      "        dup = articles.find('div', {'class':'dedupHeadlines'}).text\n",
      "        \n",
      "    factiva_list.append({'title': title,\n",
      "                 'date': date,\n",
      "                 'dup': dup})\n",
      "148/118:\n",
      "#title\n",
      "factiva_list =[]\n",
      "for article in articles:\n",
      "    title = article.find(\"a\").text\n",
      "    date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "    \n",
      "    if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "        dup = None\n",
      "    else:\n",
      "        dup = articles.find('div', {'class':'dedupHeadlines'}).text\n",
      "        \n",
      "    factiva_list.append({'title': title,\n",
      "                 'date': date,\n",
      "                 'dup': dup})\n",
      "148/119:\n",
      "#title\n",
      "factiva_list =[]\n",
      "for article in articles:\n",
      "    title = article.find(\"a\").text\n",
      "    date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "    \n",
      "    if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "        dup = None\n",
      "    else:\n",
      "        dup = article.find('div', {'class':'dedupHeadlines'}).text\n",
      "        \n",
      "    factiva_list.append({'title': title,\n",
      "                 'date': date,\n",
      "                 'dup': dup})\n",
      "148/120: factiva_list\n",
      "148/121: len(factiva_list)\n",
      "148/122:\n",
      "comp = 'amz'\n",
      "fetch_date=f'{datetime.date(datetime.today())}'\n",
      "filename = f'{comp}_{len(art_list)}_{fetch_date}.csv'\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"title\", \"date\", \"dup\"])\n",
      "\n",
      "    for item in art_list:\n",
      "        writer.writerow([item['title'], item['date'], item['dup'])\n",
      "                         \n",
      "return print(f'Articles for {comp} has been saved in {filename}')\n",
      "148/123:\n",
      "comp = 'amz'\n",
      "fetch_date=f'{datetime.date(datetime.today())}'\n",
      "filename = f'{comp}_{len(art_list)}_{fetch_date}.csv'\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"title\", \"date\", \"dup\"])\n",
      "\n",
      "    for item in art_list:\n",
      "        writer.writerow([item['title'], item['date'], item['dup'])\n",
      "                         \n",
      "print(f'Articles for {comp} has been saved in {filename}')\n",
      "148/124:\n",
      "comp = 'amz'\n",
      "fetch_date=f'{datetime.date(datetime.today())}'\n",
      "filename = f'{comp}_{len(art_list)}_{fetch_date}.csv'\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"title\", \"date\", \"dup\"])\n",
      "\n",
      "    for item in art_list:\n",
      "        writer.writerow([item['title'], item['date'], item['dup']])\n",
      "                         \n",
      "print(f'Articles for {comp} has been saved in {filename}')\n",
      "148/125:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "#from dateutil.relativedelta import relativedelta\n",
      "import csv\n",
      "import time\n",
      "148/126:\n",
      "comp = 'amz'\n",
      "fetch_date=f'{datetime.date(datetime.today())}'\n",
      "filename = f'{comp}_{len(art_list)}_{fetch_date}.csv'\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"title\", \"date\", \"dup\"])\n",
      "\n",
      "    for item in art_list:\n",
      "        writer.writerow([item['title'], item['date'], item['dup']])\n",
      "                         \n",
      "print(f'Articles for {comp} has been saved in {filename}')\n",
      "148/127:\n",
      "#title\n",
      "art_list =[]\n",
      "\n",
      "for article in articles:\n",
      "    title = article.find(\"a\").text\n",
      "    date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "    \n",
      "    #identify duplicates\n",
      "    if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "        dup = None\n",
      "    else:\n",
      "        dup = article.find('div', {'class':'dedupHeadlines'}).text\n",
      "        \n",
      "    art_list.append({'title': title,\n",
      "                 'date': date,\n",
      "                 'dup': dup})\n",
      "148/128:\n",
      "comp = 'amz'\n",
      "fetch_date=f'{datetime.date(datetime.today())}'\n",
      "filename = f'{comp}_{len(art_list)}_{fetch_date}.csv'\n",
      "\n",
      "with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"title\", \"date\", \"dup\"])\n",
      "\n",
      "    for item in art_list:\n",
      "        writer.writerow([item['title'], item['date'], item['dup']])\n",
      "                         \n",
      "print(f'Articles for {comp} has been saved in {filename}')\n",
      "152/1:\n",
      "# Importing company list\n",
      "comp_list = []  \n",
      "\n",
      "# opening the file using \"with\" statement\n",
      "with open('comp-usa.csv', 'r') as comp_data:\n",
      "    for comp in csv.DictReader(comp_data):\n",
      "        comp_list.append(comp)\n",
      "        \n",
      "comp_list\n",
      "152/2:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "import csv\n",
      "import time\n",
      "152/3:\n",
      "# Importing company list\n",
      "comp_list = []  \n",
      "\n",
      "# opening the file using \"with\" statement\n",
      "with open('comp-usa.csv', 'r') as comp_data:\n",
      "    for comp in csv.DictReader(comp_data):\n",
      "        comp_list.append(comp)\n",
      "        \n",
      "comp_list\n",
      "152/4:\n",
      "# Importing company list\n",
      "comp_list = []  \n",
      "\n",
      "# opening the file using \"with\" statement\n",
      "with open('comp-usa.csv', 'r') as comp_data:\n",
      "    for comp in csv.DictReader(comp_data):\n",
      "        comp_list.append(comp)\n",
      "len(comp_list)\n",
      "152/5:\n",
      "# Importing company list\n",
      "comp_list = []  \n",
      "\n",
      "# opening the file using \"with\" statement\n",
      "with open('comp-usa.csv', 'r') as comp_data:\n",
      "    for comp in csv.DictReader(comp_data):\n",
      "        comp_list.append(comp)\n",
      "\n",
      "len(comp_list)\n",
      "152/6:\n",
      "def getArticles(filename, comp):\n",
      "\n",
      "    # Opening the html file\n",
      "    HTMLFile = open(filename, \"r\", encoding='Latin1')\n",
      "\n",
      "    # Reading the file\n",
      "    index = HTMLFile.read()\n",
      "\n",
      "    # Creating a BeautifulSoup object and specifying the parser\n",
      "    factiva = bts(index, 'lxml')\n",
      "    \n",
      "    # Get articles from soup\n",
      "    art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "    articles = art.find_all(\"tr\", {\"class\":'headline'})\n",
      "    \n",
      "    print(f'{comp}: {len(articles)} articles have been scraped')\n",
      "\n",
      "    return articles\n",
      "152/7:\n",
      "# Function to get all articles of a company:\n",
      "\n",
      "def getArticles(filename, comp):\n",
      "\n",
      "    # Opening the html file\n",
      "    HTMLFile = open(filename, \"r\", encoding='Latin1')\n",
      "\n",
      "    # Reading the file\n",
      "    index = HTMLFile.read()\n",
      "\n",
      "    # Creating a BeautifulSoup object and specifying the parser\n",
      "    factiva = bts(index, 'lxml')\n",
      "    \n",
      "    # Get articles from soup\n",
      "    art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "    articles = art.find_all(\"tr\", {\"class\":'headline'})\n",
      "    \n",
      "    print(f'{comp}: {len(articles)} articles have been scraped')\n",
      "\n",
      "    return articles\n",
      "152/8: comp_list[0]\n",
      "152/9:\n",
      "filename = 'AMZN.html'\n",
      "comp = 'AMZN'\n",
      "\n",
      "articles = getArticles(filename, comp)\n",
      "152/10:\n",
      "filename = 'AMZN.html'\n",
      "comp = 'AMZN'\n",
      "\n",
      "articles = getArticles(filename, comp)\n",
      "152/11:\n",
      "# Function saving articles into dictionary\n",
      "def saveArticles(articles, comp):\n",
      "    art_list =[]\n",
      "    \n",
      "    for article in articles:\n",
      "        title = article.find(\"a\").text\n",
      "        date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "\n",
      "        #identify duplicates\n",
      "        if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "            dup = None\n",
      "        else:\n",
      "            dup = article.find('div', {'class':'dedupHeadlines'}).text\n",
      "\n",
      "        art_list.append({'comp': comp,\n",
      "                         'title': title,\n",
      "                         'date': date,\n",
      "                         'dup': dup})\n",
      "152/12:\n",
      "# Function saving articles into dictionary\n",
      "def saveArticles(articles, comp):\n",
      "    art_list =[]\n",
      "    \n",
      "    for article in articles:\n",
      "        title = article.find(\"a\").text\n",
      "        date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "\n",
      "        #identify duplicates\n",
      "        if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "            dup = None\n",
      "        else:\n",
      "            dup = article.find('div', {'class':'dedupHeadlines'}).text\n",
      "\n",
      "        art_list.append({'comp': comp,\n",
      "                         'title': title,\n",
      "                         'date': date,\n",
      "                         'dup': dup})\n",
      "    return art_list\n",
      "152/13:\n",
      "# Function saving articles into dictionary\n",
      "def saveArticles(articles, comp):\n",
      "    art_list =[]\n",
      "    \n",
      "    for article in articles:\n",
      "        title = article.find(\"a\").text\n",
      "        date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "\n",
      "        #identify duplicates\n",
      "        if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "            dup = None\n",
      "        else:\n",
      "            dup = article.find('div', {'class':'dedupHeadlines'}).text\n",
      "\n",
      "        art_list.append({'comp': comp,\n",
      "                         'title': title,\n",
      "                         'date': date,\n",
      "                         'dup': dup})\n",
      "    return art_list\n",
      "152/14:\n",
      "def saveFactiva (art_list, comp):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{comp}_{len(art_list)}_{fetch_date}.csv'\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"date\", \"dup\"])\n",
      "\n",
      "        for item in art_list:\n",
      "            writer.writerow([item['title'], item['date'], item['dup']])\n",
      "\n",
      "    return print(f'Articles for {comp} has been saved in {filename}')\n",
      "152/15:\n",
      "filename = 'AMZN.html'\n",
      "comp = 'AMZN'\n",
      "\n",
      "articles = getArticles(filename, comp)\n",
      "\n",
      "art_list = saveArticles(articles, comp)\n",
      "\n",
      "saveFactiva (art_list, comp)\n",
      "152/16:\n",
      "def saveFactiva (art_list, comp):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{comp}_{len(art_list)}_{fetch_date}.csv'\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"comp\", \"title\", \"date\", \"dup\"])\n",
      "\n",
      "        for item in art_list:\n",
      "            writer.writerow([item['comp'], item['title'], item['date'], item['dup']])\n",
      "\n",
      "    return print(f'Articles for {comp} has been saved in {filename}')\n",
      "152/17:\n",
      "filename = 'AMZN.html'\n",
      "comp = 'AMZN'\n",
      "\n",
      "articles = getArticles(filename, comp)\n",
      "\n",
      "art_list = saveArticles(articles, comp)\n",
      "\n",
      "saveFactiva (art_list, comp)\n",
      "152/18:\n",
      "filename = 'AMZN.html'\n",
      "comp = 'AMZN'\n",
      "\n",
      "articles = getArticles(filename, comp)\n",
      "\n",
      "art_list = saveArticles(articles, comp)\n",
      "\n",
      "saveFactiva (art_list, comp)\n",
      "152/19:\n",
      "# Function saving articles into dictionary\n",
      "def saveArticles(articles, art_list, comp):\n",
      "      \n",
      "    for article in articles:\n",
      "        title = article.find(\"a\").text\n",
      "        date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "\n",
      "        #identify duplicates\n",
      "        if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "            dup = None\n",
      "        else:\n",
      "            dup = article.find('div', {'class':'dedupHeadlines'}).text\n",
      "\n",
      "        art_list.append({'comp': comp,\n",
      "                         'title': title,\n",
      "                         'date': date,\n",
      "                         'dup': dup})\n",
      "        \n",
      "        print(f'{comp} articles has been saved. Total articles: {len(art_list)}')\n",
      "        \n",
      "    return art_list\n",
      "152/20:\n",
      "def saveFactiva (art_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    csv_name = f'factiva_{len(art_list)}_{fetch_date}.csv'\n",
      "\n",
      "    with open(csv_name, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"comp\", \"title\", \"date\", \"dup\"])\n",
      "\n",
      "        for item in art_list:\n",
      "            writer.writerow([item['comp'], item['title'], item['date'], item['dup']])\n",
      "\n",
      "    return print(f'{len(art_list)} articles has been saved in {csv_name}')\n",
      "152/21:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "import csv\n",
      "import time\n",
      "152/22:\n",
      "# Function to get all articles of a company:\n",
      "\n",
      "def getArticles(filename, comp):\n",
      "\n",
      "    # Opening the html file\n",
      "    HTMLFile = open(filename, \"r\", encoding='Latin1')\n",
      "\n",
      "    # Reading the file\n",
      "    index = HTMLFile.read()\n",
      "\n",
      "    # Creating a BeautifulSoup object and specifying the parser\n",
      "    factiva = bts(index, 'lxml')\n",
      "    \n",
      "    # Get articles from soup\n",
      "    art = factiva.find(\"div\",{\"class\":\"headlines\", \"id\":\"headlines\"})\n",
      "    articles = art.find_all(\"tr\", {\"class\":'headline'})\n",
      "    \n",
      "    print(f'{comp}: {len(articles)} articles have been scraped')\n",
      "\n",
      "    return articles\n",
      "152/23:\n",
      "# Function saving articles into dictionary\n",
      "def saveArticles(articles, art_list, comp):\n",
      "      \n",
      "    for article in articles:\n",
      "        title = article.find(\"a\").text\n",
      "        date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "\n",
      "        #identify duplicates\n",
      "        if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "            dup = None\n",
      "        else:\n",
      "            dup = article.find('div', {'class':'dedupHeadlines'}).text\n",
      "\n",
      "        art_list.append({'comp': comp,\n",
      "                         'title': title,\n",
      "                         'date': date,\n",
      "                         'dup': dup})\n",
      "        \n",
      "        print(f'{comp} articles has been saved. Total articles: {len(art_list)}')\n",
      "        \n",
      "    return art_list\n",
      "152/24:\n",
      "def saveFactiva (art_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    csv_name = f'factiva_{len(art_list)}_{fetch_date}.csv'\n",
      "\n",
      "    with open(csv_name, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"comp\", \"title\", \"date\", \"dup\"])\n",
      "\n",
      "        for item in art_list:\n",
      "            writer.writerow([item['comp'], item['title'], item['date'], item['dup']])\n",
      "\n",
      "    return print(f'{len(art_list)} articles has been saved in {csv_name}')\n",
      "152/25:\n",
      "# Importing company list\n",
      "comp_list = []  \n",
      "\n",
      "# opening the file using \"with\" statement\n",
      "with open('comp-usa.csv', 'r') as comp_data:\n",
      "    for comp in csv.DictReader(comp_data):\n",
      "        comp_list.append(comp)\n",
      "\n",
      "len(comp_list)\n",
      "152/26:\n",
      "art_list =[]\n",
      "\n",
      "for company in comp_list:\n",
      "    articles = getArticles(filename, comp)\n",
      "    art_list = saveArticles(articles, comp)\n",
      "    saveFactiva (art_list)\n",
      "152/27:\n",
      "art_list =[]\n",
      "\n",
      "for company in comp_list:\n",
      "    articles = getArticles(filename, comp)\n",
      "    art_list = saveArticles(articles, art_list, comp)\n",
      "    saveFactiva (art_list)\n",
      "152/28:\n",
      "# Function saving articles into dictionary\n",
      "def saveArticles(articles, art_list, comp):\n",
      "      \n",
      "    for article in articles:\n",
      "        comp = comp_list['comp']\n",
      "        title = article.find(\"a\").text\n",
      "        date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "\n",
      "        #identify duplicates\n",
      "        if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "            dup = None\n",
      "        else:\n",
      "            dup = article.find('div', {'class':'dedupHeadlines'}).text\n",
      "\n",
      "        art_list.append({'comp': comp,\n",
      "                         'title': title,\n",
      "                         'date': date,\n",
      "                         'dup': dup})\n",
      "        \n",
      "        print(f'{comp} articles has been saved. Total articles: {len(art_list)}')\n",
      "        \n",
      "    return art_list\n",
      "152/29:\n",
      "def saveFactiva (art_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    csv_name = f'factiva_{len(art_list)}_{fetch_date}.csv'\n",
      "\n",
      "    with open(csv_name, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"comp\", \"title\", \"date\", \"dup\"])\n",
      "\n",
      "        for item in art_list:\n",
      "            writer.writerow([item['comp'], item['title'], item['date'], item['dup']])\n",
      "\n",
      "    return print(f'{len(art_list)} articles has been saved in {csv_name}')\n",
      "152/30:\n",
      "# Importing company list\n",
      "comp_list = []  \n",
      "\n",
      "# opening the file using \"with\" statement\n",
      "with open('comp-usa.csv', 'r') as comp_data:\n",
      "    for comp in csv.DictReader(comp_data):\n",
      "        comp_list.append(comp)\n",
      "\n",
      "len(comp_list)\n",
      "152/31:\n",
      "art_list =[]\n",
      "\n",
      "for company in comp_list:\n",
      "    articles = getArticles(filename, comp)\n",
      "    art_list = saveArticles(articles, art_list, comp)\n",
      "    saveFactiva (art_list)\n",
      "152/32:\n",
      "art_list =[]\n",
      "\n",
      "for company in comp_list:\n",
      "    articles = getArticles(filename, comp)\n",
      "152/33:\n",
      "# Importing company list\n",
      "comp_list = []  \n",
      "\n",
      "# opening the file using \"with\" statement\n",
      "with open('comp-usa.csv', 'r') as comp_data:\n",
      "    for comp in csv.DictReader(comp_data):\n",
      "        comp_list.append(comp)\n",
      "\n",
      "len(comp_list)\n",
      "\n",
      "comp_list\n",
      "152/34:\n",
      "art_list =[]\n",
      "\n",
      "for company in comp_list:\n",
      "    filename = comp_list['filename']\n",
      "    comp = comp_list['comp']\n",
      "    articles = getArticles(filename, comp)\n",
      "152/35:\n",
      "# Importing company list\n",
      "comp_list = []  \n",
      "\n",
      "# opening the file using \"with\" statement\n",
      "with open('comp-usa.csv', 'r') as comp_data:\n",
      "    for comp in csv.DictReader(comp_data):\n",
      "        comp_list.append(comp)\n",
      "\n",
      "len(comp_list)\n",
      "\n",
      "type(comp_list)\n",
      "152/36:\n",
      "art_list =[]\n",
      "\n",
      "for company in comp_list:\n",
      "    filename = company['filename']\n",
      "    comp = company['comp']\n",
      "    articles = getArticles(filename, comp)\n",
      "152/37:\n",
      "art_list =[]\n",
      "\n",
      "for company in comp_list:\n",
      "    filename = company['filename']\n",
      "    comp = company['comp']\n",
      "    articles = getArticles(filename, comp)\n",
      "    art_list = saveArticles(articles, art_list, comp)\n",
      "    saveFactiva (art_list)\n",
      "152/38:\n",
      "# Function saving articles into dictionary\n",
      "def saveArticles(articles, art_list, comp):\n",
      "      \n",
      "    for article in articles:\n",
      "        title = article.find(\"a\").text\n",
      "        date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "\n",
      "        #identify duplicates\n",
      "        if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "            dup = None\n",
      "        else:\n",
      "            dup = article.find('div', {'class':'dedupHeadlines'}).text\n",
      "\n",
      "        art_list.append({'comp': comp,\n",
      "                         'title': title,\n",
      "                         'date': date,\n",
      "                         'dup': dup})\n",
      "        \n",
      "        print(f'{comp} articles has been saved. Total articles: {len(art_list)}')\n",
      "        \n",
      "    return art_list\n",
      "152/39:\n",
      "# Importing company list\n",
      "comp_list = []  \n",
      "\n",
      "# opening the file using \"with\" statement\n",
      "with open('comp-usa.csv', 'r') as comp_data:\n",
      "    for comp in csv.DictReader(comp_data):\n",
      "        comp_list.append(comp)\n",
      "\n",
      "len(comp_list)\n",
      "\n",
      "type(comp_list)\n",
      "152/40:\n",
      "art_list =[]\n",
      "\n",
      "for company in comp_list:\n",
      "    filename = company['filename']\n",
      "    comp = company['comp']\n",
      "    articles = getArticles(filename, comp)\n",
      "    art_list = saveArticles(articles, art_list, comp)\n",
      "    saveFactiva (art_list)\n",
      "152/41:\n",
      "art_list =[]\n",
      "\n",
      "for company in comp_list:\n",
      "    filename = company['filename']\n",
      "    comp = company['comp']\n",
      "    articles = getArticles(filename, comp)\n",
      "    art_list = saveArticles(articles, art_list, comp)\n",
      "    print(f'{comp} articles has been saved. Total articles: {len(art_list)}')\n",
      "    saveFactiva (art_list)\n",
      "152/42:\n",
      "# Function saving articles into dictionary\n",
      "def saveArticles(articles, art_list, comp):\n",
      "      \n",
      "    for article in articles:\n",
      "        title = article.find(\"a\").text\n",
      "        date = article.find_all(\"td\")[2].find(\"div\").text\n",
      "\n",
      "        #identify duplicates\n",
      "        if article.find('div', {'class':'dedupHeadlines'}) == None:\n",
      "            dup = None\n",
      "        else:\n",
      "            dup = article.find('div', {'class':'dedupHeadlines'}).text\n",
      "\n",
      "        art_list.append({'comp': comp,\n",
      "                         'title': title,\n",
      "                         'date': date,\n",
      "                         'dup': dup})  \n",
      "         \n",
      "    return art_list\n",
      "152/43:\n",
      "def saveFactiva (art_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    csv_name = f'factiva_{len(art_list)}_{fetch_date}.csv'\n",
      "\n",
      "    with open(csv_name, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"comp\", \"title\", \"date\", \"dup\"])\n",
      "\n",
      "        for item in art_list:\n",
      "            writer.writerow([item['comp'], item['title'], item['date'], item['dup']])\n",
      "\n",
      "    return print(f'{len(art_list)} articles has been saved in {csv_name}')\n",
      "152/44:\n",
      "# Importing company list\n",
      "comp_list = []  \n",
      "\n",
      "# opening the file using \"with\" statement\n",
      "with open('comp-usa.csv', 'r') as comp_data:\n",
      "    for comp in csv.DictReader(comp_data):\n",
      "        comp_list.append(comp)\n",
      "\n",
      "len(comp_list)\n",
      "\n",
      "type(comp_list)\n",
      "152/45:\n",
      "art_list =[]\n",
      "\n",
      "for company in comp_list:\n",
      "    filename = company['filename']\n",
      "    comp = company['comp']\n",
      "    articles = getArticles(filename, comp)\n",
      "    art_list = saveArticles(articles, art_list, comp)\n",
      "    print(f'{comp} articles has been saved. Total articles: {len(art_list)}')\n",
      "    saveFactiva (art_list)\n",
      "152/46:\n",
      "art_list =[]\n",
      "\n",
      "for company in comp_list:\n",
      "    filename = company['filename']\n",
      "    comp = company['comp']\n",
      "    articles = getArticles(filename, comp)\n",
      "    art_list = saveArticles(articles, art_list, comp)\n",
      "    print(f'{comp} articles has been saved. Total articles: {len(art_list)}')\n",
      "\n",
      "saveFactiva (art_list)\n",
      "153/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "import csv\n",
      "import time\n",
      "153/2:\n",
      "#Function for parsing the URLs\n",
      "def cookSoup(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "153/3:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(soup, cat_url, country, cat):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "    #No of pages (rounding up all numbers) = math.ceil(int(total_prod)/40)\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/40)\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{cat_url}?from={40 * i}&size=40'\n",
      "            url_list.append(page)\n",
      "    print (f'{country}_{cat}: There are {total_prod} products ({total_page} pages)')\n",
      "    return url_list\n",
      "153/4:\n",
      "# Function to get main data\n",
      "def getDecathlonData(country_url, url_list, country, cat, prod_list):\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. absent)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat,\n",
      "                             'EN_cat': cat})\n",
      "            \n",
      "    print(f'{country}_{cat}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      "153/5:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{len(prod_list)}_{fetch_date}.csv'.replace(\" \",\"_\").replace(\",\",\"\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"EN_cat\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['EN_cat'], item['url']])\n",
      "    \n",
      "    return print(f'{country}: {len(prod_list)} products have been saved in {filename}')\n",
      "153/6:\n",
      "# Function to get main data\n",
      "def getDecathlonData(country_url, url_list, country, cat, prod_list):\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. absent)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat,\n",
      "                             'EN_cat': cat})\n",
      "            \n",
      "    print(f'{country}_{cat}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      "153/7:\n",
      "# Importing country - category data\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "153/8: cat_list[1]\n",
      "153/9:\n",
      "#Looping over several categories\n",
      "\n",
      "prod_list = []\n",
      "\n",
      "for category in cat_list:\n",
      "    country = category['country']\n",
      "    country_url = category['country_url']\n",
      "    cat = category['cat']\n",
      "    cat_url = category['cat_url']\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat, prod_list)\n",
      "\n",
      "saveDecathlonData(country, prod_list)\n",
      "154/1:\n",
      "import pandas as pd\n",
      "import yfinance as yf\n",
      "from yahoofinancials import YahooFinancials\n",
      "154/2:\n",
      "import pandas as pd\n",
      "import yfinance as yf\n",
      "from yahoofinancials import YahooFinancials\n",
      "154/3:\n",
      "import pandas as pd\n",
      "import yfinance as yf\n",
      "from yahoofinancials import YahooFinancials\n",
      "154/4:\n",
      "import pandas as pd\n",
      "import yfinance as yf\n",
      "from yahoofinancials import YahooFinancials\n",
      "154/5:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "tsla_df.head()\n",
      "154/6:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df.head()\n",
      "154/7:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df.head()\n",
      "\n",
      "len(amzn_df.head)\n",
      "154/8:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df.head()\n",
      "\n",
      "len(amzn_df)\n",
      "154/9:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df\n",
      "154/10:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "type(amzn_df)\n",
      "154/11:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "head(amzn_df)\n",
      "154/12:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df.head\n",
      "154/13:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df.head()\n",
      "154/14:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "df = to_dict(amzn_df)\n",
      "154/15:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "df.to_dict('dict')\n",
      "154/16:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df.to_dict('dict')\n",
      "154/17:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df.to_dict('records')\n",
      "154/18:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False).to_dict('records')\n",
      "len(amzn_df)\n",
      "154/19:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False).to_dict('records')\n",
      "amzn_df[1]\n",
      "154/20:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False).to_dict('records')\n",
      "amzn_df[1]['Open']\n",
      "154/21:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False).to_dict('records')\n",
      "amzn_df[1]\n",
      "154/22:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False).to_dict('records')\n",
      "amzn_df[1]\n",
      "154/23: amzn_df[1]['Open']\n",
      "154/24: amzn_df[1]['comp'] = 'AMZN'\n",
      "154/25: amzn_df[1]['comp'] = 'AMZN'\n",
      "154/26: amzn_df[1]\n",
      "154/27: amzn_df[1]\n",
      "154/28:\n",
      "def stockPrice(comp):\n",
      "    stock = yf.download(comp, \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False).to_dict('records')\n",
      "    for item in df:\n",
      "        item['comp'] = comp\n",
      "    return stock\n",
      "154/29:\n",
      "def stockPrice(comp):\n",
      "    stock = yf.download(comp, \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False).to_dict('records')\n",
      "    for item in df:\n",
      "        item['comp'] = comp\n",
      "    return stock\n",
      "154/30:\n",
      "comp = 'CVS'\n",
      "stockPrice(comp)\n",
      "154/31:\n",
      "def stockPrice(comp):\n",
      "    stock = yf.download(comp, \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False).to_dict('records')\n",
      "    for item in stock:\n",
      "        item['comp'] = comp\n",
      "    return stock\n",
      "154/32:\n",
      "comp = 'CVS'\n",
      "stockPrice(comp)\n",
      "154/33:\n",
      "comp = 'CVS'\n",
      "stockPrice(comp)\n",
      "154/34:\n",
      "comp = 'CVS'\n",
      "cvs = stockPrice(comp)\n",
      "154/35: len(cvs)\n",
      "154/36:\n",
      "comp_list=list(ACI, AMZN, ANF, ASO, BBBY, BNED, BURL, CHS, CONN, COST, \n",
      "               CVS, CWH, DDS, DG, DLTR, DXLG, FL, GCO, GES, GME, GPS, GRWG, HD, \n",
      "               IFMK, JWN, KR, LOW, M, ODP, QRTEA, RAD, ROST, SCVL, SFM, SPTN, TCS, \n",
      "               TDUP, TGT, TSCO, ULTA, W, WBA, WINA, WMK, WMT, WOOF)\n",
      "\n",
      "type(comp_list)\n",
      "154/37:\n",
      "comp_list=list(\"ACI\", \"AMZN\", \"ANF\", \"ASO\", \"BBBY\", \"BNED\", \"BURL\", \"CHS\", \"CONN\", \n",
      "               \"COST\", \"CVS\", \"CWH\", \"DDS\", \"DG\", \"DLTR\", \"DXLG\", \"FL\", \"GCO\", \"GES\", \"GME\", \n",
      "               \"GPS\", \"GRWG\", \"HD\", \"IFMK\", \"JWN\", \"KR\", \"LOW\", \"M\", \"ODP\", \"QRTEA\", \"RAD\", \n",
      "               \"ROST\", \"SCVL\", \"SFM\", \"SPTN\", \"TCS\", \"TDUP\", \"TGT\", \"TSCO\", \"ULTA\", \"W\", \"WBA\", \n",
      "               \"WINA\", \"WMK\", \"WMT\", \"WOOF\")\n",
      "\n",
      "type(comp_list)\n",
      "154/38:\n",
      "comp_list=list[\"ACI\", \"AMZN\", \"ANF\", \"ASO\", \"BBBY\", \"BNED\", \"BURL\", \"CHS\", \"CONN\", \n",
      "               \"COST\", \"CVS\", \"CWH\", \"DDS\", \"DG\", \"DLTR\", \"DXLG\", \"FL\", \"GCO\", \"GES\", \"GME\", \n",
      "               \"GPS\", \"GRWG\", \"HD\", \"IFMK\", \"JWN\", \"KR\", \"LOW\", \"M\", \"ODP\", \"QRTEA\", \"RAD\", \n",
      "               \"ROST\", \"SCVL\", \"SFM\", \"SPTN\", \"TCS\", \"TDUP\", \"TGT\", \"TSCO\", \"ULTA\", \"W\", \"WBA\", \n",
      "               \"WINA\", \"WMK\", \"WMT\", \"WOOF\"]\n",
      "\n",
      "type(comp_list)\n",
      "154/39:\n",
      "comp_list=list[\"ACI\", \"AMZN\", \"ANF\", \"ASO\", \"BBBY\", \"BNED\", \"BURL\", \"CHS\", \"CONN\", \n",
      "               \"COST\", \"CVS\", \"CWH\", \"DDS\", \"DG\", \"DLTR\", \"DXLG\", \"FL\", \"GCO\", \"GES\", \"GME\", \n",
      "               \"GPS\", \"GRWG\", \"HD\", \"IFMK\", \"JWN\", \"KR\", \"LOW\", \"M\", \"ODP\", \"QRTEA\", \"RAD\", \n",
      "               \"ROST\", \"SCVL\", \"SFM\", \"SPTN\", \"TCS\", \"TDUP\", \"TGT\", \"TSCO\", \"ULTA\", \"W\", \"WBA\", \n",
      "               \"WINA\", \"WMK\", \"WMT\", \"WOOF\"]\n",
      "\n",
      "len(comp_list)\n",
      "154/40:\n",
      "comp_list=[\"ACI\", \"AMZN\", \"ANF\", \"ASO\", \"BBBY\", \"BNED\", \"BURL\", \"CHS\", \"CONN\", \n",
      "               \"COST\", \"CVS\", \"CWH\", \"DDS\", \"DG\", \"DLTR\", \"DXLG\", \"FL\", \"GCO\", \"GES\", \"GME\", \n",
      "               \"GPS\", \"GRWG\", \"HD\", \"IFMK\", \"JWN\", \"KR\", \"LOW\", \"M\", \"ODP\", \"QRTEA\", \"RAD\", \n",
      "               \"ROST\", \"SCVL\", \"SFM\", \"SPTN\", \"TCS\", \"TDUP\", \"TGT\", \"TSCO\", \"ULTA\", \"W\", \"WBA\", \n",
      "               \"WINA\", \"WMK\", \"WMT\", \"WOOF\"]\n",
      "\n",
      "len(comp_list)\n",
      "154/41:\n",
      "comp_list=[\"ACI\", \"AMZN\", \"ANF\", \"ASO\", \"BBBY\", \"BNED\", \"BURL\", \"CHS\", \"CONN\", \n",
      "               \"COST\", \"CVS\", \"CWH\", \"DDS\", \"DG\", \"DLTR\", \"DXLG\", \"FL\", \"GCO\", \"GES\", \"GME\", \n",
      "               \"GPS\", \"GRWG\", \"HD\", \"IFMK\", \"JWN\", \"KR\", \"LOW\", \"M\", \"ODP\", \"QRTEA\", \"RAD\", \n",
      "               \"ROST\", \"SCVL\", \"SFM\", \"SPTN\", \"TCS\", \"TDUP\", \"TGT\", \"TSCO\", \"ULTA\", \"W\", \"WBA\", \n",
      "               \"WINA\", \"WMK\", \"WMT\", \"WOOF\"]\n",
      "\n",
      "type(comp_list)\n",
      "154/42:\n",
      "comp_list=[\"ACI\", \"AMZN\", \"ANF\", \"ASO\", \"BBBY\", \"BNED\", \"BURL\", \"CHS\", \"CONN\", \n",
      "               \"COST\", \"CVS\", \"CWH\", \"DDS\", \"DG\", \"DLTR\", \"DXLG\", \"FL\", \"GCO\", \"GES\", \"GME\", \n",
      "               \"GPS\", \"GRWG\", \"HD\", \"IFMK\", \"JWN\", \"KR\", \"LOW\", \"M\", \"ODP\", \"QRTEA\", \"RAD\", \n",
      "               \"ROST\", \"SCVL\", \"SFM\", \"SPTN\", \"TCS\", \"TDUP\", \"TGT\", \"TSCO\", \"ULTA\", \"W\", \"WBA\", \n",
      "               \"WINA\", \"WMK\", \"WMT\", \"WOOF\"]\n",
      "154/43:\n",
      "stock = []\n",
      "for comp in comp_list:\n",
      "    stock.append(stockPrice(comp))\n",
      "    \n",
      "len(stock)\n",
      "154/44:\n",
      "stock = []\n",
      "for comp in comp_list:\n",
      "    stock.append(stockPrice(comp))\n",
      "    \n",
      "stock[1]\n",
      "154/45: stock[46]\n",
      "154/46: stock[45]\n",
      "154/47:\n",
      "stock = []\n",
      "for comp in comp_list:\n",
      "    stock_comp = stockPrice(comp)\n",
      "    for date_comp in stock_comp:\n",
      "        stock.append(date_comp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/48: len(stock)\n",
      "154/49: len(stock)\n",
      "154/50: len(stock)\n",
      "154/51: 46*757\n",
      "154/52: 45*757\n",
      "154/53: 42*757\n",
      "154/54: len(comp_list)*757\n",
      "154/55: stock[1]\n",
      "154/56: csv\n",
      "154/57:\n",
      "comp = 'CVS'\n",
      "cvs = stockPrice(comp)\n",
      "154/58: csv\n",
      "154/59: cvs\n",
      "154/60:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df\n",
      "154/61:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df.to_dict('records')\n",
      "154/62:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df\n",
      "154/63:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "type(amzn_df)\n",
      "154/64:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "len(amzn_df)\n",
      "154/65:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df[1]\n",
      "154/66:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "154/67:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False).to_dict('records', index=True)\n",
      "amzn_df\n",
      "154/68:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df['Date']\n",
      "154/69:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df\n",
      "154/70:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "amzn_df\n",
      "154/71: amzn_df.to_csv(\"amzn.csv\", index=True)\n",
      "154/72:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "amzn_df.to_csv(\"amzn.csv\", index=True)\n",
      "154/73:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "amzn_df.to_csv(\"amzn.csv\", index=False)\n",
      "154/74:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "amzn_df.to_csv(\"amzn.csv\", index=True)\n",
      "154/75:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "amzn_df.to_dict(\"amzn\", index=True)\n",
      "154/76:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "amzn_df.to_dict(\"amzn\")\n",
      "154/77:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "amzn_df.to_dict(\"records\")\n",
      "154/78:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "#amzn_df.to_dict(\"records\")\n",
      "\n",
      "amzn_df.reset_index(inplace=True)\n",
      "154/79:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "#amzn_df.to_dict(\"records\")\n",
      "\n",
      "amzn_df.reset_index(inplace=True)\n",
      "154/80:\n",
      "def stockPrice(comp):\n",
      "    stock = yf.download(comp, \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False).to_dict('records')\n",
      "    for item in stock:\n",
      "        item['comp'] = comp\n",
      "    return stock\n",
      "154/81:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "#amzn_df.to_dict(\"records\")\n",
      "\n",
      "amzn_df.reset_index(inplace=True)\n",
      "\n",
      "amzn_df\n",
      "154/82:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False).reset_index(inplace=True).to_dict(\"records\")\n",
      "154/83:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False).reset_index(inplace=True)\n",
      "154/84:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False).reset_index(inplace=True)\n",
      "154/85:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False).reset_index(inplace=True)\n",
      "\n",
      "type(amzn_df)\n",
      "154/86:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False).reset_index(inplace=True)\n",
      "\n",
      "amzn_df\n",
      "154/87:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "\n",
      "amzn = amzn_df.reset_index(inplace=True).to_dict(\"records\")\n",
      "154/88:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "#amzn_df.to_dict(\"records\")\n",
      "\n",
      "\n",
      "\n",
      "amzn_df\n",
      "154/89:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "amzn_df.reset_index(inplace = True).to_dict(\"records\")\n",
      "154/90:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "amzn_df.reset_index(inplace = True)\n",
      "154/91:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "amzn_df.reset_index(inplace = True)\n",
      "154/92:\n",
      "amzn_df = yf.download('AMZN', \n",
      "                      start='2019-01-01', \n",
      "                      end='2022-01-01', \n",
      "                      progress=False)\n",
      "amzn_df.reset_index(inplace = True)\n",
      "154/93: amzn_df\n",
      "154/94: amzn_df.to_dict(\"records\")\n",
      "154/95:\n",
      "def stockPrice(comp):\n",
      "    stock_df = yf.download(comp, \n",
      "                      start='2019-01-01', \n",
      "                      end='2021-12-31', \n",
      "                      progress=False)\n",
      "    stock_reset = stock_df.reset_index(inplace = True)\n",
      "    stock = stock_df.to_dict('records')\n",
      "    for item in stock:\n",
      "        item['comp'] = comp\n",
      "    return stock\n",
      "154/96:\n",
      "comp = 'CVS'\n",
      "cvs = stockPrice(comp)\n",
      "154/97:\n",
      "comp = 'CVS'\n",
      "cvs = stockPrice(comp)\n",
      "154/98: cvs[1]\n",
      "154/99:\n",
      "stock = []\n",
      "for comp in comp_list:\n",
      "    stock_comp = stockPrice(comp)\n",
      "    stock.extend(stock_comp)\n",
      "\n",
      "len(stock)\n",
      "154/100:\n",
      "stock = []\n",
      "for comp in comp_list:\n",
      "    stock_comp = stockPrice(comp)\n",
      "    stock.extend(stock_comp)\n",
      "\n",
      "len(stock)\n",
      "154/101: stock[0]\n",
      "154/102: stock[3]\n",
      "154/103: stock[32900]\n",
      "154/104:\n",
      "with open('stock.csv', \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"date\", \"open\", \"close\", \"adj_close\", \"comp\"])\n",
      "\n",
      "        for item in stock:\n",
      "            writer.writerow([item['Date'], item['Open'], item['Close'], item['Adj Close'], item['comp']])\n",
      "154/105:\n",
      "import csv\n",
      "with open('stock.csv', \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"date\", \"open\", \"close\", \"adj_close\", \"comp\"])\n",
      "\n",
      "        for item in stock:\n",
      "            writer.writerow([item['Date'], item['Open'], item['Close'], item['Adj Close'], item['comp']])\n",
      "158/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "import csv\n",
      "import time\n",
      "158/2:\n",
      "#Function for parsing the URLs\n",
      "def cookSoup(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "158/3:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(soup, cat_url, country, cat):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/40)\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{cat_url}?from={40 * i}&size=40'\n",
      "            url_list.append(page)\n",
      "    print (f'{country}_{cat}: There are {total_prod} products ({total_page} pages)')\n",
      "    return url_list\n",
      "158/4:\n",
      "# Function to get main data\n",
      "def getDecathlonData(country_url, url_list, country, cat, prod_list):\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. absent)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat,\n",
      "                             'EN_cat': cat})\n",
      "            \n",
      "    print(f'{country}_{cat}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      "158/5:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{len(prod_list)}_{fetch_date}.csv'.replace(\" \",\"_\").replace(\",\",\"\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"EN_cat\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['EN_cat'], item['url']])\n",
      "    \n",
      "    return print(f'{country}: {len(prod_list)} products have been saved in {filename}')\n",
      "158/6:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "import csv\n",
      "import time\n",
      "158/7:\n",
      "#Function for parsing the URLs\n",
      "def cookSoup(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "158/8:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(soup, cat_url, country, cat):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/40)\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{cat_url}?from={40 * i}&size=40'\n",
      "            url_list.append(page)\n",
      "    print (f'{country}_{cat}: There are {total_prod} products ({total_page} pages)')\n",
      "    return url_list\n",
      "158/9:\n",
      "# Function to get main data\n",
      "def getDecathlonData(country_url, url_list, country, cat, prod_list):\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. absent)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat,\n",
      "                             'EN_cat': cat})\n",
      "            \n",
      "    print(f'{country}_{cat}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      "158/10:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{len(prod_list)}_{fetch_date}.csv'.replace(\" \",\"_\").replace(\",\",\"\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"EN_cat\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['EN_cat'], item['url']])\n",
      "    \n",
      "    return print(f'{country}: {len(prod_list)} products have been saved in {filename}')\n",
      "158/11:\n",
      "# Importing country - category data\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list_NL.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "158/12: cat_list[1]\n",
      "158/13:\n",
      "#Looping over several categories\n",
      "\n",
      "prod_list = []\n",
      "\n",
      "for category in cat_list:\n",
      "    country = category['country']\n",
      "    country_url = category['country_url']\n",
      "    cat = category['cat']\n",
      "    cat_url = category['cat_url']\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat, prod_list)\n",
      "\n",
      "saveDecathlonData(country, prod_list)\n",
      "158/14:\n",
      "# Importing country - category data\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list_NL.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "158/15: cat_list[1]\n",
      "158/16:\n",
      "#Looping over several categories\n",
      "\n",
      "prod_list = []\n",
      "\n",
      "for category in cat_list:\n",
      "    country = category['country']\n",
      "    country_url = category['country_url']\n",
      "    cat = category['cat']\n",
      "    cat_url = category['cat_url']\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat, prod_list)\n",
      "\n",
      "saveDecathlonData(country, prod_list)\n",
      "158/17:\n",
      "# Importing country - category data\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list_NL.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "158/18: cat_list[1]\n",
      "158/19:\n",
      "#Looping over several categories\n",
      "\n",
      "prod_list = []\n",
      "\n",
      "for category in cat_list:\n",
      "    country = category['country']\n",
      "    country_url = category['country_url']\n",
      "    cat = category['cat']\n",
      "    cat_url = category['cat_url']\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat, prod_list)\n",
      "\n",
      "saveDecathlonData(country, prod_list)\n",
      "158/20:\n",
      "# Importing country - category data\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list_NL.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "158/21: cat_list[1]\n",
      "158/22:\n",
      "# Importing country - category data\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list_NL.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "158/23: cat_list[0]\n",
      "158/24:\n",
      "#Looping over several categories\n",
      "\n",
      "prod_list = []\n",
      "\n",
      "for category in cat_list:\n",
      "    country = category['country']\n",
      "    country_url = category['country_url']\n",
      "    cat = category['cat']\n",
      "    cat_url = category['cat_url']\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat, prod_list)\n",
      "\n",
      "saveDecathlonData(country, prod_list)\n",
      "158/25:\n",
      "# Importing country - category data\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list_NL.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "158/26: cat_list[0]\n",
      "158/27:\n",
      "#Looping over several categories\n",
      "\n",
      "prod_list = []\n",
      "\n",
      "for category in cat_list:\n",
      "    country = category['country']\n",
      "    country_url = category['country_url']\n",
      "    cat = category['cat']\n",
      "    cat_url = category['cat_url']\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat, prod_list)\n",
      "\n",
      "saveDecathlonData(country, prod_list)\n",
      "160/1:\n",
      "#NL - Woman sportwear\n",
      "url = 'https://www.decathlon.nl/browse/c0-dames-sportkleding/_/N-1fj8z6z'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "162/1:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "162/2:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "162/3:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "162/4:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "162/5:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "162/6:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "162/7: prod\n",
      "162/8:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "162/9: de1_list\n",
      "159/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "import csv\n",
      "import time\n",
      "159/2:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from datetime import datetime, timedelta\n",
      "import csv\n",
      "import time\n",
      "159/3:\n",
      "#Function for parsing the URLs\n",
      "def cookSoup(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "159/4:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(soup, cat_url, country, cat):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":\"plp-bar-info svelte-1uqvrhu\"}).find(\"span\", {\"class\":\"svelte-1uqvrhu\"}).text\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/40)\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{cat_url}?from={40 * i}&size=40'\n",
      "            url_list.append(page)\n",
      "    print (f'{country}_{cat}: There are {total_prod} products ({total_page} pages)')\n",
      "    return url_list\n",
      "159/5:\n",
      "# Function to get main data\n",
      "def getDecathlonData(country_url, url_list, country, cat, prod_list):\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=\"dpb-holder loaded svelte-wmr4s2\")\n",
      "        for product in prod:\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_cat = page_soup.find(\"h1\").text\n",
      "            prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "            prod_url = f'{country_url}{link}'\n",
      "            \n",
      "            #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #product prices in case of product being discounted or not\n",
      "            if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "            else:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "            \n",
      "            #product sticker (2 cases: present vs. absent)\n",
      "\n",
      "            if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'category' : prod_cat,\n",
      "                             'EN_cat': cat})\n",
      "            \n",
      "    print(f'{country}_{cat}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      "159/6:\n",
      "# Function to save data into csv:\n",
      "def saveDecathlonData(country, prod_list):\n",
      "    fetch_date=f'{datetime.date(datetime.today())}'\n",
      "    filename = f'{country}_{len(prod_list)}_{fetch_date}.csv'.replace(\" \",\"_\").replace(\",\",\"\")\n",
      "\n",
      "    with open(filename, \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "        writer = csv.writer(csv_file, delimiter = \",\")\n",
      "        writer.writerow([\"title\", \"sku\", \"reg_price\", \"act_price\", \"brand\", \"sticker\", \"category\", \"EN_cat\", \"url\"])\n",
      "\n",
      "        for item in prod_list:\n",
      "            writer.writerow([item['title'], item['sku'], item['regular price'], item['actual price'], item['brand'], item['sticker'], item['category'], item['EN_cat'], item['url']])\n",
      "    \n",
      "    return print(f'{country}: {len(prod_list)} products have been saved in {filename}')\n",
      "159/7:\n",
      "# Importing country - category data\n",
      "cat_list = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_list.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_list.append(cat)\n",
      "159/8: cat_list[0]\n",
      "159/9:\n",
      "#Looping over several categories\n",
      "\n",
      "prod_list = []\n",
      "\n",
      "for category in cat_list:\n",
      "    country = category['country']\n",
      "    country_url = category['country_url']\n",
      "    cat = category['cat']\n",
      "    cat_url = category['cat_url']\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat)\n",
      "    prod_list = getDecathlonData(country_url, url_list, country, cat, prod_list)\n",
      "\n",
      "saveDecathlonData(country, prod_list)\n",
      "162/10:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "162/11:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "162/12:\n",
      "#DE - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung-damen/_/N-qjh2ro'\n",
      "\n",
      "de1 = getAndParseURL(url)\n",
      "\n",
      "prod = de1.find_all(class_=\"dpb-holder loaded svelte-d5pqmr\")\n",
      "162/13: prod\n",
      "162/14: len(prod)\n",
      "162/15:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-wmr4s2\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "162/16:\n",
      "de_base = \"https://www.decathlon.de\"\n",
      "de1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-d5pqmr\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-d5pqmr\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "162/17: de1_list\n",
      "162/18: len(de1_list)\n",
      "160/2:\n",
      "#NL - Woman sportwear\n",
      "url = 'https://www.decathlon.nl/browse/c0-dames-sportkleding/_/N-1fj8z6z'\n",
      "\n",
      "nl1 = getAndParseURL(url)\n",
      "\n",
      "prod = nl1.find_all(class_=\"dpb-holder loaded svelte-1e4me6r\")\n",
      "160/3:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "160/4:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "160/5:\n",
      "#NL - Woman sportwear\n",
      "url = 'https://www.decathlon.nl/browse/c0-dames-sportkleding/_/N-1fj8z6z'\n",
      "\n",
      "nl1 = getAndParseURL(url)\n",
      "\n",
      "prod = nl1.find_all(class_=\"dpb-holder loaded svelte-1e4me6r\")\n",
      "160/6:\n",
      "nl_base = \"https://www.decathlon.nl\"\n",
      "nl1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = de1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1e4me6r\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1e4me6r\"}).text\n",
      "    prod_url = f'{de_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/7:\n",
      "nl_base = \"https://www.decathlon.nl\"\n",
      "nl1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = nl1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1e4me6r\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1e4me6r\"}).text\n",
      "    prod_url = f'{nl_base}{link}'\n",
      "    \n",
      "    #if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "        \n",
      "    if product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}) == None:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z prc__active-price--sale\"}).text.replace('\\n','').replace('*','')\n",
      "    else:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    de1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/8:\n",
      "#NL - Woman sportwear\n",
      "url = 'https://www.decathlon.nl/browse/c0-dames-sportkleding/_/N-1fj8z6z'\n",
      "\n",
      "nl1 = getAndParseURL(url)\n",
      "\n",
      "prod = nl1.find_all(class_=\"dpb-holder loaded svelte-1e4me6r\")\n",
      "160/9: prod[1]\n",
      "160/10: prod[1].find(\"a\").attrs[\"href\"]\n",
      "160/11: prod[1].find(\"a\").attrs[\"href\"]\n",
      "160/12: nl1.find(\"h1\").text\n",
      "160/13: nl1.find(\"h1\").text\n",
      "160/14: prod[1].find(\"h2\",{\"class\":\"svelte-1e4me6r\"}).text\n",
      "160/15: prod[1].find(\"h2\",{\"class\":\"svelte-1e4me6r\"}).text\n",
      "160/16: prod[1].find(\"strong\",{\"class\":\"svelte-1e4me6r\"}).text\n",
      "160/17: prod[1].find(\"strong\",{\"class\":\"svelte-1e4me6r\"}).text\n",
      "160/18: prod[1].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "160/19: a <- prod[1].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "160/20: a = prod[1].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "160/21:\n",
      "a = prod[1].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "a\n",
      "160/22: a = prod[1].find(\"a\").attrs[\"href\"]\n",
      "160/23:\n",
      "a = prod[1].find(\"a\").attrs[\"href\"]\n",
      "a.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "160/24:\n",
      "a = prod[1].find(\"a\").attrs[\"href\"]\n",
      "a.partition(\"?mc=\")\n",
      "160/25:\n",
      "a = prod[1].find(\"a\").attrs[\"href\"]\n",
      "prod_id = link.partition(\"?mc=\")[2]\n",
      "160/26:\n",
      "a = prod[1].find(\"a\").attrs[\"href\"]\n",
      "a.partition(\"?mc=\")[2]\n",
      "160/27: prod[1].find(\"div\", {\"class\":\"prc__active-price svelte-1kkqb6z\"})\n",
      "160/28: prod[1].find(\"div\", {\"class\":\"prc__active-price\"})\n",
      "160/29: prod[1].find(\"div\", {\"class\":\"prc__active-price\"})\n",
      "160/30: prod[1].find(\"div\", {\"class\":\"prc__active-price\"}).text\n",
      "160/31: prod[0].find(\"div\", {\"class\":\"prc__active-price\"}).text\n",
      "160/32: prod[0].find(\"div\", {\"class\":\"prc__previous\"}).text\n",
      "160/33: prod[1].find(\"div\", {\"class\":\"prc__previous\"}).text\n",
      "160/34: prod[1].find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "160/35: prod[0].find(\"span\", {\"class\":\"prc__previous\"}).text\n",
      "160/36: prod[0].find(\"span\", {\"class\":\"prc__info-addon\"}).text\n",
      "160/37: prod[1].find(\"div\", {\"class\":\"prc__active-price\"})\n",
      "160/38: prod[0].find(\"span\", {\"class\":\"prc__info-addon\"})\n",
      "160/39: prod[1].find(\"span\", {\"class\":\"prc__info-addon\"})\n",
      "160/40: prod[0].find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "160/41: prod[0].find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "160/42: prod[1].find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "160/43: prod[1].find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "160/44: prod[1].find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "160/45:\n",
      "#NL - Woman sportwear\n",
      "url = 'https://www.decathlon.nl/browse/c0-dames-sportkleding/_/N-1fj8z6z'\n",
      "\n",
      "nl1 = getAndParseURL(url)\n",
      "\n",
      "prod = nl1.find_all(class_=\"dpb-holder loaded svelte-1e4me6r\")\n",
      "160/46:\n",
      "nl_base = \"https://www.decathlon.nl\"\n",
      "nl1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = nl1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1e4me6r\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1e4me6r\"}).text\n",
      "    prod_url = f'{nl_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    nl1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/47: nl1_list\n",
      "160/48:\n",
      "#NL - Woman sportwear\n",
      "url = 'https://www.decathlon.fr/browse/c0-femme/_/N-ry4jwt'\n",
      "\n",
      "nl1 = getAndParseURL(url)\n",
      "\n",
      "prod = nl1.find_all(class_=\"dpb-holder loaded svelte-1e4me6r\")\n",
      "160/49:\n",
      "a = prod[1].find(\"a\").attrs[\"href\"]\n",
      "a.partition(\"?mc=\")[2]\n",
      "160/50: prod\n",
      "160/51:\n",
      "#NL - Woman sportwear\n",
      "url = 'https://www.decathlon.fr/browse/c0-femme/_/N-ry4jwt'\n",
      "\n",
      "nl1 = getAndParseURL(url)\n",
      "\n",
      "prod = nl1.find_all(class_=\"dpb-holder loaded svelte-aim4wv\")\n",
      "160/52: prod\n",
      "160/53: len(prod)\n",
      "160/54:\n",
      "nl_base = \"https://www.decathlon.fr\"\n",
      "nl1_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = nl1.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-aim4wv\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-aim4wv\"}).text\n",
      "    prod_url = f'{nl_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    nl1_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/55: nl1_list\n",
      "160/56:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "160/57:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "160/58:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "160/59:\n",
      "#NL - Woman sportwear\n",
      "url = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjj'\n",
      "\n",
      "doc = getAndParseURL(url)\n",
      "\n",
      "prod = doc.find_all(class_=\"dpb-holder loaded svelte-d5pqmr\")\n",
      "160/60: len(prod)\n",
      "160/61: doc.find(\"h1\").text\n",
      "160/62: prod[1].find(\"h2\",{\"class\":\"svelte-1e4me6r\"}).text\n",
      "160/63: prod[1].find(\"h2\",{\"class\":\"svelte-d5pqmr\"}).text\n",
      "160/64: prod[1].find(\"strong\",{\"class\":\"svelte-d5pqmr\"}).text\n",
      "160/65: prod[0].find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "160/66: prod[1].find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "160/67: prod[15].find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "160/68: prod[15].find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "160/69:\n",
      "url_base = \"https://www.decathlon.de\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-d5pqmr\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-d5pqmr\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/70: prod_list\n",
      "160/71:\n",
      "#NL - Woman sportwear\n",
      "url = 'https://www.decathlon.co.uk/browse/c0-women/_/N-1an7dur'\n",
      "\n",
      "doc = getAndParseURL(url)\n",
      "\n",
      "prod = doc.find_all(class_=\"dpb-holder loaded svelte-1ls8j3w\")\n",
      "160/72: len(prod)\n",
      "160/73: prod[1].find(\"strong\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "160/74: prod[1].find(\"h2\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "160/75:\n",
      "url_base = \"https://www.decathlon.co.uk\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/76: prod_list\n",
      "160/77:\n",
      "url_base = \"https://www.decathlon.co.uk\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/78: prod_list\n",
      "160/79:\n",
      "url_base = \"https://www.decathlon.co.uk\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}).replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/80:\n",
      "url_base = \"https://www.decathlon.co.uk\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}).replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/81:\n",
      "url_base = \"https://www.decathlon.co.uk\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/82:\n",
      "url_base = \"https://www.decathlon.co.uk\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}).replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/83:\n",
      "url_base = \"https://www.decathlon.co.uk\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}).text.replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/84: prod_list\n",
      "160/85:\n",
      "url_base = \"https://www.decathlon.co.uk\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-1bh0x5d\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/86:\n",
      "#Woman sportwear\n",
      "url = 'https://www.decathlon.ch/de/browse/c0-damen/c1-damenbekleidung/_/N-1ezwtdc'\n",
      "\n",
      "doc = getAndParseURL(url)\n",
      "\n",
      "prod = doc.find_all(class_=\"dpb-holder loaded svelte-aim4wv\")\n",
      "160/87: len(prod)\n",
      "160/88:\n",
      "#Woman sportwear\n",
      "url = 'https://www.decathlon.ch/de/browse/c0-damen/c1-damenbekleidung/_/N-1ezwtdc'\n",
      "\n",
      "doc = getAndParseURL(url)\n",
      "\n",
      "prod = doc.find_all(class_=\"dpb-holder loaded svelte-aim4wv\")\n",
      "\n",
      "per_page = 20\n",
      "160/89: doc.find(\"h1\").text\n",
      "160/90: prod[1].find(\"h2\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "160/91: prod[1].find(\"h2\",{\"class\":\"svelte-aim4wv\"}).text\n",
      "160/92: prod[1].find(\"strong\",{\"class\":\"svelte-aim4wv\"}).text\n",
      "160/93: prod[0].find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "160/94:\n",
      "url_base = \"https://www.decathlon.ch/de/\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-1ls8j3w\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/95:\n",
      "url_base = \"https://www.decathlon.ch/de/\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-Z19lq375\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-Z19lq375\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/96:\n",
      "url_base = \"https://www.decathlon.ch/de/\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":\"svelte-aim4wv\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":\"svelte-aim4wv\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": \"sticker svelte-15lojui\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/97: prod_list\n",
      "160/98:\n",
      "#Woman sportwear\n",
      "url = 'https://www.decathlon.ch/de/browse/c0-damen/c1-damenbekleidung/_/N-1ezwtdc'\n",
      "code = 'aim4wv'\n",
      "doc = getAndParseURL(url)\n",
      "\n",
      "prod = doc.find_all(class_=f\"dpb-holder loaded svelte-{code}\")\n",
      "\n",
      "#per_page = 20\n",
      "160/99:\n",
      "#Woman sportwear\n",
      "url = 'https://www.decathlon.co.uk/browse/c0-women/c1-clothing/_/N-7reina'\n",
      "code = '1ls8j3w'\n",
      "stk_code = '1bh0x5d'\n",
      "doc = getAndParseURL(url)\n",
      "\n",
      "prod = doc.find_all(class_=f\"dpb-holder loaded svelte-{code}\")\n",
      "\n",
      "#per_page = 20\n",
      "160/100: len(prod)\n",
      "160/101:\n",
      "url_base = \"https://www.decathlon.co.uk\"\n",
      "prod_list = []\n",
      "\n",
      "for product in prod:\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_cat = doc.find(\"h1\").text\n",
      "    prod_title = product.find(\"h2\",{\"class\":f\"svelte-{code}\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":f\"svelte-{code}\"}).text\n",
      "    prod_url = f'{url_base}{link}'\n",
      "    \n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "    \n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "    \n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "    \n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        \n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": f\"sticker svelte-{stk_code}\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{stk_code}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "    \n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'category' : prod_cat})\n",
      "160/102: prod_list\n",
      "163/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "163/2:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "163/3: sport = doc.find_all(class_=\"filter-content-input-group\")\n",
      "163/4:\n",
      "#DE\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "\n",
      "doc = getAndParseURL(url)\n",
      "163/5:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "163/6:\n",
      "#Function for parsing the URLs\n",
      "def getAndParseURL(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "163/7:\n",
      "#DE\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "\n",
      "doc = getAndParseURL(url)\n",
      "163/8:\n",
      "#DE\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "\n",
      "doc = getAndParseURL(filter_link)\n",
      "163/9: sport = doc.find_all(class_=\"filter-content-input-group\")\n",
      "163/10:\n",
      "sport = doc.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "len(sport)\n",
      "163/11: sport\n",
      "163/12: sport[4]\n",
      "163/13: sport[0]\n",
      "163/14: $ chromedriver\n",
      "163/15:\n",
      "options = webdriver.ChromeOptions()\n",
      "options.add_argument('--ignore-certificate-errors')\n",
      "options.add_argument(\"--test-type\")\n",
      "options.binary_location = \"/usr/bin/chromium\"\n",
      "driver = webdriver.Chrome(chrome_options=options)\n",
      "driver.get('https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd')\n",
      "163/16:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "163/17:\n",
      "options = webdriver.ChromeOptions()\n",
      "options.add_argument('--ignore-certificate-errors')\n",
      "options.add_argument(\"--test-type\")\n",
      "options.binary_location = \"/usr/bin/chromium\"\n",
      "driver = webdriver.Chrome(chrome_options=options)\n",
      "driver.get('https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd')\n",
      "164/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "164/2:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(browser,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters js svelte-11vrp39\")))\n",
      "\n",
      "buttons = driver.find_elements_by_class_name(\"cta cta--outline cta--2nd cta--small\")\n",
      "for button in buttons:\n",
      "    button.click()\n",
      "164/3:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters js svelte-11vrp39\")))\n",
      "\n",
      "buttons = driver.find_elements_by_class_name(\"cta cta--outline cta--2nd cta--small\")\n",
      "for button in buttons:\n",
      "    button.click()\n",
      "164/4:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters js svelte-11vrp39\")))\n",
      "\n",
      "buttons = driver.find_elements_by_class_name(\"cta cta--outline cta--2nd cta--small\")\n",
      "for button in buttons:\n",
      "    button.click()\n",
      "164/5:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "#buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters js svelte-11vrp39\")))\n",
      "\n",
      "buttons = driver.find_elements_by_class_name(\"cta cta--outline cta--2nd cta--small\")\n",
      "for button in buttons:\n",
      "    button.click()\n",
      "164/6:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "#buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters js svelte-11vrp39\")))\n",
      "\n",
      "buttons = driver.find_elements(by = By.CLASS_NAME, value = \"cta cta--outline cta--2nd cta--small\")\n",
      "for button in buttons:\n",
      "    button.click()\n",
      "164/7: buttons\n",
      "164/8:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters js svelte-11vrp39\")))\n",
      "\n",
      "buttons = driver.find_elements(by = By.CLASS_NAME, value = \"cta cta--outline cta--2nd cta--small\")\n",
      "for button in buttons:\n",
      "    button.click()\n",
      "164/9:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "#buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters js svelte-11vrp39\")))\n",
      "\n",
      "buttons = driver.find_elements(By.CLASS_NAME, \"cta cta--outline cta--2nd cta--small\")\n",
      "\n",
      "for button in buttons:\n",
      "    button.click()\n",
      "164/10: buttons\n",
      "164/11:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "#buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters js svelte-11vrp39\")))\n",
      "\n",
      "buttons = driver.find_elements(By.CLASS_NAME, \"cta cta--outline cta--2nd cta--small\")\n",
      "164/12: buttons\n",
      "164/13:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "#buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters js svelte-11vrp39\")))\n",
      "\n",
      "buttons = driver.find_elements(By.CLASS_NAME, \"cta--outline\")\n",
      "164/14: buttons\n",
      "164/15:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "#buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters js svelte-11vrp39\")))\n",
      "\n",
      "buttons = driver.find_elements(By.CLASS_NAME, \"cta--outline\")\n",
      "\n",
      "for button in buttons:\n",
      "    button.click()\n",
      "164/16: buttons[0]\n",
      "164/17:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "#buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters js svelte-11vrp39\")))\n",
      "\n",
      "buttons = driver.find_elements(By.CLASS_NAME, \"cta--outline\")\n",
      "\n",
      "for button in buttons:\n",
      "    print(button)\n",
      "164/18:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "buttons = driver.find_elements(By.CLASS_NAME, \"cta--outline\")\n",
      "\n",
      "for button in buttons:\n",
      "    print(button)\n",
      "164/19:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "buttons = driver.find_elements(By.CLASS_NAME, \"cta--outline\")\n",
      "\n",
      "for button in buttons:\n",
      "    button.click()\n",
      "164/20:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "buttons = driver.find_elements(By.XPATH, '//button[@class='cta']')\n",
      "\n",
      "#for button in buttons:\n",
      "#    button.click()\n",
      "164/21:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "buttons = driver.find_elements(By.XPATH, '//button[@class=\"cta\"]')\n",
      "\n",
      "#for button in buttons:\n",
      "#    button.click()\n",
      "164/22: buttons\n",
      "164/23:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "#buttons = driver.find_elements(By.XPATH, '//button[@class=\"cta\"]')\n",
      "\n",
      "#for button in buttons:\n",
      "#    button.click()\n",
      "164/24: buttons\n",
      "165/1:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "#buttons = driver.find_elements(By.XPATH, '//button[@class=\"cta\"]')\n",
      "\n",
      "#for button in buttons:\n",
      "#    button.click()\n",
      "165/2:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "165/3:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "#buttons = driver.find_elements(By.XPATH, '//button[@class=\"cta\"]')\n",
      "\n",
      "#for button in buttons:\n",
      "#    button.click()\n",
      "165/4:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "buttons = driver.find_elements(By.XPATH, '//button[@type=\"button\"]')\n",
      "\n",
      "buttons[1]\n",
      "\n",
      "#for button in buttons:\n",
      "#    button.click()\n",
      "165/5:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "buttons = driver.find_elements(By.XPATH, '//button[@type=\"button\"]')\n",
      "\n",
      "for button in buttons:\n",
      "    button.click()\n",
      "165/6:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "for button in buttons:\n",
      "    button.click()\n",
      "165/7: buttons\n",
      "165/8: len(buttons)\n",
      "165/9:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "button[5].click()\n",
      "165/10:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"filters\")))\n",
      "\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "buttons[5].click()\n",
      "165/11:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "buttons[5].click()\n",
      "165/12:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "buttons[5].click()\n",
      "165/13:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "\n",
      "driver.execute_script(\"arguments[0].click();\", buttons[5])\n",
      "#buttons[5].click()\n",
      "165/14:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "\n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "165/15: html_source = driver.page_source\n",
      "165/16:\n",
      "driver = webdriver.Chrome()\n",
      "filter_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    soup = bts(driver.page_source)\n",
      "165/17: soup\n",
      "165/18:\n",
      "sport = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "len(sport)\n",
      "165/19:\n",
      "sport = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "len(sport)\n",
      "165/20: sport[0]\n",
      "165/21:\n",
      "sport = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "sport[len(sport)-1]\n",
      "165/22:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "sport <- [len(filter)-1]\n",
      "165/23:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "sport = [len(filter)-1]\n",
      "165/24: sport[0]\n",
      "165/25:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "sport = [len(filter)-1]\n",
      "165/26: sport\n",
      "165/27:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "sport = filter[len(filter)-1]\n",
      "165/28: sport\n",
      "165/29: len(sport)\n",
      "165/30: sport[1]\n",
      "165/31: sport[0]\n",
      "165/32: sport\n",
      "165/33: type(sport)\n",
      "165/34: sport.find_all(\"a\").attrs[\"href\"]\n",
      "165/35:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "sport = filter[len(filter)-1]\n",
      "165/36: sport.find_all(\"a\")\n",
      "165/37:\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = sport.text\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                     'sport_link': sport_link})\n",
      "165/38:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "sports = filter[len(filter)-1]\n",
      "165/39:\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = sport.text\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                     'sport_link': sport_link})\n",
      "165/40: sport_list\n",
      "165/41: sport_list[0]\n",
      "165/42:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = sport.text\n",
      "    re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", sport_type)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                     'sport_link': sport_link})\n",
      "165/43: sport_list[0]\n",
      "165/44:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = sport.text\n",
      "    re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", sport_type)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                     'sport_link': sport_link})\n",
      "165/45:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = sport.text\n",
      "    re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", sport_type)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/46: sport_list[0]\n",
      "165/47:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = sport.text\n",
      "    re.sub(\"\\(\\.*?\\)\\\", \"\", sport_type)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/48:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = sport.text\n",
      "    re.sub(\"(.*?)\", \"\", sport_type)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/49: sport_list[0]\n",
      "165/50:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = re.sub(\"(.*?)\",\"\", sport.text)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/51: sport_list[0]\n",
      "165/52:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = sport.text\n",
      "    re.sub(\"(.*?)\",\"\", sport.text)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/53: sport_list[0]\n",
      "165/54:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", sport.text)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/55: sport_list[0]\n",
      "165/56:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = re.sub(\"\\(\\.*?\\)\\\", \"\", sport.text)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/57:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = re.sub(\"[\\(\\.*?\\)\\]\", \"\", sport.text)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/58:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = re.sub(\"[\\(\\.*?\\)\\]\", \"\", sport.text)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/59:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = re.sub(\"[\\(].*?[\\)]\", \"\", sport.text)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/60: sport_list[0]\n",
      "165/61:\n",
      "import re\n",
      "\n",
      "sport_list = []\n",
      "for sport in sports.find_all(\"a\"):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport.text)\n",
      "    sport_link = sport.attrs[\"href\"]\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/62: sport_list[0]\n",
      "165/63: sport_list\n",
      "165/64:\n",
      "a = '/browse/c0-damen/_/N-1wmfzjjZhpsxkdZ4505jj'\n",
      "\n",
      "a - '/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "165/65:\n",
      "a = '/browse/c0-damen/_/N-1wmfzjjZhpsxkdZ4505jj'\n",
      "\n",
      "b = '/browse/c0-damen/_/N-1wmfzjjZhpsxkd'\n",
      "\n",
      "a.replace(b,\"\")\n",
      "165/66:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import re\n",
      "165/67:\n",
      "driver = webdriver.Chrome()\n",
      "\n",
      "base_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_code = 'Zhpsxkd'\n",
      "\n",
      "filter_link = f{base_link}{base_code}\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/68:\n",
      "driver = webdriver.Chrome()\n",
      "\n",
      "base_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_code = 'Zhpsxkd'\n",
      "\n",
      "filter_link = f'{base_link}{base_code}''\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/69:\n",
      "driver = webdriver.Chrome()\n",
      "\n",
      "base_link = 'https://www.decathlon.de/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_code = 'Zhpsxkd'\n",
      "\n",
      "filter_link = f'{base_link}{base_code}'\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/70:\n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "165/71:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_link': sport_link})\n",
      "\n",
      "for i in range(1, len(sport_url) + 1):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport.text) #remove all numbers\n",
      "    sport_code = sport.attrs[\"href\"].replace(filter_link,\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_link': sport_link})\n",
      "165/72: sport_list\n",
      "165/73: sport_url\n",
      "165/74: sport_list[0]\n",
      "165/75:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url) + 1):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport.text) #remove all numbers\n",
      "    sport_code = sport.attrs[\"href\"].replace(filter_link,\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/76:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url) + 1):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport.text) #remove all numbers\n",
      "    sport_code = sport.attrs[\"href\"].replace(filter_link,\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/77:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url) + 1):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport.text) #remove all numbers\n",
      "    sport_code = sport.attrs[\"href\"].replace(filter_link,\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/78: sport_list[0]\n",
      "165/79: sport_list\n",
      "165/80: filter_link\n",
      "165/81: sport_list[1]\n",
      "165/82:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url) + 1):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport.text) #remove all numbers\n",
      "    sport_code = sport.attrs[\"href\"].text.replace(filter_link,\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/83:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url) + 1):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport.text) #remove all numbers\n",
      "    sport_code = sport.attrs[\"href\"].replace(filter_link,\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/84:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url) + 1):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(filter_link,\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/85: sport_url[1].text\n",
      "165/86: len(sport_url) + 1\n",
      "165/87: print(range(1,135))\n",
      "165/88: range(1,135)\n",
      "165/89:\n",
      "for i in range(1,135):\n",
      "    print(i)\n",
      "165/90: len(sport_url)\n",
      "165/91:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url) - 1):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(filter_link,\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/92: sport_list\n",
      "165/93:\n",
      "driver = webdriver.Chrome()\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "\n",
      "filter_link = f'{base_index}{base_link}{base_code}'\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/94:\n",
      "driver = webdriver.Chrome()\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "\n",
      "filter_link = f'{base_index}{base_link}{base_code}'\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/95:\n",
      "driver = webdriver.Chrome()\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{base_code}'\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/96:\n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "165/97:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url) - 1):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(base_cat,\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/98: sport_list\n",
      "165/99:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url) - 1):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{base_code}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/100: sport_list\n",
      "165/101: len(sport_list)\n",
      "165/102:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{base_code}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/103: len(sport_list)\n",
      "165/104:\n",
      "driver = webdriver.Chrome()\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-herren/_/N-1m7gkpt'\n",
      "base_sport = 'Zhpsxkd'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{base_code}'\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/105:\n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "165/106:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{base_code}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/107: len(sport_list)\n",
      "165/108:\n",
      "driver = webdriver.Chrome()\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-kinder/_/N-pavoh3'\n",
      "base_sport = 'Zhpsxkd'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{base_code}'\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/109:\n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "165/110:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{base_code}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/111: len(sport_list)\n",
      "165/112:\n",
      "driver = webdriver.Chrome()\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-accessoires/_/N-xx62mz'\n",
      "base_sport = 'Zhpsxkd'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{base_code}'\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/113:\n",
      "driver = webdriver.Chrome()\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-kinder/_/N-pavoh3'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_code}'\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/114:\n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "165/115:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_code\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_code}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/116: len(sport_list)\n",
      "165/117:\n",
      "driver = webdriver.Chrome()\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-kinder/_/N-pavoh3'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/118:\n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "165/119:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/120: len(sport_list)\n",
      "165/121:\n",
      "driver = webdriver.Chrome()\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "165/122:\n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "165/123:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/124: len(sport_list)\n",
      "165/125:\n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "165/126:\n",
      "sport_list = []\n",
      "\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/127: len(sport_list)\n",
      "165/128:\n",
      "def getSportList(base_index, base_cat, sold_by, base_sport):\n",
      "    filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "    driver.get(filter_link)\n",
      "\n",
      "    #accept cookie popup\n",
      "    WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "    #get all buttons and click on them all (show more) by \n",
      "    buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "    buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "    #click all buttons\n",
      "    for button in buttons:\n",
      "        driver.execute_script(\"arguments[0].click();\", button)\n",
      "        #getting the soup\n",
      "        soup = bts(driver.page_source)\n",
      "    \n",
      "    # access all filters:\n",
      "    filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "    # access sport filter at the last position:\n",
      "    sports = filter[len(filter)-1]\n",
      "    \n",
      "    sport_list = []\n",
      "    sport_url = sports.find_all(\"a\")\n",
      "\n",
      "    #first item\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "    sport_code = base_sport\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "\n",
      "    for i in range(1, len(sport_url)):\n",
      "        sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "        sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "        sport_list.append({'sport_type': sport_type,\n",
      "                           'sport_code': sport_code})\n",
      "    return (sport_list)\n",
      "165/129:\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "sport_list = getSportList(base_index, base_cat, sold_by, base_sport)\n",
      "165/130:\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "sport_list = getSportList(base_index, base_cat, sold_by, base_sport)\n",
      "165/131:\n",
      "def getSportSoup(base_index, base_cat, sold_by, base_sport):\n",
      "    filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "    driver.get(filter_link)\n",
      "\n",
      "    #accept cookie popup\n",
      "    WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "    #get all buttons and click on them all (show more) by \n",
      "    buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "    buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "    #click all buttons\n",
      "    for button in buttons:\n",
      "        driver.execute_script(\"arguments[0].click();\", button)\n",
      "        #getting the soup\n",
      "        soup = bts(driver.page_source)\n",
      "    return soup\n",
      "165/132:\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "soup = getSportSoup(base_index, base_cat, sold_by, base_sport)\n",
      "165/133:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import re\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "165/134:\n",
      "def getSportSoup(base_index, base_cat, sold_by, base_sport):\n",
      "    filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "    driver.get(filter_link)\n",
      "\n",
      "    #accept cookie popup\n",
      "    WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "    #get all buttons and click on them all (show more) by \n",
      "    buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "    buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "    #click all buttons\n",
      "    for button in buttons:\n",
      "        driver.execute_script(\"arguments[0].click();\", button)\n",
      "        #getting the soup\n",
      "        soup = bts(driver.page_source)\n",
      "    return soup\n",
      "165/135:\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "soup = getSportSoup(base_index, base_cat, sold_by, base_sport)\n",
      "165/136:\n",
      "def getSportList(base_index, base_cat, sold_by, base_sport):\n",
      "    filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "    driver.get(filter_link)\n",
      "\n",
      "    #accept cookie popup\n",
      "    WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "    #get all buttons and click on them all (show more) by \n",
      "    buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "    buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "    #click all buttons\n",
      "    for button in buttons:\n",
      "        driver.execute_script(\"arguments[0].click();\", button)\n",
      "        #getting the soup\n",
      "        soup = bts(driver.page_source)\n",
      "    \n",
      "    # access all filters:\n",
      "    filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "    # access sport filter at the last position:\n",
      "    sports = filter[len(filter)-1]\n",
      "\n",
      "    sport_list = []\n",
      "    sport_url = sports.find_all(\"a\")\n",
      "\n",
      "    #first item\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "    sport_code = base_sport\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "\n",
      "    for i in range(1, len(sport_url)):\n",
      "        sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "        sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "        sport_list.append({'sport_type': sport_type,\n",
      "                           'sport_code': sport_code})\n",
      "    return (sport_list)\n",
      "165/137:\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "sport_list = getSportList(base_index, base_cat, sold_by, base_sport)\n",
      "165/138:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import re\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "165/139:\n",
      "def getSportSoup(base_index, base_cat, sold_by, base_sport):\n",
      "    filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "    driver.get(filter_link)\n",
      "\n",
      "    #accept cookie popup\n",
      "    WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "    #get all buttons and click on them all (show more) by \n",
      "    buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "    buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "    #click all buttons\n",
      "    for button in buttons:\n",
      "        driver.execute_script(\"arguments[0].click();\", button)\n",
      "        #getting the soup\n",
      "        soup = bts(driver.page_source)\n",
      "    \n",
      "    return soup\n",
      "165/140:\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "sport_soup = getSportSoup(base_index, base_cat, sold_by, base_sport)\n",
      "165/141:\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "soup = getSportSoup(base_index, base_cat, sold_by, base_sport)\n",
      "165/142:\n",
      "#input\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "    \n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "\n",
      "sport_list = []\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/143:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import re\n",
      "165/144:\n",
      "#input\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "driver = webdriver.Chrome()\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "    \n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "\n",
      "sport_list = []\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/145:\n",
      "#input\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "driver = webdriver.Chrome()\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "    \n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "\n",
      "sport_list = []\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "165/146: len(sport_list)\n",
      "165/147:\n",
      "# get category links\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "165/148:\n",
      "# get category links\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "\n",
      "categories[0]\n",
      "165/149:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "\n",
      "for cat in categories:\n",
      "    level_1 = cat.find(\"h1\").text\n",
      "    cat_name = cat.find(\"a\").text\n",
      "    cat_link = cat.find(\"a\").attrs[\"href\"]\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'cat_name': cat_name,\n",
      "                     'cat_link': cat_link})\n",
      "\n",
      "cat_list\n",
      "165/150:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "\n",
      "categories\n",
      "165/151:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "\n",
      "categories\n",
      "165/152:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "\n",
      "categories[1]\n",
      "165/153:\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    cat_name = cat.find(\"a\").text\n",
      "    cat_link = cat.find(\"a\").attrs[\"href\"]\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'cat_name': cat_name,\n",
      "                     'cat_link': cat_link})\n",
      "\n",
      "cat_list\n",
      "165/154:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "\n",
      "categories[1].find(\"a\")\n",
      "165/155:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "165/156: categories[0].find(\"a\")\n",
      "165/157: categories[0]\n",
      "165/158: categories[0].text\n",
      "165/159: categories[0].find('img')\n",
      "165/160: categories[0].find('img').attrs['alt']\n",
      "165/161: categories[0].find('a').attrs['href']\n",
      "165/162: categories[0]\n",
      "165/163: categories[0].find(\"href\")\n",
      "165/164: categories[0].find(\"a\")\n",
      "165/165: categories[0]\n",
      "165/166: categories[0].find(\"a\")\n",
      "165/167: categories[0].find(\"img\")\n",
      "165/168: categories[0].attrs[\"href\"]\n",
      "165/169:\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    cat_name = cat.find('img').attrs['alt']\n",
      "    cat_link = cat.attrs[\"href\"]\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'cat_name': cat_name,\n",
      "                     'cat_link': cat_link})\n",
      "\n",
      "cat_list\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165/170: categories[0].find('img').attrs['alt']\n",
      "165/171:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "165/172:\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    cat_name = cat.find('img').attrs['alt']\n",
      "    cat_link = cat.attrs[\"href\"]\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'cat_name': cat_name,\n",
      "                     'cat_link': cat_link})\n",
      "\n",
      "cat_list\n",
      "165/173: categories[7].find('img').attrs['alt']\n",
      "165/174: categories[6].find('img').attrs['alt']\n",
      "165/175: categories[5].find('img').attrs['alt']\n",
      "165/176: categories[4].find('img').attrs['alt']\n",
      "165/177: categories[3].find('img').attrs['alt']\n",
      "165/178: categories[2].find('img').attrs['alt']\n",
      "165/179: categories[1].find('img').attrs['alt']\n",
      "165/180: categories[2].find('img').attrs['alt']\n",
      "165/181: categories[2].text\n",
      "165/182: categories[0].text\n",
      "165/183: categories[1].text\n",
      "165/184: categories[2].text\n",
      "165/185: categories\n",
      "166/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import re\n",
      "166/2:\n",
      "#input\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "driver = webdriver.Chrome()\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "    \n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "\n",
      "sport_list = []\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "166/3:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import re\n",
      "166/4:\n",
      "#input\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "driver = webdriver.Chrome()\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "    \n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "\n",
      "sport_list = []\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "166/5:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "166/6: categories\n",
      "166/7: categories[0].attrs['data-help']\n",
      "166/8:\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    cat_name = cat.attrs['data-help']\n",
      "    cat_link = cat.attrs[\"href\"]\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'cat_name': cat_name,\n",
      "                     'cat_link': cat_link})\n",
      "\n",
      "cat_list\n",
      "166/9:\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    level_2 = cat.attrs['data-help']\n",
      "    cat_link = cat.attrs[\"href\"].replace[base_sport,\"\"]\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'cat_name': cat_name,\n",
      "                     'cat_link': cat_link})\n",
      "\n",
      "cat_list\n",
      "166/10:\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    level_2 = cat.attrs['data-help']\n",
      "    cat_link = cat.attrs[\"href\"].replace(base_sport,\"\")\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'cat_name': cat_name,\n",
      "                     'cat_link': cat_link})\n",
      "\n",
      "cat_list\n",
      "166/11:\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    level_2 = cat.attrs['data-help']\n",
      "    cat_link = cat.attrs[\"href\"].replace(f'{base_sport}',\"\")\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'cat_name': cat_name,\n",
      "                     'cat_link': cat_link})\n",
      "\n",
      "cat_list\n",
      "166/12:\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    level_2 = cat.attrs['data-help']\n",
      "    cat_link = cat.attrs[\"href\"].replace(f'{base_sport}',\"\")\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'cat_name': cat_name,\n",
      "                     'cat_link': cat_link})\n",
      "\n",
      "cat_list\n",
      "166/13:\n",
      "a = '/browse/c0-damen/c1-sportbekleidung/_/N-1peigq4Z1da2q0eZhpsxkd'\n",
      "\n",
      "a.replace(base_sport,\"\")\n",
      "166/14:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "166/15:\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    level_2 = cat.attrs['data-help']\n",
      "    cat_link = cat.attrs[\"href\"].replace(base_sport,\"\")\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'cat_name': cat_name,\n",
      "                     'cat_link': cat_link})\n",
      "\n",
      "cat_list\n",
      "166/16:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    level_2 = cat.attrs['data-help']\n",
      "    cat_link = cat.attrs[\"href\"].replace(base_sport,\"\")\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'cat_name': cat_name,\n",
      "                     'cat_link': cat_link})\n",
      "166/17: cat_list\n",
      "166/18:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    level_2 = cat.attrs['data-help']\n",
      "    cat_link = cat.attrs[\"href\"].replace(base_sport,\"\")\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'level_2': level_2,\n",
      "                     'cat_link': cat_link})\n",
      "166/19: cat_list\n",
      "166/20:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    level_2 = cat.attrs['data-help']\n",
      "    cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}''\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'level_2': level_2,\n",
      "                     'cat_link': cat_link})\n",
      "166/21:\n",
      "# get category links\n",
      "cat_list = []\n",
      "\n",
      "categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "\n",
      "for cat in categories:\n",
      "    level_1 = soup.find(\"h1\").text\n",
      "    level_2 = cat.attrs['data-help']\n",
      "    cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "    cat_list.append({'level_1': level_1,\n",
      "                     'level_2': level_2,\n",
      "                     'cat_link': cat_link})\n",
      "166/22: cat_list\n",
      "166/23: len(cat_list)\n",
      "166/24:\n",
      "len(cat_list)\n",
      "len(sport_list)\n",
      "166/25:\n",
      "len(cat_list)\n",
      "len(sport_list)\n",
      "166/26: print(len(cat_list), len(sport_list))\n",
      "166/27:\n",
      "print(len(cat_list), len(sport_list))\n",
      "\n",
      "cat_list[0]\n",
      "166/28:\n",
      "print(len(cat_list), len(sport_list))\n",
      "\n",
      "cat_list[0]\n",
      "sport_list[0]\n",
      "166/29: cat_list[0]\n",
      "166/30: cat_list[0][level_1]\n",
      "166/31: cat_list[0](level_1)\n",
      "166/32: cat_list[0](\"level_1\")\n",
      "166/33: cat_list[0][\"level_1\"]\n",
      "166/34:\n",
      "#constructing links for categories level_2 + filter\n",
      "cat_level3=[]\n",
      "for cat in cat_list:\n",
      "    for sport in sport_list:\n",
      "        level_1 = cat[\"level_1\"]\n",
      "        level_2 = cat[\"level_2\"]\n",
      "        sport_type = sport[\"sport_type\"]\n",
      "        url = f'{cat['cat_link']}{sport['sport_code']}'\n",
      "        cat_level3.append({'level_1': level_1,\n",
      "                           'level_2': level_2,\n",
      "                           'sport_type': sport_type,\n",
      "                           'url': url})\n",
      "166/35: cat_list[0]\n",
      "166/36:\n",
      "#constructing links for categories level_2 + filter\n",
      "cat_level3=[]\n",
      "for cat in cat_list:\n",
      "    for sport in sport_list:\n",
      "        level_1 = cat[\"level_1\"]\n",
      "        level_2 = cat[\"level_2\"]\n",
      "        sport_type = sport[\"sport_type\"]\n",
      "        url = f'{cat[\"cat_link\"]}{sport[\"sport_code\"]}'\n",
      "        cat_level3.append({'level_1': level_1,\n",
      "                           'level_2': level_2,\n",
      "                           'sport_type': sport_type,\n",
      "                           'url': url})\n",
      "166/37:\n",
      "#constructing links for categories level_2 + filter\n",
      "cat_level3=[]\n",
      "for cat in cat_list:\n",
      "    for sport in sport_list:\n",
      "        level_1 = cat[\"level_1\"]\n",
      "        level_2 = cat[\"level_2\"]\n",
      "        sport_type = sport[\"sport_type\"]\n",
      "        url = f'{cat[\"cat_link\"]}{sport[\"sport_code\"]}'\n",
      "        cat_level3.append({'level_1': level_1,\n",
      "                           'level_2': level_2,\n",
      "                           'sport_type': sport_type,\n",
      "                           'url': url})\n",
      "166/38: cat_level3[0]\n",
      "166/39: len(cat_level3)\n",
      "166/40:\n",
      "cat_list= []\n",
      "\n",
      "cat_list = getCatlink(soup, cat_list)\n",
      "166/41:\n",
      "# Function to get category links\n",
      "def getCatLink(soup, cat_list):\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    return cat_list\n",
      "166/42:\n",
      "cat_list= []\n",
      "\n",
      "cat_list = getCatlink(soup, cat_list)\n",
      "166/43:\n",
      "# Function to get category links\n",
      "def getCatLink(soup, cat_list):\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    return cat_list\n",
      "166/44:\n",
      "# Function to get category links\n",
      "def getCatLink(soup, cat_list):\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    return cat_list\n",
      "166/45:\n",
      "cat_list= []\n",
      "\n",
      "cat_list = getCatLink(soup, cat_list)\n",
      "166/46:\n",
      "print(len(cat_list), len(sport_list))\n",
      "\n",
      "cat_list[0]\n",
      "166/47:\n",
      "# Function to get category links\n",
      "def getCatLink(base_index, base_cat, cat_list):\n",
      "    url = f'{base_index}{base_cat}'\n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    return cat_list\n",
      "166/48:\n",
      "# Importing category_url data\n",
      "cat_url = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_url.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_url.append(cat)\n",
      "166/49:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import re\n",
      "import csv\n",
      "166/50:\n",
      "# Importing category_url data\n",
      "cat_url = []  \n",
      "# opening the file using \"with\" \n",
      "# statement\n",
      "with open('cat_url.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_url.append(cat)\n",
      "166/51: cat_url[0]\n",
      "166/52:\n",
      "#getting category links for all genders:\n",
      "cat_list =[]\n",
      "for i in cat_url:\n",
      "    base_index = i[\"base_index\"]\n",
      "    base_cat = i[\"base_cat\"]\n",
      "    cat_list = getCatLink(base_index, base_cat, cat_list)\n",
      "166/53:\n",
      "# Function to get category links\n",
      "def getCatLink(base_index, base_cat, cat_list):\n",
      "    url = f'{base_index}{base_cat}'\n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    return cat_list\n",
      "166/54:\n",
      "#getting category links for all genders:\n",
      "cat_list =[]\n",
      "for i in cat_url:\n",
      "    base_index = i[\"base_index\"]\n",
      "    base_cat = i[\"base_cat\"]\n",
      "    cat_list = getCatLink(base_index, base_cat, cat_list)\n",
      "166/55: cat_list[0]\n",
      "166/56: len(cat_list)\n",
      "166/57: cat_list\n",
      "166/58:\n",
      "#combine link for category level 2 and filtered by sport\n",
      "\n",
      "cat_level3=[]\n",
      "\n",
      "for cat in cat_list:\n",
      "    for sport in sport_list:\n",
      "        level_1 = cat[\"level_1\"]\n",
      "        level_2 = cat[\"level_2\"]\n",
      "        sport_type = sport[\"sport_type\"]\n",
      "        url = f'{cat[\"cat_link\"]}{sport[\"sport_code\"]}'\n",
      "        cat_level3.append({'level_1': level_1,\n",
      "                           'level_2': level_2,\n",
      "                           'sport_type': sport_type,\n",
      "                           'url': url})\n",
      "166/59: len(cat_level3)\n",
      "166/60: cat_level3[]\n",
      "166/61: cat_level3[0]\n",
      "166/62:\n",
      "#saving cat_level3\n",
      "\n",
      "with open(\"DE_cat_level3.csv\", \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"cat_l1\", \"cat_l2\", \"sport_type\", \"url\"])\n",
      "\n",
      "    for item in cat_level3:\n",
      "        writer.writerow([item['level_1'], item['level_2'], item['sport_type'], item['url']])\n",
      "166/63:\n",
      "# Function to get category links\n",
      "def getCatLink(base_index, base_cat, sold_by, cat_list):\n",
      "    url = f'{base_index}{base_cat}{sold_by}'\n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    return cat_list\n",
      "166/64:\n",
      "# Importing category_url data\n",
      "cat_url = []  \n",
      "# opening the file using \"with\" statement\n",
      "with open('cat_url.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_url.append(cat)\n",
      "166/65:\n",
      "#getting category links for all genders:\n",
      "cat_list =[]\n",
      "for i in cat_url:\n",
      "    base_index = i[\"base_index\"]\n",
      "    base_cat = i[\"base_cat\"]\n",
      "    cat_list = getCatLink(base_index, base_cat, sold_by, cat_list)\n",
      "166/66:\n",
      "# Importing category_url data\n",
      "cat_url = []  \n",
      "# opening the file using \"with\" statement\n",
      "with open('cat_url.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_url.append(cat)\n",
      "166/67:\n",
      "#getting category links for all genders:\n",
      "cat_list =[]\n",
      "for i in cat_url:\n",
      "    base_index = i[\"base_index\"]\n",
      "    base_cat = i[\"base_cat\"]\n",
      "    sold_by = i[\"sold_by\"]\n",
      "    cat_list = getCatLink(base_index, base_cat, sold_by, cat_list)\n",
      "166/68: cat_list\n",
      "166/69:\n",
      "#combine link for category level 2 and filtered by sport\n",
      "\n",
      "cat_level3=[]\n",
      "\n",
      "for cat in cat_list:\n",
      "    for sport in sport_list:\n",
      "        level_1 = cat[\"level_1\"]\n",
      "        level_2 = cat[\"level_2\"]\n",
      "        sport_type = sport[\"sport_type\"]\n",
      "        url = f'{cat[\"cat_link\"]}{sport[\"sport_code\"]}'\n",
      "        cat_level3.append({'level_1': level_1,\n",
      "                           'level_2': level_2,\n",
      "                           'sport_type': sport_type,\n",
      "                           'url': url})\n",
      "166/70: cat_level3[0]\n",
      "166/71:\n",
      "#saving cat_level3\n",
      "\n",
      "with open(\"DE_cat_level3.csv\", \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"cat_l1\", \"cat_l2\", \"sport_type\", \"url\"])\n",
      "\n",
      "    for item in cat_level3:\n",
      "        writer.writerow([item['level_1'], item['level_2'], item['sport_type'], item['url']])\n",
      "166/72:\n",
      "#saving cat_level3\n",
      "\n",
      "with open(\"DE_cat_level3.csv\", \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"cat_l1\", \"cat_l2\", \"sport_type\", \"url\"])\n",
      "\n",
      "    for item in cat_level3:\n",
      "        writer.writerow([item['level_1'], item['level_2'], item['sport_type'], item['url']])\n",
      "171/1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import re\n",
      "import csv\n",
      "171/2:\n",
      "#input\n",
      "country = 'DE'\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "driver = webdriver.Chrome()\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "    \n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "\n",
      "sport_list = []\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "171/3:\n",
      "# Function to get category links\n",
      "def getCatLink(base_index, base_cat, sold_by, cat_list):\n",
      "    url = f'{base_index}{base_cat}{sold_by}'\n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    return cat_list\n",
      "171/4:\n",
      "# Importing category_url data\n",
      "cat_url = []  \n",
      "# opening the file using \"with\" statement\n",
      "with open('cat_url.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_url.append(cat)\n",
      "171/5:\n",
      "#getting category links for all genders:\n",
      "cat_list =[]\n",
      "for i in cat_url:\n",
      "    base_index = i[\"base_index\"]\n",
      "    base_cat = i[\"base_cat\"]\n",
      "    sold_by = i[\"sold_by\"]\n",
      "    cat_list = getCatLink(base_index, base_cat, sold_by, cat_list)\n",
      "171/6: cat_list\n",
      "171/7:\n",
      "#combine link for category level 2 and filtered by sport\n",
      "\n",
      "cat_level3=[]\n",
      "\n",
      "for cat in cat_list:\n",
      "    for sport in sport_list:\n",
      "        country = country\n",
      "        level_1 = cat[\"level_1\"]\n",
      "        level_2 = cat[\"level_2\"]\n",
      "        sport_type = sport[\"sport_type\"]\n",
      "        url = f'{cat[\"cat_link\"]}{sport[\"sport_code\"]}'\n",
      "        cat_level3.append({'country': country,\n",
      "                           'level_1': level_1,\n",
      "                           'level_2': level_2,\n",
      "                           'sport_type': sport_type,\n",
      "                           'url': url})\n",
      "171/8: cat_level3[0]\n",
      "171/9:\n",
      "#saving cat_level3\n",
      "\n",
      "with open(\"DE_cat_level3.csv\", \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"country\",\"cat_l1\", \"cat_l2\", \"sport_type\", \"url\"])\n",
      "\n",
      "    for item in cat_level3:\n",
      "        writer.writerow([item['country'], item['level_1'], item['level_2'], item['sport_type'], item['url']])\n",
      "171/10: len(cat_level3)\n",
      "171/11:\n",
      "#saving cat_level3\n",
      "\n",
      "with open(\"DE_cat_level3.csv\", \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"country\",\"cat_level1\", \"cat_level2\", \"sport_type\", \"cat_url\"])\n",
      "\n",
      "    for item in cat_level3:\n",
      "        writer.writerow([item['country'], item['level_1'], item['level_2'], item['sport_type'], item['url']])\n",
      "171/12:\n",
      "#saving cat_level3\n",
      "\n",
      "with open(f\"{country}_cat_level3.csv\", \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"country\",\"cat_level1\", \"cat_level2\", \"sport_type\", \"cat_url\"])\n",
      "\n",
      "    for item in cat_level3:\n",
      "        writer.writerow([item['country'], item['level_1'], item['level_2'], item['sport_type'], item['url']])\n",
      "171/13:\n",
      "#Function for parsing the URLs\n",
      "def cookSoup(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "171/14:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(soup, cat_url, country, l1, l2, l3, total_page_src, per_page):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":f\"plp-bar-info svelte-{total_page_src}\"}).find(\"span\", {\"class\":f\"svelte-{total_page_src}\"}).text\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/per_page)\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{cat_url}?from={per_page * i}&size={per_page}'\n",
      "            url_list.append(page)\n",
      "    print (f'{country}_{l1}_{l2}_{l3}: There are {total_prod} products ({total_page} pages)')\n",
      "    return url_list\n",
      "171/15:\n",
      "# Function to get main data\n",
      "def getDecathlonData(base_index, url_list, country, l1, l2, l3, prod_list, prod_list_src, sticker_src):\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "        \n",
      "        for product in prod:\n",
      "            cat1 = l1\n",
      "            cat2 = l2\n",
      "            sport = l3\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "            prod_url = f'{base_index}{link}'\n",
      "\n",
      "            #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #Prices:\n",
      "            #for product without discount\n",
      "            if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "\n",
      "            #for product with discount\n",
      "            else:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "            #label:\n",
      "            if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'cat_1' : cat1,\n",
      "                             'cat_2' : cat2,\n",
      "                             'cat_3' : cat3})\n",
      "\n",
      "    print(f'{country}_{l1}_{l2}_{l3}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      "171/16:\n",
      "# country input\n",
      "country = 'DE'\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "prod_list_src = 'd5pqm'\n",
      "sticker_src = '15lojui'\n",
      "total_page_src = '1uqvrhu'\n",
      "per_page = 40\n",
      "171/17:\n",
      "#combine link for category level 2 and filtered by sport\n",
      "\n",
      "cat_level3=[]\n",
      "\n",
      "for cat in cat_list:\n",
      "    for sport in sport_list:\n",
      "        country = country\n",
      "        level_1 = cat[\"level_1\"]\n",
      "        level_2 = cat[\"level_2\"]\n",
      "        sport_type = sport[\"sport_type\"]\n",
      "        url = f'{cat[\"cat_link\"]}{sport[\"sport_code\"]}'\n",
      "        cat_level3.append({'country': country,\n",
      "                           'cat1': level_1,\n",
      "                           'cat2': level_2,\n",
      "                           'cat3': sport_type,\n",
      "                           'cat_url': url})\n",
      "171/18: len(cat_level3)\n",
      "171/19:\n",
      "#saving cat_level3\n",
      "\n",
      "with open(f\"{country}_cat_level3.csv\", \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"country\",\"cat1\", \"cat2\", \"cat3\", \"cat_url\"])\n",
      "\n",
      "    for item in cat_level3:\n",
      "        writer.writerow([item['country'], item['cat1'], item['cat2'], item['cat3'], item['cat_url']])\n",
      "171/20: cat_level3[1]\n",
      "171/21:\n",
      "prod_list = []\n",
      "\n",
      "for item in cat_level3:\n",
      "    country = item[\"country\"]\n",
      "    cat_url = item[\"cat_url\"]\n",
      "    cat1 = item[\"cat1\"]\n",
      "    cat2 = item[\"cat2\"]\n",
      "    cat3 = item[\"cat3\"]\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page)\n",
      "    prod_list = getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src)\n",
      "171/22:\n",
      "country = cat_level3[0][\"country\"]\n",
      "cat_url = cat_level3[0][\"cat_url\"]\n",
      "cat1 = cat_level3[0][\"cat1\"]\n",
      "cat2 = cat_level3[0][\"cat2\"]\n",
      "cat3 = cat_level3[0][\"cat3\"]\n",
      "soup = cookSoup(cat_url)\n",
      "171/23: soup\n",
      "171/24: url_list = pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page)\n",
      "171/25: prod_list = getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src)\n",
      "171/26: url_list = pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page)\n",
      "171/27:\n",
      "for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "        \n",
      "        for product in prod:\n",
      "            cat1 = cat1\n",
      "            cat2 = cat2\n",
      "            sport = cat3\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "            prod_url = f'{base_index}{link}'\n",
      "\n",
      "            #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #Prices:\n",
      "            #for product without discount\n",
      "            if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "\n",
      "            #for product with discount\n",
      "            else:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "            #label:\n",
      "            if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'cat_1' : cat1,\n",
      "                             'cat_2' : cat2,\n",
      "                             'cat_3' : cat3})\n",
      "\n",
      "    print(f'{country}_{l1}_{l2}_{l3}: {len(prod_list)} products have been scraped!')\n",
      "171/29: url_list\n",
      "171/30:\n",
      "for url in url_list:\n",
      "    page_soup = cookSoup(url)\n",
      "    prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "\n",
      "    for product in prod:\n",
      "        cat1 = cat1\n",
      "        cat2 = cat2\n",
      "        sport = cat3\n",
      "        link = product.find(\"a\").attrs[\"href\"]\n",
      "        prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "        brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "        prod_url = f'{base_index}{link}'\n",
      "\n",
      "        #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "        #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "        if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "            prod_id = link.partition(\"?mc=\")[2]\n",
      "        else:\n",
      "            prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "        #Prices:\n",
      "        #for product without discount\n",
      "        if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "            reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "            act_price = None\n",
      "\n",
      "        #for product with discount\n",
      "        else:\n",
      "            reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "            act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "        #label:\n",
      "        if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "            prod_sticker = None\n",
      "        else:\n",
      "            prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "        prod_list.append({'title': prod_title,\n",
      "                         'sku': prod_id,\n",
      "                         'regular price': reg_price,\n",
      "                         'actual price' : act_price,\n",
      "                         'brand': brand_name,\n",
      "                         'url' : prod_url,\n",
      "                         'sticker' : prod_sticker,\n",
      "                         'cat_1' : cat1,\n",
      "                         'cat_2' : cat2,\n",
      "                         'cat_3' : cat3})\n",
      "\n",
      "    print(f'{country}_{l1}_{l2}_{l3}: {len(prod_list)} products have been scraped!')\n",
      "171/31:\n",
      "for url in url_list:\n",
      "    page_soup = cookSoup(url)\n",
      "    prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "\n",
      "    for product in prod:\n",
      "        cat1 = cat1\n",
      "        cat2 = cat2\n",
      "        sport = cat3\n",
      "        link = product.find(\"a\").attrs[\"href\"]\n",
      "        prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "        brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "        prod_url = f'{base_index}{link}'\n",
      "\n",
      "        #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "        #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "        if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "            prod_id = link.partition(\"?mc=\")[2]\n",
      "        else:\n",
      "            prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "        #Prices:\n",
      "        #for product without discount\n",
      "        if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "            reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "            act_price = None\n",
      "\n",
      "        #for product with discount\n",
      "        else:\n",
      "            reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "            act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "        #label:\n",
      "        if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "            prod_sticker = None\n",
      "        else:\n",
      "            prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "        prod_list.append({'title': prod_title,\n",
      "                         'sku': prod_id,\n",
      "                         'regular price': reg_price,\n",
      "                         'actual price' : act_price,\n",
      "                         'brand': brand_name,\n",
      "                         'url' : prod_url,\n",
      "                         'sticker' : prod_sticker,\n",
      "                         'cat_1' : cat1,\n",
      "                         'cat_2' : cat2,\n",
      "                         'cat_3' : cat3})\n",
      "\n",
      "    print(f'{country}_{cat1}_{cat2}_{cat3}: {len(prod_list)} products have been scraped!')\n",
      "171/32:\n",
      "page_soup = cookSoup(url_list[0])\n",
      "prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "171/33:\n",
      "page_soup = cookSoup(url_list[0])\n",
      "prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "prod\n",
      "171/34:\n",
      "# country input\n",
      "country = 'DE'\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "prod_list_src = 'd5pqmr'\n",
      "sticker_src = '15lojui'\n",
      "total_page_src = '1uqvrhu'\n",
      "per_page = 40\n",
      "171/35:\n",
      "page_soup = cookSoup(url_list[0])\n",
      "prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "prod\n",
      "171/36:\n",
      "prod_list = []\n",
      "\n",
      "for item in cat_level3:\n",
      "    country = item[\"country\"]\n",
      "    cat_url = item[\"cat_url\"]\n",
      "    cat1 = item[\"cat1\"]\n",
      "    cat2 = item[\"cat2\"]\n",
      "    cat3 = item[\"cat3\"]\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page)\n",
      "    prod_list = getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src)\n",
      "171/37:\n",
      "prod_list = []\n",
      "\n",
      "for item in cat_level3:\n",
      "    country = item[\"country\"]\n",
      "    cat_url = item[\"cat_url\"]\n",
      "    cat1 = item[\"cat1\"]\n",
      "    cat2 = item[\"cat2\"]\n",
      "    cat3 = item[\"cat3\"]\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page)\n",
      "    prod_list = getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src)\n",
      "171/38:\n",
      "page_soup = cookSoup('https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung/_/N-1peigq4Z1da2q0eZexlast')\n",
      "prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "171/39: prod\n",
      "171/40: prod[0].find(\"span\", {\"class\":\"prc__previous\"})\n",
      "171/41: prod[0].find(\"span\", {\"class\":\"prc__info-addon\"})\n",
      "171/42: prod[0]\n",
      "171/43: len(prod)\n",
      "171/44: prod[0]\n",
      "171/45: prod[1]\n",
      "171/46:\n",
      "page_soup = cookSoup('https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung/_/N-1peigq4Z1da2q0eZexlast')\n",
      "prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-d5pqmr\")\n",
      "171/47: prod[0]\n",
      "171/48: prod[0].find(\"span\", {\"class\":\"prc__info-addon\"})\n",
      "171/49: prod[0].find(\"div\", {\"class\":\"prc__active-price\"})\n",
      "171/50: prod[1].find(\"div\", {\"class\":\"prc__active-price\"})\n",
      "171/51: prod[113].find(\"div\", {\"class\":\"prc__active-price\"})\n",
      "171/52: prod[112].find(\"div\", {\"class\":\"prc__active-price\"})\n",
      "171/53: prod[111]\n",
      "171/54: len(prod)\n",
      "171/55: prod[40]\n",
      "171/56: prod[39]\n",
      "171/57: prod[39].find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "171/58: prod[0].find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "171/59:\n",
      "for product in prod:\n",
      "    print(product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text)\n",
      "171/60:\n",
      "for product in prod:\n",
      "    print(product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text)\n",
      "    print(product.find(\"div\", {\"class\":\"prc__active-price\"}))\n",
      "171/61:\n",
      "for product in prod:\n",
      "    print(product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text)\n",
      "    print(product.find(\"div\", {\"class\":\"prc__active-price\"}).text)\n",
      "171/62:\n",
      "for product in prod:\n",
      "    print(product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text)\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        print(f'no discount {product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')}')\n",
      "    else:\n",
      "        print(f'with discount {product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')}')\n",
      "171/63:\n",
      "for product in prod:\n",
      "    print(product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text)\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        print(f\"no discount: {product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')}\")\n",
      "    else:\n",
      "        print(f\"with discount: {product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')}\")\n",
      "171/64:\n",
      "for product in prod:\n",
      "    print(product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text)\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        print(\"no discount\")\n",
      "        print(product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*',''))\n",
      "    else:\n",
      "        print(\"with discount\")\n",
      "        print(product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')})\n",
      "171/65:\n",
      "for product in prod:\n",
      "    print(product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text)\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        print(\"no discount\")\n",
      "        print(product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*',''))\n",
      "    else:\n",
      "        print(\"with discount\")\n",
      "        print(product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*',''))\n",
      "171/66:\n",
      "for product in prod:\n",
      "    cat1 = cat1\n",
      "    cat2 = cat2\n",
      "    sport = cat3\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "    prod_url = f'{base_index}{link}'\n",
      "\n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "\n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'cat_1' : cat1,\n",
      "                     'cat_2' : cat2,\n",
      "                     'cat_3' : cat3})\n",
      "171/67: len(prod_list)\n",
      "171/68:\n",
      "page_soup = cookSoup('https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung/_/N-1peigq4Z1da2q0eZexlast')\n",
      "prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "171/69:\n",
      "page_soup = cookSoup('https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung/_/N-1peigq4Z1da2q0eZexlast')\n",
      "prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "171/70: len(prod)\n",
      "171/71:\n",
      "for product in prod:\n",
      "    print(product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text)\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        print(\"no discount\")\n",
      "        print(product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*',''))\n",
      "    else:\n",
      "        print(\"with discount\")\n",
      "        print(product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*',''))\n",
      "171/72:\n",
      "for product in prod:\n",
      "    cat1 = cat1\n",
      "    cat2 = cat2\n",
      "    sport = cat3\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "    prod_url = f'{base_index}{link}'\n",
      "\n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "\n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'cat_1' : cat1,\n",
      "                     'cat_2' : cat2,\n",
      "                     'cat_3' : cat3})\n",
      "171/73: len(prod_list)\n",
      "171/74:\n",
      "prod_list = []\n",
      "for product in prod:\n",
      "    cat1 = cat1\n",
      "    cat2 = cat2\n",
      "    sport = cat3\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "    prod_url = f'{base_index}{link}'\n",
      "\n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "\n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'cat_1' : cat1,\n",
      "                     'cat_2' : cat2,\n",
      "                     'cat_3' : cat3})\n",
      "171/75: len(prod_list)\n",
      "171/76: len(prod_list)\n",
      "171/77: prod_list[0]\n",
      "171/78:\n",
      "prod_list = []\n",
      "\n",
      "for item in cat_level3:\n",
      "    country = item[\"country\"]\n",
      "    cat_url = item[\"cat_url\"]\n",
      "    cat1 = item[\"cat1\"]\n",
      "    cat2 = item[\"cat2\"]\n",
      "    cat3 = item[\"cat3\"]\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page)\n",
      "    prod_list = getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src)\n",
      "171/79:\n",
      "page_soup = cookSoup('https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung/_/N-1peigq4Z1da2q0eZexlast?from=40&size=40')\n",
      "prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "171/80: len(prod)\n",
      "171/81:\n",
      "prod_list = []\n",
      "for product in prod:\n",
      "    cat1 = cat1\n",
      "    cat2 = cat2\n",
      "    sport = cat3\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "    prod_url = f'{base_index}{link}'\n",
      "\n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "\n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'cat_1' : cat1,\n",
      "                     'cat_2' : cat2,\n",
      "                     'cat_3' : cat3})\n",
      "171/82: len(prod_list)\n",
      "171/83: prod_list[0]\n",
      "171/84:\n",
      "page_soup = cookSoup('https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung/_/N-1peigq4Z1da2q0eZexlast?from=80&size=40')\n",
      "prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "171/85: len(prod)\n",
      "171/86:\n",
      "prod_list = []\n",
      "for product in prod:\n",
      "    cat1 = cat1\n",
      "    cat2 = cat2\n",
      "    sport = cat3\n",
      "    link = product.find(\"a\").attrs[\"href\"]\n",
      "    prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "    brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "    prod_url = f'{base_index}{link}'\n",
      "\n",
      "    #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "    #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "    if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "        prod_id = link.partition(\"?mc=\")[2]\n",
      "    else:\n",
      "        prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "    #Prices:\n",
      "    #for product without discount\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = None\n",
      "\n",
      "    #for product with discount\n",
      "    else:\n",
      "        reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "        act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "    #label:\n",
      "    if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "        prod_sticker = None\n",
      "    else:\n",
      "        prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "    prod_list.append({'title': prod_title,\n",
      "                     'sku': prod_id,\n",
      "                     'regular price': reg_price,\n",
      "                     'actual price' : act_price,\n",
      "                     'brand': brand_name,\n",
      "                     'url' : prod_url,\n",
      "                     'sticker' : prod_sticker,\n",
      "                     'cat_1' : cat1,\n",
      "                     'cat_2' : cat2,\n",
      "                     'cat_3' : cat3})\n",
      "171/87: %history -g\n",
      "171/88:\n",
      "for product in prod:\n",
      "    print(product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text)\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        print(\"no discount\")\n",
      "        print(product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*',''))\n",
      "    else:\n",
      "        print(\"with discount\")\n",
      "        print(product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')})\n",
      "171/89:\n",
      "for product in prod:\n",
      "    print(product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text)\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        print(\"no discount\")\n",
      "        print(product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*',''))\n",
      "    else:\n",
      "        print(\"with discount\")\n",
      "        print(product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*',''))\n",
      "   1:\n",
      "#Set up\n",
      "from bs4 import BeautifulSoup as bts\n",
      "import pandas as pd\n",
      "import requests\n",
      "import math #for rounding numbers\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import re\n",
      "import csv\n",
      "   2:\n",
      "# country input\n",
      "country = 'DE'\n",
      "base_index = 'https://www.decathlon.de'\n",
      "base_cat = '/browse/c0-damen/_/N-1wmfzjj'\n",
      "base_sport = 'Zhpsxkd'\n",
      "sold_by = 'Z1da2q0e'\n",
      "prod_list_src = 'd5pqmr'\n",
      "sticker_src = '15lojui'\n",
      "total_page_src = '1uqvrhu'\n",
      "per_page = 40\n",
      "   3:\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "driver = webdriver.Chrome()\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "    \n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "\n",
      "sport_list = []\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "   4:\n",
      "# Function to get category links\n",
      "def getCatLink(base_index, base_cat, sold_by, cat_list):\n",
      "    url = f'{base_index}{base_cat}{sold_by}'\n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    return cat_list\n",
      "   5:\n",
      "# Importing category_url data\n",
      "cat_url = []  \n",
      "# opening the file using \"with\" statement\n",
      "with open('cat_url.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_url.append(cat)\n",
      "   6:\n",
      "#getting category links for all genders:\n",
      "cat_list =[]\n",
      "for i in cat_url:\n",
      "    base_index = i[\"base_index\"]\n",
      "    base_cat = i[\"base_cat\"]\n",
      "    sold_by = i[\"sold_by\"]\n",
      "    cat_list = getCatLink(base_index, base_cat, sold_by, cat_list)\n",
      "   7: cat_list\n",
      "   8:\n",
      "#combine link for category level 2 and filtered by sport\n",
      "\n",
      "cat_level3=[]\n",
      "\n",
      "for cat in cat_list:\n",
      "    for sport in sport_list:\n",
      "        country = country\n",
      "        level_1 = cat[\"level_1\"]\n",
      "        level_2 = cat[\"level_2\"]\n",
      "        sport_type = sport[\"sport_type\"]\n",
      "        url = f'{cat[\"cat_link\"]}{sport[\"sport_code\"]}'\n",
      "        cat_level3.append({'country': country,\n",
      "                           'cat1': level_1,\n",
      "                           'cat2': level_2,\n",
      "                           'cat3': sport_type,\n",
      "                           'cat_url': url})\n",
      "   9: cat_level3[1]\n",
      "  10:\n",
      "#saving cat_level3\n",
      "\n",
      "with open(f\"{country}_cat_level3.csv\", \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"country\",\"cat1\", \"cat2\", \"cat3\", \"cat_url\"])\n",
      "\n",
      "    for item in cat_level3:\n",
      "        writer.writerow([item['country'], item['cat1'], item['cat2'], item['cat3'], item['cat_url']])\n",
      "  11:\n",
      "#Function for parsing the URLs\n",
      "def cookSoup(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "  12:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":f\"plp-bar-info svelte-{total_page_src}\"}).find(\"span\", {\"class\":f\"svelte-{total_page_src}\"}).text\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/per_page)\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{cat_url}?from={per_page * i}&size={per_page}'\n",
      "            url_list.append(page)\n",
      "    print (f'{country}_{cat1}_{cat2}_{cat3}: There are {total_prod} products ({total_page} pages)')\n",
      "    return url_list\n",
      "  13:\n",
      "# Function to get main data\n",
      "def getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src):\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "        \n",
      "        for product in prod:\n",
      "            cat1 = cat1\n",
      "            cat2 = cat2\n",
      "            sport = cat3\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "            prod_url = f'{base_index}{link}'\n",
      "\n",
      "            #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #Prices:\n",
      "            #for product without discount\n",
      "            if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "\n",
      "            #for product with discount\n",
      "            else:\n",
      "                reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "            #label:\n",
      "            if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'cat_1' : cat1,\n",
      "                             'cat_2' : cat2,\n",
      "                             'cat_3' : cat3})\n",
      "\n",
      "    print(f'{country}_{l1}_{l2}_{l3}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      "  14:\n",
      "page_soup = cookSoup('https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung/_/N-1peigq4Z1da2q0eZexlast?from=80&size=40')\n",
      "prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "  15:\n",
      "for product in prod:\n",
      "    print(product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text)\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        print(\"no discount\")\n",
      "        print(product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*',''))\n",
      "    else:\n",
      "        print(\"with discount\")\n",
      "        print(product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*',''))\n",
      "  16:\n",
      "# Function to get main data\n",
      "def getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src):\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "        \n",
      "        for product in prod:\n",
      "            cat1 = cat1\n",
      "            cat2 = cat2\n",
      "            sport = cat3\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "            prod_url = f'{base_index}{link}'\n",
      "\n",
      "            #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #Prices:\n",
      "            #for product without discount\n",
      "            if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "\n",
      "            #for product with discount\n",
      "            else:\n",
      "                if product.find(\"span\", {\"class\":\"prc__previous\"}) == None:\n",
      "                    reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "                    act_price = None\n",
      "                else:\n",
      "                    reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                    act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "            #label:\n",
      "            if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'cat_1' : cat1,\n",
      "                             'cat_2' : cat2,\n",
      "                             'cat_3' : cat3})\n",
      "\n",
      "    print(f'{country}_{l1}_{l2}_{l3}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      "  17:\n",
      "page_soup = cookSoup('https://www.decathlon.de/browse/c0-damen/c1-sportbekleidung/_/N-1peigq4Z1da2q0eZexlast?from=80&size=40')\n",
      "prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "  18:\n",
      "for product in prod:\n",
      "    print(product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text)\n",
      "    if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "        print(\"no discount\")\n",
      "        print(product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*',''))\n",
      "    else:\n",
      "        if product.find(\"span\", {\"class\":\"prc__previous\"}) == None:\n",
      "            print(\"no discount\")\n",
      "            print(product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*',''))\n",
      "        else:\n",
      "            print(\"with discount\")\n",
      "            print(product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*',''))\n",
      "  19:\n",
      "# Function to get main data\n",
      "def getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src):\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "        \n",
      "        for product in prod:\n",
      "            cat1 = cat1\n",
      "            cat2 = cat2\n",
      "            sport = cat3\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "            prod_url = f'{base_index}{link}'\n",
      "\n",
      "            #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #Prices:\n",
      "            #for product without discount\n",
      "            if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "\n",
      "            #for product with discount\n",
      "            else:\n",
      "                if product.find(\"span\", {\"class\":\"prc__previous\"}) == None:\n",
      "                    reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "                    act_price = None\n",
      "                else:\n",
      "                    reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                    act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "            #label:\n",
      "            if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'cat_1' : cat1,\n",
      "                             'cat_2' : cat2,\n",
      "                             'cat_3' : cat3})\n",
      "\n",
      "    print(f'{country}_{l1}_{l2}_{l3}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      "  20:\n",
      "prod_list = []\n",
      "\n",
      "for item in cat_level3:\n",
      "    country = item[\"country\"]\n",
      "    cat_url = item[\"cat_url\"]\n",
      "    cat1 = item[\"cat1\"]\n",
      "    cat2 = item[\"cat2\"]\n",
      "    cat3 = item[\"cat3\"]\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page)\n",
      "    prod_list = getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src)\n",
      "  21:\n",
      "# Function to get main data\n",
      "def getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src):\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "        \n",
      "        for product in prod:\n",
      "            cat1 = cat1\n",
      "            cat2 = cat2\n",
      "            sport = cat3\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "            prod_url = f'{base_index}{link}'\n",
      "\n",
      "            #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #Prices:\n",
      "            #for product without discount\n",
      "            if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "\n",
      "            #for product with discount\n",
      "            else:\n",
      "                if product.find(\"span\", {\"class\":\"prc__previous\"}) == None:\n",
      "                    reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "                    act_price = None\n",
      "                else:\n",
      "                    reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                    act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "            #label:\n",
      "            if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'cat_1' : cat1,\n",
      "                             'cat_2' : cat2,\n",
      "                             'cat_3' : cat3})\n",
      "\n",
      "    print(f'{country}_{cat1}_{cat2}_{cat3}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      "  22:\n",
      "prod_list = []\n",
      "\n",
      "for item in cat_level3:\n",
      "    country = item[\"country\"]\n",
      "    cat_url = item[\"cat_url\"]\n",
      "    cat1 = item[\"cat1\"]\n",
      "    cat2 = item[\"cat2\"]\n",
      "    cat3 = item[\"cat3\"]\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page)\n",
      "    prod_list = getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src)\n",
      "  23: cat_list\n",
      "  24: cat_list\n",
      "  25: cat_list[1:2]\n",
      "  26: cat_list[1:3]\n",
      "  27: cat_list[0:3]\n",
      "  28:\n",
      "# Function to get category links\n",
      "def getCatLink(base_index, base_cat, sold_by, cat_list):\n",
      "    url = f'{base_index}{base_cat}{sold_by}'\n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories[0: len(categories)]:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    return cat_list\n",
      "  29:\n",
      "# Importing category_url data\n",
      "cat_url = []  \n",
      "# opening the file using \"with\" statement\n",
      "with open('cat_url.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_url.append(cat)\n",
      "  30:\n",
      "#getting category links for all genders:\n",
      "cat_list =[]\n",
      "for i in cat_url:\n",
      "    base_index = i[\"base_index\"]\n",
      "    base_cat = i[\"base_cat\"]\n",
      "    sold_by = i[\"sold_by\"]\n",
      "    cat_list = getCatLink(base_index, base_cat, sold_by, cat_list)\n",
      "  31: cat_list\n",
      "  32:\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "driver = webdriver.Chrome()\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "    \n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the last position:\n",
      "sports = filter[len(filter)-1]\n",
      "\n",
      "sport_list = []\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "  33:\n",
      "# Function to get category links\n",
      "def getCatLink(base_index, base_cat, sold_by, cat_list):\n",
      "    url = f'{base_index}{base_cat}{sold_by}'\n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories[0: len(categories)]:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    return cat_list\n",
      "  34:\n",
      "# Importing category_url data\n",
      "cat_url = []  \n",
      "# opening the file using \"with\" statement\n",
      "with open('cat_url.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_url.append(cat)\n",
      "  35:\n",
      "#getting category links for all genders:\n",
      "cat_list =[]\n",
      "for i in cat_url:\n",
      "    base_index = i[\"base_index\"]\n",
      "    base_cat = i[\"base_cat\"]\n",
      "    sold_by = i[\"sold_by\"]\n",
      "    cat_list = getCatLink(base_index, base_cat, sold_by, cat_list)\n",
      "  36: cat_list\n",
      "  37: len(categories)\n",
      "  38:\n",
      "for i in range[0:10]:\n",
      "    print(i)\n",
      "  39:\n",
      "for i in range(0:10):\n",
      "    print(i)\n",
      "  40:\n",
      "for i in range(0,10):\n",
      "    print(i)\n",
      "  41:\n",
      "for i in range(0,len(cat_list)):\n",
      "    print(i)\n",
      "  42: len(cat_list)\n",
      "  43:\n",
      "for cat in cat_list[0:3]:\n",
      "    print cat\n",
      "  44:\n",
      "for cat in cat_list[0:3]:\n",
      "    print (cat)\n",
      "  45:\n",
      "# Function to get category links\n",
      "def getCatLink(base_index, base_cat, sold_by, cat_list):\n",
      "    url = f'{base_index}{base_cat}{sold_by}'\n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories[0: len(categories)-1]:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    return cat_list\n",
      "  46:\n",
      "# Importing category_url data\n",
      "cat_url = []  \n",
      "# opening the file using \"with\" statement\n",
      "with open('cat_url.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_url.append(cat)\n",
      "  47:\n",
      "#getting category links for all genders:\n",
      "cat_list =[]\n",
      "for i in cat_url:\n",
      "    base_index = i[\"base_index\"]\n",
      "    base_cat = i[\"base_cat\"]\n",
      "    sold_by = i[\"sold_by\"]\n",
      "    cat_list = getCatLink(base_index, base_cat, sold_by, cat_list)\n",
      "  48: cat_list\n",
      "  49: len(cat_list)\n",
      "  50: 7 + 5 + 7\n",
      "  51:\n",
      "#combine link for category level 2 and filtered by sport\n",
      "\n",
      "cat_level3=[]\n",
      "\n",
      "for cat in cat_list:\n",
      "    for sport in sport_list:\n",
      "        country = country\n",
      "        level_1 = cat[\"level_1\"]\n",
      "        level_2 = cat[\"level_2\"]\n",
      "        sport_type = sport[\"sport_type\"]\n",
      "        url = f'{cat[\"cat_link\"]}{sport[\"sport_code\"]}'\n",
      "        cat_level3.append({'country': country,\n",
      "                           'cat1': level_1,\n",
      "                           'cat2': level_2,\n",
      "                           'cat3': sport_type,\n",
      "                           'cat_url': url})\n",
      "  52: cat_level3[1]\n",
      "  53:\n",
      "#saving cat_level3\n",
      "\n",
      "with open(f\"{country}_cat_level3.csv\", \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"country\",\"cat1\", \"cat2\", \"cat3\", \"cat_url\"])\n",
      "\n",
      "    for item in cat_level3:\n",
      "        writer.writerow([item['country'], item['cat1'], item['cat2'], item['cat3'], item['cat_url']])\n",
      "  54:\n",
      "#Function for parsing the URLs\n",
      "def cookSoup(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "  55:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":f\"plp-bar-info svelte-{total_page_src}\"}).find(\"span\", {\"class\":f\"svelte-{total_page_src}\"}).text\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/per_page)\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{cat_url}?from={per_page * i}&size={per_page}'\n",
      "            url_list.append(page)\n",
      "    print (f'{country}_{cat1}_{cat2}_{cat3}: There are {total_prod} products ({total_page} pages)')\n",
      "    return url_list\n",
      "  56:\n",
      "# Function to get main data\n",
      "def getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src):\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "        \n",
      "        for product in prod:\n",
      "            cat1 = cat1\n",
      "            cat2 = cat2\n",
      "            sport = cat3\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "            prod_url = f'{base_index}{link}'\n",
      "\n",
      "            #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #Prices:\n",
      "            #for product without discount\n",
      "            if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "\n",
      "            #for product with discount\n",
      "            else:\n",
      "                if product.find(\"span\", {\"class\":\"prc__previous\"}) == None:\n",
      "                    reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "                    act_price = None\n",
      "                else:\n",
      "                    reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                    act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "            #label:\n",
      "            if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'cat_1' : cat1,\n",
      "                             'cat_2' : cat2,\n",
      "                             'cat_3' : cat3})\n",
      "\n",
      "    print(f'{country}_{cat1}_{cat2}_{cat3}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      "  57:\n",
      "prod_list = []\n",
      "\n",
      "for item in cat_level3:\n",
      "    country = item[\"country\"]\n",
      "    cat_url = item[\"cat_url\"]\n",
      "    cat1 = item[\"cat1\"]\n",
      "    cat2 = item[\"cat2\"]\n",
      "    cat3 = item[\"cat3\"]\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page)\n",
      "    prod_list = getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src)\n",
      "  58: len(prod_list)\n",
      "  59:\n",
      "#saving cat_level3\n",
      "with open(f\"{country}_decathlon.csv\", \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"country\",\"title\", \"sku\", \"reg_pr\", \"act_pr\", \"brand\", \"sticker\", \"cat_1\", \"cat_2\", \"cat_3\", \"url\"])\n",
      "\n",
      "    for item in prod_list:\n",
      "        writer.writerow([country, item['title'], item['sku'], item['regular price'], \n",
      "                         item['actual price'], item['brand'], item['sticker'], \n",
      "                         item['cat_1'], item['cat_2'], item['cat_3'], item['url']])\n",
      "  60:\n",
      "# country input\n",
      "country = 'FR'\n",
      "base_index = 'https://www.decathlon.fr'\n",
      "base_cat = '/browse/c0-femme/_/N-ry4jwt'\n",
      "base_sport = 'Z1l5trmt'\n",
      "sold_by = 'Z1o76joc'\n",
      "prod_list_src = 'aim4wv'\n",
      "sticker_src = '1p0v3i7'\n",
      "total_page_src = '1uqvrhu'\n",
      "per_page = 40\n",
      "  61:\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "driver = webdriver.Chrome()\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "    \n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the second position:\n",
      "sports = filter[filter_position]\n",
      "\n",
      "sport_list = []\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "  62:\n",
      "# country input\n",
      "country = 'FR'\n",
      "base_index = 'https://www.decathlon.fr'\n",
      "base_cat = '/browse/c0-femme/_/N-ry4jwt'\n",
      "base_sport = 'Z1l5trmt'\n",
      "sold_by = 'Z1o76joc'\n",
      "prod_list_src = 'aim4wv'\n",
      "sticker_src = '1p0v3i7'\n",
      "total_page_src = '1uqvrhu'\n",
      "per_page = 40\n",
      "filter_position = 1\n",
      "  63:\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "driver = webdriver.Chrome()\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "    \n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the second position:\n",
      "sports = filter[filter_position]\n",
      "\n",
      "sport_list = []\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "  64: len(sport_list)\n",
      "  65: sport_list\n",
      "  66:\n",
      "# country input\n",
      "country = 'FR'\n",
      "base_index = 'https://www.decathlon.fr'\n",
      "base_cat = '/browse/c0-femme/_/N-ry4jwt'\n",
      "base_sport = 'Z1l5trmt'\n",
      "sold_by = 'Z1o76joc'\n",
      "prod_list_src = 'aim4wv'\n",
      "sticker_src = '1p0v3i7'\n",
      "total_page_src = '1uqvrhu'\n",
      "per_page = 40\n",
      "filter_position = 0\n",
      "  67:\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "driver = webdriver.Chrome()\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "    \n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the second position:\n",
      "sports = filter[filter_position]\n",
      "\n",
      "sport_list = []\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = sport_url[i].attrs[\"href\"].replace(f'{base_cat}{sold_by}{base_sport}',\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "  68: sport_list\n",
      "  69: sport_url\n",
      "  70: sport_url[0].attrs[\"href\"]\n",
      "  71: sport_url[1].attrs[\"href\"]\n",
      "  72: sport_url[2].attrs[\"href\"]\n",
      "  73: sport_list[2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  74: sport_url[2].attrs[\"href\"]\n",
      "  75: sport_url[1].attrs[\"href\"]\n",
      "  76: re.sub(\"\\/.*?\\/\", \"\", sport_url[2].attrs[\"href\"])\n",
      "  77: re.sub(\"\\/.*?\\_\", \"\", sport_url[2].attrs[\"href\"])\n",
      "  78: re.sub(\"\\/.*?\\_/\", \"\", sport_url[2].attrs[\"href\"])\n",
      "  79:\n",
      "# country input\n",
      "country = 'FR'\n",
      "base_index = 'https://www.decathlon.fr'\n",
      "base_cat = '/browse/c0-femme/_/N-ry4jwt'\n",
      "base_sport = 'Z1l5trmt'\n",
      "sold_by = 'Z1o76joc'\n",
      "prod_list_src = 'aim4wv'\n",
      "sticker_src = '1p0v3i7'\n",
      "total_page_src = '1uqvrhu'\n",
      "per_page = 40\n",
      "filter_position = 0\n",
      "fem_src = 'N-ry4jwt'\n",
      "  80: re.sub(\"\\/.*?\\_/\", \"\", sport_url[2].attrs[\"href\"]).replace(fem_src, \"\").replace(base_sport, \"\"). replace(sold_by,\"\")\n",
      "  81:\n",
      "filter_link = f'{base_index}{base_cat}{sold_by}{base_sport}'\n",
      "driver = webdriver.Chrome()\n",
      "driver.get(filter_link)\n",
      "\n",
      "#accept cookie popup\n",
      "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//*[@id=\"didomi-notice-agree-button\"]'))).click()\n",
      "\n",
      "\n",
      "#get all buttons and click on them all (show more) by \n",
      "buttons = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')))\n",
      "buttons = driver.find_elements(By.XPATH, '(//button[@class=\"cta cta--outline cta--2nd cta--small\"])')\n",
      "\n",
      "#click all buttons\n",
      "for button in buttons:\n",
      "    driver.execute_script(\"arguments[0].click();\", button)\n",
      "    #getting the soup\n",
      "    soup = bts(driver.page_source)\n",
      "    \n",
      "# access all filters:\n",
      "filter = soup.find_all(class_=\"filter-content-input-group\")\n",
      "\n",
      "# access sport filter at the second position:\n",
      "sports = filter[filter_position]\n",
      "\n",
      "sport_list = []\n",
      "sport_url = sports.find_all(\"a\")\n",
      "\n",
      "#first item\n",
      "sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[0].text)\n",
      "sport_code = base_sport\n",
      "sport_list.append({'sport_type': sport_type,\n",
      "                   'sport_code': sport_code})\n",
      "\n",
      "for i in range(1, len(sport_url)):\n",
      "    sport_type = re.sub(\"\\(.*?\\)\", \"\", sport_url[i].text) #remove all numbers\n",
      "    sport_code = re.sub(\"\\/.*?\\_/\", \"\", sport_url[i].attrs[\"href\"]).replace(fem_src, \"\").replace(base_sport, \"\"). replace(sold_by,\"\")\n",
      "    sport_list.append({'sport_type': sport_type,\n",
      "                       'sport_code': sport_code})\n",
      "  82: sport_list\n",
      "  83:\n",
      "# Importing category_url data\n",
      "cat_url = []  \n",
      "# opening the file using \"with\" statement\n",
      "with open('FR_cat_url.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_url.append(cat)\n",
      "  84: cat_url\n",
      "  85:\n",
      "#getting category links for all genders:\n",
      "cat_list =[]\n",
      "for i in cat_url:\n",
      "    base_index = i[\"base_index\"]\n",
      "    base_cat = i[\"base_cat\"]\n",
      "    sold_by = i[\"sold_by\"]\n",
      "    cat_list = getCatLink(base_index, base_cat, sold_by, cat_list)\n",
      "  86: cat_list\n",
      "  87: len(cat_list)\n",
      "  88:\n",
      "#combine link for category level 2 and filtered by sport\n",
      "\n",
      "cat_level3=[]\n",
      "\n",
      "for cat in cat_list:\n",
      "    for sport in sport_list:\n",
      "        country = country\n",
      "        level_1 = cat[\"level_1\"]\n",
      "        level_2 = cat[\"level_2\"]\n",
      "        sport_type = sport[\"sport_type\"]\n",
      "        url = f'{cat[\"cat_link\"]}{sport[\"sport_code\"]}'\n",
      "        cat_level3.append({'country': country,\n",
      "                           'cat1': level_1,\n",
      "                           'cat2': level_2,\n",
      "                           'cat3': sport_type,\n",
      "                           'cat_url': url})\n",
      "  89: cat_level3[1]\n",
      "  90:\n",
      "#saving cat_level3\n",
      "\n",
      "with open(f\"{country}_cat_level3.csv\", \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"country\",\"cat1\", \"cat2\", \"cat3\", \"cat_url\"])\n",
      "\n",
      "    for item in cat_level3:\n",
      "        writer.writerow([item['country'], item['cat1'], item['cat2'], item['cat3'], item['cat_url']])\n",
      "  91:\n",
      "#Function for parsing the URLs\n",
      "def cookSoup(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      "  92:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":f\"plp-bar-info svelte-{total_page_src}\"}).find(\"span\", {\"class\":f\"svelte-{total_page_src}\"}).text\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/per_page)\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{cat_url}?from={per_page * i}&size={per_page}'\n",
      "            url_list.append(page)\n",
      "    print (f'{country}_{cat1}_{cat2}_{cat3}: There are {total_prod} products ({total_page} pages)')\n",
      "    return url_list\n",
      "  93:\n",
      "# Function to get main data\n",
      "def getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src):\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "        \n",
      "        for product in prod:\n",
      "            cat1 = cat1\n",
      "            cat2 = cat2\n",
      "            sport = cat3\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "            prod_url = f'{base_index}{link}'\n",
      "\n",
      "            #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #Prices:\n",
      "            #for product without discount\n",
      "            if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "\n",
      "            #for product with discount\n",
      "            else:\n",
      "                if product.find(\"span\", {\"class\":\"prc__previous\"}) == None:\n",
      "                    reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "                    act_price = None\n",
      "                else:\n",
      "                    reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                    act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "            #label:\n",
      "            if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'cat_1' : cat1,\n",
      "                             'cat_2' : cat2,\n",
      "                             'cat_3' : cat3})\n",
      "\n",
      "    print(f'{country}_{cat1}_{cat2}_{cat3}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      "  94:\n",
      "prod_list = []\n",
      "\n",
      "for item in cat_level3:\n",
      "    country = item[\"country\"]\n",
      "    cat_url = item[\"cat_url\"]\n",
      "    cat1 = item[\"cat1\"]\n",
      "    cat2 = item[\"cat2\"]\n",
      "    cat3 = item[\"cat3\"]\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page)\n",
      "    prod_list = getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src)\n",
      "  95:\n",
      "# Function to get category links\n",
      "def getCatLink(base_index, base_cat, sold_by, cat_list):\n",
      "    url = f'{base_index}{base_cat}{sold_by}'\n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories[0: len(categories)-1]:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    cat_list = [cat for cat in cat_list if cat['level_2'] != 'Bons plans']\n",
      "    return cat_list\n",
      "  96:\n",
      "# Importing category_url data\n",
      "cat_url = []  \n",
      "# opening the file using \"with\" statement\n",
      "with open('FR_cat_url.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_url.append(cat)\n",
      "  97: cat_url\n",
      "  98:\n",
      "#getting category links for all genders:\n",
      "cat_list =[]\n",
      "for i in cat_url:\n",
      "    base_index = i[\"base_index\"]\n",
      "    base_cat = i[\"base_cat\"]\n",
      "    sold_by = i[\"sold_by\"]\n",
      "    cat_list = getCatLink(base_index, base_cat, sold_by, cat_list)\n",
      "  99: cat_list\n",
      " 100:\n",
      "# Function to get category links\n",
      "def getCatLink(base_index, base_cat, sold_by, cat_list):\n",
      "    url = f'{base_index}{base_cat}{sold_by}'\n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories[0: len(categories)]:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    cat_list = [cat for cat in cat_list if cat['level_2'] != 'Bons plans']\n",
      "    return cat_list\n",
      " 101:\n",
      "# Importing category_url data\n",
      "cat_url = []  \n",
      "# opening the file using \"with\" statement\n",
      "with open('FR_cat_url.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_url.append(cat)\n",
      " 102: cat_url\n",
      " 103: len(cat_list)\n",
      " 104: cat_url\n",
      " 105:\n",
      "#getting category links for all genders:\n",
      "cat_list =[]\n",
      "for i in cat_url:\n",
      "    base_index = i[\"base_index\"]\n",
      "    base_cat = i[\"base_cat\"]\n",
      "    sold_by = i[\"sold_by\"]\n",
      "    cat_list = getCatLink(base_index, base_cat, sold_by, cat_list)\n",
      " 106:\n",
      "# Function to get category links\n",
      "def getCatLink(base_index, base_cat, sold_by, cat_list):\n",
      "    url = f'{base_index}{base_cat}{sold_by}'\n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories[0: len(categories)]:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    cat_list = [cat for cat in cat_list if cat['level_2'] != 'Bons plans']\n",
      "    return cat_list\n",
      " 107:\n",
      "\n",
      "len(cat_list)\n",
      " 108: len(cat_list)\n",
      " 109: cat_list\n",
      " 110:\n",
      "# Function to get category links\n",
      "def getCatLink(base_index, base_cat, sold_by, cat_list):\n",
      "    url = f'{base_index}{base_cat}{sold_by}'\n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories[0: len(categories) + 1]:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    \n",
      "    #remove category \"bons plans\" from being scraped\n",
      "    cat_list = [cat for cat in cat_list if cat['level_2'] != 'Bons plans']\n",
      "    \n",
      "    return cat_list\n",
      " 111:\n",
      "# Importing category_url data\n",
      "cat_url = []  \n",
      "# opening the file using \"with\" statement\n",
      "with open('FR_cat_url.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_url.append(cat)\n",
      " 112: cat_url\n",
      " 113:\n",
      "#getting category links for all genders:\n",
      "cat_list =[]\n",
      "for i in cat_url:\n",
      "    base_index = i[\"base_index\"]\n",
      "    base_cat = i[\"base_cat\"]\n",
      "    sold_by = i[\"sold_by\"]\n",
      "    cat_list = getCatLink(base_index, base_cat, sold_by, cat_list)\n",
      " 114: len(cat_list)\n",
      " 115: cat_list\n",
      " 116:\n",
      "# Function to get category links\n",
      "def getCatLink(base_index, base_cat, sold_by, cat_list):\n",
      "    url = f'{base_index}{base_cat}{sold_by}'\n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    categories = soup.find_all(class_=\"category-link svelte-14kqft6\")\n",
      "    for cat in categories[0: len(categories) - 1]:\n",
      "        level_1 = soup.find(\"h1\").text\n",
      "        level_2 = cat.attrs['data-help']\n",
      "        cat_link = f'{base_index}{cat.attrs[\"href\"].replace(base_sport,\"\")}'\n",
      "        cat_list.append({'level_1': level_1,\n",
      "                         'level_2': level_2,\n",
      "                         'cat_link': cat_link})\n",
      "    \n",
      "    #remove category \"bons plans\" from being scraped\n",
      "    cat_list = [cat for cat in cat_list if cat['level_2'] != 'Bons plans']\n",
      "    \n",
      "    return cat_list\n",
      " 117:\n",
      "# Importing category_url data\n",
      "cat_url = []  \n",
      "# opening the file using \"with\" statement\n",
      "with open('FR_cat_url.csv', 'r') as cat_data:\n",
      "    for cat in csv.DictReader(cat_data):\n",
      "        cat_url.append(cat)\n",
      " 118: cat_url\n",
      " 119:\n",
      "#getting category links for all genders:\n",
      "cat_list =[]\n",
      "for i in cat_url:\n",
      "    base_index = i[\"base_index\"]\n",
      "    base_cat = i[\"base_cat\"]\n",
      "    sold_by = i[\"sold_by\"]\n",
      "    cat_list = getCatLink(base_index, base_cat, sold_by, cat_list)\n",
      " 120: len(cat_list)\n",
      " 121: cat_list\n",
      " 122:\n",
      "#combine link for category level 2 and filtered by sport\n",
      "\n",
      "cat_level3=[]\n",
      "\n",
      "for cat in cat_list:\n",
      "    for sport in sport_list:\n",
      "        country = country\n",
      "        level_1 = cat[\"level_1\"]\n",
      "        level_2 = cat[\"level_2\"]\n",
      "        sport_type = sport[\"sport_type\"]\n",
      "        url = f'{cat[\"cat_link\"]}{sport[\"sport_code\"]}'\n",
      "        cat_level3.append({'country': country,\n",
      "                           'cat1': level_1,\n",
      "                           'cat2': level_2,\n",
      "                           'cat3': sport_type,\n",
      "                           'cat_url': url})\n",
      " 123: cat_level3[1]\n",
      " 124:\n",
      "#saving cat_level3\n",
      "\n",
      "with open(f\"{country}_cat_level3.csv\", \"w\", encoding='utf-8-sig', newline='') as csv_file:\n",
      "    writer = csv.writer(csv_file, delimiter = \",\")\n",
      "    writer.writerow([\"country\",\"cat1\", \"cat2\", \"cat3\", \"cat_url\"])\n",
      "\n",
      "    for item in cat_level3:\n",
      "        writer.writerow([item['country'], item['cat1'], item['cat2'], item['cat3'], item['cat_url']])\n",
      " 125:\n",
      "#Function for parsing the URLs\n",
      "def cookSoup(url): \n",
      "    result = requests.get(url, headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "    soup = bts(result.text, 'html.parser')\n",
      "    return soup\n",
      " 126:\n",
      "# Function for pagination - creating a list of urls from a category\n",
      "def pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page):\n",
      "    url_list = [cat_url]\n",
      "    total_prod = soup.find(\"div\",{\"class\":f\"plp-bar-info svelte-{total_page_src}\"}).find(\"span\", {\"class\":f\"svelte-{total_page_src}\"}).text\n",
      "    #Create list of urls within the cat\n",
      "    total_page = math.ceil(int(total_prod)/per_page)\n",
      "    for i in range(1, total_page):\n",
      "            page = f'{cat_url}?from={per_page * i}&size={per_page}'\n",
      "            url_list.append(page)\n",
      "    print (f'{country}_{cat1}_{cat2}_{cat3}: There are {total_prod} products ({total_page} pages)')\n",
      "    return url_list\n",
      " 127:\n",
      "# Function to get main data\n",
      "def getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src):\n",
      "    for url in url_list:\n",
      "        page_soup = cookSoup(url)\n",
      "        prod = page_soup.find_all(class_=f\"dpb-holder loaded svelte-{prod_list_src}\")\n",
      "        \n",
      "        for product in prod:\n",
      "            cat1 = cat1\n",
      "            cat2 = cat2\n",
      "            sport = cat3\n",
      "            link = product.find(\"a\").attrs[\"href\"]\n",
      "            prod_title = product.find(\"h2\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "            brand_name = product.find(\"strong\",{\"class\":f\"svelte-{prod_list_src}\"}).text\n",
      "            prod_url = f'{base_index}{link}'\n",
      "\n",
      "            #SKU: if all sku contains only 7 characters: prod_id = product.find(\"a\").attrs[\"href\"].partition(\"?mc=\")[2][0:7]\n",
      "            #taking sku's even in case of more than 7 character id's:\n",
      "\n",
      "            if link.partition(\"?mc=\")[2].rpartition('&')[0] == '':\n",
      "                prod_id = link.partition(\"?mc=\")[2]\n",
      "            else:\n",
      "                prod_id = link.partition(\"?mc=\")[2].rpartition('&')[0]\n",
      "\n",
      "            #Prices:\n",
      "            #for product without discount\n",
      "            if product.find(\"span\", {\"class\":\"prc__info-addon\"}) == None:\n",
      "                reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "                act_price = None\n",
      "\n",
      "            #for product with discount\n",
      "            else:\n",
      "                if product.find(\"span\", {\"class\":\"prc__previous\"}) == None:\n",
      "                    reg_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "                    act_price = None\n",
      "                else:\n",
      "                    reg_price = product.find(\"span\", {\"class\":\"prc__previous\"}).text.replace('\\n','').replace('*','')\n",
      "                    act_price = product.find(\"div\", {\"class\":\"prc__active-price\"}).text.replace('\\n','').replace('*','')\n",
      "\n",
      "            #label:\n",
      "            if product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}) == None:\n",
      "                prod_sticker = None\n",
      "            else:\n",
      "                prod_sticker = product.find(\"div\", {\"class\": f\"sticker svelte-{sticker_src}\"}).text.replace('\\n','').replace('*','').replace(' ','')\n",
      "\n",
      "            prod_list.append({'title': prod_title,\n",
      "                             'sku': prod_id,\n",
      "                             'regular price': reg_price,\n",
      "                             'actual price' : act_price,\n",
      "                             'brand': brand_name,\n",
      "                             'url' : prod_url,\n",
      "                             'sticker' : prod_sticker,\n",
      "                             'cat_1' : cat1,\n",
      "                             'cat_2' : cat2,\n",
      "                             'cat_3' : cat3})\n",
      "\n",
      "    print(f'{country}_{cat1}_{cat2}_{cat3}: {len(prod_list)} products have been scraped!')\n",
      "        \n",
      "    return prod_list\n",
      " 128:\n",
      "prod_list = []\n",
      "\n",
      "for item in cat_level3:\n",
      "    country = item[\"country\"]\n",
      "    cat_url = item[\"cat_url\"]\n",
      "    cat1 = item[\"cat1\"]\n",
      "    cat2 = item[\"cat2\"]\n",
      "    cat3 = item[\"cat3\"]\n",
      "    soup = cookSoup(cat_url)\n",
      "    url_list = pageCreation(soup, cat_url, country, cat1, cat2, cat3, total_page_src, per_page)\n",
      "    prod_list = getDecathlonData(base_index, url_list, country, cat1, cat2, cat3, prod_list, prod_list_src, sticker_src)\n",
      " 129:\n",
      "# debugging\n",
      "result = requests.get('https://www.decathlon.fr/browse/c0-femme/c1-chaussant/sport-foot5/_/N-1ntm14cZith9kxZ1o76joc', headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "soup = bts(result.text, 'html.parser')\n",
      "url_list = []\n",
      "total_prod = soup.find(\"div\",{\"class\":f\"plp-bar-info svelte-{total_page_src}\"}).find(\"span\", {\"class\":f\"svelte-{total_page_src}\"}).text\n",
      "\n",
      "#Create list of urls within the cat\n",
      "total_page = math.ceil(int(total_prod)/per_page)\n",
      "for i in range(1, total_page):\n",
      "    page = f'{cat_url}?from={per_page * i}&size={per_page}'\n",
      "    url_list.append(page)\n",
      " 130:\n",
      "# debugging\n",
      "result = requests.get('https://www.decathlon.fr/browse/c0-femme/c1-chaussant/sport-foot5/_/N-1ntm14cZith9kxZ1o76joc', headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "soup = bts(result.text, 'html.parser')\n",
      "url_list = []\n",
      "total_prod = soup.find(\"div\",{\"class\":f\"plp-bar-info svelte-{total_page_src}\"}).find(\"span\", {\"class\":f\"svelte-{total_page_src}\"}).text\n",
      "\n",
      "#Create list of urls within the cat\n",
      "total_page = math.ceil(int(total_prod)/per_page)\n",
      "for i in range(1, total_page):\n",
      "    page = f'{cat_url}?from={per_page * i}&size={per_page}'\n",
      "    url_list.append(page)\n",
      "\n",
      "        \n",
      "url_list\n",
      " 131:\n",
      "# debugging\n",
      "result = requests.get('https://www.decathlon.fr/browse/c0-femme/c1-chaussant/sport-foot5/_/N-1ntm14cZith9kxZ1o76joc', headers = {\"User-Agent\":\"Mozilla/5.0\"})\n",
      "soup = bts(result.text, 'html.parser')\n",
      "url_list = []\n",
      "total_prod = soup.find(\"div\",{\"class\":f\"plp-bar-info svelte-{total_page_src}\"}).find(\"span\", {\"class\":f\"svelte-{total_page_src}\"}).text\n",
      "\n",
      "#Create list of urls within the cat\n",
      "total_page = math.ceil(int(total_prod)/per_page)\n",
      "for i in range(1, total_page):\n",
      "    page = f'{cat_url}?from={per_page * i}&size={per_page}'\n",
      "    url_list.append(page)\n",
      "\n",
      "        \n",
      "url_list\n",
      " 132: soup\n",
      " 133: soup.find(\"div\",{\"class\":f\"plp-bar-info svelte-{total_page_src}\"})\n",
      " 134: soup.find(\"div\",{\"class\":f\"plp-bar-info svelte-{total_page_src}\"}).find(\"span\", {\"class\":f\"svelte-{total_page_src}\"}).text\n",
      " 135: soup.find(\"div\",{\"class\":f\"plp-bar-info svelte-{total_page_src}\"}).find(\"span\", {\"class\":f\"svelte-{total_page_src}\"}).text\n",
      " 136: total_page\n",
      " 137: total_page\n",
      " 138: page = f'{cat_url}?from={per_page * i}&size={per_page}'\n",
      " 139: page = f'{cat_url}?from={per_page * 1}&size={per_page}'\n",
      " 140: page = f'{cat_url}?from={per_page * 1}&size={per_page}'\n",
      " 141: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39304a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
